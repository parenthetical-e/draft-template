
@article{2012,
  title = {Executive Functions: What They Are, How They Work, and Why They Evolved},
  shorttitle = {Executive Functions},
  year = {2012},
  month = dec,
  volume = {50},
  pages = {50-2366-50-2366},
  issn = {0009-4978, 1523-8253},
  doi = {10.5860/CHOICE.50-2366},
  file = {2012 - Executive functions what they are, how they work,.pdf},
  journal = {Choice Reviews Online},
  language = {en},
  number = {04}
}

@article{2019,
  title = {Correction for {{Street}} et al., {{Coevolution}} of Cultural Intelligence, Extended Life History, Sociality, and Brain Size in Primates},
  year = {2019},
  month = feb,
  volume = {116},
  pages = {3929--3932},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1900438116},
  file = {2019 - Correction for Street et al., Coevolution of cultu.pdf},
  journal = {Proc Natl Acad Sci USA},
  language = {en},
  number = {9}
}

@book{Aaron2013,
  title = {Combinatorial {{Game Theory}}},
  author = {Aaron, Siegel},
  year = {2013},
  publisher = {{American Mathematical Society}},
  series = {Graduate {{Studies}} in {{Mathematics}} ({{Book}} 146)}
}

@inproceedings{Abadi2016,
  title = {Deep {{Learning}} with {{Differential Privacy}}},
  booktitle = {Proceedings of the 2016 {{ACM SIGSAC Conference}} on {{Computer}} and {{Communications Security}} - {{CCS}}'16},
  author = {Abadi, Martin and Chu, Andy and Goodfellow, Ian and McMahan, H. Brendan and Mironov, Ilya and Talwar, Kunal and Zhang, Li},
  year = {2016},
  pages = {308--318},
  publisher = {{ACM Press}},
  address = {{Vienna, Austria}},
  doi = {10.1145/2976749.2978318},
  abstract = {Machine learning techniques based on neural networks are achieving remarkable results in a wide variety of domains. Often, the training of models requires large, representative datasets, which may be crowdsourced and contain sensitive information. The models should not expose private information in these datasets. Addressing this goal, we develop new algorithmic techniques for learning and a refined analysis of privacy costs within the framework of differential privacy. Our implementation and experiments demonstrate that we can train deep neural networks with non-convex objectives, under a modest privacy budget, and at a manageable cost in software complexity, training efficiency, and model quality.},
  file = {Abadi et al. - 2016 - Deep Learning with Differential Privacy.pdf},
  isbn = {978-1-4503-4139-4},
  language = {en}
}

@article{Abbott1993,
  title = {Analysis of {{Neuron Models}} with {{Dynamically Regulated Conductances}}},
  author = {Abbott, L. F. and LeMasson, Gwendal},
  year = {1993},
  month = nov,
  volume = {5},
  pages = {823--842},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/neco.1993.5.6.823},
  file = {1993 - Abbott, LeMasson - Analysis of Neuron Models with Dynamically Regulated Conductances.pdf},
  journal = {Neural Computation},
  language = {en},
  number = {6}
}

@incollection{Abbott2005,
  title = {Drivers and Modulators from Push-Pull and Balanced Synaptic Input},
  booktitle = {Progress in {{Brain Research}}},
  author = {Abbott, L.F. and Chance, Frances S.},
  year = {2005},
  volume = {149},
  pages = {147--155},
  publisher = {{Elsevier}},
  doi = {10.1016/S0079-6123(05)49011-1},
  abstract = {In 1998, Sherman and Guillery proposed that there are two types of inputs to cortical neurons; drivers and modulators. These two forms of input are required to explain how, for example, sensory driven responses are controlled and modified by attention and other internally generated gating signals. One might imagine that driver signals are carried by fast ionotropic receptors, whereas modulators correspond to slower metabotropic receptors. Instead, we have proposed a novel mechanism by which both driver and modulator inputs could be carried by transmission through the same types of ionotropic receptors. In this scheme, the distinction between driver and modulator inputs is functional and changeable rather than anatomical and fixed. Driver inputs are carried by excitation and inhibition acting in a push-pull manner. This means that increases in excitation are accompanied by decreases in inhibition and vice versa. Modulators correspond to excitation and inhibition that covary so that they increase or decrease together. Theoretical and experimental work has shown that such an arrangement modulates the gain of a neuron, rather than driving it to respond. Constructing drivers and modulators in this manner allows individual excitatory synaptic inputs to play either role, and indeed to switch between roles, depending on how they are linked with inhibition.},
  file = {2005 - Abbott, Chance - Drivers and modulators from push-pull and balanced synaptic input.pdf},
  isbn = {978-0-444-51679-4},
  language = {en}
}

@article{Abbott2015,
  title = {Random Walks on Semantic Networks Can Resemble Optimal Foraging.},
  author = {Abbott, Joshua T. and Austerweil, Joseph L. and Griffiths, Thomas L.},
  year = {2015},
  month = jul,
  volume = {122},
  pages = {558--569},
  issn = {1939-1471, 0033-295X},
  doi = {10.1037/a0038693},
  abstract = {When people are asked to retrieve members of a category from memory, clusters of semantically related items tend to be retrieved together. A recent article by Hills, Jones, and Todd (2012) argued that this pattern reflects a process similar to optimal strategies for foraging for food in patchy spatial environments, with an individual making a strategic decision to switch away from a cluster of related information as it becomes depleted. We demonstrate that similar behavioral phenomena also emerge from a random walk on a semantic network derived from human word-association data. Random walks provide an alternative account of how people search their memories, postulating an undirected rather than a strategic search process. We show that results resembling optimal foraging are produced by random walks when related items are close together in the semantic network. These findings are reminiscent of arguments from the debate on mental imagery, showing how different processes can produce similar results when operating on different representations.},
  file = {Abbott et al. - 2015 - Random walks on semantic networks can resemble opt.pdf},
  journal = {Psychological Review},
  language = {en},
  number = {3}
}

@article{Abbott2016,
  title = {Building Functional Networks of Spiking Model Neurons},
  author = {Abbott, L F and DePasquale, Brian and Memmesheimer, Raoul-Martin},
  year = {2016},
  month = mar,
  volume = {19},
  pages = {350--355},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.4241},
  file = {Abbott et al. - 2016 - Building functional networks of spiking model neur.pdf},
  journal = {Nature Neuroscience},
  language = {en},
  number = {3}
}

@article{Abdi2009,
  title = {How to Compute Reliability Estimates and Display Confidence and Tolerance Intervals for Pattern Classifiers Using the {{Bootstrap}} and 3-Way Multidimensional Scaling ({{DISTATIS}})},
  author = {Abdi, Herv{\'e} and Dunlop, Joseph P. and Williams, Lynne J.},
  year = {2009},
  month = mar,
  volume = {45},
  pages = {89--95},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2008.11.008},
  abstract = {When used to analyze brain imaging data, pattern classifiers typically produce results that can be interpreted as a measure of discriminability or as a distance between some experimental categories. These results can be analyzed with techniques such as multidimensional scaling (MDS), which represent the experimental categories as points on a map. While such a map reveals the configuration of the categories, it does not provide a reliability estimate of the position of the experimental categories, and therefore cannot be used for inferential purposes. In this paper, we present a procedure that provides reliability estimates for pattern classifiers. This procedure combines bootstrap estimation (to estimate the variability of the experimental conditions) and a new 3-way extension of MDS, called DISTATIS, that can be used to integrate the distance matrices generated by the bootstrap procedure and to represent the results as MDS-like maps. Reliability estimates are expressed as (1) tolerance intervals which reflect the accuracy of the assignment of scans to experimental categories and as (2) confidence intervals which generalize standard hypothesis testing. When more than two categories are involved in the application of a pattern classifier, the use of confidence intervals for null hypothesis testing inflates Type I error. We address this problem with a Bonferonni-like correction. Our methodology is illustrated with the results of a pattern classifier described by O'Toole et al. (O'Toole, A., Jiang, F., Abdi, H., Haxby, J., 2005. Partially distributed representations of objects and faces in ventral temporal cortex. J. Cogn. Neurosci. 17, 580\textendash 590) who re-analyzed data originally collected by Haxby et al. (Haxby, J., Gobbini, M., Furey, M., Ishai, A., Schouten, J., Pietrini, P., 2001. Distributed and overlapping representation of faces and objects in ventral temporal cortex. Science 293, 2425\textendash 2430).},
  file = {2009 - Abdi, Dunlop, Williams - How to compute reliability estimates and display confidence and tolerance intervals for pattern classifi.pdf},
  journal = {NeuroImage},
  language = {en},
  number = {1}
}

@article{Abrams2004,
  title = {Chimera {{States}} for {{Coupled Oscillators}}},
  author = {Abrams, Daniel M. and Strogatz, Steven H.},
  year = {2004},
  month = oct,
  volume = {93},
  issn = {0031-9007, 1079-7114},
  doi = {10.1103/PhysRevLett.93.174102},
  file = {2004 - Abrams, Strogatz - Chimera states for coupled oscillators.pdf},
  journal = {Physical Review Letters},
  language = {en},
  number = {17}
}

@techreport{Ackerman2020,
  title = {Astrocytes Close a Critical Period of Motor Circuit Plasticity},
  author = {Ackerman, Sarah D. and {Perez-Catalan}, Nelson A. and Freeman, Marc R. and Doe, Chris Q.},
  year = {2020},
  month = may,
  institution = {{Neuroscience}},
  doi = {10.1101/2020.05.15.098608},
  abstract = {Abstract                        Critical periods \textendash{} brief intervals where neural circuits can be modified by sensory input \textendash{} are necessary for proper neural circuit assembly. Extended critical periods are associated with neurodevelopmental disorders, including schizophrenia and autism; however, the mechanisms that ensure timely critical period closure remain unknown. Here, we define the extent of a critical period in the developing             Drosophila             motor circuit, and identify astrocytes as essential for proper critical period termination. During the critical period, decreased activity produces larger motor dendrites with fewer inhibitory inputs; conversely, increased motor neuron activity produces smaller motor dendrites with fewer excitatory inputs. Importantly, activity has little effect on dendrite morphology after critical period closure. Astrocytes invade the neuropil just prior to critical period closure, and astrocyte ablation prolongs the critical period. Finally, we use a genetic screen to identify astrocyte-motor neuron signaling pathways that close the critical period, including Neuroligin-Neurexin signaling. Reduced signaling destabilizes dendritic microtubules, increases dendrite dynamicity, and impairs locomotor behavior, underscoring the importance of critical period closure. Previous work defines astroglia as regulators of plasticity at individual synapses; here, we show that astrocytes also regulate large-scale structural plasticity to motor dendrite, and thus, circuit architecture to ensure proper locomotor behavior.},
  file = {Ackerman et al. - 2020 - Astrocytes close a critical period of motor circui.pdf},
  language = {en},
  type = {Preprint}
}

@article{Adamou2019,
  title = {Microfoundations of {{Discounting}}},
  author = {Adamou, Alexander and Berman, Yonatan and Mavroyiannis, Diomides and Peters, Ole},
  year = {2019},
  issn = {1556-5068},
  doi = {10.2139/ssrn.3463229},
  abstract = {An important question in economics is how people choose between different payments in the future. The classical normative model predicts that a decision maker discounts a later payment relative to an earlier one by an exponential function of the time between them. Descriptive models use non-exponential functions to fit observed behavioral phenomena, such as preference reversal. Here we propose a model of discounting, consistent with standard axioms of choice, in which decision makers maximize the growth rate of their wealth. Four specifications of the model produce four forms of discounting \textendash{} no discounting, exponential, hyperbolic, and a hybrid of exponential and hyperbolic \textendash{} two of which predict preference reversal. Our model requires no assumption of behavioral bias or payment risk.},
  file = {Adamou et al. - 2019 - Microfoundations of Discounting.pdf},
  journal = {SSRN Journal},
  language = {en}
}

@article{Adams2012,
  title = {Neuroethology of Decision-Making},
  author = {Adams, Geoffrey K and Watson, Karli K and Pearson, John and Platt, Michael L},
  year = {2012},
  month = dec,
  volume = {22},
  pages = {982--989},
  issn = {09594388},
  doi = {10.1016/j.conb.2012.07.009},
  file = {Adams et al. - 2012 - Neuroethology of decision-making.pdf},
  journal = {Current Opinion in Neurobiology},
  language = {en},
  number = {6}
}

@article{Advani,
  title = {Statistical Mechanics of High-Dimensional Inference},
  author = {Advani, Madhu and Ganguli, Surya},
  pages = {12},
  file = {Advani and Ganguli - Statistical mechanics of high-dimensional inferenc.pdf},
  language = {en}
}

@article{Aflalo,
  title = {Decoding Motor Imagery from the Posterior Parietal Cortex of a Tetraplegic Human},
  author = {Aflalo, Tyson and Kellis, Spencer and Klaes, Christian and Lee, Brian and Shi, Ying and Pejsa, Kelsie and Shanfield, Kathleen and {Hayes-Jackson}, Stephanie and Aisen, Mindy and Heck, Christi and Liu, Charles and Andersen, Richard A},
  pages = {6},
  file = {Aflalo et al. - Decoding motor imagery from the posterior parietal.pdf},
  language = {en}
}

@article{Afraimovich2008,
  title = {Winnerless Competition Principle and Prediction of the Transient Dynamics in a {{Lotka}}\textendash{{Volterra}} Model},
  author = {Afraimovich, Valentin and Tristan, Irma and Huerta, Ramon and Rabinovich, Mikhail I.},
  year = {2008},
  month = dec,
  volume = {18},
  pages = {043103},
  issn = {1054-1500, 1089-7682},
  doi = {10.1063/1.2991108},
  file = {Afraimovich et al. - 2008 - Winnerless competition principle and prediction of.pdf},
  journal = {Chaos},
  language = {en},
  number = {4}
}

@article{Afshar2011,
  title = {Single-{{Trial Neural Correlates}} of {{Arm Movement Preparation}}},
  author = {Afshar, Afsheen and Santhanam, Gopal and Yu, Byron M. and Ryu, Stephen I. and Sahani, Maneesh and Shenoy, Krishna V.},
  year = {2011},
  month = aug,
  volume = {71},
  pages = {555--564},
  issn = {08966273},
  doi = {10.1016/j.neuron.2011.05.047},
  abstract = {The process by which neural circuitry in the brain plans and executes movements is not well understood. Until recently, most available data were limited either to single-neuron electrophysiological recordings or to measures of aggregate field or metabolism. Neither approach reveals how individual neurons' activities are coordinated within the population, and thus inferences about how the neural circuit forms a motor plan for an upcoming movement have been indirect. Here we build on recent advances in the measurement and description of population activity to frame and test an ``initial condition hypothesis'' of arm movement preparation and initiation. This hypothesis leads to a model in which the timing of movements may be predicted on each trial using neurons' moment-by-moment firing rates and rates of change of those rates. Using simultaneous microelectrode array recordings from premotor cortex of monkeys performing delayed-reach movements, we compare such single-trial predictions to those of other theories. We show that our model can explain approximately 4-fold more arm-movement reaction-time variance than the best alternative method. Thus, the initial condition hypothesis elucidates a view of the relationship between single-trial preparatory neural population dynamics and single-trial behavior.},
  file = {2011 - Afshar et al. - Single-trial neural correlates of arm movement preparation.pdf},
  journal = {Neuron},
  language = {en},
  number = {3}
}

@article{Afsharpour1985,
  title = {Light Microscopic Analysis of Golgi-Impregnated Rat Subthalamic Neurons},
  author = {Afsharpour, Salman},
  year = {1985},
  month = jun,
  volume = {236},
  pages = {1--13},
  issn = {0021-9967, 1096-9861},
  doi = {10.1002/cne.902360102},
  file = {1985 - Afsharpour - Light microscopic analysis of golgi‐impregnated rat subthalamic neurons.pdf},
  journal = {The Journal of Comparative Neurology},
  language = {en},
  number = {1}
}

@article{Aguilera2013,
  title = {The Situated {{HKB}} Model: How Sensorimotor Spatial Coupling Can Alter Oscillatory Brain Dynamics},
  shorttitle = {The Situated {{HKB}} Model},
  author = {Aguilera, Miguel and Bedia, Manuel G. and Santos, Bruno A. and Barandiaran, Xabier E.},
  year = {2013},
  volume = {7},
  issn = {1662-5188},
  doi = {10.3389/fncom.2013.00117},
  abstract = {Despite the increase of both dynamic and embodied/situated approaches in cognitive science, there is still little research on how coordination dynamics under a closed sensorimotor loop might induce qualitatively different patterns of neural oscillations compared to those found in isolated systems. We take as a departure point the Haken-Kelso-Bunz (HKB) model, a generic model for dynamic coordination between two oscillatory components, which has proven useful for a vast range of applications in cognitive science and whose dynamical properties are well understood. In order to explore the properties of this model under closed sensorimotor conditions we present what we call the situated HKB model: a robotic model that performs a gradient climbing task and whose ``brain'' is modeled by the HKB equation. We solve the differential equations that define the agent-environment coupling for increasing values of the agent's sensitivity (sensor gain), finding different behavioral strategies. These results are compared with two different models: a decoupled HKB with no sensory input and a passively-coupled HKB that is also decoupled but receives a structured input generated by a situated agent. We can precisely quantify and qualitatively describe how the properties of the system, when studied in coupled conditions, radically change in a manner that cannot be deduced from the decoupled HKB models alone. We also present the notion of neurodynamic signature as the dynamic pattern that correlates with a specific behavior and we show how only a situated agent can display this signature compared to an agent that simply receives the exact same sensory input. To our knowledge, this is the first analytical solution of the HKB equation in a sensorimotor loop and qualitative and quantitative analytic comparison of spatially coupled vs. decoupled oscillatory controllers. Finally, we discuss the limitations and possible generalization of our model to contemporary neuroscience and philosophy of mind.},
  file = {Aguilera et al. - 2013 - The situated HKB model how sensorimotor spatial c.pdf},
  journal = {Frontiers in Computational Neuroscience},
  language = {en}
}

@article{Ahilan2019,
  title = {Learning to Use Past Evidence in a Sophisticated World Model},
  author = {Ahilan, Sanjeevan and Solomon, Rebecca B. and Breton, Yannick-Andr{\'e} and Conover, Kent and Niyogi, Ritwik K. and Shizgal, Peter and Dayan, Peter},
  editor = {Haefner, Ralf},
  year = {2019},
  month = jun,
  volume = {15},
  pages = {e1007093},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1007093},
  abstract = {Humans and other animals are able to discover underlying statistical structure in their environments and exploit it to achieve efficient and effective performance. However, such structure is often difficult to learn and use because it is obscure, involving long-range temporal dependencies. Here, we analysed behavioural data from an extended experiment with rats, showing that the subjects learned the underlying statistical structure, albeit suffering at times from immediate inferential imperfections as to their current state within it. We accounted for their behaviour using a Hidden Markov Model, in which recent observations are integrated with evidence from the past. We found that over the course of training, subjects came to track their progress through the task more accurately, a change that our model largely attributed to improved integration of past evidence. This learning reflected the structure of the task, decreasing reliance on recent observations, which were potentially misleading.},
  file = {Ahilan et al. - 2019 - Learning to use past evidence in a sophisticated w.pdf},
  journal = {PLoS Comput Biol},
  language = {en},
  number = {6}
}

@article{Ahilan2020,
  title = {Correcting {{Experience Replay}} for {{Multi}}-{{Agent Communication}}},
  author = {Ahilan, Sanjeevan and Dayan, Peter},
  year = {2020},
  month = oct,
  abstract = {We consider the problem of learning to communicate using multi-agent reinforcement learning (MARL). A common approach is to learn off-policy, using data sampled from a replay buffer. However, messages received in the past may not accurately reflect the current communication policy of each agent, and this complicates learning. We therefore introduce a `communication correction' which accounts for the non-stationarity of observed communication induced by multi-agent learning. It works by relabelling the received message to make it likely under the communicator's current policy, and thus be a better reflection of the receiver's current environment. To account for cases in which agents are both senders and receivers, we introduce an ordered relabelling scheme. Our correction is computationally efficient and can be integrated with a range of off-policy algorithms. It substantially improves the ability of communicating MARL systems to learn across a variety of cooperative and competitive tasks.},
  archiveprefix = {arXiv},
  eprint = {2010.01192},
  eprinttype = {arxiv},
  file = {Ahilan and Dayan - 2020 - Correcting Experience Replay for Multi-Agent Commu.pdf},
  journal = {arXiv:2010.01192 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Multiagent Systems},
  language = {en},
  primaryclass = {cs}
}

@article{Ahn2008,
  title = {Comparison of {{Decision Learning Models Using}} the {{Generalization Criterion Method}}},
  author = {Ahn, Woo-Young and Busemeyer, Jerome and Wagenmakers, Eric-Jan and Stout, Julie},
  year = {2008},
  month = dec,
  volume = {32},
  pages = {1376--1402},
  issn = {0364-0213},
  doi = {10.1080/03640210802352992},
  abstract = {It is a hallmark of a good model to make accurate a priori predictions to new conditions (Busemeyer \& Wang, 2000). This study compared 8 decision learning models with respect to their generalizability. Participants performed 2 tasks (the Iowa Gambling Task and the Soochow Gambling Task), and each model made a priori predictions by estimating the parameters for each participant from 1 task and using those same parameters to predict on the other task. Three methods were used to evaluate the models at the individual level of analysis. The first method used a post hoc fit criterion, the second method used a generalization criterion for short-term predictions, and the third method again used a generalization criterion for long-term predictions. The results suggest that the models with the prospect utility function can make generalizable predictions to new conditions, and different learning models are needed for making short- versus long-term predictions on simple gambling tasks.},
  file = {2008 - Ahn et al. - Comparison of decision learning models using the generalization criterion method.pdf},
  journal = {Cognitive Science: A Multidisciplinary Journal},
  language = {en},
  number = {8}
}

@article{Ahrens1974,
  title = {Computer Methods for Sampling from Gamma, Beta, Poisson and Bionomial Distributions},
  author = {Ahrens, J. H. and Dieter, U.},
  year = {1974},
  month = sep,
  volume = {12},
  pages = {223--246},
  issn = {0010-485X, 1436-5057},
  doi = {10.1007/BF02293108},
  abstract = {Zusammenfassung Computer Methods for Sampling from Gamma, Beta, Poisson and Binomial Distributions. Accurate computer methods are evaluated which transform uniformly distributed random numbers into quantities that follow gamma, beta, Poisson, binomial and negative-binomial distributions. All algorithms are designed for variable parameters. The known convenient methods are slow when the parameters are large. Therefore new procedures are introduced which can cope efficiently with parameters of all sizes. Some algorithms require sampling from the normal distribution as an intermediate step. In the reported computer experiments the normal deviates were obtained from a recent method which is also described.},
  file = {1974 - Ahrens, Dieter - Computer methods for sampling from gamma, beta, poisson and bionomial distributions.pdf},
  journal = {Computing},
  language = {en},
  number = {3}
}

@article{Aitchison2016,
  title = {The {{Hamiltonian Brain}}: {{Efficient Probabilistic Inference}} with {{Excitatory}}-{{Inhibitory Neural Circuit Dynamics}}},
  shorttitle = {The {{Hamiltonian Brain}}},
  author = {Aitchison, Laurence and Lengyel, M{\'a}t{\'e}},
  editor = {Kording, Konrad P.},
  year = {2016},
  month = dec,
  volume = {12},
  pages = {e1005186},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1005186},
  file = {Aitchison and Lengyel - 2016 - The Hamiltonian Brain Efficient Probabilistic Inf.pdf},
  journal = {PLOS Computational Biology},
  language = {en},
  number = {12}
}

@article{Akam2014,
  title = {Oscillatory Multiplexing of Population Codes for Selective Communication in the Mammalian Brain},
  author = {Akam, Thomas and Kullmann, Dimitri M.},
  year = {2014},
  month = feb,
  volume = {15},
  pages = {111--122},
  issn = {1471-003X, 1471-0048},
  doi = {10.1038/nrn3668},
  abstract = {Mammalian brains exhibit population oscillations, the structures of which vary in time and space according to behavioural state. A proposed function of these oscillations is to control the flow of signals among anatomically connected networks. However, the nature of neural coding that may support selective communication that depends on oscillations has received relatively little attention. Here, we consider the role of multiplexing, whereby multiple information streams share a common neural substrate. We suggest that multiplexing implemented through periodic modulation of firing-rate population codes enables flexible reconfiguration of effective connectivity among brain areas.},
  file = {Akam and Kullmann - 2014 - Oscillatory multiplexing of population codes for s.pdf},
  journal = {Nature Reviews Neuroscience},
  language = {en},
  number = {2}
}

@article{Akiba2019,
  title = {Optuna: {{A Next}}-Generation {{Hyperparameter Optimization Framework}}},
  shorttitle = {Optuna},
  author = {Akiba, Takuya and Sano, Shotaro and Yanase, Toshihiko and Ohta, Takeru and Koyama, Masanori},
  year = {2019},
  month = jul,
  abstract = {The purpose of this study is to introduce new design-criteria for next-generation hyperparameter optimization software. The criteria we propose include (1) define-by-run API that allows users to construct the parameter search space dynamically, (2) efficient implementation of both searching and pruning strategies, and (3) easy-to-setup, versatile architecture that can be deployed for various purposes, ranging from scalable distributed computing to light-weight experiment conducted via interactive interface. In order to prove our point, we will introduce Optuna, an optimization software which is a culmination of our effort in the development of a next generation optimization software. As an optimization software designed with define-by-run principle, Optuna is particularly the first of its kind. We will present the design-techniques that became necessary in the development of the software that meets the above criteria, and demonstrate the power of our new design through experimental results and real world applications. Our software is available under the MIT license (https://github.com/pfnet/optuna/).},
  archiveprefix = {arXiv},
  eprint = {1907.10902},
  eprinttype = {arxiv},
  file = {Akiba et al. - 2019 - Optuna A Next-generation Hyperparameter Optimizat.pdf},
  journal = {arXiv:1907.10902 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryclass = {cs, stat}
}

@article{Albers2018,
  title = {Decoupling of {{BOLD}} Amplitude and Pattern Classification of Orientation-Selective Activity in Human Visual Cortex},
  author = {Albers, Anke Marit and Meindertsma, Thomas and Toni, Ivan and {de Lange}, Floris P.},
  year = {2018},
  month = oct,
  volume = {180},
  pages = {31--40},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2017.09.046},
  file = {Albers et al. - 2018 - Decoupling of BOLD amplitude and pattern classific.pdf},
  journal = {NeuroImage},
  language = {en}
}

@article{Albin1989,
  title = {The Functional Anatomy of Basal Ganglia Disorders},
  author = {Albin, Roger L. and Young, Anne B. and Penney, John B.},
  year = {1989},
  month = jan,
  volume = {12},
  pages = {366--375},
  issn = {01662236},
  doi = {10.1016/0166-2236(89)90074-X},
  file = {1983 - Albin, Penney, Young - The Functional Anatomy of Basal Ganglia Disorders.pdf},
  journal = {Trends in Neurosciences},
  language = {en},
  number = {10}
}

@article{Alekseichuk2016,
  title = {Spatial {{Working Memory}} in {{Humans Depends}} on {{Theta}} and {{High Gamma Synchronization}} in the {{Prefrontal Cortex}}},
  author = {Alekseichuk, Ivan and Turi, Zsolt and {Amador de Lara}, Gabriel and Antal, Andrea and Paulus, Walter},
  year = {2016},
  month = jun,
  volume = {26},
  pages = {1513--1521},
  issn = {09609822},
  doi = {10.1016/j.cub.2016.04.035},
  abstract = {Previous, albeit correlative, findings have shown that the neural mechanisms underlying working memory critically require cross-structural and cross-frequency coupling mechanisms between theta and gamma neural oscillations. However, the direct causality between cross-frequency coupling and working memory performance remains to be demonstrated. Here we externally modulated the interaction of theta and gamma rhythms in the prefrontal cortex using novel cross-frequency protocols of transcranial alternating current stimulation to affect spatial working memory performance in humans. Enhancement of working memory performance and increase of global neocortical connectivity were observed when bursts of high gamma oscillations (80\textendash 100 Hz) coincided with the peaks of the theta waves, whereas superimposition on the trough of the theta wave and low gamma frequency protocols were ineffective. Thus, our results demonstrate the sensitivity of working memory performance and global neocortical connectivity to the phase and rhythm of the externally driven thetagamma cross-frequency synchronization.},
  file = {Alekseichuk et al. - 2016 - Spatial Working Memory in Humans Depends on Theta .pdf},
  journal = {Current Biology},
  language = {en},
  number = {12}
}

@article{Aljadeff2015,
  title = {Eigenvalues of Block Structured Asymmetric Random Matrices},
  author = {Aljadeff, Johnatan and Renfrew, David and Stern, Merav},
  year = {2015},
  month = oct,
  volume = {56},
  pages = {103502},
  issn = {0022-2488, 1089-7658},
  doi = {10.1063/1.4931476},
  abstract = {We study the spectrum of an asymmetric random matrix with block structured variances. The rows and columns of the random square matrix are divided into \$D\$ partitions with arbitrary size (linear in \$N\$). The parameters of the model are the variances of elements in each block, summarized in \$g\textbackslash in\textbackslash mathbb\{R\}\^\{D\textbackslash times D\}\_+\$. Using the Hermitization approach and by studying the matrix-valued Stieltjes transform we show that these matrices have a circularly symmetric spectrum, we give an explicit formula for their spectral radius and a set of implicit equations for the full density function. We discuss applications of this model to neural networks.},
  archiveprefix = {arXiv},
  eprint = {1411.2688},
  eprinttype = {arxiv},
  file = {2015 - Aljadeff, Renfrew, Stern - Eigenvalues of block structured asymmetric random matrices.pdf;Aljadeff et al. - 2015 - Eigenvalues of block structured asymmetric random .pdf},
  journal = {Journal of Mathematical Physics},
  keywords = {Mathematics - Probability},
  language = {en},
  number = {10}
}

@article{Aljadeff2015a,
  title = {Transition to {{Chaos}} in {{Random Networks}} with {{Cell}}-{{Type}}-{{Specific Connectivity}}},
  author = {Aljadeff, Johnatan and Stern, Merav and Sharpee, Tatyana},
  year = {2015},
  month = feb,
  volume = {114},
  issn = {0031-9007, 1079-7114},
  doi = {10.1103/PhysRevLett.114.088101},
  file = {2015 - Aljadeff, Stern, Sharpee - Transition to chaos in random networks with cell-type-specific connectivity.pdf;Aljadeff et al. - 2015 - Transition to Chaos in Random Networks with Cell-T.pdf},
  journal = {Physical Review Letters},
  language = {en},
  number = {8}
}

@article{Aljadeff2016,
  title = {On the Low Dimensional Dynamics of Structured Random Networks},
  author = {Aljadeff, Johnatan and Renfrew, David and Vegu{\'e}, Marina and Sharpee, Tatyana O.},
  year = {2016},
  month = feb,
  volume = {93},
  issn = {2470-0045, 2470-0053},
  doi = {10.1103/PhysRevE.93.022302},
  abstract = {Using a generalized random recurrent neural network model, and by extending our recently developed mean-field approach [J. Aljadeff, M. Stern, T. Sharpee, Phys. Rev. Lett. 114, 088101 (2015)], we study the relationship between the network connectivity structure and its low dimensional dynamics. Each connection in the network is a random number with mean 0 and variance that depends on pre- and post-synaptic neurons through a sufficiently smooth function \$g\$ of their identities. We find that these networks undergo a phase transition from a silent to a chaotic state at a critical point we derive as a function of \$g\$. Above the critical point, although unit activation levels are chaotic, their autocorrelation functions are restricted to a low dimensional subspace. This provides a direct link between the network's structure and some of its functional characteristics. We discuss example applications of the general results to neuroscience where we derive the support of the spectrum of connectivity matrices with heterogeneous and possibly correlated degree distributions, and to ecology where we study the stability of the cascade model for food web structure.},
  archiveprefix = {arXiv},
  eprint = {1509.02546},
  eprinttype = {arxiv},
  file = {2015 - Aljadeff et al. - On the low dimensional dynamics of structured random networks.pdf;Aljadeff et al. - 2016 - On the low dimensional dynamics of structured rand.pdf},
  journal = {Physical Review E},
  keywords = {Condensed Matter - Disordered Systems and Neural Networks,Quantitative Biology - Neurons and Cognition},
  language = {en},
  number = {2}
}

@article{Alkemade2015,
  title = {Topographic Organization of the Human and Non-Human Primate Subthalamic Nucleus},
  author = {Alkemade, Anneke and Schnitzler, Alfons and Forstmann, Birte U.},
  year = {2015},
  month = nov,
  volume = {220},
  pages = {3075--3086},
  issn = {1863-2653, 1863-2661},
  doi = {10.1007/s00429-015-1047-2},
  file = {2015 - Alkemade, Schnitzler, Forstmann - Topographic organization of the human and non-human primate subthalamic nucleus.pdf;Alkemade et al. - 2015 - Topographic organization of the human and non-huma.pdf},
  journal = {Brain Structure and Function},
  language = {en},
  number = {6}
}

@article{Almeida2015,
  title = {Neural Circuit Basis of Visuo-Spatial Working Memory Precision: A Computational and Behavioral Study},
  shorttitle = {Neural Circuit Basis of Visuo-Spatial Working Memory Precision},
  author = {Almeida, Rita and Barbosa, Jo{\~a}o and Compte, Albert},
  year = {2015},
  month = sep,
  volume = {114},
  pages = {1806--1818},
  issn = {0022-3077, 1522-1598},
  doi = {10.1152/jn.00362.2015},
  abstract = {The amount of information that can be retained in working-memory (WM) is limited. Limitations of WM have been the subject of intense research, especially trying to specify algorithmic models for WM. Comparatively, neural circuit perspectives have barely been used to test WM limitations in behavioral experiments. Here, we used a neuronal microcircuit model for visuo-spatial WM (vsWM) to investigate memory of several items. The model assumes that there is a topographic organization of the circuit responsible for spatial memory retention. This assumption leads to specific predictions, which we tested in psychophysics experiments. According to the model, nearby locations should be recalled with a bias, as if the two memory traces merged during the delay period. Another prediction is that the previously reported loss of memory precision for increasing number of memory items (memory load) should vanish when the distances between items are controlled for. Both predictions were confirmed experimentally. Taken together, our findings provide support for a topographic neural-circuit organization of vsWM, they suggest that interference between similar memories underlies some WM limitations, and they put forward a circuit-based explanation that reconciles previous conflicting results on the dependence of WM precision with load.},
  file = {2015 - Almeida, Barbosa, Compte - Neural circuit basis of visuo-spatial working memory precision a computational and behavioral study.pdf;Almeida et al. - 2015 - Neural circuit basis of visuo-spatial working memo.pdf},
  journal = {Journal of Neurophysiology},
  language = {en},
  number = {3}
}

@article{Alonso-Frech2006,
  title = {Slow Oscillatory Activity and Levodopa-Induced Dyskinesias in {{Parkinson}}'s Disease},
  author = {{Alonso-Frech}, F.},
  year = {2006},
  month = jul,
  volume = {129},
  pages = {1748--1757},
  issn = {0006-8950, 1460-2156},
  doi = {10.1093/brain/awl103},
  file = {2006 - Alonso-Frech et al. - Slow oscillatory activity and levodopa-induced dyskinesias in Parkinson's disease.pdf},
  journal = {Brain},
  language = {en},
  number = {7}
}

@techreport{Alonso2019,
  title = {Temperature Compensation in a Small Rhythmic Circuit},
  author = {Alonso, Leandro M. and Marder, Eve},
  year = {2019},
  month = jul,
  institution = {{Neuroscience}},
  doi = {10.1101/716761},
  abstract = {Temperature affects the conductances and kinetics of the ionic channels that underlie neuronal activity. Each membrane conductance has a different characteristic temperature sensitivity, which raises the question of how neurons and neuronal circuits can operate robustly over wide temperature ranges. To address this, we employed computational models of the pyloric network of crabs and lobsters. We employed a landscape optimization scheme introduced previously (Alonso and Marder, 2019) to produce multiple different models that exhibit triphasic pyloric rhythms over a range of temperatures. We use the currentscapes introduced in (Alonso and Marder, 2019) to explore the dynamics of model currents and how they change with temperature. We found that temperature changes the relative contributions of the currents to neuronal activity so that rhythmic activity smoothly slides through changes in mechanisms. Moreover, the responses of the models to extreme perturbations\textemdash such as gradually decreasing a current type\textemdash are often qualitatively different at different temperatures.},
  file = {Alonso and Marder - 2019 - Temperature compensation in a small rhythmic circu.pdf},
  language = {en},
  type = {Preprint}
}

@article{Alvarez-Salvado2018,
  title = {Elementary Sensory-Motor Transformations Underlying Olfactory Navigation in Walking Fruit-Flies},
  author = {{\'A}lvarez-Salvado, Efr{\'e}n and Licata, Angela M and Connor, Erin G and McHugh, Margaret K and King, Benjamin MN and Stavropoulos, Nicholas and Victor, Jonathan D and Crimaldi, John P and Nagel, Katherine I},
  year = {2018},
  month = aug,
  volume = {7},
  pages = {e37815},
  issn = {2050-084X},
  doi = {10.7554/eLife.37815},
  abstract = {Odor attraction in walking Drosophila melanogaster is commonly used to relate neural function to behavior, but the algorithms underlying attraction are unclear. Here, we develop a high-throughput assay to measure olfactory behavior in response to well-controlled sensory stimuli. We show that odor evokes two behaviors: an upwind run during odor (ON response), and a local search at odor offset (OFF response). Wind orientation requires antennal mechanoreceptors, but search is driven solely by odor. Using dynamic odor stimuli, we measure the dependence of these two behaviors on odor intensity and history. Based on these data, we develop a navigation model that recapitulates the behavior of flies in our apparatus, and generates realistic trajectories when run in a turbulent boundary layer plume. The ability to parse olfactory navigation into quantifiable elementary sensori-motor transformations provides a foundation for dissecting neural circuits that govern olfactory behavior.           ,              All kinds of animals use their sense of smell to find things. Doing this is difficult because odors in air travel as plumes, which meander downwind and break apart. Scientists are interested in learning the rules that animals use to decipher these odor signals and trace them back to their source. For example, do animals use patterns of timing in the odor, differences between smell at the two nostrils, or the direction of the wind? Scientists would also like to know how animal's brain circuits decipher this information.             Tiny fruit flies make a good model for studying the way animals detect odors because scientists have already learned a great deal about how their brains work. There are also many tools available to help scientists study the brain circuits of fruit flies.             Now, \'Alvarez-Salvado et al. show that fruit flies use multiple senses to track odors to their source. In the experiments, fruit flies that were blind and could not fly were placed in tiny wind tunnels and their behavior in response to a smell or no smell in the tunnel was carefully documented. When the flies detected an odor, they turned to face the wind using their antennae to detect wind direction and run toward it. When flies lost track of an odor they began to search for it at the spot where they last smelled it. Next, \'Alvarez-Salvado et al. created a computer model that recreated the flies' behavior and was able to find the odor source as well as real flies. The model added together these basic behaviors to successfully recreate the flies' odor-search strategy.             Other animals are often better than humans at finding odor sources. As a result, people use pigs to find truffles and dogs to find lost hikers. The computer model \'Alvarez-Salvado et al. developed might help design robots that can search for truffles, hikers, or landmines, without risking the lives of animals. It might also be useful for designing autonomous vehicles that must respond to many types of information in changing environments to make decisions.},
  file = {Álvarez-Salvado et al. - 2018 - Elementary sensory-motor transformations underlyin.pdf},
  journal = {eLife},
  language = {en}
}

@article{Alvarez2004,
  title = {Simulating Cortical Network Activity States Constrained by Intracellular Recordings},
  author = {Alvarez, Fabi{\'a}n P. and Destexhe, Alain},
  year = {2004},
  month = jun,
  volume = {58-60},
  pages = {285--290},
  issn = {09252312},
  doi = {10.1016/j.neucom.2004.01.057},
  abstract = {We present a method for studying states of network activity while incorporating constraints provided by intracellular measurements. Taking into account measurements of the average membrane potential, input resistance changes and membrane potential uctuations, narrows down the possible region of parameter space (connectivity, quantal conductances) where this activity can appear in networks. Searching in those speci\"yc regions greatly enhances the e ciency of the network-level modeling because irrelevant parameter combinations are automatically eliminated. We illustrate this approach by modeling self-sustained stochastic states in networks of excitatory and inhibitory neurons, based on intracellular recordings in vivo.},
  file = {2004 - Alvarez, Destexhe - Simulating cortical network activity states constrained by intracellular recordings.pdf},
  journal = {Neurocomputing},
  language = {en}
}

@article{Amilhon2015,
  title = {Parvalbumin {{Interneurons}} of {{Hippocampus Tune Population Activity}} at {{Theta Frequency}}},
  author = {Amilhon, B{\'e}n{\'e}dicte and Huh, Carey Y.L. and Manseau, Fr{\'e}d{\'e}ric and Ducharme, Guillaume and Nichol, Heather and Adamantidis, Antoine and Williams, Sylvain},
  year = {2015},
  month = jun,
  volume = {86},
  pages = {1277--1289},
  issn = {08966273},
  doi = {10.1016/j.neuron.2015.05.027},
  abstract = {Hippocampal theta rhythm arises from a combination of recently described intrinsic theta oscillators and inputs from multiple brain areas. Interneurons expressing the markers parvalbumin (PV) and somatostatin (SOM) are leading candidates to participate in intrinsic rhythm generation and principal cell (PC) coordination in distal CA1 and subiculum. We tested their involvement by optogenetically activating and silencing PV or SOM interneurons in an intact hippocampus preparation that preserves intrinsic connections and oscillates spontaneously at theta frequencies. Despite evidence suggesting that SOM interneurons are crucial for theta, optogenetic manipulation of these interneurons modestly influenced theta rhythm. However, SOM interneurons were able to strongly modulate temporoammonic inputs. In contrast, activation of PV interneurons powerfully controlled PC network and rhythm generation optimally at 8 Hz, while continuously silencing them disrupted theta. Our results thus demonstrate a pivotal role of PV but not SOM interneurons for PC synchronization and the emergence of intrinsic hippocampal theta.},
  file = {Amilhon et al. - 2015 - Parvalbumin Interneurons of Hippocampus Tune Popul.pdf},
  journal = {Neuron},
  language = {en},
  number = {5}
}

@article{Amir2019,
  title = {The Developmental Origins of Risk and Time Preferences across Diverse Societies.},
  author = {Amir, Dorsa and Jordan, Matthew R. and McAuliffe, Katherine and Valeggia, Claudia R. and Sugiyama, Lawrence S. and Bribiescas, Richard G. and Snodgrass, J. Josh and Dunham, Yarrow},
  year = {2019},
  month = sep,
  issn = {1939-2222, 0096-3445},
  doi = {10.1037/xge0000675},
  abstract = {Risk and time preferences have often been viewed as reflecting inherent traits such as impatience and self-control. Here, we offer an alternative perspective, arguing that they are flexible and environmentally-informed. In Study 1, we investigated risk and time preferences among children in the United States, India, and Argentina, as well as forager-horticulturalist Shuar children in Amazonian Ecuador. We find striking cross-cultural differences in behavior: children in India, the U.S., and Argentina are more risk-seeking and future-oriented, while Shuar children are more risk-averse and exhibit more heterogeneous time preferences, on average preferring more today choices. To explore one of the socioecological forces that may be shaping these preferences, in Study 2, we compared the behavior of more and less- marketintegrated Shuar children, finding that those in market-integrated regions are more futureoriented and risk-seeking. These findings indicate that cross-cultural differences in risk and time preferences can be traced into childhood and may be influenced by the local environment. More broadly, our results contribute to a growing understanding of plasticity and variation in the development of behavior.},
  file = {Amir et al. - 2019 - The developmental origins of risk and time prefere.pdf},
  journal = {Journal of Experimental Psychology: General},
  language = {en}
}

@article{Amirnovin2004,
  title = {Visually {{Guided Movements Suppress Subthalamic Oscillations}} in {{Parkinson}}'s {{Disease Patients}}},
  author = {Amirnovin, R.},
  year = {2004},
  month = dec,
  volume = {24},
  pages = {11302--11306},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.3242-04.2004},
  file = {2004 - Amirnovin - Visually Guided Movements Suppress Subthalamic Oscillations in Parkinson's Disease Patients.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {50}
}

@article{Anastassiou2015,
  title = {Ephaptic Coupling to Endogenous Electric Field Activity: Why Bother?},
  shorttitle = {Ephaptic Coupling to Endogenous Electric Field Activity},
  author = {Anastassiou, Costas A and Koch, Christof},
  year = {2015},
  month = apr,
  volume = {31},
  pages = {95--103},
  issn = {09594388},
  doi = {10.1016/j.conb.2014.09.002},
  file = {Anastassiou and Koch - 2015 - Ephaptic coupling to endogenous electric field act.pdf},
  journal = {Current Opinion in Neurobiology},
  language = {en}
}

@article{Anderson2000,
  title = {The {{Contribution}} of {{Noise}} to {{Contrast Invariance}} of {{Orientation Tuning}} in {{Cat Visual Cortex}}},
  author = {Anderson, J. S.},
  year = {2000},
  month = dec,
  volume = {290},
  pages = {1968--1972},
  issn = {00368075, 10959203},
  doi = {10.1126/science.290.5498.1968},
  file = {2000 - Anderson et al. - The contribution of noise to contrast invariance of orientation tuning in cat visual cortex.pdf},
  journal = {Science},
  language = {en},
  number = {5498}
}

@article{Anderson2021,
  title = {On Reaction Network Implementations of Neural Networks},
  author = {Anderson, David F. and Joshi, Badal and Deshpande, Abhishek},
  year = {2021},
  month = apr,
  volume = {18},
  pages = {rsif.2021.0031, 20210031},
  issn = {1742-5662},
  doi = {10.1098/rsif.2021.0031},
  abstract = {This paper is concerned with the utilization of deterministically modelled chemical reaction networks for the implementation of (feed-forward) neural networks. We develop a general mathematical framework and prove that the ordinary differential equations (ODEs) associated with certain reaction network implementations of neural networks have desirable properties including (i) existence of unique positive fixed points that are smooth in the parameters of the model (necessary for gradient descent) and (ii) fast convergence to the fixed point regardless of initial condition (necessary for efficient implementation). We do so by first making a connection between neural networks and fixed points for systems of ODEs, and then by constructing reaction networks with the correct associated set of ODEs. We demonstrate the theory by constructing a reaction network that implements a neural network with a smoothed ReLU activation function, though we also demonstrate how to generalize the construction to allow for other activation functions (each with the desirable properties listed previously). As there are multiple types of `networks' used in this paper, we also give a careful introduction to both reaction networks and neural networks, in order to disambiguate the overlapping vocabulary in the two settings and to clearly highlight the role of each network's properties.},
  file = {Anderson et al. - 2021 - On reaction network implementations of neural netw.pdf},
  journal = {J. R. Soc. Interface.},
  language = {en},
  number = {177}
}

@article{Andrychowicz,
  title = {Hindsight {{Experience Replay}}},
  author = {Andrychowicz, Marcin and Wolski, Filip and Ray, Alex and Schneider, Jonas and Fong, Rachel and Welinder, Peter and McGrew, Bob and Tobin, Josh and Abbeel, Pieter and Zaremba, Wojciech},
  pages = {15},
  abstract = {Dealing with sparse rewards is one of the biggest challenges in Reinforcement Learning (RL). We present a novel technique called Hindsight Experience Replay which allows sample-efficient learning from rewards which are sparse and binary and therefore avoid the need for complicated reward engineering. It can be combined with an arbitrary off-policy RL algorithm and may be seen as a form of implicit curriculum. We demonstrate our approach on the task of manipulating objects with a robotic arm. In particular, we run experiments on three different tasks: pushing, sliding, and pick-and-place, in each case using only binary rewards indicating whether or not the task is completed. Our ablation studies show that Hindsight Experience Replay is a crucial ingredient which makes training possible in these challenging environments. We show that our policies trained on a physics simulation can be deployed on a physical robot and successfully complete the task. The video presenting our experiments is available at https://goo.gl/SMrQnI.},
  file = {Andrychowicz et al. - Hindsight Experience Replay.pdf},
  language = {en}
}

@article{Apesteguia2007,
  title = {Imitation\textemdash Theory and Experimental Evidence},
  author = {Apesteguia, Jose and Huck, Steffen and Oechssler, J{\"o}rg},
  year = {2007},
  month = sep,
  volume = {136},
  pages = {217--235},
  issn = {00220531},
  doi = {10.1016/j.jet.2006.07.006},
  abstract = {We introduce a generalized theoretical approach to study imitation models and subject the models to rigorous experimental testing. In our theoretical analysis we \TH nd that the different predictions of previous imitation models are due to different informational assumptions, not to different behavioral rules. It is more important whom one imitates rather than how. In a laboratory experiment we test the different theories by systematically varying information conditions. We \TH nd that the generalized imitation model predicts the differences between treatments well. The data also provide support for imitation on the individual level, both in terms of choice and in terms of perception. But imitation is not unconditional. Rather individuals' propensity to imitate more successful actions is increasing in payoff differences.},
  file = {2007 - Steffen Huck - Imitation - theory and experimental evidence.pdf},
  journal = {Journal of Economic Theory},
  language = {en},
  number = {1}
}

@article{Aplin2019,
  title = {Culture and Cultural Evolution in Birds: A Review of the Evidence},
  shorttitle = {Culture and Cultural Evolution in Birds},
  author = {Aplin, Lucy M.},
  year = {2019},
  month = jan,
  volume = {147},
  pages = {179--187},
  issn = {00033472},
  doi = {10.1016/j.anbehav.2018.05.001},
  file = {Aplin - 2019 - Culture and cultural evolution in birds a review .pdf},
  journal = {Animal Behaviour},
  language = {en}
}

@article{Apps2015,
  title = {Vicarious {{Reinforcement Learning Signals When Instructing Others}}},
  author = {Apps, M. A. J. and Lesage, E. and Ramnani, N.},
  year = {2015},
  month = feb,
  volume = {35},
  pages = {2904--2913},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.3669-14.2015},
  file = {Apps et al. - 2015 - Vicarious Reinforcement Learning Signals When Inst.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {7}
}

@article{Arakaki2017,
  title = {Capturing the Diversity of Biological Tuning Curves Using Generative Adversarial Networks},
  author = {Arakaki, Takafumi and Barello, Gregory and Ahmadian, Yashar},
  year = {2017},
  month = jul,
  doi = {10.1101/167916},
  abstract = {Tuning curves characterizing the response selectivities of biological neurons often exhibit large degrees of irregularity and diversity across neurons. Theoretical network models that feature heterogeneous cell populations or random connectivity also give rise to diverse tuning curves. However, a general framework for fitting such models to experimentally measured tuning curves is lacking. We address this problem by proposing to view mechanistic network models as generative models whose parameters can be optimized to fit the distribution of experimentally measured tuning curves. A major obstacle for fitting such models is that their likelihood function is not explicitly available or is highly intractable to compute. Recent advances in machine learning provide ways for fitting generative models without the need to evaluate the likelihood and its gradient. Generative Adversarial Networks (GAN) provide one such framework which has been successful in traditional machine learning tasks. We apply this approach in two separate experiments, showing how GANs can be used to fit commonly used mechanistic models in theoretical neuroscience to datasets of measured tuning curves. This fitting procedure avoids the computationally expensive step of inferring latent variables, e.g., the biophysical parameters of individual cells or the particular realization of the full synaptic connectivity matrix, and directly learns model parameters which characterize the statistics of connectivity or of single-cell properties. Another strength of this approach is that it fits the entire, joint distribution of experimental tuning curves, instead of matching a few summary statistics picked a priori by the user. More generally, this framework opens the door to fitting theoretically motivated dynamical network models directly to simultaneously or non-simultaneously recorded neural responses.},
  file = {Arakaki et al. - 2017 - Capturing the diversity of biological tuning curve.pdf},
  journal = {bioRxiv},
  language = {en}
}

@article{Arandia-Romero2016,
  title = {Multiplicative and {{Additive Modulation}} of {{Neuronal Tuning}} with {{Population Activity Affects Encoded Information}}},
  author = {{Arandia-Romero}, I{\~n}igo and Tanabe, Seiji and Drugowitsch, Jan and Kohn, Adam and {Moreno-Bote}, Rub{\'e}n},
  year = {2016},
  month = mar,
  volume = {89},
  pages = {1305--1316},
  issn = {08966273},
  doi = {10.1016/j.neuron.2016.01.044},
  abstract = {Numerous studies have shown that neuronal responses are modulated by stimulus properties and also by the state of the local network. However, little is known about how activity fluctuations of neuronal populations modulate the sensory tuning of cells and affect their encoded information. We found that fluctuations in ongoing and stimulus-evoked population activity in primate visual cortex modulate the tuning of neurons in a multiplicative and additive manner. While distributed on a continuum, neurons with stronger multiplicative effects tended to have less additive modulation and vice versa. The information encoded by multiplicatively modulated neurons increased with greater population activity, while that of additively modulated neurons decreased. These effects offset each other so that population activity had little effect on total information. Our results thus suggest that intrinsic activity fluctuations may act as a ``traffic light'' that determines which subset of neurons is most informative.},
  file = {Arandia-Romero et al. - 2016 - Multiplicative and Additive Modulation of Neuronal.pdf},
  journal = {Neuron},
  language = {en},
  number = {6}
}

@article{Araque1999,
  title = {Tripartite Synapses: Glia, the Unacknowledged Partner},
  shorttitle = {Tripartite Synapses},
  author = {Araque, Alfonso and Parpura, Vladimir and Sanzgiri, Rita P. and Haydon, Philip G.},
  year = {1999},
  month = may,
  volume = {22},
  pages = {208--215},
  issn = {01662236},
  doi = {10.1016/S0166-2236(98)01349-6},
  file = {Araque et al. - 1999 - Tripartite synapses glia, the unacknowledged part.pdf},
  journal = {Trends in Neurosciences},
  language = {en},
  number = {5}
}

@article{Arbesman2009,
  title = {Superlinear Scaling for Innovation in Cities},
  author = {Arbesman, Samuel and Kleinberg, Jon M. and Strogatz, Steven H.},
  year = {2009},
  month = jan,
  volume = {79},
  issn = {1539-3755, 1550-2376},
  doi = {10.1103/PhysRevE.79.016115},
  file = {2009 - Arbesman, Kleinberg, Strogatz - Superlinear scaling for innovation in cities.pdf},
  journal = {Physical Review E},
  language = {en},
  number = {1}
}

@article{Archer,
  title = {Low-Dimensional Models of Neural Population Activity in Sensory Cortical Circuits},
  author = {Archer, Evan W and Koster, Urs and Pillow, Jonathan W and Macke, Jakob H},
  pages = {9},
  abstract = {Neural responses in visual cortex are influenced by visual stimuli and by ongoing spiking activity in local circuits. An important challenge in computational neuroscience is to develop models that can account for both of these features in large multi-neuron recordings and to reveal how stimulus representations interact with and depend on cortical dynamics. Here we introduce a statistical model of neural population activity that integrates a nonlinear receptive field model with a latent dynamical model of ongoing cortical activity. This model captures temporal dynamics and correlations due to shared stimulus drive as well as common noise. Moreover, because the nonlinear stimulus inputs are mixed by the ongoing dynamics, the model can account for a multiple idiosyncratic receptive field shapes with a small number of nonlinear inputs to a low-dimensional dynamical model. We introduce a fast estimation method using online expectation maximization with Laplace approximations, for which inference scales linearly in both population size and recording duration. We test this model to multi-channel recordings from primary visual cortex and show that it accounts for neural tuning properties as well as cross-neural correlations.},
  file = {Archer et al. - Low-dimensional models of neural population activi.pdf},
  language = {en}
}

@article{Arizono2020,
  title = {Structural Basis of Astrocytic {{Ca2}}+ Signals at Tripartite Synapses},
  author = {Arizono, Misa and Inavalli, V. V. G. Krishna and Panatier, Aude and Pfeiffer, Thomas and Angibaud, Julie and Levet, Florian and Ter Veer, Mirelle J. T. and Stobart, Jillian and Bellocchio, Luigi and Mikoshiba, Katsuhiko and Marsicano, Giovanni and Weber, Bruno and Oliet, St{\'e}phane H. R. and N{\"a}gerl, U. Valentin},
  year = {2020},
  month = dec,
  volume = {11},
  pages = {1906},
  issn = {2041-1723},
  doi = {10.1038/s41467-020-15648-4},
  file = {Arizono et al. - 2020 - Structural basis of astrocytic Ca2+ signals at tri.pdf},
  journal = {Nat Commun},
  language = {en},
  number = {1}
}

@article{Arnoldt2015,
  title = {Toward the {{Darwinian}} Transition: {{Switching}} between Distributed and Speciated States in a Simple Model of Early Life},
  shorttitle = {Toward the {{Darwinian}} Transition},
  author = {Arnoldt, Hinrich and Strogatz, Steven H. and Timme, Marc},
  year = {2015},
  month = nov,
  volume = {92},
  issn = {1539-3755, 1550-2376},
  doi = {10.1103/PhysRevE.92.052909},
  file = {2015 - Arnoldt, Strogatz, Timme - Toward the Darwinian transition Switching between distributed and speciated states in a simple model o.pdf;Arnoldt et al. - 2015 - Toward the Darwinian transition Switching between.pdf},
  journal = {Physical Review E},
  language = {en},
  number = {5}
}

@article{Arpit,
  title = {A {{Closer Look}} at {{Memorization}} in {{Deep Networks}}},
  author = {Arpit, Devansh and Jastrzebski, Stanis{\l}aw and Ballas, Nicolas and Krueger, David and Bengio, Emmanuel and Kanwal, Maxinder S and Maharaj, Tegan and Fischer, Asja and Courville, Aaron and Bengio, Yoshua and {Lacoste-Julien}, Simon},
  pages = {10},
  abstract = {We examine the role of memorization in deep learning, drawing connections to capacity, generalization, and adversarial robustness. While deep networks are capable of memorizing noise data, our results suggest that they tend to prioritize learning simple patterns first. In our experiments, we expose qualitative differences in gradient-based optimization of deep neural networks (DNNs) on noise vs. real data. We also demonstrate that for appropriately tuned explicit regularization (e.g., dropout) we can degrade DNN training performance on noise datasets without compromising generalization on real data. Our analysis suggests that the notions of effective capacity which are dataset independent are unlikely to explain the generalization performance of deep networks when trained with gradient based methods because training data itself plays an important role in determining the degree of memorization.},
  file = {Arpit et al. - A Closer Look at Memorization in Deep Networks.pdf},
  language = {en}
}

@article{Asano2020,
  title = {A {{CRITICAL ANALYSIS OF SELF}}-{{SUPERVISION}}, {{OR WHAT WE CAN LEARN FROM A SINGLE IMAGE}}},
  author = {Asano, Yuki M and Rupprecht, Christian and Vedaldi, Andrea},
  year = {2020},
  pages = {16},
  abstract = {We look critically at popular self-supervision techniques for learning deep convolutional neural networks without manual labels. We show that three different and representative methods, BiGAN, RotNet and DeepCluster, can learn the first few layers of a convolutional network from a single image as well as using millions of images and manual labels, provided that strong data augmentation is used. However, for deeper layers the gap with manual supervision cannot be closed even if millions of unlabelled images are used for training. We conclude that: (1) the weights of the early layers of deep networks contain limited information about the statistics of natural images, that (2) such low-level statistics can be learned through self-supervision just as well as through strong supervision, and that (3) the low-level statistics can be captured via synthetic transformations instead of using a large image dataset.},
  file = {Asano et al. - 2020 - A CRITICAL ANALYSIS OF SELF-SUPERVISION, OR WHAT W.pdf},
  language = {en}
}

@article{Asanovic,
  title = {The {{Landscape}} of {{Parallel Computing Research}}: {{A View}} from {{Berkeley}}},
  author = {Asanovic, Krste and Bodik, Ras and Catanzaro, Bryan Christopher and Gebis, Joseph James and Husbands, Parry and Keutzer, Kurt and Patterson, David A and Plishker, William Lester and Shalf, John and Williams, Samuel Webb and Yelick, Katherine A},
  pages = {56},
  abstract = {The recent switch to parallel microprocessors is a milestone in the history of computing. Industry has laid out a roadmap for multicore designs that preserves the programming paradigm of the past via binary compatibility and cache coherence. Conventional wisdom is now to double the number of cores on a chip with each silicon generation.},
  file = {Asanovic et al. - The Landscape of Parallel Computing Research A Vi.pdf},
  language = {en}
}

@article{Asmuth2009,
  title = {A {{Bayesian Sampling Approach}} to {{Exploration}} in {{Reinforcement Learning}}},
  author = {Asmuth, John and Li, Lihong and Littman, Michael L and Nouri, Ali and Wingate, David},
  year = {2009},
  pages = {8},
  abstract = {We present a modular approach to reinforcement learning that uses a Bayesian representation of the uncertainty over models. The approach, BOSS (Best of Sampled Set), drives exploration by sampling multiple models from the posterior and selecting actions optimistically. It extends previous work by providing a rule for deciding when to resample and how to combine the models. We show that our algorithm achieves nearoptimal reward with high probability with a sample complexity that is low relative to the speed at which the posterior distribution converges during learning. We demonstrate that BOSS performs quite favorably compared to state-of-the-art reinforcement-learning approaches and illustrate its flexibility by pairing it with a non-parametric model that generalizes across states.},
  file = {Asmuth et al. - 2009 - A Bayesian Sampling Approach to Exploration in Rei.pdf},
  language = {en}
}

@article{Assisi2011,
  title = {Using the {{Structure}} of {{Inhibitory Networks}} to {{Unravel Mechanisms}} of {{Spatiotemporal Patterning}}},
  author = {Assisi, Collins and Stopfer, Mark and Bazhenov, Maxim},
  year = {2011},
  month = jan,
  volume = {69},
  pages = {373--386},
  issn = {08966273},
  doi = {10.1016/j.neuron.2010.12.019},
  abstract = {Neuronal networks exhibit a rich dynamical repertoire, a consequence of both the intrinsic properties of neurons and the structure of the network. It has been hypothesized that inhibitory interneurons corral principal neurons into transiently synchronous ensembles that encode sensory information and subserve behavior. How does the structure of the inhibitory network facilitate such spatiotemporal patterning? We established a relationship between an important structural property of a network, its colorings, and the dynamics it constrains. Using a model of the insect antennal lobe, we show that our description allows the explicit identification of the groups of inhibitory interneurons that switch, during odor stimulation, between activity and quiescence in a coordinated manner determined by features of the network structure. This description optimally matches the perspective of the downstream neurons looking for synchrony in ensembles of presynaptic cells and allows a low-dimensional description of seemingly complex high-dimensional network activity.},
  file = {2011 - Assisi, Stopfer, Bazhenov - Using the structure of inhibitory networks to unravel mechanisms of spatiotemporal patterning.pdf},
  journal = {Neuron},
  language = {en},
  number = {2}
}

@article{Atallah2009,
  title = {Instantaneous {{Modulation}} of {{Gamma Oscillation Frequency}} by {{Balancing Excitation}} with {{Inhibition}}},
  author = {Atallah, Bassam V. and Scanziani, Massimo},
  year = {2009},
  month = may,
  volume = {62},
  pages = {566--577},
  issn = {08966273},
  doi = {10.1016/j.neuron.2009.04.027},
  abstract = {Neurons recruited for local computations exhibit rhythmic activity at gamma frequencies. The amplitude and frequency of these oscillations are continuously modulated depending on stimulus and behavioral state. This modulation is believed to crucially control information flow across cortical areas. Here we report that in the rat hippocampus gamma oscillation amplitude and frequency vary rapidly, from one cycle to the next. Strikingly, the amplitude of one oscillation predicts the interval to the next. Using in vivo and in vitro whole-cell recordings, we identify the underlying mechanism. We show that cycle-bycycle fluctuations in amplitude reflect changes in synaptic excitation spanning over an order of magnitude. Despite these rapid variations, synaptic excitation is immediately and proportionally counterbalanced by inhibition. These rapid adjustments in inhibition instantaneously modulate oscillation frequency. So, by rapidly balancing excitation with inhibition, the hippocampal network is able to swiftly modulate gamma oscillations over a wide band of frequencies.},
  file = {2009 - Atallah, Scanziani - Instantaneous Modulation of Gamma Oscillation Frequency by Balancing Excitation with Inhibition.pdf},
  journal = {Neuron},
  language = {en},
  number = {4}
}

@article{Attwell2002,
  title = {The Neural Basis of Functional Brain Imaging Signals},
  author = {Attwell, David and Iadecola, Costantino},
  year = {2002},
  month = dec,
  volume = {25},
  pages = {621--625},
  issn = {01662236},
  doi = {10.1016/S0166-2236(02)02264-6},
  file = {2002 - Attwell, Iadecola - The neural basis of functional brain imaging signals.pdf},
  journal = {Trends in Neurosciences},
  language = {en},
  number = {12}
}

@incollection{Auersperg2015,
  title = {Exploration {{Technique}} and {{Technical Innovations}} in {{Corvids}} and {{Parrots}}},
  booktitle = {Animal {{Creativity}} and {{Innovation}}},
  author = {Auersperg, Alice M.I.},
  year = {2015},
  pages = {45--72},
  publisher = {{Elsevier}},
  doi = {10.1016/B978-0-12-800648-1.00003-6},
  file = {Auersperg - 2015 - Exploration Technique and Technical Innovations in.pdf},
  isbn = {978-0-12-800648-1},
  language = {en}
}

@incollection{Auersperg2015a,
  title = {Exploration {{Technique}} and {{Technical Innovations}} in {{Corvids}} and {{Parrots}}},
  booktitle = {Animal {{Creativity}} and {{Innovation}}},
  author = {Auersperg, Alice M.I.},
  year = {2015},
  pages = {45--72},
  publisher = {{Elsevier}},
  doi = {10.1016/B978-0-12-800648-1.00003-6},
  file = {Auersperg - 2015 - Exploration Technique and Technical Innovations in.pdf},
  isbn = {978-0-12-800648-1},
  language = {en}
}

@article{Aussel2018,
  title = {A Detailed Anatomical and Mathematical Model of the Hippocampal Formation for the Generation of Sharp-Wave Ripples and Theta-Nested Gamma Oscillations},
  author = {Aussel, Am{\'e}lie and Buhry, Laure and Tyvaert, Louise and Ranta, Radu},
  year = {2018},
  month = dec,
  volume = {45},
  pages = {207--221},
  issn = {0929-5313, 1573-6873},
  doi = {10.1007/s10827-018-0704-x},
  abstract = {The mechanisms underlying the broad variety of oscillatory rhythms measured in the hippocampus during the sleep-wake cycle are not yet fully understood. In this article, we propose a computational model of the hippocampal formation based on a realistic topology and synaptic connectivity, and we analyze the effect of different changes on the network, namely the variation of synaptic conductances, the variations of the CAN channel conductance and the variation of inputs. By using a detailed simulation of intracerebral recordings, we show that this model is able to reproduce both the theta-nested gamma oscillations that are seen in awake brains and the sharp-wave ripple complexes measured during slow-wave sleep. The results of our simulations support the idea that the functional connectivity of the hippocampus, modulated by the sleep-wake variations in Acetylcholine concentration, is a key factor in controlling its rhythms.},
  file = {Aussel et al. - 2018 - A detailed anatomical and mathematical model of th.pdf},
  journal = {Journal of Computational Neuroscience},
  language = {en},
  number = {3}
}

@article{Austerweil,
  title = {Human Memory Search as a Random Walk in a Semantic Network},
  author = {Austerweil, Joseph L and Abbott, Joshua T and Griffiths, Thomas L},
  pages = {9},
  abstract = {The human mind has a remarkable ability to store a vast amount of information in memory, and an even more remarkable ability to retrieve these experiences when needed. Understanding the representations and algorithms that underlie human memory search could potentially be useful in other information retrieval settings, including internet search. Psychological studies have revealed clear regularities in how people search their memory, with clusters of semantically related items tending to be retrieved together. These findings have recently been taken as evidence that human memory search is similar to animals foraging for food in patchy environments, with people making a rational decision to switch away from a cluster of related information as it becomes depleted. We demonstrate that the results that were taken as evidence for this account also emerge from a random walk on a semantic network, much like the random web surfer model used in internet search engines. This offers a simpler and more unified account of how people search their memory, postulating a single process rather than one process for exploring a cluster and one process for switching between clusters.},
  file = {Austerweil et al. - Human memory search as a random walk in a semantic.pdf},
  language = {en}
}

@article{Ay2015,
  title = {Information {{Geometry}} on {{Complexity}} and {{Stochastic Interaction}}},
  author = {Ay, Nihat},
  year = {2015},
  month = apr,
  volume = {17},
  pages = {2432--2458},
  issn = {1099-4300},
  doi = {10.3390/e17042432},
  abstract = {Interdependencies of stochastically interacting units are usually quantified by the Kullback-Leibler divergence of a stationary joint probability distribution on the set of all configurations from the corresponding factorized distribution. This is a spatial approach which does not describe the intrinsically temporal aspects of interaction. In the present paper, the setting is extended to a dynamical version where temporal interdependencies are also captured by using information geometry of Markov chain manifolds.},
  file = {2015 - Ay - Information geometry on complexity and stochastic interaction.pdf;Ay - 2015 - Information Geometry on Complexity and Stochastic .pdf},
  journal = {Entropy},
  language = {en},
  number = {4}
}

@article{Ayala-Orozco2004,
  title = {Levy Walk Patterns in the Foraging Movements of Spider Monkeys ({{Ateles}} Geoffroyi)},
  author = {{Ayala-Orozco}, Barbara and Cocho, Germinal and Larralde, Hernen and {Ramos-Fernendez}, Gabriel and Mateos, Jose L. and Miramontes, Octavio},
  year = {2004},
  month = jan,
  volume = {55},
  pages = {223--230},
  issn = {0340-5443, 1432-0762},
  doi = {10.1007/s00265-003-0700-6},
  file = {Ayala-Orozco et al. - 2004 - L�vy walk patterns in the foraging movements of sp.pdf},
  journal = {Behavioral Ecology and Sociobiology},
  language = {en},
  number = {3}
}

@article{Ayala-Orozco2004a,
  title = {Levy Walk Patterns in the Foraging Movements of Spider Monkeys ({{Ateles}} Geoffroyi)},
  author = {{Ayala-Orozco}, Barbara and Cocho, Germinal and Larralde, Hernon and {Ramos-Fernendez}, Gabriel and Mateos, Jose L. and Miramontes, Octavio},
  year = {2004},
  month = jan,
  volume = {55},
  pages = {223--230},
  issn = {0340-5443, 1432-0762},
  doi = {10.1007/s00265-003-0700-6},
  journal = {Behavioral Ecology and Sociobiology},
  number = {3}
}

@article{Ayaz2009,
  title = {Gain {{Modulation}} of {{Neuronal Responses}} by {{Subtractive}} and {{Divisive Mechanisms}} of {{Inhibition}}},
  author = {Ayaz, Asli and Chance, Frances S.},
  year = {2009},
  month = feb,
  volume = {101},
  pages = {958--968},
  issn = {0022-3077, 1522-1598},
  doi = {10.1152/jn.90547.2008},
  file = {2012 - Ayaz, Chance - Gain Modulation of Neuronal Responses by Subtractive and Divisive Mechanisms of Inhibition Gain Modulation of Neur.pdf},
  journal = {Journal of Neurophysiology},
  language = {en},
  number = {2}
}

@article{Azevedo2009,
  title = {Equal Numbers of Neuronal and Nonneuronal Cells Make the Human Brain an Isometrically Scaled-up Primate Brain},
  author = {Azevedo, Frederico A.C. and Carvalho, Ludmila R.B. and Grinberg, Lea T. and Farfel, Jos{\'e} Marcelo and Ferretti, Renata E.L. and Leite, Renata E.P. and Filho, Wilson Jacob and Lent, Roberto and {Herculano-Houzel}, Suzana},
  year = {2009},
  month = apr,
  volume = {513},
  pages = {532--541},
  issn = {00219967, 10969861},
  doi = {10.1002/cne.21974},
  file = {Azevedo et al. - 2009 - Equal numbers of neuronal and nonneuronal cells ma.pdf},
  journal = {J. Comp. Neurol.},
  language = {en},
  number = {5}
}

@article{Azodi-Avval2015,
  title = {Phase-Dependent Modulation as a Novel Approach for Therapeutic Brain Stimulation},
  author = {{Azodi-Avval}, Ramin and Gharabaghi, Alireza},
  year = {2015},
  month = feb,
  volume = {9},
  issn = {1662-5188},
  doi = {10.3389/fncom.2015.00026},
  file = {2015 - Azodi-Avval, Gharabaghi - Phase-dependent modulation as a novel approach for therapeutic brain stimulation.pdf;Azodi-Avval and Gharabaghi - 2015 - Phase-dependent modulation as a novel approach for 2.pdf;Azodi-Avval and Gharabaghi - 2015 - Phase-dependent modulation as a novel approach for.pdf},
  journal = {Frontiers in Computational Neuroscience},
  language = {en}
}

@article{Azulay2018,
  title = {Why Do Deep Convolutional Networks Generalize so Poorly to Small Image Transformations?},
  author = {Azulay, Aharon and Weiss, Yair},
  year = {2018},
  month = may,
  abstract = {Deep convolutional network architectures are often assumed to guarantee generalization for small image translations and deformations. In this paper we show that modern CNNs (VGG16, ResNet50, and InceptionResNetV2) can drastically change their output when an image is translated in the image plane by a few pixels, and that this failure of generalization also happens with other realistic small image transformations. Furthermore, the deeper the network the more we see these failures to generalize. We show that these failures are related to the fact that the architecture of modern CNNs ignores the classical sampling theorem so that generalization is not guaranteed. We also show that biases in the statistics of commonly used image datasets makes it unlikely that CNNs will learn to be invariant to these transformations. Taken together our results suggest that the performance of CNNs in object recognition falls far short of the generalization capabilities of humans.},
  archiveprefix = {arXiv},
  eprint = {1805.12177},
  eprinttype = {arxiv},
  file = {Azulay and Weiss - 2018 - Why do deep convolutional networks generalize so p.pdf},
  journal = {arXiv:1805.12177 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  language = {en},
  primaryclass = {cs}
}

@article{Bachman2019,
  title = {Learning {{Representations}} by {{Maximizing Mutual Information Across Views}}},
  author = {Bachman, Philip and Hjelm, R. Devon and Buchwalter, William},
  year = {2019},
  month = jun,
  abstract = {We propose an approach to self-supervised representation learning based on maximizing mutual information between features extracted from multiple views of a shared context. For example, one could produce multiple views of a local spatiotemporal context by observing it from different locations (e.g., camera positions within a scene), and via different modalities (e.g., tactile, auditory, or visual). Or, an ImageNet image could provide a context from which one produces multiple views by repeatedly applying data augmentation. Maximizing mutual information between features extracted from these views requires capturing information about high-level factors whose influence spans multiple views \textendash{} e.g., presence of certain objects or occurrence of certain events. Following our proposed approach, we develop a model which learns image representations that significantly outperform prior methods on the tasks we consider. Most notably, using self-supervised learning, our model learns representations which achieve 68.1\% accuracy on ImageNet using standard linear evaluation. This beats prior results by over 12\% and concurrent results by 7\%. When we extend our model to use mixture-based representations, segmentation behaviour emerges as a natural side-effect. Our code is available online: https://github.com/Philip-Bachman/amdim-public.},
  archiveprefix = {arXiv},
  eprint = {1906.00910},
  eprinttype = {arxiv},
  file = {Bachman et al. - 2019 - Learning Representations by Maximizing Mutual Info.pdf},
  journal = {arXiv:1906.00910 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryclass = {cs, stat}
}

@article{Badre2012,
  title = {Mechanisms of {{Hierarchical Reinforcement Learning}} in {{Cortico}}-{{Striatal Circuits}} 2: {{Evidence}} from {{fMRI}}},
  shorttitle = {Mechanisms of {{Hierarchical Reinforcement Learning}} in {{Cortico}}-{{Striatal Circuits}} 2},
  author = {Badre, D. and Frank, M. J.},
  year = {2012},
  month = mar,
  volume = {22},
  pages = {527--536},
  issn = {1047-3211, 1460-2199},
  doi = {10.1093/cercor/bhr117},
  abstract = {The frontal lobes may be organized hierarchically such that more rostral frontal regions modulate cognitive control operations in caudal regions. In our companion paper (Frank MJ, Badre D. 2011. Mechanisms of hierarchical reinforcement learning in corticostriatal circuits I: computational analysis. 22:509--526), we provide novel neural circuit and algorithmic models of hierarchical cognitive control in cortico--striatal circuits. Here, we test key model predictions using functional magnetic resonance imaging (fMRI). Our neural circuit model proposes that contextual representations in rostral frontal cortex influence the striatal gating of contextual representations in caudal frontal cortex. Reinforcement learning operates at each level, such that the system adaptively learns to gate higher order contextual information into rostral regions. Our algorithmic Bayesian ``mixture of experts'' model captures the key computations of this neural model and provides trial-by-trial estimates of the learner's latent hypothesis states. In the present paper, we used these quantitative estimates to reanalyze fMRI data from a hierarchical reinforcement learning task reported in Badre D, Kayser AS, D'Esposito M. 2010. Frontal cortex and the discovery of abstract action rules. Neuron. 66:315\textendash 326. Results validate key predictions of the models and provide evidence for an individual cortico--striatal circuit for reinforcement learning of hierarchical structure at a specific level of policy abstraction. These findings are initially consistent with the proposal that hierarchical control in frontal cortex may emerge from interactions among nested cortico--striatal circuits at different levels of abstraction.},
  file = {2012 - Badre, Frank - Mechanisms of hierarchical reinforcement learning in cortico-striatal circuits 2 Evidence from fMRI.pdf},
  journal = {Cerebral Cortex},
  language = {en},
  number = {3}
}

@article{Baez2016,
  title = {Relative {{Entropy}} in {{Biological Systems}}},
  author = {Baez, John C. and Pollard, Blake S.},
  year = {2016},
  month = feb,
  volume = {18},
  pages = {46},
  issn = {1099-4300},
  doi = {10.3390/e18020046},
  abstract = {In this paper we review various information-theoretic characterizations of the approach to equilibrium in biological systems. The replicator equation, evolutionary game theory, Markov processes and chemical reaction networks all describe the dynamics of a population or probability distribution. Under suitable assumptions, the distribution will approach an equilibrium with the passage of time. Relative entropy\textemdash that is, the Kullback\textendash Leibler divergence, or various generalizations of this\textemdash provides a quantitative measure of how far from equilibrium the system is. We explain various theorems that give conditions under which relative entropy is nonincreasing. In biochemical applications these results can be seen as versions of the Second Law of Thermodynamics, stating that free energy can never increase with the passage of time. In ecological applications, they make precise the notion that a population gains information from its environment as it approaches equilibrium.},
  archiveprefix = {arXiv},
  eprint = {1512.02742},
  eprinttype = {arxiv},
  file = {Baez and Pollard - 2016 - Relative Entropy in Biological Systems.pdf},
  journal = {Entropy},
  keywords = {Computer Science - Information Theory,Mathematics - Probability,Quantitative Biology - Quantitative Methods},
  language = {en},
  number = {2}
}

@article{Bak1988,
  title = {Self-Organized Criticality},
  author = {Bak, Per and Tang, Chao and Wiesenfeld, Kurt},
  year = {1988},
  month = jul,
  volume = {38},
  pages = {364--374},
  issn = {0556-2791},
  doi = {10.1103/PhysRevA.38.364},
  file = {1988 - Bak, Tang, Wiensenfeld - Self-organized criticality.pdf},
  journal = {Physical Review A},
  language = {en},
  number = {1}
}

@article{Baker2018,
  title = {Algorithms for {{Olfactory Search}} across {{Species}}},
  author = {Baker, Keeley L. and Dickinson, Michael and Findley, Teresa M. and Gire, David H. and Louis, Matthieu and Suver, Marie P. and Verhagen, Justus V. and Nagel, Katherine I. and Smear, Matthew C.},
  year = {2018},
  month = oct,
  volume = {38},
  pages = {9383--9389},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.1668-18.2018},
  file = {Baker et al. - 2018 - Algorithms for Olfactory Search across Species.pdf},
  journal = {J. Neurosci.},
  language = {en},
  number = {44}
}

@article{Baker2018a,
  title = {Algorithms for {{Olfactory Search}} across {{Species}}},
  author = {Baker, Keeley L. and Dickinson, Michael and Findley, Teresa M. and Gire, David H. and Louis, Matthieu and Suver, Marie P. and Verhagen, Justus V. and Nagel, Katherine I. and Smear, Matthew C.},
  year = {2018},
  month = oct,
  volume = {38},
  pages = {9383--9389},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.1668-18.2018},
  file = {Baker et al. - 2018 - Algorithms for Olfactory Search across Species 2.pdf},
  journal = {J. Neurosci.},
  language = {en},
  number = {44}
}

@article{Bakhtin2012,
  title = {A Neural Computation Model for Decision-Making Times},
  author = {Bakhtin, Yuri and Correll, Joshua},
  year = {2012},
  month = oct,
  volume = {56},
  pages = {333--340},
  issn = {00222496},
  doi = {10.1016/j.jmp.2012.05.005},
  abstract = {We introduce two new models for decision-making times for a two-choice decision task with no a priori bias. One of the models is the mean-field Curie\textendash Weiss model of neural computation, and the other is based on dynamics near an unstable equilibrium under a small noise perturbation. As in the existing literature, we interpret exit times as reaction times and show that our models lead to a specific shape of the exit time distributions in the vanishing noise limit. We test the distribution shape against experimental data and show that for almost 90\% of the participants, reaction times are described well by the model. Among the features of our model are: the dependence of the exit distribution only on two parameters, the elegance of rigorous mathematical analysis, and the microscopic nature of the noise.},
  file = {2012 - Bakhtin, Correll - A neural computation model for decision-making times.pdf},
  journal = {Journal of Mathematical Psychology},
  language = {en},
  number = {5}
}

@article{Baladron2012,
  title = {Mean-Field Description and Propagation of Chaos in Networks of {{Hodgkin}}-{{Huxley}} and {{FitzHugh}}-{{Nagumo}} Neurons},
  author = {Baladron, Javier and Fasoli, Diego and Faugeras, Olivier and Touboul, Jonathan},
  year = {2012},
  volume = {2},
  pages = {10},
  issn = {2190-8567},
  doi = {10.1186/2190-8567-2-10},
  file = {2012 - Baladron et al. - Mean-field description and propagation of chaos in networks of Hodgkin-Huxley and FitzHugh-Nagumo neurons.pdf},
  journal = {The Journal of Mathematical Neuroscience},
  language = {en},
  number = {1}
}

@article{Balduzzi2018,
  title = {The {{Mechanics}} of N-{{Player Differentiable Games}}},
  author = {Balduzzi, David and Racaniere, Sebastien and Martens, James and Foerster, Jakob and Tuyls, Karl and Graepel, Thore},
  year = {2018},
  month = feb,
  abstract = {The cornerstone underpinning deep learning is the guarantee that gradient descent on an objective converges to local minima. Unfortunately, this guarantee fails in settings, such as generative adversarial nets, where there are multiple interacting losses. The behavior of gradient-based methods in games is not well understood \textendash{} and is becoming increasingly important as adversarial and multiobjective architectures proliferate. In this paper, we develop new techniques to understand and control the dynamics in general games. The key result is to decompose the second-order dynamics into two components. The first is related to potential games, which reduce to gradient descent on an implicit function; the second relates to Hamiltonian games, a new class of games that obey a conservation law, akin to conservation laws in classical mechanical systems. The decomposition motivates Symplectic Gradient Adjustment (SGA), a new algorithm for finding stable fixed points in general games. Basic experiments show SGA is competitive with recently proposed algorithms for finding stable fixed points in GANs \textendash{} whilst at the same time being applicable to \textendash{} and having guarantees in \textendash{} much more general games.},
  archiveprefix = {arXiv},
  eprint = {1802.05642},
  eprinttype = {arxiv},
  file = {Balduzzi et al. - 2018 - The Mechanics of n-Player Differentiable Games.pdf},
  journal = {arXiv:1802.05642 [cs]},
  keywords = {Computer Science - Computer Science and Game Theory,Computer Science - Machine Learning,Computer Science - Multiagent Systems,Computer Science - Neural and Evolutionary Computing},
  language = {en},
  primaryclass = {cs}
}

@article{Balkenius,
  title = {Cognitive {{Modeling}} with {{Context Sensitive Reinforcement Learning}}},
  author = {Balkenius, Christian and Winberg, Stefan},
  pages = {12},
  file = {2004 - Balkenius, Winberg - Cognitive modeling with context sensitive reinforcement learning.pdf},
  language = {en}
}

@article{Bandt2002,
  title = {Permutation {{Entropy}}: {{A Natural Complexity Measure}} for {{Time Series}}},
  shorttitle = {Permutation {{Entropy}}},
  author = {Bandt, Christoph and Pompe, Bernd},
  year = {2002},
  month = apr,
  volume = {88},
  issn = {0031-9007, 1079-7114},
  doi = {10.1103/PhysRevLett.88.174102},
  file = {2002 - Bandt, Pompe - Permutation entropy a natural complexity measure for time series.pdf},
  journal = {Physical Review Letters},
  language = {en},
  number = {17}
}

@article{Banerjee2021,
  title = {Reinforcement-Guided Learning in Frontal Neocortex: Emerging Computational Concepts},
  shorttitle = {Reinforcement-Guided Learning in Frontal Neocortex},
  author = {Banerjee, Abhishek and Rikhye, Rajeev V and Marblestone, Adam},
  year = {2021},
  month = apr,
  volume = {38},
  pages = {133--140},
  issn = {23521546},
  doi = {10.1016/j.cobeha.2021.02.019},
  file = {Banerjee et al. - 2021 - Reinforcement-guided learning in frontal neocortex.pdf},
  journal = {Current Opinion in Behavioral Sciences},
  language = {en}
}

@article{Banks2009,
  title = {Natural Strategies for Search},
  author = {Banks, Alec and Vincent, Jonathan and Phalp, Keith},
  year = {2009},
  month = sep,
  volume = {8},
  pages = {547--570},
  issn = {1567-7818, 1572-9796},
  doi = {10.1007/s11047-008-9087-7},
  abstract = {In recent years a considerable amount of natural computing research has been undertaken to exploit the analogy between, say, searching a given problem space for an optimal solution and the natural process of foraging for food. Such analogies have led to useful solutions in areas such as optimisation, prominent examples being ant colony systems and particle swarm optimisation. However, these solutions often rely on well defined fitness landscapes that are not always be available in more general search scenarios. This paper surveys a wide variety of behaviours observed within the natural world, and aims to highlight general cooperative group behaviours, search strategies and communication methods that might be useful within a wider computing context, beyond optimisation, where information from the fitness landscape may be sparse, but new search paradigms could be developed that capitalise on research into biological systems that have developed over millennia within the natural world.},
  file = {Banks et al. - 2009 - Natural strategies for search.pdf},
  journal = {Nat Comput},
  language = {en},
  number = {3}
}

@article{Bannister2005,
  title = {Inter- and Intra-Laminar Connections of Pyramidal Cells in the Neocortex},
  author = {Bannister, A. Peter},
  year = {2005},
  month = oct,
  volume = {53},
  pages = {95--103},
  issn = {01680102},
  doi = {10.1016/j.neures.2005.06.019},
  abstract = {The flow of excitation through cortical columns has long since been predicted by studying the axonal projection patterns of excitatory neurones situated within different laminae. In grossly simplified terms and assuming random connectivity, such studies predict that input from the thalamus terminates primarily in layer 4, is relayed `forward' to layer 3, then to layers 5 and 6 from where the modified signal may exit the cortex. Projection patterns also indicate `back' projections from layer 5 to 3 and layer 6 to 4. More recently it has become clear that the interconnections between these layers are not random; forward projections primarily contact specific pyramidal subclasses and intracortical back projections innervate interneurones. This indicates that presynaptic axons or postsynaptic dendrites are capable of selecting their synaptic partners and that this selectivity is layer dependent.},
  file = {2005 - Bannister - Inter- and intra-laminar connections of pyramidal cells in the neocortex.pdf},
  journal = {Neuroscience Research},
  language = {en},
  number = {2}
}

@article{Bansal2018,
  title = {{{EMERGENT COMPLEXITY VIA MULTI}}-{{AGENT COMPETITION}}},
  author = {Bansal, Trapit and Pachocki, Jakub and Sidor, Szymon and Sutskever, Ilya and Mordatch, Igor},
  year = {2018},
  pages = {12},
  abstract = {Reinforcement learning algorithms can train agents that solve problems in complex, interesting environments. Normally, the complexity of the trained agent is closely related to the complexity of the environment. This suggests that a highly capable agent requires a complex environment for training. In this paper, we point out that a competitive multi-agent environment trained with self-play can produce behaviors that are far more complex than the environment itself. We also point out that such environments come with a natural curriculum, because for any skill level, an environment full of agents of this level will have the right level of difficulty.},
  file = {Bansal et al. - 2018 - EMERGENT COMPLEXITY VIA MULTI-AGENT COMPETITION.pdf},
  language = {en}
}

@article{Baranes2013,
  title = {Active Learning of Inverse Models with Intrinsically Motivated Goal Exploration in Robots},
  author = {Baranes, Adrien and Oudeyer, Pierre-Yves},
  year = {2013},
  month = jan,
  volume = {61},
  pages = {49--73},
  issn = {09218890},
  doi = {10.1016/j.robot.2012.05.008},
  abstract = {We introduce the Self-Adaptive Goal Generation Robust Intelligent Adaptive Curiosity (SAGG-RIAC) architecture as an intrinsically motivated goal exploration mechanism which allows active learning of inverse models in high-dimensional redundant robots. This allows a robot to efficiently and actively learn distributions of parameterized motor skills/policies that solve a corresponding distribution of parameterized tasks/goals. The architecture makes the robot sample actively novel parameterized tasks in the task space, based on a measure of competence progress, each of which triggers low-level goal-directed learning of the motor policy parameters that allow to solve it. For both learning and generalization, the system leverages regression techniques which allow to infer the motor policy parameters corresponding to a given novel parameterized task, and based on the previously learnt correspondences between policy and task parameters.},
  file = {Baranes and Oudeyer - 2013 - Active learning of inverse models with intrinsical.pdf},
  journal = {Robotics and Autonomous Systems},
  language = {en},
  number = {1}
}

@article{Barbieri2011,
  title = {On the Trajectories and Performance of {{Infotaxis}}, an Information-Based Greedy Search Algorithm},
  author = {Barbieri, Carlo and Cocco, Simona and Monasson, R{\'e}mi},
  year = {2011},
  month = apr,
  volume = {94},
  pages = {20005},
  issn = {0295-5075, 1286-4854},
  doi = {10.1209/0295-5075/94/20005},
  abstract = {We present a continuous-space version of Infotaxis, a search algorithm where a searcher greedily moves to maximize the gain in information about the position of the target to be found. Using a combination of analytical and numerical tools we study the nature of the trajectories in two and three dimensions. The probability that the search is successful and the running time of the search are estimated. A possible extension to non-greedy search is suggested.},
  archiveprefix = {arXiv},
  eprint = {1010.2728},
  eprinttype = {arxiv},
  file = {Barbieri et al. - 2011 - On the trajectories and performance of Infotaxis, .pdf},
  journal = {EPL},
  keywords = {Condensed Matter - Disordered Systems and Neural Networks,Physics - Biological Physics,Quantitative Biology - Quantitative Methods},
  language = {en},
  number = {2}
}

@article{Barbieri2011a,
  title = {On the Trajectories and Performance of {{Infotaxis}}, an Information-Based Greedy Search Algorithm},
  author = {Barbieri, Carlo and Cocco, Simona and Monasson, R{\'e}mi},
  year = {2011},
  month = apr,
  volume = {94},
  pages = {20005},
  issn = {0295-5075, 1286-4854},
  doi = {10.1209/0295-5075/94/20005},
  abstract = {We present a continuous-space version of Infotaxis, a search algorithm where a searcher greedily moves to maximize the gain in information about the position of the target to be found. Using a combination of analytical and numerical tools we study the nature of the trajectories in two and three dimensions. The probability that the search is successful and the running time of the search are estimated. A possible extension to non-greedy search is suggested.},
  archiveprefix = {arXiv},
  eprint = {1010.2728},
  eprinttype = {arxiv},
  file = {Barbieri et al. - 2011 - On the trajectories and performance of Infotaxis,  2.pdf},
  journal = {EPL},
  keywords = {Condensed Matter - Disordered Systems and Neural Networks,Physics - Biological Physics,Quantitative Biology - Quantitative Methods},
  language = {en},
  number = {2}
}

@article{Barbieri2014,
  title = {Stimulus {{Dependence}} of {{Local Field Potential Spectra}}: {{Experiment}} versus {{Theory}}},
  shorttitle = {Stimulus {{Dependence}} of {{Local Field Potential Spectra}}},
  author = {Barbieri, F. and Mazzoni, A. and Logothetis, N. K. and Panzeri, S. and Brunel, N.},
  year = {2014},
  month = oct,
  volume = {34},
  pages = {14589--14605},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.5365-13.2014},
  file = {Barbieri et al. - 2014 - Stimulus Dependence of Local Field Potential Spect.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {44}
}

@article{Barbour2001,
  title = {An {{Evaluation}} of {{Synapse Independence}}},
  author = {Barbour, Boris},
  year = {2001},
  month = oct,
  volume = {21},
  pages = {7969--7984},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.21-20-07969.2001},
  file = {2001 - Barbour - An Evaluation of Synapse Independence.pdf},
  journal = {The Journal of Neuroscience},
  language = {en},
  number = {20}
}

@article{Bargmann2013,
  title = {From the Connectome to Brain Function},
  author = {Bargmann, Cornelia I and Marder, Eve},
  year = {2013},
  month = jun,
  volume = {10},
  pages = {483--490},
  issn = {1548-7091, 1548-7105},
  doi = {10.1038/nmeth.2451},
  file = {2013 - Bargmann, Marder - From the connectome to brain function.pdf},
  journal = {Nature Methods},
  language = {en},
  number = {6}
}

@article{Barraclough2004,
  title = {Prefrontal Cortex and Decision Making in a Mixed-Strategy Game},
  author = {Barraclough, Dominic J and Conroy, Michelle L and Lee, Daeyeol},
  year = {2004},
  month = apr,
  volume = {7},
  pages = {404--410},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn1209},
  file = {2004 - Barraclough, Conroy, Lee - Prefrontal cortex and decision making in a mixed-strategy game.pdf},
  journal = {Nature Neuroscience},
  language = {en},
  number = {4}
}

@article{Barreiro2010,
  title = {Time Scales of Spike-Train Correlation for Neural Oscillators with Common Drive},
  author = {Barreiro, Andrea K. and {Shea-Brown}, Eric and Thilo, Evan L.},
  year = {2010},
  month = jan,
  volume = {81},
  issn = {1539-3755, 1550-2376},
  doi = {10.1103/PhysRevE.81.011916},
  file = {2010 - Barreiro, Shea-Brown, Thilo - Time scales of spike-train correlation for neural oscillators with common drive.pdf},
  journal = {Physical Review E},
  language = {en},
  number = {1}
}

@article{Barreto2018,
  title = {Successor {{Features}} for {{Transfer}} in {{Reinforcement Learning}}},
  author = {Barreto, Andre and Dabney, Will and Munos, Remi and Hunt, Jonathan J and Schaul, Tom},
  year = {2018},
  volume = {1606.05312v2},
  pages = {1--11},
  abstract = {Transfer in reinforcement learning refers to the notion that generalization should occur not only within a task but also across tasks. We propose a transfer framework for the scenario where the reward function changes between tasks but the environment's dynamics remain the same. Our approach rests on two key ideas: successor features, a value function representation that decouples the dynamics of the environment from the rewards, and generalized policy improvement, a generalization of dynamic programming's policy improvement operation that considers a set of policies rather than a single one. Put together, the two ideas lead to an approach that integrates seamlessly within the reinforcement learning framework and allows the free exchange of information across tasks. The proposed method also provides performance guarantees for the transferred policy even before any learning has taken place. We derive two theorems that set our approach in firm theoretical ground and present experiments that show that it successfully promotes transfer in practice, significantly outperforming alternative methods in a sequence of navigation tasks and in the control of a simulated robotic arm.},
  file = {Barreto et al. - Successor Features for Transfer in Reinforcement L.pdf},
  journal = {Arxiv},
  language = {en}
}

@article{Barrett2008,
  title = {Optimal {{Learning Rules}} for {{Discrete Synapses}}},
  author = {Barrett, Adam B. and {van Rossum}, M. C. W.},
  editor = {Graham, Lyle J.},
  year = {2008},
  month = nov,
  volume = {4},
  pages = {e1000230},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1000230},
  abstract = {There is evidence that biological synapses have a limited number of discrete weight states. Memory storage with such synapses behaves quite differently from synapses with unbounded, continuous weights, as old memories are automatically overwritten by new memories. Consequently, there has been substantial discussion about how this affects learning and storage capacity. In this paper, we calculate the storage capacity of discrete, bounded synapses in terms of Shannon information. We use this to optimize the learning rules and investigate how the maximum information capacity depends on the number of synapses, the number of synaptic states, and the coding sparseness. Below a certain critical number of synapses per neuron (comparable to numbers found in biology), we find that storage is similar to unbounded, continuous synapses. Hence, discrete synapses do not necessarily have lower storage capacity.},
  file = {2008 - Barrett, Van Rossum - Optimal learning rules for discrete synapses.pdf},
  journal = {PLoS Computational Biology},
  language = {en},
  number = {11}
}

@article{Barron1993,
  title = {Universal Approximation Bounds for Superpositions of a Sigmoidal Function},
  author = {Barron, A.R.},
  year = {1993},
  month = may,
  volume = {39},
  pages = {930--945},
  issn = {0018-9448, 1557-9654},
  doi = {10.1109/18.256500},
  file = {Barron - 1993 - Universal approximation bounds for superpositions .pdf},
  journal = {IEEE Trans. Inform. Theory},
  language = {en},
  number = {3}
}

@article{Barth2012,
  title = {Experimental Evidence for Sparse Firing in the Neocortex},
  author = {Barth, Alison L. and Poulet, James F.A.},
  year = {2012},
  month = jun,
  volume = {35},
  pages = {345--355},
  issn = {01662236},
  doi = {10.1016/j.tins.2012.03.008},
  file = {2012 - Barth, Poulet - Experimental evidence for sparse firing in the neocortex.pdf},
  journal = {Trends in Neurosciences},
  language = {en},
  number = {6}
}

@article{Bartlett1998,
  title = {Boosting the Margin: A New Explanation for the Effectiveness of Voting Methods},
  shorttitle = {Boosting the Margin},
  author = {Bartlett, Peter and Freund, Yoav and Lee, Wee Sun and Schapire, Robert E.},
  year = {1998},
  month = oct,
  volume = {26},
  issn = {0090-5364},
  doi = {10.1214/aos/1024691352},
  file = {Bartlett et al. - 1998 - Boosting the margin a new explanation for the eff.pdf},
  journal = {Ann. Statist.},
  language = {en},
  number = {5}
}

@article{Bartlett1998a,
  title = {Boosting the Margin: A New Explanation for the Effectiveness of Voting Methods},
  shorttitle = {Boosting the Margin},
  author = {Bartlett, Peter and Freund, Yoav and Lee, Wee Sun and Schapire, Robert E.},
  year = {1998},
  month = oct,
  volume = {26},
  issn = {0090-5364},
  doi = {10.1214/aos/1024691352},
  file = {Bartlett et al. - 1998 - Boosting the margin a new explanation for the eff 2.pdf},
  journal = {Ann. Statist.},
  language = {en},
  number = {5}
}

@incollection{Barto2013,
  title = {Intrinsic {{Motivation}} and {{Reinforcement Learning}}},
  booktitle = {Intrinsically {{Motivated Learning}} in {{Natural}} and {{Artificial Systems}}},
  author = {Barto, Andrew G.},
  editor = {Baldassarre, Gianluca and Mirolli, Marco},
  year = {2013},
  pages = {17--47},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-32375-1_2},
  abstract = {Psychologists distinguish between extrinsically motivated behavior, which is behavior undertaken to achieve some externally supplied reward, such as a prize, a high grade, or a high-paying job, and intrinsically motivated behavior, which is behavior done for its own sake. Is an analogous distinction meaningful for machine learning systems? Can we say of a machine learning system that it is motivated to learn, and if so, is it possible to provide it with an analog of intrinsic motivation? Despite the fact that a formal distinction between extrinsic and intrinsic motivation is elusive, this chapter argues that the answer to both questions is assuredly ``yes'' and that the machine learning framework of reinforcement learning is particularly appropriate for bringing learning together with what in animals one would call motivation. Despite the common perception that a reinforcement learning agent's reward has to be extrinsic because the agent has a distinct input channel for reward signals, reinforcement learning provides a natural framework for incorporating principles of intrinsic motivation.},
  file = {Barto - 2013 - Intrinsic Motivation and Reinforcement Learning.pdf},
  isbn = {978-3-642-32374-4 978-3-642-32375-1},
  language = {en}
}

@article{Bartolo2020,
  title = {Information-{{Limiting Correlations}} in {{Large Neural Populations}}},
  author = {Bartolo, Ramon and Saunders, Richard C. and Mitz, Andrew R. and Averbeck, Bruno B.},
  year = {2020},
  month = feb,
  volume = {40},
  pages = {1668--1678},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.2072-19.2019},
  file = {Bartolo et al. - 2020 - Information-Limiting Correlations in Large Neural .pdf},
  journal = {J. Neurosci.},
  language = {en},
  number = {8}
}

@article{Bartos2007,
  title = {Synaptic Mechanisms of Synchronized Gamma Oscillations in Inhibitory Interneuron Networks},
  author = {Bartos, Marlene and Vida, Imre and Jonas, Peter},
  year = {2007},
  month = jan,
  volume = {8},
  pages = {45--56},
  issn = {1471-003X, 1471-0048},
  doi = {10.1038/nrn2044},
  abstract = {Gamma frequency oscillations are thought to provide a temporal structure for information processing in the brain. They contribute to cognitive functions, such as memory formation and sensory processing, and are disturbed in some psychiatric disorders. Fast-spiking, parvalbumin-expressing, soma-inhibiting interneurons have a key role in the generation of these oscillations. Experimental analysis in the hippocampus and the neocortex reveals that synapses among these interneurons are highly specialized. Computational analysis further suggests that synaptic specialization turns interneuron networks into robust gamma frequency oscillators.},
  file = {2007 - Bartos, Vida, Jonas - Synaptic mechanisms of synchronized gamma oscillations in inhibitory interneuron networks(2).pdf},
  journal = {Nature Reviews Neuroscience},
  language = {en},
  number = {1}
}

@article{Bartumeus2002,
  title = {Optimizing the {{Encounter Rate}} in {{Biological Interactions}}: {{L\'evy}} versus {{Brownian Strategies}}},
  shorttitle = {Optimizing the {{Encounter Rate}} in {{Biological Interactions}}},
  author = {Bartumeus, F. and Catalan, J. and Fulco, U. L. and Lyra, M. L. and Viswanathan, G. M.},
  year = {2002},
  month = feb,
  volume = {88},
  pages = {097901},
  issn = {0031-9007, 1079-7114},
  doi = {10.1103/PhysRevLett.88.097901},
  file = {Bartumeus et al. - 2002 - Optimizing the Encounter Rate in Biological Intera.pdf},
  journal = {Phys. Rev. Lett.},
  language = {en},
  number = {9}
}

@article{Bartumeus2002a,
  title = {Optimizing the {{Encounter Rate}} in {{Biological Interactions}}: {{L\'evy}} versus {{Brownian Strategies}}},
  shorttitle = {Optimizing the {{Encounter Rate}} in {{Biological Interactions}}},
  author = {Bartumeus, F. and Catalan, J. and Fulco, U. L. and Lyra, M. L. and Viswanathan, G. M.},
  year = {2002},
  month = feb,
  volume = {88},
  pages = {097901},
  issn = {0031-9007, 1079-7114},
  doi = {10.1103/PhysRevLett.88.097901},
  file = {Bartumeus et al. - 2002 - Optimizing the Encounter Rate in Biological Intera 2.pdf},
  journal = {Phys. Rev. Lett.},
  language = {en},
  number = {9}
}

@article{Bartumeus2002b,
  title = {Optimizing the {{Encounter Rate}} in {{Biological Interactions}}: {{L\'evy}} versus {{Brownian Strategies}}},
  shorttitle = {Optimizing the {{Encounter Rate}} in {{Biological Interactions}}},
  author = {Bartumeus, F. and Catalan, J. and Fulco, U. L. and Lyra, M. L. and Viswanathan, G. M.},
  year = {2002},
  month = feb,
  volume = {88},
  pages = {097901},
  issn = {0031-9007, 1079-7114},
  doi = {10.1103/PhysRevLett.88.097901},
  file = {Bartumeus et al. - 2002 - Optimizing the Encounter Rate in Biological Intera 3.pdf},
  journal = {Phys. Rev. Lett.},
  language = {en},
  number = {9}
}

@article{Bartumeus2003,
  title = {Helical {{Levy}} Walks: {{Adjusting}} Searching Statistics to Resource Availability in Microzooplankton},
  shorttitle = {Helical {{Levy}} Walks},
  author = {Bartumeus, F. and Peters, F. and Pueyo, S. and Marrase, C. and Catalan, J.},
  year = {2003},
  month = oct,
  volume = {100},
  pages = {12771--12775},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.2137243100},
  file = {Bartumeus et al. - 2003 - Helical Levy walks Adjusting searching statistics.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {22}
}

@article{Bartumeus2003a,
  title = {Helical {{Levy}} Walks: {{Adjusting}} Searching Statistics to Resource Availability in Microzooplankton},
  shorttitle = {Helical {{Levy}} Walks},
  author = {Bartumeus, F. and Peters, F. and Pueyo, S. and Marrase, C. and Catalan, J.},
  year = {2003},
  month = oct,
  volume = {100},
  pages = {12771--12775},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.2137243100},
  file = {Bartumeus et al. - 2003 - Helical Levy walks Adjusting searching statistics 2.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {22}
}

@article{Bartumeus2009,
  title = {Optimal Search Behavior and Classic Foraging Theory},
  author = {Bartumeus, F and Catalan, J},
  year = {2009},
  month = oct,
  volume = {42},
  pages = {434002},
  issn = {1751-8113, 1751-8121},
  doi = {10.1088/1751-8113/42/43/434002},
  abstract = {Random walk methods and diffusion theory pervaded ecological sciences as methods to analyze and describe animal movement. Consequently, statistical physics was mostly seen as a toolbox rather than as a conceptual framework that could contribute to theory on evolutionary biology and ecology. However, the existence of mechanistic relationships and feedbacks between behavioral processes and statistical patterns of movement suggests that, beyond movement quantification, statistical physics may prove to be an adequate framework to understand animal behavior across scales from an ecological and evolutionary perspective. Recently developed random search theory has served to critically re-evaluate classic ecological questions on animal foraging. For instance, during the last few years, there has been a growing debate on whether search behavior can include traits that improve success by optimizing random (stochastic) searches. Here, we stress the need to bring together the general encounter problem within foraging theory, as a mean for making progress in the biological understanding of random searching. By sketching the assumptions of optimal foraging theory (OFT) and by summarizing recent results on random search strategies, we pinpoint ways to extend classic OFT, and integrate the study of search strategies and its main results into the more general theory of optimal foraging.},
  file = {Bartumeus and Catalan - 2009 - Optimal search behavior and classic foraging theor.pdf},
  journal = {J. Phys. A: Math. Theor.},
  language = {en},
  number = {43}
}

@article{Bartumeus2014,
  title = {Stochastic {{Optimal Foraging}}: {{Tuning Intensive}} and {{Extensive Dynamics}} in {{Random Searches}}},
  shorttitle = {Stochastic {{Optimal Foraging}}},
  author = {Bartumeus, Frederic and Raposo, Ernesto P. and Viswanathan, Gandhimohan M. and {da Luz}, Marcos G. E.},
  editor = {McDonnell, Mark D.},
  year = {2014},
  month = sep,
  volume = {9},
  pages = {e106373},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0106373},
  abstract = {Recent theoretical developments had laid down the proper mathematical means to understand how the structural complexity of search patterns may improve foraging efficiency. Under information-deprived scenarios and specific landscape configurations, Le\textasciiacute vy walks and flights are known to lead to high search efficiencies. Based on a one-dimensional comparative analysis we show a mechanism by which, at random, a searcher can optimize the encounter with close and distant targets. The mechanism consists of combining an optimal diffusivity (optimally enhanced diffusion) with a minimal diffusion constant. In such a way the search dynamics adequately balances the tension between finding close and distant targets, while, at the same time, shifts the optimal balance towards relatively larger close-to-distant target encounter ratios. We find that introducing a multiscale set of reorientations ensures both a thorough local space exploration without oversampling and a fast spreading dynamics at the large scale. Le\textasciiacute vy reorientation patterns account for these properties but other reorientation strategies providing similar statistical signatures can mimic or achieve comparable efficiencies. Hence, the present work unveils general mechanisms underlying efficient random search, beyond the Le\textasciiacute vy model. Our results suggest that animals could tune key statistical movement properties (e.g. enhanced diffusivity, minimal diffusion constant) to cope with the very general problem of balancing out intensive and extensive random searching. We believe that theoretical developments to mechanistically understand stochastic search strategies, such as the one here proposed, are crucial to develop an empirically verifiable and comprehensive animal foraging theory.},
  file = {Bartumeus et al. - 2014 - Stochastic Optimal Foraging Tuning Intensive and .pdf},
  journal = {PLoS ONE},
  language = {en},
  number = {9}
}

@article{Bartunov,
  title = {Assessing the {{Scalability}} of {{Biologically}}-{{Motivated}}  {{Deep Learning Algorithms}} and {{Architectures}}},
  author = {Bartunov, Sergey and Santoro, Adam and Richards, Blake A and Hinton, Geoffrey E and Lillicrap, Timothy P},
  pages = {14},
  abstract = {The backpropagation of error algorithm (BP) is often said to be impossible to implement in a real brain. The recent success of deep networks in machine learning and AI, however, has inspired proposals for understanding how the brain might learn across multiple layers, and hence how it might implement or approximate BP. As of yet, none of these proposals have been rigorously evaluated on tasks where BP-guided deep learning has proved critical, or in architectures more structured than simple fully-connected networks. Here we present the first results on scaling up biologically motivated models of deep learning on datasets which need deep networks with appropriate architectures to achieve good performance. We present results on the MNIST, CIFAR10, and ImageNet datasets and explore variants of target-propagation (TP) and feedback alignment (FA) algorithms, and explore performance in both fully- and locally-connected architectures. We also introduce weight-transport-free variants of difference target propagation (DTP) modified to remove backpropagation from the penultimate layer. Many of these algorithms perform well for MNIST, but for CIFAR and ImageNet we find that TP and FA variants perform significantly worse than BP, especially for networks composed of locally connected units, opening questions about whether new architectures and algorithms are required to scale these approaches. Our results and implementation details help establish baselines for biologically motivated deep learning schemes going forward.},
  file = {Bartunov et al. - Assessing the Scalability of Biologically-Motivate.pdf},
  language = {en}
}

@article{Bastide2015,
  title = {Pathophysiology of {{L}}-Dopa-Induced Motor and Non-Motor Complications in {{Parkinson}}'s Disease},
  author = {Bastide, Matthieu F. and Meissner, Wassilios G. and Picconi, Barbara and Fasano, Stefania and Fernagut, Pierre-Olivier and Feyder, Michael and Francardo, Veronica and Alcacer, Cristina and Ding, Yunmin and Brambilla, Riccardo and Fisone, Gilberto and Jon Stoessl, A. and Bourdenx, Mathieu and Engeln, Michel and Navailles, Sylvia and De Deurwaerd{\`e}re, Philippe and Ko, Wai Kin D. and Simola, Nicola and Morelli, Micaela and Groc, Laurent and Rodriguez, Maria-Cruz and Gurevich, Eugenia V. and Quik, Maryka and Morari, Michele and Mellone, Manuela and Gardoni, Fabrizio and Tronci, Elisabetta and Guehl, Dominique and Tison, Fran{\c c}ois and Crossman, Alan R. and Kang, Un Jung and {Steece-Collier}, Kathy and Fox, Susan and Carta, Manolo and Angela Cenci, M. and B{\'e}zard, Erwan},
  year = {2015},
  month = sep,
  volume = {132},
  pages = {96--168},
  issn = {03010082},
  doi = {10.1016/j.pneurobio.2015.07.002},
  abstract = {Involuntary movements, or dyskinesia, represent a debilitating complication of levodopa (L-dopa) therapy for Parkinson's disease (PD). L-dopa-induced dyskinesia (LID) are ultimately experienced by the vast majority of patients. In addition, psychiatric conditions often manifested as compulsive behaviours, are emerging as a serious problem in the management of L-dopa therapy. The present review attempts to provide an overview of our current understanding of dyskinesia and other L-dopainduced dysfunctions, a field that dramatically evolved in the past twenty years. In view of the extensive literature on LID, there appeared a critical need to re-frame the concepts, to highlight the most suitable models, to review the central nervous system (CNS) circuitry that may be involved, and to propose a pathophysiological framework was timely and necessary. An updated review to clarify our understanding of LID and other L-dopa-related side effects was therefore timely and necessary.},
  file = {2015 - Bastide et al. - Pathophysiology of L-dopa-induced motor and non-motor complications in Parkinson's disease.pdf;Bastide et al. - 2015 - Pathophysiology of L-dopa-induced motor and non-mo.pdf},
  journal = {Progress in Neurobiology},
  language = {en}
}

@article{Bastin2014,
  title = {Inhibitory Control and Error Monitoring by Human Subthalamic Neurons},
  author = {Bastin, J and Polosan, M and Benis, D and Goetz, L and Bhattacharjee, M and Piallat, B and Krainik, A and Bougerol, T and Chabard{\`e}s, S and David, O},
  year = {2014},
  month = sep,
  volume = {4},
  pages = {e439-e439},
  issn = {2158-3188},
  doi = {10.1038/tp.2014.73},
  file = {2014 - Bastin et al. - Inhibitory control and error monitoring by human subthalamic neurons.pdf;Bastin et al. - 2014 - Inhibitory control and error monitoring by human s.pdf},
  journal = {Translational Psychiatry},
  language = {en},
  number = {9}
}

@article{Bastos2015,
  title = {Communication through Coherence with Inter-Areal Delays},
  author = {Bastos, Andre M and Vezoli, Julien and Fries, Pascal},
  year = {2015},
  month = apr,
  volume = {31},
  pages = {173--180},
  issn = {09594388},
  doi = {10.1016/j.conb.2014.11.001},
  file = {Bastos et al. - 2015 - Communication through coherence with inter-areal d.pdf},
  journal = {Current Opinion in Neurobiology},
  language = {en}
}

@article{Batty,
  title = {{{BehaveNet}}: Nonlinear Embedding and {{Bayesian}} Neural Decoding of Behavioral Videos},
  author = {Batty, Eleanor and Whiteway, Matthew R and Saxena, Shreya and Biderman, Dan and Abe, Taiga and Musall, Simon and Gillis, Winthrop and Markowitz, Jeffrey E and Churchland, Anne and Cunningham, John and Datta, Sandeep Robert and Linderman, Scott W and Paninski, Liam},
  pages = {12},
  abstract = {A fundamental goal of systems neuroscience is to understand the relationship between neural activity and behavior. Behavior has traditionally been characterized by low-dimensional, task-related variables such as movement speed or response times. More recently, there has been a growing interest in automated analysis of high-dimensional video data collected during experiments. Here we introduce a probabilistic framework for the analysis of behavioral video and neural activity. This framework provides tools for compression, segmentation, generation, and decoding of behavioral videos. Compression is performed using a convolutional autoencoder (CAE), which yields a low-dimensional continuous representation of behavior. We then use an autoregressive hidden Markov model (ARHMM) to segment the CAE representation into discrete ``behavioral syllables.'' The resulting generative model can be used to simulate behavioral video data. Finally, based on this generative model, we develop a novel Bayesian decoding approach that takes in neural activity and outputs probabilistic estimates of the full-resolution behavioral video. We demonstrate this framework on two different experimental paradigms using distinct behavioral and neural recording technologies.},
  file = {Batty et al. - BehaveNet nonlinear embedding and Bayesian neural.pdf},
  language = {en}
}

@article{Baumgartner1999,
  title = {Assessment of Cluster Homogeneity in {{fMRI}} Data Using {{Kendall}}'s Coefficient of Concordance},
  author = {Baumgartner, R. and Somorjai, R. and Summers, R. and Richter, W.},
  year = {1999},
  month = dec,
  volume = {17},
  pages = {1525--1532},
  issn = {0730725X},
  doi = {10.1016/S0730-725X(99)00101-0},
  file = {1999 - Concordance - q Technical Note ASSESSMENT OF CLUSTER HOMOGENEITY IN fMRI DATA USING KENDALL ’ S COEFFICIENT OF CONCORDANCE.pdf},
  journal = {Magnetic Resonance Imaging},
  language = {en},
  number = {10}
}

@article{Baumgartner2001,
  title = {Graphical Display of {{fMRI}} Data: Visualizing Multidimensional Space},
  shorttitle = {Graphical Display of {{fMRI}} Data},
  author = {Baumgartner, R and Somorjai, R},
  year = {2001},
  month = feb,
  volume = {19},
  pages = {283--286},
  issn = {0730725X},
  doi = {10.1016/S0730-725X(01)00296-X},
  abstract = {Visualization of multidimensional data is an integral part of computational statistics and exploratory data analysis (EDA). We show how visualization of fMRI time-courses may be used to reveal the fMRI data structure. We consider fMRI time-courses (TCs) as points in multidimensional space. In simulated and in vivo data, we show that minimum spanning tree (MST)-based sequencing of multivariate time-courses, in combination with a homogeneity map visualization, allows for effective and useful graphical display of the groups of coactivated time-courses obtained by temporal clustering. This display may serve as a tool for investigation of brain connectivity. We also suggest a simple overall display of the entire fMRI data set. \textcopyright{} 2001 Elsevier Science Inc. All rights reserved.},
  file = {2001 - Baumgartner, Somorjai - Graphical display of fMRI data visualizing multidimensional space.pdf},
  journal = {Magnetic Resonance Imaging},
  language = {en},
  number = {2}
}

@article{Baxter2000,
  title = {A {{Model}} of {{Inductive Bias Learning}}},
  author = {Baxter, J.},
  year = {2000},
  month = mar,
  volume = {12},
  pages = {149--198},
  issn = {1076-9757},
  doi = {10.1613/jair.731},
  abstract = {A major problem in machine learning is that of inductive bias: how to choose a learner's hypothesis space so that it is large enough to contain a solution to the problem being learnt, yet small enough to ensure reliable generalization from reasonably-sized training sets. Typically such bias is supplied by hand through the skill and insights of experts. In this paper a model for automatically learning bias is investigated. The central assumption of the model is that the learner is embedded within an environment of related learning tasks. Within such an environment the learner can sample from multiple tasks, and hence it can search for a hypothesis space that contains good solutions to many of the problems in the environment. Under certain restrictions on the set of all hypothesis spaces available to the learner, we show that a hypothesis space that performs well on a sufficiently large number of training tasks will also perform well when learning novel tasks in the same environment. Explicit bounds are also derived demonstrating that learning multiple tasks within an environment of related tasks can potentially give much better generalization than learning a single task.},
  file = {Baxter - 2000 - A Model of Inductive Bias Learning.pdf},
  journal = {jair},
  language = {en}
}

@article{Bays2014,
  title = {Noise in {{Neural Populations Accounts}} for {{Errors}} in {{Working Memory}}},
  author = {Bays, P. M.},
  year = {2014},
  month = mar,
  volume = {34},
  pages = {3632--3645},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.3204-13.2014},
  file = {Bays - 2014 - Noise in Neural Populations Accounts for Errors in.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {10}
}

@article{Bazargani2016a,
  title = {Astrocyte Calcium Signaling: The Third Wave},
  shorttitle = {Astrocyte Calcium Signaling},
  author = {Bazargani, Narges and Attwell, David},
  year = {2016},
  month = feb,
  volume = {19},
  pages = {182--189},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.4201},
  file = {Bazargani and Attwell - 2016 - Astrocyte calcium signaling the third wave 2.pdf},
  journal = {Nat Neurosci},
  language = {en},
  number = {2}
}

@article{Bazazi2012,
  title = {Intermittent {{Motion}} in {{Desert Locusts}}: {{Behavioural Complexity}} in {{Simple Environments}}},
  shorttitle = {Intermittent {{Motion}} in {{Desert Locusts}}},
  author = {Bazazi, Sepideh and Bartumeus, Frederic and Hale, Joseph J. and Couzin, Iain D.},
  editor = {Fryxell, John M.},
  year = {2012},
  month = may,
  volume = {8},
  pages = {e1002498},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1002498},
  abstract = {Animals can exhibit complex movement patterns that may be the result of interactions with their environment or may be directly the mechanism by which their behaviour is governed. In order to understand the drivers of these patterns we examine the movement behaviour of individual desert locusts in a homogenous experimental arena with minimal external cues. Locust motion is intermittent and we reveal that as pauses become longer, the probability that a locust changes direction from its previous direction of travel increases. Long pauses (of greater than 100 s) can be considered reorientation bouts, while shorter pauses (of less than 6 s) appear to act as periods of resting between displacements. We observe powerlaw behaviour in the distribution of move and pause lengths of over 1.5 orders of magnitude. While Le\textasciiacute vy features do exist, locusts' movement patterns are more fully described by considering moves, pauses and turns in combination. Further analysis reveals that these combinations give rise to two behavioural modes that are organized in time: local search behaviour (long exploratory pauses with short moves) and relocation behaviour (long displacement moves with shorter resting pauses). These findings offer a new perspective on how complex animal movement patterns emerge in nature.},
  file = {Bazazi et al. - 2012 - Intermittent Motion in Desert Locusts Behavioural.pdf},
  journal = {PLoS Comput Biol},
  language = {en},
  number = {5}
}

@techreport{Beam2019,
  title = {A Computational Knowledge Engine for Human Neuroscience},
  author = {Beam, Elizabeth and Potts, Christopher and Poldrack, Russell A. and Etkin, Amit},
  year = {2019},
  month = jul,
  institution = {{Neuroscience}},
  doi = {10.1101/701540},
  abstract = {Functional neuroimaging has been a mainstay of human neuroscience for the past 25 years. The goal for this research has largely been to understand how activity across brain structures relates to mental constructs and computations. However, interpretation of fMRI data has often occurred within knowledge frameworks crafted by experts, which have the potential to reify historical trends and amplify the subjective biases that limit the replicability of findings.             1             In other words, we lack a comprehensive data-driven ontology for structure-function mapping in the human brain, through which we can also test the explanatory value of current dominant conceptual frameworks. Ontologies in other fields are popular tools for automated data synthesis,             2, 3             yet relatively few attempts have been made to engineer ontologies in a data-driven manner.             4             Here, we employ a computational approach to derive a data-driven ontology for neurobiological domains that synthesizes the texts and data of nearly 20,000 human neuroimaging articles. The data-driven ontology includes 6 domains, each defined by a circuit of brain structures and its associated mental functions. Several of these domains are omitted from the leading framework in neuroscience, while others uncover novel combinations of mental functions related to common brain circuitry. Crucially, the structure-function links in each domain better replicate across articles in held-out data than those mapped from the dominant frameworks in neuroscience and psychiatry. We further show that the data-driven ontology partitions the literature into modular subfields, for which the domains serve as generalizable archetypes of the structure-function patterns observed in single articles. The approach to computational ontology we present here is the most comprehensive functional characterization of human brain circuits quantifiable with fMRI. Moreover, our methods can be extended to synthesize other scientific literatures, yielding ontologies that are built up from the data of the field.},
  file = {Beam et al. - 2019 - A computational knowledge engine for human neurosc.pdf},
  language = {en},
  type = {Preprint}
}

@article{Beattie2016,
  title = {{{DeepMind Lab}}},
  author = {Beattie, Charles and Leibo, Joel Z. and Teplyashin, Denis and Ward, Tom and Wainwright, Marcus and K{\"u}ttler, Heinrich and Lefrancq, Andrew and Green, Simon and Vald{\'e}s, V{\'i}ctor and Sadik, Amir and Schrittwieser, Julian and Anderson, Keith and York, Sarah and Cant, Max and Cain, Adam and Bolton, Adrian and Gaffney, Stephen and King, Helen and Hassabis, Demis and Legg, Shane and Petersen, Stig},
  year = {2016},
  month = dec,
  abstract = {DeepMind Lab is a first-person 3D game platform designed for research and development of general artificial intelligence and machine learning systems. DeepMind Lab can be used to study how autonomous artificial agents may learn complex tasks in large, partially observed, and visually diverse worlds. DeepMind Lab has a simple and flexible API enabling creative task-designs and novel AI-designs to be explored and quickly iterated upon. It is powered by a fast and widely recognised game engine, and tailored for effective use by the research community.},
  archiveprefix = {arXiv},
  eprint = {1612.03801},
  eprinttype = {arxiv},
  file = {Beattie et al. - 2016 - DeepMind Lab.pdf},
  journal = {arXiv:1612.03801 [cs]},
  keywords = {Computer Science - Artificial Intelligence},
  language = {en},
  primaryclass = {cs}
}

@article{Beauchamp2020,
  title = {Dynamic {{Stimulation}} of {{Visual Cortex Produces Form Vision}} in {{Sighted}} and {{Blind Humans}}},
  author = {Beauchamp, Michael S. and Oswalt, Denise and Sun, Ping and Foster, Brett L. and Magnotti, John F. and Niketeghad, Soroush and Pouratian, Nader and Bosking, William H. and Yoshor, Daniel},
  year = {2020},
  month = may,
  volume = {181},
  pages = {774-783.e5},
  issn = {00928674},
  doi = {10.1016/j.cell.2020.04.033},
  abstract = {A visual cortical prosthesis (VCP) has long been proposed as a strategy for restoring useful vision to the blind, under the assumption that visual percepts of small spots of light produced with electrical stimulation of visual cortex (phosphenes) will combine into coherent percepts of visual forms, like pixels on a video screen. We tested an alternative strategy in which shapes were traced on the surface of visual cortex by stimulating electrodes in dynamic sequence. In both sighted and blind participants, dynamic stimulation enabled accurate recognition of letter shapes predicted by the brain's spatial map of the visual world. Forms were presented and recognized rapidly by blind participants, up to 86 forms per minute. These findings demonstrate that a brain prosthetic can produce coherent percepts of visual forms.},
  file = {Beauchamp et al. - 2020 - Dynamic Stimulation of Visual Cortex Produces Form.pdf},
  journal = {Cell},
  language = {en},
  number = {4}
}

@article{Bedau2000,
  title = {Open {{Problems}} in {{Artificial Life}}},
  author = {Bedau, Mark A. and McCaskill, John S. and Packard, Norman H. and Rasmussen, Steen and Adami, Chris and Green, David G. and Ikegami, Takashi and Kaneko, Kunihiko and Ray, Thomas S.},
  year = {2000},
  month = oct,
  volume = {6},
  pages = {363--376},
  issn = {1064-5462, 1530-9185},
  doi = {10.1162/106454600300103683},
  abstract = {This article lists fourteen open problems in artificial life, each of which is a grand challenge requiring a major advance on a fundamental issue for its solution. Each problem is briefly explained, and, where deemed helpful, some promising paths to its solution are indicated.},
  file = {Bedau et al. - 2000 - Open Problems in Artificial Life.pdf},
  journal = {Artificial Life},
  language = {en},
  number = {4}
}

@article{Beddington1975,
  title = {Dynamic Complexity in Predator-Prey Models Framed in Difference Equations},
  author = {Beddington, J. R. and Free, C. A. and Lawton, J. H.},
  year = {1975},
  month = may,
  volume = {255},
  pages = {58--60},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/255058a0},
  file = {Beddington et al. - 1975 - Dynamic complexity in predator-prey models framed .pdf},
  journal = {Nature},
  language = {en},
  number = {5503}
}

@article{Bednekoff1997,
  title = {Clark's Nutcracker Spatial Memory: Many Errors Might Not Be Due to Forgetting},
  shorttitle = {Clark's Nutcracker Spatial Memory},
  author = {Bednekoff, Peter A and Balda, Russell P},
  year = {1997},
  month = sep,
  volume = {54},
  pages = {691--698},
  issn = {00033472},
  doi = {10.1006/anbe.1997.0473},
  abstract = {Clark's nutcrackers, Nucifraga columbiana, rely upon cached seeds for both winter survival and breeding. Laboratory studies have confirmed that nutcrackers use spatial memory to recover their caches. In the laboratory, however, nutcrackers seem to perform less accurately than they do in nature. Two lines of evidence indicate that nutcrackers make `errors' in the laboratory that are not due to failures of memory. First, when digging in sand-filled cups, nutcrackers were 89\% accurate when they plunged their bills directly into the middle of cups but only 21\% accurate when they swept their bills across the cups. Second, nutcrackers were more accurate when the cost of probing was increased by covering sand-filled cups with either petri dishes or heavy glass bowls. Birds recovered caches in order of increasing costs. As costs increased, nutcrackers made somewhat fewer errors nearer to cache sites before recovering the caches and dramatically fewer errors further away from cache sites or near cache sites after recovering the caches. Some errors may be a form of environmental sampling. We conclude that the impressive achievements documented by previous studies are underestimates of the spatial memory abilities of Clark's nutcrackers.},
  file = {Bednekoff and Balda - 1997 - Clark's nutcracker spatial memory many errors mig.pdf},
  journal = {Animal Behaviour},
  language = {en},
  number = {3}
}

@article{Beer2000,
  title = {Dynamical Approaches to Cognitive Science},
  author = {Beer, Randall D.},
  year = {2000},
  month = mar,
  volume = {4},
  pages = {91--99},
  issn = {13646613},
  doi = {10.1016/S1364-6613(99)01440-0},
  file = {Beer - 2000 - Dynamical approaches to cognitive science.pdf},
  journal = {Trends in Cognitive Sciences},
  language = {en},
  number = {3}
}

@article{Beggs2015,
  title = {Editorial: {{Can There Be}} a {{Physics}} of the {{Brain}}?},
  shorttitle = {Editorial},
  author = {Beggs, John},
  year = {2015},
  month = jun,
  volume = {114},
  issn = {0031-9007, 1079-7114},
  doi = {10.1103/PhysRevLett.114.220001},
  file = {Beggs - 2015 - Editorial Can There Be a Physics of the Brain.pdf},
  journal = {Physical Review Letters},
  language = {en},
  number = {22}
}

@article{Behabadi2014,
  title = {Mechanisms Underlying Subunit Independence in Pyramidal Neuron Dendrites},
  author = {Behabadi, B. F. and Mel, B. W.},
  year = {2014},
  month = jan,
  volume = {111},
  pages = {498--503},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1217645111},
  file = {2013 - Behabadi, Mel - Mechanisms underlying subunit independence in pyramidal neuron dendrites.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {1}
}

@article{Behrens2007,
  title = {Learning the Value of Information in an Uncertain World},
  author = {Behrens, Timothy E J and Woolrich, Mark W and Walton, Mark E and Rushworth, Matthew F S},
  year = {2007},
  month = sep,
  volume = {10},
  pages = {1214--1221},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn1954},
  file = {Behrens et al. - 2007 - Learning the value of information in an uncertain .pdf},
  journal = {Nat Neurosci},
  language = {en},
  number = {9}
}

@article{Beierlein2003,
  title = {Two {{Dynamically Distinct Inhibitory Networks}} in {{Layer}} 4 of the {{Neocortex}}},
  author = {Beierlein, Michael and Gibson, Jay R. and Connors, Barry W.},
  year = {2003},
  month = nov,
  volume = {90},
  pages = {2987--3000},
  issn = {0022-3077, 1522-1598},
  doi = {10.1152/jn.00283.2003},
  file = {2003 - Beierlein - Two Dynamically Distinct Inhibitory Networks in Layer 4 of the Neocortex.pdf;2003 - Beierlein - Two Dynamically Distinct Inhibitory Networks in Layer 4 of the Neocortex(2).pdf},
  journal = {Journal of Neurophysiology},
  language = {en},
  number = {5}
}

@article{Belghazi,
  title = {Mutual {{Information Neural Estimation}}},
  author = {Belghazi, Mohamed Ishmael and Baratin, Aristide and Rajeswar, Sai and Ozair, Sherjil and Bengio, Yoshua and Courville, Aaron and Hjelm, R Devon},
  pages = {18},
  abstract = {We argue that the estimation of mutual information between high dimensional continuous random variables can be achieved by gradient descent over neural networks. We present a Mutual Information Neural Estimator (MINE) that is linearly scalable in dimensionality as well as in sample size, trainable through back-prop, and strongly consistent. We present a handful of applications on which MINE can be used to minimize or maximize mutual information. We apply MINE to improve adversarially trained generative models. We also use MINE to implement the Information Bottleneck, applying it to supervised classification; our results demonstrate substantial improvement in flexibility and performance in these settings.},
  file = {Belghazi et al. - Mutual Information Neural Estimation.pdf},
  language = {en}
}

@article{Bellemare2013,
  title = {The {{Arcade Learning Environment}}: {{An Evaluation Platform}} for {{General Agents}}},
  shorttitle = {The {{Arcade Learning Environment}}},
  author = {Bellemare, Marc G. and Naddaf, Yavar and Veness, Joel and Bowling, Michael},
  year = {2013},
  month = jun,
  volume = {47},
  pages = {253--279},
  issn = {1076-9757},
  doi = {10.1613/jair.3912},
  abstract = {In this article we introduce the Arcade Learning Environment (ALE): both a challenge problem and a platform and methodology for evaluating the development of general, domain-independent AI technology. ALE provides an interface to hundreds of Atari 2600 game environments, each one different, interesting, and designed to be a challenge for human players. ALE presents significant research challenges for reinforcement learning, model learning, model-based planning, imitation learning, transfer learning, and intrinsic motivation. Most importantly, it provides a rigorous testbed for evaluating and comparing approaches to these problems. We illustrate the promise of ALE by developing and benchmarking domain-independent agents designed using well-established AI techniques for both reinforcement learning and planning. In doing so, we also propose an evaluation methodology made possible by ALE, reporting empirical results on over 55 different games. All of the software, including the benchmark agents, is publicly available.},
  archiveprefix = {arXiv},
  eprint = {1207.4708},
  eprinttype = {arxiv},
  file = {2013 - Bellemare, Veness - The Arcade Learning Environment An Evaluation Platform for General Agents.pdf},
  journal = {Journal of Artificial Intelligence Research},
  keywords = {Computer Science - Artificial Intelligence},
  language = {en}
}

@article{Bellemare2016,
  title = {Unifying {{Count}}-{{Based Exploration}} and {{Intrinsic Motivation}}},
  author = {Bellemare, Marc G. and Srinivasan, Sriram and Ostrovski, Georg and Schaul, Tom and Saxton, David and Munos, Remi},
  year = {2016},
  month = jun,
  abstract = {We consider an agent's uncertainty about its environment and the problem of generalizing this uncertainty across states. Specifically, we focus on the problem of exploration in non-tabular reinforcement learning. Drawing inspiration from the intrinsic motivation literature, we use density models to measure uncertainty, and propose a novel algorithm for deriving a pseudo-count from an arbitrary density model. This technique enables us to generalize count-based exploration algorithms to the non-tabular case. We apply our ideas to Atari 2600 games, providing sensible pseudo-counts from raw pixels. We transform these pseudo-counts into exploration bonuses and obtain significantly improved exploration in a number of hard games, including the infamously difficult MONTEZUMA'S REVENGE.},
  archiveprefix = {arXiv},
  eprint = {1606.01868},
  eprinttype = {arxiv},
  file = {Bellemare et al. - 2016 - Unifying Count-Based Exploration and Intrinsic Mot.pdf},
  journal = {arXiv:1606.01868 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryclass = {cs, stat}
}

@article{Bellemare2017,
  title = {A {{Distributional Perspective}} on {{Reinforcement Learning}}},
  author = {Bellemare, Marc G. and Dabney, Will and Munos, R{\'e}mi},
  year = {2017},
  month = jul,
  abstract = {In this paper we argue for the fundamental importance of the value distribution: the distribution of the random return received by a reinforcement learning agent. This is in contrast to the common approach to reinforcement learning which models the expectation of this return, or value. Although there is an established body of literature studying the value distribution, thus far it has always been used for a specific purpose such as implementing risk-aware behaviour. We begin with theoretical results in both the policy evaluation and control settings, exposing a significant distributional instability in the latter. We then use the distributional perspective to design a new algorithm which applies Bellman's equation to the learning of approximate value distributions. We evaluate our algorithm using the suite of games from the Arcade Learning Environment. We obtain both state-of-the-art results and anecdotal evidence demonstrating the importance of the value distribution in approximate reinforcement learning. Finally, we combine theoretical and empirical evidence to highlight the ways in which the value distribution impacts learning in the approximate setting.},
  archiveprefix = {arXiv},
  eprint = {1707.06887},
  eprinttype = {arxiv},
  file = {Bellemare et al. - 2017 - A Distributional Perspective on Reinforcement Lear.pdf},
  journal = {arXiv:1707.06887 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryclass = {cs, stat}
}

@article{Bellmann1954,
  title = {The Theory of Dynamic Programming},
  author = {Bellmann, Richard},
  year = {1954},
  volume = {60},
  pages = {503--515},
  journal = {Bull. Amer. Math. Soc},
  number = {6}
}

@article{Bellot-Saez2017,
  title = {Astrocytic Modulation of Neuronal Excitability through {{K}} + Spatial Buffering},
  author = {{Bellot-Saez}, Alba and K{\'e}kesi, Orsolya and Morley, John W. and Buskila, Yossi},
  year = {2017},
  month = jun,
  volume = {77},
  pages = {87--97},
  issn = {01497634},
  doi = {10.1016/j.neubiorev.2017.03.002},
  abstract = {The human brain contains two major cell populations, neurons and glia. While neurons are electrically excitable and capable of discharging short voltage pulses known as action potentials, glial cells are not. However, astrocytes, the prevailing subtype of glia in the cortex, are highly connected and can modulate the excitability of neurons by changing the concentration of potassium ions in the extracellular environment, a process called K+ clearance.},
  file = {Bellot-Saez et al. - 2017 - Astrocytic modulation of neuronal excitability thr.pdf},
  journal = {Neuroscience \& Biobehavioral Reviews},
  language = {en}
}

@article{Bellot-Saez2017a,
  title = {Astrocytic Modulation of Neuronal Excitability through {{K}} + Spatial Buffering},
  author = {{Bellot-Saez}, Alba and K{\'e}kesi, Orsolya and Morley, John W. and Buskila, Yossi},
  year = {2017},
  month = jun,
  volume = {77},
  pages = {87--97},
  issn = {01497634},
  doi = {10.1016/j.neubiorev.2017.03.002},
  file = {Bellot-Saez et al. - 2017 - Astrocytic modulation of neuronal excitability thr 2.pdf},
  journal = {Neuroscience \& Biobehavioral Reviews},
  language = {en}
}

@article{Bellot-Saez2018,
  title = {Astrocytic Modulation of Cortical Oscillations},
  author = {{Bellot-Saez}, Alba and Cohen, Greg and {van Schaik}, Andr{\'e} and Ooi, Lezanne and W Morley, John and Buskila, Yossi},
  year = {2018},
  month = dec,
  volume = {8},
  pages = {11565},
  issn = {2045-2322},
  doi = {10.1038/s41598-018-30003-w},
  file = {Bellot-Saez et al. - 2018 - Astrocytic modulation of cortical oscillations.pdf},
  journal = {Sci Rep},
  language = {en},
  number = {1}
}

@article{Ben-Yakov2012,
  title = {Loss of Reliable Temporal Structure in Event-Related Averaging of Naturalistic Stimuli},
  author = {{Ben-Yakov}, Aya and Honey, Christopher J. and Lerner, Yulia and Hasson, Uri},
  year = {2012},
  month = oct,
  volume = {63},
  pages = {501--506},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2012.07.008},
  abstract = {To separate neural signals from noise, brain responses measured in neuroimaging are routinely averaged across space and time. However, such procedures may obscure some properties of neural activity. Recently, multi-voxel pattern analysis methods have demonstrated that patterns of activity across voxels contain valuable information that is concealed by spatial averaging. Here we show that temporal patterns of neural activity contain information that can discriminate different stimuli, even within brain regions that show no net activation to that stimulus class. Furthermore, we find that in many brain regions, responses to natural stimuli are highly context dependent. In such cases, prototypical event-related responses do not even exist for individual stimuli, so that averaging responses to the same stimulus within different contexts may worsen the effective signal-to-noise. As a result, analysis of the temporal structures of single events can reveal aspects of neural dynamics which cannot be detected using standard event-related averaging methods.},
  file = {2012 - Ben-Yakov et al. - Loss of reliable temporal structure in event-related averaging of naturalistic stimuli.pdf},
  journal = {NeuroImage},
  language = {en},
  number = {1}
}

@article{Benaim2009,
  title = {Learning in Games with Unstable Equilibria},
  author = {Bena{\"i}m, Michel and Hofbauer, Josef and Hopkins, Ed},
  year = {2009},
  month = jul,
  volume = {144},
  pages = {1694--1709},
  issn = {00220531},
  doi = {10.1016/j.jet.2008.09.003},
  abstract = {We propose a new concept for the analysis of games, the TASP, which gives a precise prediction about non-equilibrium play in games whose Nash equilibria are mixed and are unstable under fictitious play-like learning processes. We show that, when players learn using weighted stochastic fictitious play and so place greater weight on more recent experience, the time average of play often converges in these ``unstable'' games, even while mixed strategies and beliefs continue to cycle. This time average, the TASP, is related to the best response cycle first identified by Shapley (1964). Though conceptually distinct from Nash equilibrium, for many games the TASP is close enough to Nash to create the appearance of convergence to equilibrium. We discuss how these theoretical results may help to explain data from recent experimental studies of price dispersion.},
  file = {2009 - Benaïm, Hofbauer, Hopkins - Learning in games with unstable equilibria.pdf},
  journal = {Journal of Economic Theory},
  language = {en},
  number = {4}
}

@article{Bench2013,
  title = {On the {{Function}} of {{Boredom}}},
  author = {Bench, Shane and Lench, Heather},
  year = {2013},
  month = aug,
  volume = {3},
  pages = {459--472},
  issn = {2076-328X},
  doi = {10.3390/bs3030459},
  abstract = {Boredom is frequently considered inconsequential and has received relatively little research attention. We argue that boredom has important implications for human functioning, based on emotion theory and empirical evidence. Specifically, we argue that boredom motivates pursuit of new goals when the previous goal is no longer beneficial. Exploring alternate goals and experiences allows the attainment of goals that might be missed if people fail to reengage. Similar to other discrete emotions, we propose that boredom has specific and unique impacts on behavior, cognition, experience and physiology. Consistent with a broader argument that boredom encourages the behavioral pursuit of alternative goals, we argue that, while bored, attention to the current task is reduced, the experience of boredom is negative and aversive, and that boredom increases autonomic arousal to ready the pursuit of alternatives. By motivating desire for change from the current state, boredom increases opportunities to attain social, cognitive, emotional and experiential stimulation that could have been missed. We review the limited extant literature to support these claims, and call for more experimental boredom research.},
  file = {Bench and Lench - 2013 - On the Function of Boredom.pdf},
  journal = {Behavioral Sciences},
  language = {en},
  number = {3}
}

@article{Bench2013a,
  title = {On the {{Function}} of {{Boredom}}},
  author = {Bench, Shane and Lench, Heather},
  year = {2013},
  month = aug,
  volume = {3},
  pages = {459--472},
  issn = {2076-328X},
  doi = {10.3390/bs3030459},
  abstract = {Boredom is frequently considered inconsequential and has received relatively little research attention. We argue that boredom has important implications for human functioning, based on emotion theory and empirical evidence. Specifically, we argue that boredom motivates pursuit of new goals when the previous goal is no longer beneficial. Exploring alternate goals and experiences allows the attainment of goals that might be missed if people fail to reengage. Similar to other discrete emotions, we propose that boredom has specific and unique impacts on behavior, cognition, experience and physiology. Consistent with a broader argument that boredom encourages the behavioral pursuit of alternative goals, we argue that, while bored, attention to the current task is reduced, the experience of boredom is negative and aversive, and that boredom increases autonomic arousal to ready the pursuit of alternatives. By motivating desire for change from the current state, boredom increases opportunities to attain social, cognitive, emotional and experiential stimulation that could have been missed. We review the limited extant literature to support these claims, and call for more experimental boredom research.},
  file = {Bench and Lench - 2013 - On the Function of Boredom 2.pdf},
  journal = {Behavioral Sciences},
  language = {en},
  number = {3}
}

@article{Bendor2001,
  title = {{{ASPIRATION}}-{{BASED REINFORCEMENT LEARNING IN REPEATED INTERACTION GAMES}}: {{AN OVERVIEW}}},
  shorttitle = {{{ASPIRATION}}-{{BASED REINFORCEMENT LEARNING IN REPEATED INTERACTION GAMES}}},
  author = {Bendor, Jonathan and Mookherjee, Dilip and Ray, Debraj},
  year = {2001},
  month = jun,
  volume = {03},
  pages = {159--174},
  issn = {0219-1989, 1793-6675},
  doi = {10.1142/S0219198901000348},
  file = {2001 - BENDOR, MOOKHERJEE, RAY - Aspiration-Based Reinforcement Learning in Repeated Interaction Games an Overview.pdf},
  journal = {International Game Theory Review},
  language = {en},
  number = {02n03}
}

@article{Bengio2009,
  title = {Learning {{Deep Architectures}} for {{AI}}},
  author = {Bengio, Y.},
  year = {2009},
  volume = {2},
  pages = {1--127},
  issn = {1935-8237, 1935-8245},
  doi = {10.1561/2200000006},
  abstract = {Theoretical results suggest that in order to learn the kind of complicated functions that can represent highlevel abstractions (e.g. in vision, language, and other AI-level tasks), one may need deep architectures. Deep architectures are composed of multiple levels of non-linear operations, such as in neural nets with many hidden layers or in complicated propositional formulae re-using many sub-formulae. Searching the parameter space of deep architectures is a difficult task, but learning algorithms such as those for Deep Belief Networks have recently been proposed to tackle this problem with notable success, beating the state-of-the-art in certain areas. This paper discusses the motivations and principles regarding learning algorithms for deep architectures, in particular those exploiting as building blocks unsupervised learning of single-layer models such as Restricted Boltzmann Machines, used to construct deeper models such as Deep Belief Networks.},
  file = {2009 - Bengio - Learning Deep Architectures for AI.pdf},
  journal = {Foundations and Trends\textregistered{} in Machine Learning},
  language = {en},
  number = {1}
}

@article{Bengio2015,
  title = {{{STDP}} as Presynaptic Activity Times Rate of Change of Postsynaptic Activity},
  author = {Bengio, Yoshua and Mesnard, Thomas and Fischer, Asja and Zhang, Saizheng and Wu, Yuhuai},
  year = {2015},
  month = sep,
  abstract = {We introduce a weight update formula that is expressed only in terms of firing rates and their derivatives and that results in changes consistent with those associated with spike-timing dependent plasticity (STDP) rules and biological observations, even though the explicit timing of spikes is not needed. The new rule changes a synaptic weight in proportion to the product of the presynaptic firing rate and the temporal rate of change of activity on the postsynaptic side. These quantities are interesting for studying theoretical explanation for synaptic changes from a machine learning perspective. In particular, if neural dynamics moved neural activity towards reducing some objective function, then this STDP rule would correspond to stochastic gradient descent on that objective function.},
  archiveprefix = {arXiv},
  eprint = {1509.05936},
  eprinttype = {arxiv},
  file = {2015 - Bengio et al. - STDP as presynaptic activity times rate of change of postsynaptic activity.pdf;Bengio et al. - 2015 - STDP as presynaptic activity times rate of change .pdf},
  journal = {arXiv:1509.05936 [cs, q-bio]},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Quantitative Biology - Neurons and Cognition},
  language = {en},
  primaryclass = {cs, q-bio}
}

@article{Bengio2015a,
  title = {Towards {{Biologically Plausible Deep Learning}}},
  author = {Bengio, Yoshua and Lee, Dong-Hyun and Bornschein, Jorg and Mesnard, Thomas and Lin, Zhouhan},
  year = {2015},
  month = feb,
  abstract = {Neuroscientists have long criticised deep learning algorithms as incompatible with current knowledge of neurobiology. We explore more biologically plausible versions of deep representation learning, focusing here mostly on unsupervised learning but developing a learning mechanism that could account for supervised, unsupervised and reinforcement learning. The starting point is that the basic learning rule believed to govern synaptic weight updates (Spike-TimingDependent Plasticity) can be interpreted as gradient descent on some objective function so long as the neuronal dynamics push firing rates towards better values of the objective function (be it supervised, unsupervised, or reward-driven). The second main idea is that this corresponds to a form of the variational EM algorithm, i.e., with approximate rather than exact posteriors, implemented by neural dynamics. Another contribution of this paper is that the gradients required for updating the hidden states in the above variational interpretation can be estimated using an approximation that only requires propagating activations forward and backward, with pairs of layers learning to form a denoising auto-encoder. Finally, we extend the theory about the probabilistic interpretation of auto-encoders to justify improved sampling schemes based on the generative interpretation of denoising auto-encoders, and we validate all these ideas on generative learning tasks.},
  archiveprefix = {arXiv},
  eprint = {1502.04156},
  eprinttype = {arxiv},
  file = {2015 - Bengio et al. - Towards Biologically Plausible Deep Learning.pdf;Bengio et al. - 2015 - Towards Biologically Plausible Deep Learning.pdf},
  journal = {arXiv:1502.04156 [cs]},
  keywords = {Computer Science - Machine Learning},
  language = {en},
  primaryclass = {cs}
}

@article{Bengio2016,
  title = {Towards {{Biologically Plausible Deep Learning}}},
  author = {Bengio, Yoshua and Lee, Dong-Hyun and Bornschein, Jorg and Mesnard, Thomas and Lin, Zhouhan},
  year = {2016},
  month = aug,
  abstract = {Neuroscientists have long criticised deep learning algorithms as incompatible with current knowledge of neurobiology. We explore more biologically plausible versions of deep representation learning, focusing here mostly on unsupervised learning but developing a learning mechanism that could account for supervised, unsupervised and reinforcement learning. The starting point is that the basic learning rule believed to govern synaptic weight updates (Spike-TimingDependent Plasticity) arises out of a simple update rule that makes a lot of sense from a machine learning point of view and can be interpreted as gradient descent on some objective function so long as the neuronal dynamics push firing rates towards better values of the objective function (be it supervised, unsupervised, or rewarddriven). The second main idea is that this corresponds to a form of the variational EM algorithm, i.e., with approximate rather than exact posteriors, implemented by neural dynamics. Another contribution of this paper is that the gradients required for updating the hidden states in the above variational interpretation can be estimated using an approximation that only requires propagating activations forward and backward, with pairs of layers learning to form a denoising auto-encoder. Finally, we extend the theory about the probabilistic interpretation of auto-encoders to justify improved sampling schemes based on the generative interpretation of denoising auto-encoders, and we validate all these ideas on generative learning tasks.},
  archiveprefix = {arXiv},
  eprint = {1502.04156},
  eprinttype = {arxiv},
  file = {Bengio et al. - 2016 - Towards Biologically Plausible Deep Learning.pdf},
  journal = {arXiv:1502.04156 [cs]},
  keywords = {Computer Science - Machine Learning},
  language = {en},
  primaryclass = {cs}
}

@article{Benichou2011,
  title = {Intermittent Search Strategies},
  author = {B{\'e}nichou, O. and Loverdo, C. and Moreau, M. and Voituriez, R.},
  year = {2011},
  month = mar,
  volume = {83},
  pages = {81--129},
  issn = {0034-6861, 1539-0756},
  doi = {10.1103/RevModPhys.83.81},
  file = {Bénichou et al. - 2011 - Intermittent search strategies.pdf},
  journal = {Rev. Mod. Phys.},
  language = {en},
  number = {1}
}

@article{Bereby-Meyer2006,
  title = {The {{Speed}} of {{Learning}} in {{Noisy Games}}: {{Partial Reinforcement}} and the {{Sustainability}} of {{Cooperation}}},
  shorttitle = {The {{Speed}} of {{Learning}} in {{Noisy Games}}},
  author = {{Bereby-Meyer}, Yoella and Roth, Alvin E},
  year = {2006},
  month = aug,
  volume = {96},
  pages = {1029--1042},
  issn = {0002-8282},
  doi = {10.1257/aer.96.4.1029},
  abstract = {In an experiment, players' ability to learn to cooperate in the repeated prisoner's dilemma was substantially diminished when the payoffs were noisy, even though players could monitor one another's past actions perfectly. In contrast, in one-time play against a succession of opponents, noisy payoffs increased cooperation, by slowing the rate at which cooperation decays. These observations are consistent with the robust observation from the psychology literature that partial reinforcement (adding randomness to the link between an action and its consequences while holding expected payoffs constant) slows learning. This effect is magnified in the repeated game: When others are slow to learn to cooperate, the benefits of cooperation are reduced, which further hampers cooperation. These results show that a small change in the payoff environment, which changes the speed of individual learning, can have a large effect on collective behavior. And they show that there may be interesting comparative dynamics that can be derived from careful attention to the fact that at least some economic behavior is learned from experience.},
  file = {2006 - Bereby-Meyer, Roth - The speed of learning in noisy games Partial reinforcement and the sustainability of cooperation.pdf},
  journal = {American Economic Review},
  language = {en},
  number = {4}
}

@article{Berg2000,
  title = {Motile {{Behavior}} of {{Bacteria}}},
  author = {Berg, Howard C.},
  year = {2000},
  month = jan,
  volume = {53},
  pages = {24--29},
  issn = {0031-9228, 1945-0699},
  doi = {10.1063/1.882934},
  file = {Berg - 2000 - Motile Behavior of Bacteria.pdf},
  journal = {Physics Today},
  language = {en},
  number = {1}
}

@article{Berg2006,
  title = {Exploratory {{Whisking}} by {{Rat Is Not Phase Locked}} to the {{Hippocampal Theta Rhythm}}},
  author = {Berg, R. W. and Whitmer, D. and Kleinfeld, D.},
  year = {2006},
  month = jun,
  volume = {26},
  pages = {6518--6522},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.0190-06.2006},
  file = {Berg et al. - 2006 - Exploratory Whisking by Rat Is Not Phase Locked to.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {24}
}

@article{Berger-Tal2010,
  title = {Complex State-Dependent Games between Owls and Gerbils},
  author = {{Berger-Tal}, Oded and Mukherjee, Shomen and Kotler, Burt P. and Brown, Joel S.},
  year = {2010},
  month = mar,
  volume = {13},
  pages = {302--310},
  issn = {1461023X, 14610248},
  doi = {10.1111/j.1461-0248.2010.01447.x},
  abstract = {Predator\textendash prey interactions are often behaviourally sophisticated games in which the predator and prey are players. Past studies teach us that hungrier prey take higher risks when foraging and that hungrier predators increase their foraging activity and are willing to take higher risks of injury. Yet no study has looked at the simultaneous responses of predator and prey to their own and each other\~Os hunger levels in a controlled environment. We looked for evidence of a state-dependent game between predators and their prey by simultaneously manipulating the hunger state of barn owls, and Allenby\~Os gerbils as prey. The owls significantly increased their activity when hungry. However, they did not appear to respond to changes in the hunger state of the gerbils. The gerbils reacted strongly to the owls\~O state, as well as to their own state when the risk was perceived as high. Our study shows that predator\textendash prey interactions give rise to a complex state-dependent game.},
  file = {Berger-Tal et al. - 2010 - Complex state-dependent games between owls and ger.pdf},
  journal = {Ecology Letters},
  language = {en},
  number = {3}
}

@article{Berger-Tal2014,
  title = {The {{Exploration}}-{{Exploitation Dilemma}}: {{A Multidisciplinary Framework}}},
  shorttitle = {The {{Exploration}}-{{Exploitation Dilemma}}},
  author = {{Berger-Tal}, Oded and Nathan, Jonathan and Meron, Ehud and Saltz, David},
  editor = {Daunizeau, Jean},
  year = {2014},
  month = apr,
  volume = {9},
  pages = {e95693},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0095693},
  abstract = {The trade-off between the need to obtain new knowledge and the need to use that knowledge to improve performance is one of the most basic trade-offs in nature, and optimal performance usually requires some balance between exploratory and exploitative behaviors. Researchers in many disciplines have been searching for the optimal solution to this dilemma. Here we present a novel model in which the exploration strategy itself is dynamic and varies with time in order to optimize a definite goal, such as the acquisition of energy, money, or prestige. Our model produced four very distinct phases: Knowledge establishment, Knowledge accumulation, Knowledge maintenance, and Knowledge exploitation, giving rise to a multidisciplinary framework that applies equally to humans, animals, and organizations. The framework can be used to explain a multitude of phenomena in various disciplines, such as the movement of animals in novel landscapes, the most efficient resource allocation for a start-up company, or the effects of old age on knowledge acquisition in humans.},
  file = {Berger-Tal et al. - 2014 - The Exploration-Exploitation Dilemma A Multidisci.PDF},
  journal = {PLoS ONE},
  language = {en},
  number = {4}
}

@article{Berger-Tal2014a,
  title = {The {{Exploration}}-{{Exploitation Dilemma}}: {{A Multidisciplinary Framework}}},
  shorttitle = {The {{Exploration}}-{{Exploitation Dilemma}}},
  author = {{Berger-Tal}, Oded and Nathan, Jonathan and Meron, Ehud and Saltz, David},
  editor = {Daunizeau, Jean},
  year = {2014},
  month = apr,
  volume = {9},
  pages = {e95693},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0095693},
  abstract = {The trade-off between the need to obtain new knowledge and the need to use that knowledge to improve performance is one of the most basic trade-offs in nature, and optimal performance usually requires some balance between exploratory and exploitative behaviors. Researchers in many disciplines have been searching for the optimal solution to this dilemma. Here we present a novel model in which the exploration strategy itself is dynamic and varies with time in order to optimize a definite goal, such as the acquisition of energy, money, or prestige. Our model produced four very distinct phases: Knowledge establishment, Knowledge accumulation, Knowledge maintenance, and Knowledge exploitation, giving rise to a multidisciplinary framework that applies equally to humans, animals, and organizations. The framework can be used to explain a multitude of phenomena in various disciplines, such as the movement of animals in novel landscapes, the most efficient resource allocation for a start-up company, or the effects of old age on knowledge acquisition in humans.},
  file = {Berger-Tal et al. - 2014 - The Exploration-Exploitation Dilemma A Multidisci 2.PDF},
  journal = {PLoS ONE},
  language = {en},
  number = {4}
}

@article{Bergstra2012,
  title = {Random {{Search}} for {{Hyper}}-{{Parameter Optimization}}},
  author = {Bergstra, James and Bengio, Yoshua},
  year = {2012},
  volume = {13},
  pages = {281--305},
  abstract = {Grid search and manual search are the most widely used strategies for hyper-parameter optimization. This paper shows empirically and theoretically that randomly chosen trials are more efficient for hyper-parameter optimization than trials on a grid. Empirical evidence comes from a comparison with a large previous study that used grid search and manual search to configure neural networks and deep belief networks. Compared with neural networks configured by a pure grid search, we find that random search over the same domain is able to find models that are as good or better within a small fraction of the computation time. Granting random search the same computational budget, random search finds better models by effectively searching a larger, less promising configuration space. Compared with deep belief networks configured by a thoughtful combination of manual search and grid search, purely random search over the same 32-dimensional configuration space found statistically equal performance on four of seven data sets, and superior performance on one of seven. A Gaussian process analysis of the function from hyper-parameters to validation set performance reveals that for most data sets only a few of the hyper-parameters really matter, but that different hyper-parameters are important on different data sets. This phenomenon makes grid search a poor choice for configuring algorithms for new data sets. Our analysis casts some light on why recent ``High Throughput'' methods achieve surprising success\textemdash they appear to search through a large number of hyper-parameters because most hyper-parameters do not matter much. We anticipate that growing interest in large hierarchical models will place an increasing burden on techniques for hyper-parameter optimization; this work shows that random search is a natural baseline against which to judge progress in the development of adaptive (sequential) hyper-parameter optimization algorithms.},
  file = {Bergstra and Bengio - Random Search for Hyper-Parameter Optimization.pdf},
  journal = {Journal of Machine Learning Research},
  language = {en}
}

@article{Berlyne1950,
  title = {Novelty and Curiosity as Determinants of Exploratory Behaviour},
  author = {Berlyne, DE},
  year = {1950},
  volume = {41},
  pages = {68--80},
  file = {Novelty_and_curiosity_as_deter.pdf},
  journal = {British Journal of Psychology},
  number = {1}
}

@article{Berlyne1954,
  title = {A Theory of Human Curiosity},
  author = {Berlyne, D. E.},
  year = {1954},
  volume = {45},
  pages = {180--191},
  file = {Berlyne - 1954 - A theory of human curiosity.pdf},
  journal = {British Journal of Psychology. General Section},
  number = {3}
}

@book{Berlyne1960,
  title = {Conflict, Arousal, and Curiosity.},
  author = {Berlyne, D. E.},
  year = {1960},
  pages = {xii, 350},
  publisher = {{McGraw-Hill Book Company}},
  address = {{New York,  NY,  US}},
  doi = {10.1037/11164-000},
  abstract = {The topics that are to be treated in this book were unduly neglected by psychology for many years but are now beginning to come to the fore. My own researches into attention and exploratory behavior began in 1947, and at about the same time several other psychologists became independently impressed with the importance of these matters and started to study them experimentally. It is interesting that those were also the years when information theory was making its appearance and when the reticular formation of the brain stem was first attracting the notice of neurophysiologists. During the last ten years, the tempo of research into exploratory behavior and related phenomena has been steadily quickening. The book is prompted by the feeling that it is now time to pause and take stock: to review relevant data contributed by several different specialties, to consider what conclusions, whether firm or tentative, are justified at the present juncture, and to clarify what remains to be done. The primary aim of the book is, in fact, to raise problems. The book is intended as a contribution to behavior theory, i.e., to psychology conceived as a branch of science with the circumscribed objective of explaining and predicting behavior. But interest in attention and exploratory behavior and in other topics indissociably bound up with them, such as art, humor and thinking, has by no means been confined to professional psychologists. The book has two features that would have surprised me when I first set out to plan it. One is that it ends up sketching a highly modified form of drive-reduction theory. Drive-reduction theory has appeared more and more to be full of shortcomings, even for the phenomena that it was originally designed to handle. The second surprising feature is the prominence of neurophysiology. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {*Attention,*Curiosity,*Exploratory Behavior,*Physiological Arousal,*Stimulus Parameters,Motivation},
  series = {Conflict, Arousal, and Curiosity.}
}

@book{Berry1985,
  title = {Bandit Problems},
  author = {Berry, Donald A. and Fristedt, Bert},
  year = {1985},
  publisher = {{Springer Netherlands}},
  address = {{Dordrecht}},
  doi = {10.1007/978-94-015-3711-7},
  file = {Berry and Fristedt - 1985 - Bandit problems.pdf},
  isbn = {978-94-015-3713-1 978-94-015-3711-7},
  language = {en}
}

@book{Berry1985a,
  title = {Bandit Problems},
  author = {Berry, Donald A. and Fristedt, Bert},
  year = {1985},
  publisher = {{Springer Netherlands}},
  address = {{Dordrecht}},
  doi = {10.1007/978-94-015-3711-7},
  file = {Berry and Fristedt - 1985 - Bandit problems 2.pdf},
  isbn = {978-94-015-3713-1 978-94-015-3711-7},
  language = {en}
}

@article{Berseth2020,
  title = {{{SMiRL}}: {{Surprise Minimizing RL}} in {{Dynamic Environments}}},
  shorttitle = {{{SMiRL}}},
  author = {Berseth, Glen and Geng, Daniel and Devin, Coline and Rhinehart, Nicholas and Finn, Chelsea and Jayaraman, Dinesh and Levine, Sergey},
  year = {2020},
  month = feb,
  abstract = {All living organisms struggle against the forces of nature to carve out niches where they can maintain homeostasis. We propose that such a search for order amidst chaos might offer a unifying principle for the emergence of useful behaviors in artificial agents. We formalize this idea into an unsupervised reinforcement learning method called surprise minimizing RL (SMiRL). SMiRL trains an agent with the objective of maximizing the probability of observed states under a model trained on previously seen states. The resulting agents can acquire proactive behaviors that seek out and maintain stable conditions, such as balancing and damage avoidance, that are closely tied to an environment's prevailing sources of entropy, such as wind, earthquakes, and other agents. We demonstrate that our surprise minimizing agents can successfully play Tetris, Doom, control a humanoid to avoid falls and navigate to escape enemy agents, without any task-specific reward supervision. We further show that SMiRL can be used together with a standard task reward to accelerate reward-driven learning.},
  archiveprefix = {arXiv},
  eprint = {1912.05510},
  eprinttype = {arxiv},
  file = {Berseth et al. - 2020 - SMiRL Surprise Minimizing RL in Dynamic Environme.pdf},
  journal = {arXiv:1912.05510 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,G.3,Statistics - Machine Learning},
  language = {en},
  primaryclass = {cs, stat}
}

@article{Berseth2020a,
  title = {{{SMiRL}}: {{Surprise Minimizing RL}} in {{Dynamic Environments}}},
  shorttitle = {{{SMiRL}}},
  author = {Berseth, Glen and Geng, Daniel and Devin, Coline and Rhinehart, Nicholas and Finn, Chelsea and Jayaraman, Dinesh and Levine, Sergey},
  year = {2020},
  month = feb,
  abstract = {All living organisms struggle against the forces of nature to carve out a maintainable niche. We propose that such a search for order amidst chaos might offer a unifying principle for the emergence of useful behaviors in artificial agents. We formalize this idea into an unsupervised reinforcement learning method called Surprise Minimizing RL (SMiRL). SMiRL alternates between learning a density model to evaluate the surprise of a stimulus, and improving the policy to seek more predictable stimuli. This process maximizes a lower-bound on the negative entropy of the states, which can be seen as maximizing the agent's ability to maintain order in the environment. The policy seeks out stable and repeatable situations that counteract the environment's prevailing sources of entropy. This might include avoiding other hostile agents, or finding a stable, balanced pose for a bipedal robot in the face of disturbance forces. We demonstrate that our surprise minimizing agents can successfully play Tetris, Doom, control a humanoid to avoid falls, and navigate to escape enemies in a maze without any task-specific reward supervision. We further show that SMiRL can be used together with a standard task rewards to accelerate reward-driven learning.},
  archiveprefix = {arXiv},
  eprint = {1912.05510},
  eprinttype = {arxiv},
  file = {Berseth et al. - 2020 - SMiRL Surprise Minimizing RL in Dynamic Environme 2.pdf},
  journal = {arXiv:1912.05510 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,G.3,Statistics - Machine Learning},
  language = {en},
  primaryclass = {cs, stat}
}

@article{Bertens2019,
  title = {Network of {{Evolvable Neural Units}}: {{Evolving}} to {{Learn}} at a {{Synaptic Level}}},
  shorttitle = {Network of {{Evolvable Neural Units}}},
  author = {Bertens, Paul and Lee, Seong-Whan},
  year = {2019},
  month = dec,
  abstract = {Although Deep Neural Networks have seen great success in recent years through various changes in overall architectures and optimization strategies, their fundamental underlying design remains largely unchanged. Computational neuroscience on the other hand provides more biologically realistic models of neural processing mechanisms, but they are still high level abstractions of the actual experimentally observed behaviour. Here a model is proposed that bridges Neuroscience, Machine Learning and Evolutionary Algorithms to evolve individual soma and synaptic compartment models of neurons in a scalable manner. Instead of attempting to manually derive models for all the observed complexity and diversity in neural processing, we propose an Evolvable Neural Unit (ENU) that can approximate the function of each individual neuron and synapse. We demonstrate that this type of unit can be evolved to mimic Integrate-And-Fire neurons and synaptic Spike-Timing-Dependent Plasticity. Additionally, by constructing a new type of neural network where each synapse and neuron is such an evolvable neural unit, we show it is possible to evolve an agent capable of learning to solve a T-maze environment task. This network independently discovers spiking dynamics and reinforcement type learning rules, opening up a new path towards biologically inspired artificial intelligence.},
  archiveprefix = {arXiv},
  eprint = {1912.07589},
  eprinttype = {arxiv},
  file = {Bertens and Lee - 2019 - Network of Evolvable Neural Units Evolving to Lea.pdf},
  journal = {arXiv:1912.07589 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  language = {en},
  primaryclass = {cs, stat}
}

@article{Bertschinger2004,
  title = {Real-{{Time Computation}} at the {{Edge}} of {{Chaos}} in {{Recurrent Neural Networks}}},
  author = {Bertschinger, Nils and Natschl{\"a}ger, Thomas},
  year = {2004},
  month = jul,
  volume = {16},
  pages = {1413--1436},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/089976604323057443},
  file = {2004 - Bertschinger, Natschläger - Real-time computation at the edge of chaos in recurrent neural networks.pdf},
  journal = {Neural Computation},
  language = {en},
  number = {7}
}

@article{Beurrier1999,
  title = {Subthalamic {{Nucleus Neurons Switch}} from {{Single}}-{{Spike Activity}} to {{Burst}}-{{Firing Mode}}},
  author = {Beurrier, Corinne and Congar, Patrice and Bioulac, Bernard and Hammond, Constance},
  year = {1999},
  month = jan,
  volume = {19},
  pages = {599--609},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.19-02-00599.1999},
  file = {1999 - Beurrier et al. - Subthalamic nucleus neurons switch from single-spike activity to burst-firing mode.pdf},
  journal = {The Journal of Neuroscience},
  language = {en},
  number = {2}
}

@article{Beyeler2017,
  title = {Sparse Coding and Dimensionality Reduction in Cortex},
  author = {Beyeler, Michael and Rounds, Emily and Carlson, Kristofor and Dutt, Nikil and Krichmar, Jeffrey L.},
  year = {2017},
  month = jun,
  doi = {10.1101/149880},
  abstract = {Supported by recent computational studies, sparse coding and dimensionality reduction are emerging as a ubiquitous coding strategy across brain regions and modalities, allowing neurons to achieve nonnegative sparse coding (NSC) by efficiently encoding highdimensional stimulus spaces using a sparse and parts-based population code. Reducing the dimensionality of complex, multimodal sensory streams is critically important for metabolically constrained brain areas to represent the world. In this article, we provide an overview of NSC, summarize evidence for its role in neural computation in disparate regions of the brain, ranging from visual processing to spatial navigation, and speculate that specific forms of synaptic plasticity and homeostatic modulation may underlie its implementation. We suggest that NSC may be an organizing principle in the nervous system.},
  file = {Beyeler et al. - 2017 - Sparse coding and dimensionality reduction in cort.pdf},
  journal = {bioRxiv},
  language = {en}
}

@article{Bhattacharya2013,
  title = {Implementing the Cellular Mechanisms of Synaptic Transmission in a Neural Mass Model of the Thalamo-Cortical Circuitry},
  author = {Bhattacharya, Basabdatta S.},
  year = {2013},
  volume = {7},
  issn = {1662-5188},
  doi = {10.3389/fncom.2013.00081},
  abstract = {A novel direction to existing neural mass modeling technique is proposed where the commonly used ``alpha function'' for representing synaptic transmission is replaced by a kinetic framework of neurotransmitter and receptor dynamics. The aim is to underpin neuro-transmission dynamics associated with abnormal brain rhythms commonly observed in neurological and psychiatric disorders. An existing thalamocortical neural mass model is modified by using the kinetic framework for modeling synaptic transmission mediated by glutamatergic and GABA (gamma-aminobutyric-acid)-ergic receptors. The model output is compared qualitatively with existing literature on in vitro experimental studies of ferret thalamic slices, as well as on single-neuron-level model based studies of neuro-receptor and transmitter dynamics in the thalamocortical tissue. The results are consistent with these studies: the activation of ligand-gated GABA receptors is essential for generation of spindle waves in the model, while blocking this pathway leads to low-frequency synchronized oscillations such as observed in slow-wave sleep; the frequency of spindle oscillations increase with increased levels of post-synaptic membrane conductance for AMPA (alpha-amino-3-hydroxy-5-methyl-4-isoxazolepropionic-acid) receptors, and blocking this pathway effects a quiescent model output. In terms of computational efficiency, the simulation time is improved by a factor of 10 compared to a similar neural mass model based on alpha functions. This implies a dramatic improvement in computational resources for large-scale network simulation using this model. Thus, the model provides a platform for correlating high-level brain oscillatory activity with low-level synaptic attributes, and makes a significant contribution toward advancements in current neural mass modeling paradigm as a potential computational tool to better the understanding of brain oscillations in sickness and in health.},
  file = {2013 - Bhattacharya - Implementing the cellular mechanisms of synaptic transmission in a neural mass model of the thalamo-cortical circu.pdf},
  journal = {Frontiers in Computational Neuroscience},
  language = {en}
}

@article{Bi2018,
  title = {Stimulus Sensing and Signal Processing in Bacterial Chemotaxis},
  author = {Bi, Shuangyu and Sourjik, Victor},
  year = {2018},
  month = oct,
  volume = {45},
  pages = {22--29},
  issn = {13695274},
  doi = {10.1016/j.mib.2018.02.002},
  file = {Bi and Sourjik - 2018 - Stimulus sensing and signal processing in bacteria.pdf},
  journal = {Current Opinion in Microbiology},
  language = {en}
}

@article{Bi2018a,
  title = {Stimulus Sensing and Signal Processing in Bacterial Chemotaxis},
  author = {Bi, Shuangyu and Sourjik, Victor},
  year = {2018},
  month = oct,
  volume = {45},
  pages = {22--29},
  issn = {13695274},
  doi = {10.1016/j.mib.2018.02.002},
  file = {Bi and Sourjik - 2018 - Stimulus sensing and signal processing in bacteria 2.pdf},
  journal = {Current Opinion in Microbiology},
  language = {en}
}

@article{Binzegger2004,
  title = {A {{Quantitative Map}} of the {{Circuit}} of {{Cat Primary Visual Cortex}}},
  author = {Binzegger, T.},
  year = {2004},
  month = sep,
  volume = {24},
  pages = {8441--8453},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.1400-04.2004},
  file = {2004 - Binzegger - A Quantitative Map of the Circuit of Cat Primary Visual Cortex.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {39}
}

@article{Binzegger2009,
  title = {Topology and Dynamics of the Canonical Circuit of Cat {{V1}}},
  author = {Binzegger, T. and Douglas, R.J. and Martin, K.A.C.},
  year = {2009},
  month = oct,
  volume = {22},
  pages = {1071--1078},
  issn = {08936080},
  doi = {10.1016/j.neunet.2009.07.011},
  abstract = {The neocortex is a major component of the most sophisticated and economically significant computer in existence, nevertheless the organisation and operation of its computational circuit is not yet understood. Here we make some steps toward relating anatomical structure to computational function. We use methods of quantitative neuroanatomy to estimate the cortical circuit by defining the projection matrix between the various cells types of the neocortex of the cat, and then we consider the implications of this connectivity for cortical signal processing. Our analyses show that for a reasonable choice of the ratio between excitatory and inhibitory efficacy, the overall cortical circuit lies near the border of dynamical stability. We discuss a model of co-operative competitive processing that is consistent with the observed connectivity in the superficial layers of the cortex, and consider also how the topology of the overall cortical circuit could be configured dynamically through average inhibition.},
  file = {2009 - Binzegger, Douglas, Martin - Topology and dynamics of the canonical circuit of cat V1.pdf},
  journal = {Neural Networks},
  language = {en},
  number = {8}
}

@article{Bird2016,
  title = {Optimal {{Current Transfer}} in {{Dendrites}}},
  author = {Bird, Alex D. and Cuntz, Hermann},
  editor = {{van Rossum}, Mark C. W.},
  year = {2016},
  month = may,
  volume = {12},
  pages = {e1004897},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1004897},
  file = {Bird and Cuntz - 2016 - Optimal Current Transfer in Dendrites.pdf},
  journal = {PLOS Computational Biology},
  language = {en},
  number = {5}
}

@article{Bisio2014,
  title = {Emergence of {{Bursting Activity}} in {{Connected Neuronal Sub}}-{{Populations}}},
  author = {Bisio, Marta and Bosca, Alessandro and Pasquale, Valentina and Berdondini, Luca and Chiappalone, Michela},
  editor = {Vasilaki, Eleni},
  year = {2014},
  month = sep,
  volume = {9},
  pages = {e107400},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0107400},
  abstract = {Uniform and modular primary hippocampal cultures from embryonic rats were grown on commercially available microelectrode arrays to investigate network activity with respect to development and integration of different neuronal populations. Modular networks consisting of two confined active and inter-connected sub-populations of neurons were realized by means of bi-compartmental polydimethylsiloxane structures. Spontaneous activity in both uniform and modular cultures was periodically monitored, from three up to eight weeks after plating. Compared to uniform cultures and despite lower cellular density, modular networks interestingly showed higher firing rates at earlier developmental stages, and network-wide firing and bursting statistics were less variable over time. Although globally less correlated than uniform cultures, modular networks exhibited also higher intra-cluster than inter-cluster correlations, thus demonstrating that segregation and integration of activity coexisted in this simple yet powerful in vitro model. Finally, the peculiar synchronized bursting activity shown by confined modular networks preferentially propagated within one of the two compartments (`dominant'), even in cases of perfect balance of firing rate between the two sub-populations. This dominance was generally maintained during the entire monitored developmental frame, thus suggesting that the implementation of this hierarchy arose from early network development.},
  file = {Bisio et al. - 2014 - Emergence of Bursting Activity in Connected Neuron.pdf},
  journal = {PLoS ONE},
  language = {en},
  number = {9}
}

@article{Bjursten1976,
  title = {Behavioural Repertory of Cats without Cerebral Cortex from Infancy},
  author = {Bjursten, L.-M. and Norrsell, K. and Norrsell, U.},
  year = {1976},
  month = may,
  volume = {25},
  issn = {0014-4819, 1432-1106},
  doi = {10.1007/BF00234897},
  abstract = {Bilateral removal of the cerebral cortex was made in cats neonatally. Spontaneous and imposed behaviour was studied while they were growing up and after they had become adult. Special emphasis was put on the utilization of visual cues and on learning. The cats ate, drank and groomed themselves adequately. Adequate maternal and female sexual behaviour was observed. They utilized the visual and haptic senses with respect to external space. Two cats were trained to perform visual discrimination in a T-maze. The adequacy of the behaviour of these cats is compared to that of animals with similar lesions made at maturity.},
  file = {Bjursten et al. - 1976 - Behavioural repertory of cats without cerebral cor.pdf},
  journal = {Experimental Brain Research},
  language = {en},
  number = {2}
}

@article{Blelloch,
  title = {Introduction to {{Data Compression}}},
  author = {Blelloch, Guy E},
  pages = {55},
  file = {Blelloch - Introduction to Data Compression.pdf},
  language = {en}
}

@article{Blevins2019,
  title = {On the Reorderability of Node-Filtered Order Complexes},
  author = {Blevins, Ann Sizemore and Bassett, Danielle S.},
  year = {2019},
  month = may,
  abstract = {Growing graphs describe a multitude of developing processes from maturing brains to expanding vocabularies to burgeoning public transit systems. Each of these growing processes likely adheres to proliferation rules that establish an effective order of node and connection emergence. When followed, such proliferation rules allow the system to properly develop along a predetermined trajectory. But rules are rarely followed. Here we ask what topological changes in the growing graph trajectories might occur after the specific but basic perturbation of permuting the node emergence order. Specifically we harness applied topological methods to determine which of six growing graph models exhibit topology that is robust to randomizing node order, termed global reorderability, and robust to temporally-local node swaps, termed local reorderability. We find that the six graph models fall upon a spectrum of both local and global reorderability, and furthermore we provide theoretical connections between robustness to node pair ordering and robustness to arbitrary node orderings. Finally we discuss real-world applications of reorderability analyses and suggest possibilities for designing reorderable networks.},
  archiveprefix = {arXiv},
  eprint = {1905.02330},
  eprinttype = {arxiv},
  file = {Blevins and Bassett - 2019 - On the reorderability of node-filtered order compl.pdf},
  journal = {arXiv:1905.02330 [math, q-bio]},
  keywords = {55U10,Mathematics - Algebraic Topology,Quantitative Biology - Quantitative Methods},
  language = {en},
  primaryclass = {math, q-bio}
}

@article{Blum,
  title = {Applying {{Computational Complexity Theory}} and {{Cryptography}} to the {{Pursuit}} of {{Concept Understanding}}:},
  author = {Blum, Manuel},
  pages = {11},
  file = {Blum - Applying Computational Complexity Theory and Crypt.pdf},
  language = {en}
}

@article{Bobadilla-Suarez2020,
  title = {Subjective Value and Decision Entropy Are Jointly Encoded by Aligned Gradients across the Human Brain},
  author = {{Bobadilla-Suarez}, Sebastian and Guest, Olivia},
  year = {2020},
  pages = {13},
  file = {Bobadilla-Suarez and Guest - 2020 - Subjective value and decision entropy are jointly .pdf},
  language = {en}
}

@article{Bogacz2007,
  title = {The {{Basal Ganglia}} and {{Cortex Implement Optimal Decision Making Between Alternative Actions}}},
  author = {Bogacz, Rafal and Gurney, Kevin},
  year = {2007},
  month = feb,
  volume = {19},
  pages = {442--477},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/neco.2007.19.2.442},
  file = {2007 - Bogacz, Gurney - The Basal Ganglia and Cortex Implement Optimal Decision Making Between Alternative Actions.pdf},
  journal = {Neural Computation},
  language = {en},
  number = {2}
}

@article{Bolcskei2019,
  title = {Optimal {{Approximation}} with {{Sparsely Connected Deep Neural Networks}}},
  author = {B{\"o}lcskei, Helmut and Grohs, Philipp and Kutyniok, Gitta and Petersen, Philipp},
  year = {2019},
  month = jan,
  volume = {1},
  pages = {8--45},
  issn = {2577-0187},
  doi = {10.1137/18M118709X},
  abstract = {We derive fundamental lower bounds on the connectivity and the memory requirements of deep neural networks guaranteeing uniform approximation rates for arbitrary function classes in L2(Rd). In other words, we establish a connection between the complexity of a function class and the complexity of deep neural networks approximating functions from this class to within a prescribed accuracy. Additionally, we prove that our lower bounds are achievable for a broad family of function classes. Specifically, all function classes that are optimally approximated by a general class of representation systems\textemdash so-called affine systems\textemdash can be approximated by deep neural networks with minimal connectivity and memory requirements. Affine systems encompass a wealth of representation systems from applied harmonic analysis such as wavelets, ridgelets, curvelets, shearlets, {$\alpha$}-shearlets, and, more generally, {$\alpha$}-molecules. Our central result elucidates a remarkable universality property of neural networks and shows that they achieve the optimum approximation properties of all affine systems combined. As a specific example, we consider the class of {$\alpha-$}1-cartoon-like functions, which is approximated optimally by {$\alpha$}-shearlets. We also explain how our results can be extended to the approximation of functions on low-dimensional immersed manifolds. Finally, we present numerical experiments demonstrating that the standard stochastic gradient descent algorithm yields deep neural networks with close-to-optimal approximation rates. Moreover, these results indicate that stochastic gradient descent can learn approximations that are sparse in the representation systems optimally sparsifying the function class the network is trained on.},
  file = {Bölcskei et al. - 2019 - Optimal Approximation with Sparsely Connected Deep.pdf},
  journal = {SIAM Journal on Mathematics of Data Science},
  language = {en},
  number = {1}
}

@article{Bolcskei2019a,
  title = {Optimal {{Approximation}} with {{Sparsely Connected Deep Neural Networks}}},
  author = {B{\"o}lcskei, Helmut and Grohs, Philipp and Kutyniok, Gitta and Petersen, Philipp},
  year = {2019},
  month = jan,
  volume = {1},
  pages = {8--45},
  issn = {2577-0187},
  doi = {10.1137/18M118709X},
  abstract = {We derive fundamental lower bounds on the connectivity and the memory requirements of deep neural networks guaranteeing uniform approximation rates for arbitrary function classes in L2(Rd). In other words, we establish a connection between the complexity of a function class and the complexity of deep neural networks approximating functions from this class to within a prescribed accuracy. Additionally, we prove that our lower bounds are achievable for a broad family of function classes. Specifically, all function classes that are optimally approximated by a general class of representation systems\textemdash so-called affine systems\textemdash can be approximated by deep neural networks with minimal connectivity and memory requirements. Affine systems encompass a wealth of representation systems from applied harmonic analysis such as wavelets, ridgelets, curvelets, shearlets, {$\alpha$}-shearlets, and, more generally, {$\alpha$}-molecules. Our central result elucidates a remarkable universality property of neural networks and shows that they achieve the optimum approximation properties of all affine systems combined. As a specific example, we consider the class of {$\alpha-$}1-cartoon-like functions, which is approximated optimally by {$\alpha$}-shearlets. We also explain how our results can be extended to the approximation of functions on low-dimensional immersed manifolds. Finally, we present numerical experiments demonstrating that the standard stochastic gradient descent algorithm yields deep neural networks with close-to-optimal approximation rates. Moreover, these results indicate that stochastic gradient descent can learn approximations that are sparse in the representation systems optimally sparsifying the function class the network is trained on.},
  file = {Bölcskei et al. - 2019 - Optimal Approximation with Sparsely Connected Deep 2.pdf},
  journal = {SIAM Journal on Mathematics of Data Science},
  language = {en},
  number = {1}
}

@article{Bolcskei2019b,
  title = {Optimal {{Approximation}} with {{Sparsely Connected Deep Neural Networks}}},
  author = {B{\"o}lcskei, Helmut and Grohs, Philipp and Kutyniok, Gitta and Petersen, Philipp},
  year = {2019},
  month = jan,
  volume = {1},
  pages = {8--45},
  issn = {2577-0187},
  doi = {10.1137/18M118709X},
  abstract = {We derive fundamental lower bounds on the connectivity and the memory requirements of deep neural networks guaranteeing uniform approximation rates for arbitrary function classes in L2(Rd). In other words, we establish a connection between the complexity of a function class and the complexity of deep neural networks approximating functions from this class to within a prescribed accuracy.},
  file = {Bölcskei et al. - 2019 - Optimal Approximation with Sparsely Connected Deep 3.pdf},
  journal = {SIAM Journal on Mathematics of Data Science},
  language = {en},
  number = {1}
}

@article{Bollimunta2008,
  title = {Neuronal {{Mechanisms}} of {{Cortical Alpha Oscillations}} in {{Awake}}-{{Behaving Macaques}}},
  author = {Bollimunta, A. and Chen, Y. and Schroeder, C. E. and Ding, M.},
  year = {2008},
  month = oct,
  volume = {28},
  pages = {9976--9988},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.2699-08.2008},
  file = {2008 - Bollimunta et al. - Neuronal mechanisms of cortical alpha oscillations in awake-behaving macaques.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {40}
}

@article{Bonawitz2014,
  title = {Win-{{Stay}}, {{Lose}}-{{Sample}}: {{A}} Simple Sequential Algorithm for Approximating {{Bayesian}} Inference},
  shorttitle = {Win-{{Stay}}, {{Lose}}-{{Sample}}},
  author = {Bonawitz, Elizabeth and Denison, Stephanie and Gopnik, Alison and Griffiths, Thomas L.},
  year = {2014},
  month = nov,
  volume = {74},
  pages = {35--65},
  issn = {00100285},
  doi = {10.1016/j.cogpsych.2014.06.003},
  abstract = {People can behave in a way that is consistent with Bayesian models of cognition, despite the fact that performing exact Bayesian inference is computationally challenging. What algorithms could people be using to make this possible? We show that a simple sequential algorithm ``Win-Stay, Lose-Sample'', inspired by the Win-Stay, Lose-Shift (WSLS) principle, can be used to approximate Bayesian inference. We investigate the behavior of adults and preschoolers on two causal learning tasks to test whether people might use a similar algorithm. These studies use a ``mini-microgenetic method'', investigating how people sequentially update their beliefs as they encounter new evidence. Experiment 1 investigates a deterministic causal learning scenario and Experiments 2 and 3 examine how people make inferences in a stochastic scenario. The behavior of adults and preschoolers in these experiments is consistent with our Bayesian version of the WSLS principle. This algorithm provides both a practical method for performing Bayesian inference and a new way to understand people's judgments.},
  file = {Bonawitz et al. - 2014 - Win-Stay, Lose-Sample A simple sequential algorit.pdf},
  journal = {Cognitive Psychology},
  language = {en}
}

@article{Bonnefond2012,
  title = {Alpha {{Oscillations Serve}} to {{Protect Working Memory Maintenance}} against {{Anticipated Distracters}}},
  author = {Bonnefond, Mathilde and Jensen, Ole},
  year = {2012},
  month = oct,
  volume = {22},
  pages = {1969--1974},
  issn = {09609822},
  doi = {10.1016/j.cub.2012.08.029},
  abstract = {When operating in a complex world, it is essential to have mechanisms that can suppress distracting information [1, 2]. Such mechanisms might be related to neuronal oscillations, which are known to be involved in gating of incoming information [3]. We here apply a working memory (WM) task to investigate how neuronal oscillations are involved in the suppression of distracting information that can be predicted in time. We used a modified Sternberg WM task in which distracters were presented in the retention interval, while we recorded the ongoing brain activity using magnetoencephalography. The data revealed a robust adjustment of the phase of alpha oscillations in anticipation of the distracter. In trials with strong phase adjustment, response times to the memory probe were reduced. Further, the power of alpha oscillations increased prior to the distracter and predicted performance. Our findings demonstrate that the doors of perception close when a distracter is expected. The phase adjustment of the alpha rhythm adds to the computational versatility of brain oscillations, because such a mechanism allows for modulating neuronal processing on a fine temporal scale.},
  file = {2012 - Bonnefond, Jensen - Alpha oscillations serve to protect working memory maintenance against anticipated distracters.pdf},
  journal = {Current Biology},
  language = {en},
  number = {20}
}

@article{Bontrager2019,
  title = {Superstition in the {{Network}}: {{Deep Reinforcement Learning Plays Deceptive Games}}},
  shorttitle = {Superstition in the {{Network}}},
  author = {Bontrager, Philip and Khalifa, Ahmed and Anderson, Damien and Stephenson, Matthew and Salge, Christoph and Togelius, Julian},
  year = {2019},
  month = aug,
  abstract = {Deep reinforcement learning has learned to play many games well, but failed on others. To better characterize the modes and reasons of failure of deep reinforcement learners, we test the widely used Asynchronous Actor-Critic (A2C) algorithm on four deceptive games, which are specially designed to provide challenges to game-playing agents. These games are implemented in the General Video Game AI framework, which allows us to compare the behavior of reinforcement learningbased agents with planning agents based on tree search. We find that several of these games reliably deceive deep reinforcement learners, and that the resulting behavior highlights the shortcomings of the learning algorithm. The particular ways in which agents fail differ from how planning-based agents fail, further illuminating the character of these algorithms. We propose an initial typology of deceptions which could help us better understand pitfalls and failure modes of (deep) reinforcement learning.},
  archiveprefix = {arXiv},
  eprint = {1908.04436},
  eprinttype = {arxiv},
  file = {Bontrager et al. - 2019 - Superstition in the Network Deep Reinforcement Le.pdf},
  journal = {arXiv:1908.04436 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryclass = {cs, stat}
}

@article{Bopp2005,
  title = {Aging and {{Verbal Memory Span}}: {{A Meta}}-{{Analysis}}},
  shorttitle = {Aging and {{Verbal Memory Span}}},
  author = {Bopp, K. L. and Verhaeghen, P.},
  year = {2005},
  month = sep,
  volume = {60},
  pages = {P223-P233},
  issn = {1079-5014, 1758-5368},
  doi = {10.1093/geronb/60.5.P223},
  file = {2005 - Bopp, Verhaeghen - Aging and verbal memory span A meta-analysis.pdf},
  journal = {The Journals of Gerontology Series B: Psychological Sciences and Social Sciences},
  language = {en},
  number = {5}
}

@article{Borella2008,
  title = {Working Memory and Inhibition across the Adult Life-Span},
  author = {Borella, Erika and Carretti, Barbara and De Beni, Rossana},
  year = {2008},
  month = may,
  volume = {128},
  pages = {33--44},
  issn = {00016918},
  doi = {10.1016/j.actpsy.2007.09.008},
  abstract = {Research has shown that age-related changes in cognitive performance are due mostly to the decline of general factors such as working memory and inhibition. The present study is aimed at investigating age-related changes in these mechanisms across the adult life-span from 20 to 86 years of age. Results indicate a linear relationship between each working memory measure and age, independently of the nature of the task, and a quadratic relationship between the single inhibitory measures and age. Moreover, hierarchical regression analyses show that inhibition accounts for a significant, but modest, part of the age-related variance in working memory. Taken together, these results suggest that inhibition is not as crucial a contributor of age-related changes in the functional capacity of working memory across the adult life-span as previously thought.},
  file = {2008 - Borella, Carretti, De Beni - Working memory and inhibition across the adult life-span.pdf},
  journal = {Acta Psychologica},
  language = {en},
  number = {1}
}

@article{Borgers2005,
  title = {Background Gamma Rhythmicity and Attention in Cortical Local Circuits: {{A}} Computational Study},
  shorttitle = {Background Gamma Rhythmicity and Attention in Cortical Local Circuits},
  author = {Borgers, C. and Epstein, S. and Kopell, N. J.},
  year = {2005},
  month = may,
  volume = {102},
  pages = {7002--7007},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.0502366102},
  file = {Borgers et al. - 2005 - Background gamma rhythmicity and attention in cort.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {19}
}

@article{Borgers2008,
  title = {Gamma Oscillations Mediate Stimulus Competition and Attentional Selection in a Cortical Network Model},
  author = {Borgers, C. and Epstein, S. and Kopell, N. J.},
  year = {2008},
  month = nov,
  volume = {105},
  pages = {18023--18028},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.0809511105},
  file = {2008 - Börgers, Epstein, Kopell - Gamma oscillations mediate stimulus competition and attentional selection in a cortical network model.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {46}
}

@article{Borgers2014,
  title = {Approximate, Not {{Perfect Synchrony Maximizes}} the {{Downstream Effectiveness}} of {{Excitatory Neuronal Ensembles}}},
  author = {B{\"o}rgers, Christoph and Li, Jie and Kopell, Nancy},
  year = {2014},
  volume = {4},
  pages = {10},
  issn = {2190-8567},
  doi = {10.1186/2190-8567-4-10},
  abstract = {The most basic functional role commonly ascribed to synchrony in the brain is that of amplifying excitatory neuronal signals. The reasoning is straightforward: When positive charge is injected into a leaky target neuron over a time window of positive duration, some of it will have time to leak back out before an action potential is triggered in the target, and it will in that sense be wasted. If the goal is to elicit a firing response in the target using as little charge as possible, it seems best to deliver the charge all at once, i.e., in perfect synchrony. In this article, we show that this reasoning is correct only if one assumes that the input ceases when the target crosses the firing threshold, but before it actually fires. If the input ceases later\textemdash for instance, in response to a feedback signal triggered by the firing of the target\textemdash the ``most economical'' way of delivering input (the way that requires the least total amount of input) is no longer precisely synchronous, but merely approximately so. If the target is a heterogeneous network, as it always is in the brain, then ceasing the input ``when the target crosses the firing threshold'' is not an option, because there is no single moment when the firing threshold is crossed. In this sense, precise synchrony is never optimal in the brain.},
  file = {Börgers et al. - 2014 - Approximate, not Perfect Synchrony Maximizes the D.pdf},
  journal = {The Journal of Mathematical Neuroscience},
  language = {en},
  number = {1}
}

@article{Borst1999,
  title = {Information Theory and Neural Coding},
  author = {Borst, Alexander and Theunissen, Fr{\'e}d{\'e}ric E.},
  year = {1999},
  month = nov,
  volume = {2},
  pages = {947--957},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/14731},
  file = {1999 - Borst, Theunissen - Information theory and neural coding.pdf},
  journal = {Nature Neuroscience},
  language = {en},
  number = {11}
}

@article{Borst2015,
  title = {Common Circuit Design in Fly and Mammalian Motion Vision},
  author = {Borst, Alexander and Helmstaedter, Moritz},
  year = {2015},
  month = aug,
  volume = {18},
  pages = {1067--1076},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.4050},
  file = {2015 - Borst, Helmstaedter - Common circuit design in fly and mammalian motion vision.pdf;Borst and Helmstaedter - 2015 - Common circuit design in fly and mammalian motion .pdf},
  journal = {Nature Neuroscience},
  language = {en},
  number = {8}
}

@article{Bosco2015,
  title = {Correlational Effect Size Benchmarks.},
  author = {Bosco, Frank A. and Aguinis, Herman and Singh, Kulraj and Field, James G. and Pierce, Charles A.},
  year = {2015},
  month = mar,
  volume = {100},
  pages = {431--449},
  issn = {1939-1854, 0021-9010},
  doi = {10.1037/a0038047},
  abstract = {Effect size information is essential for the scientific enterprise and plays an increasingly central role in the scientific process. We extracted 147,328 correlations and developed a hierarchical taxonomy of variables reported in Journal of Applied Psychology and Personnel Psychology from 1980 to 2010 to produce empirical effect size benchmarks at the omnibus level, for 20 common research domains, and for an even finer grained level of generality. Results indicate that the usual interpretation and classification of effect sizes as small, medium, and large bear almost no resemblance to findings in the field, because distributions of effect sizes exhibit tertile partitions at values approximately one-half to one-third those intuited by Cohen (1988). Our results offer information that can be used for research planning and design purposes, such as producing better informed non-nil hypotheses and estimating statistical power and planning sample size accordingly. We also offer information useful for understanding the relative importance of the effect sizes found in a particular study in relationship to others and which research domains have advanced more or less, given that larger effect sizes indicate a better understanding of a phenomenon. Also, our study offers information about research domains for which the investigation of moderating effects may be more fruitful and provide information that is likely to facilitate the implementation of Bayesian analysis. Finally, our study offers information that practitioners can use to evaluate the relative effectiveness of various types of interventions.},
  file = {Bosco et al. - 2015 - Correlational effect size benchmarks..pdf},
  journal = {Journal of Applied Psychology},
  language = {en},
  number = {2}
}

@book{Bose2002,
  title = {Algorithms and Computation: 13th International Symposium ; Proceedings},
  shorttitle = {Algorithms and Computation},
  editor = {Bose, Prosenjit and ISAAC},
  year = {2002},
  publisher = {{Springer}},
  address = {{Berlin}},
  annotation = {OCLC: 51701610},
  file = {Bose and ISAAC - 2002 - Algorithms and computation 13th international sym.pdf},
  isbn = {978-3-540-00142-3},
  language = {en},
  number = {2518},
  series = {Lecture Notes in Computer Science}
}

@article{Bostwick2020,
  title = {Antagonistic {{Inhibitory Circuits Integrate Visual}} and {{Gravitactic Behaviors}}},
  author = {Bostwick, Michaela and Smith, Eleanor L. and Borba, Cezar and {Newman-Smith}, Erin and Guleria, Iraa and Kourakis, Matthew J. and Smith, William C.},
  year = {2020},
  month = feb,
  volume = {30},
  pages = {600-609.e2},
  issn = {09609822},
  doi = {10.1016/j.cub.2019.12.017},
  file = {Bostwick et al. - 2020 - Antagonistic Inhibitory Circuits Integrate Visual .pdf},
  journal = {Current Biology},
  language = {en},
  number = {4}
}

@techreport{Botvinik-Nezer2019,
  title = {Memory for {{Individual Items}} Is {{Related}} to {{Non}}-{{Reinforced Preference Change}}},
  author = {{Botvinik-Nezer}, Rotem and Bakkour, Akram and Salomon, Tom and Shohamy, Daphna and Schonberg, Tom},
  year = {2019},
  month = apr,
  institution = {{Neuroscience}},
  doi = {10.1101/621292},
  abstract = {It is commonly assumed that memories contribute to value-based decisions. Nevertheless, most theories of value-based decision-making do not account for memory influences on choice. Recently, new interest has emerged in the interactions between these two fundamental processes, mainly using reinforcement-based paradigms. Here, we aimed to study the role memory processes play in preference change following the non-reinforced cue-approach training (CAT) paradigm. In CAT, the mere association of cued items with a speeded motor response influences choices. Previous studies with this paradigm showed that a single training session induces a long-lasting effect of enhanced preferences for high-value trained stimuli, that is maintained for several months. We hypothesized that CAT influences memory accessibility for trained items, leading to enhanced accessibility of their positive associative memories and in turn to preference changes. In two pre-registered experiments, we tested whether memory for trained items was enhanced following CAT, in the short and in the longterm, and whether memory modifications were related to choices. We found that memory was enhanced for trained items and that better memory was correlated with enhanced preferences at the individual item level, both immediately and one month following CAT. Our findings show that memory plays a central role in value-based decision-making following CAT, even in the absence of external reinforcements. These findings contribute to new theories relating memory and value-based decision-making and set the groundwork for the implementation of novel behavioral interventions that lead to long-lasting behavioral change.},
  file = {Botvinik-Nezer et al. - 2019 - Memory for Individual Items is Related to Non-Rein.pdf},
  language = {en},
  type = {Preprint}
}

@article{Boughman2002,
  title = {How Sensory Drive Can Promote Speciation},
  author = {Boughman, Janette Wenrick},
  year = {2002},
  month = dec,
  volume = {17},
  pages = {571--577},
  issn = {01695347},
  doi = {10.1016/S0169-5347(02)02595-8},
  file = {Boughman - 2002 - How sensory drive can promote speciation.pdf},
  journal = {Trends in Ecology \& Evolution},
  language = {en},
  number = {12}
}

@article{Bourdoukan,
  title = {Enforcing Balance Allows Local Supervised Learning in Spiking Recurrent Networks},
  author = {Bourdoukan, Ralph and Den{\`e}ve, Sophie},
  pages = {9},
  abstract = {To predict sensory inputs or control motor trajectories, the brain must constantly learn temporal dynamics based on error feedback. However, it remains unclear how such supervised learning is implemented in biological neural networks. Learning in recurrent spiking networks is notoriously difficult because local changes in connectivity may have an unpredictable effect on the global dynamics. The most commonly used learning rules, such as temporal back-propagation, are not local and thus not biologically plausible. Furthermore, reproducing the Poisson-like statistics of neural responses requires the use of networks with balanced excitation and inhibition. Such balance is easily destroyed during learning. Using a top-down approach, we show how networks of integrate-and-fire neurons can learn arbitrary linear dynamical systems by feeding back their error as a feed-forward input. The network uses two types of recurrent connections: fast and slow. The fast connections learn to balance excitation and inhibition using a voltage-based plasticity rule. The slow connections are trained to minimize the error feedback using a current-based Hebbian learning rule. Importantly, the balance maintained by fast connections is crucial to ensure that global error signals are available locally in each neuron, in turn resulting in a local learning rule for the slow connections. This demonstrates that spiking networks can learn complex dynamics using purely local learning rules, using E/I balance as the key rather than an additional constraint. The resulting network implements a given function within the predictive coding scheme, with minimal dimensions and activity.},
  file = {2015 - Bourdoukan, Deneve - Enforcing balance allows local supervised learning in spiking recurrent networks.pdf;Bourdoukan and Denève - Enforcing balance allows local supervised learning.pdf},
  language = {en}
}

@inproceedings{Bourgault2002,
  title = {Information Based Adaptive Robotic Exploration},
  booktitle = {{{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{System}}},
  author = {Bourgault, F. and Makarenko, A.A. and Williams, S.B. and Grocholsky, B. and {Durrant-Whyte}, H.F.},
  year = {2002},
  volume = {1},
  pages = {540--545},
  publisher = {{IEEE}},
  address = {{Lausanne, Switzerland}},
  doi = {10.1109/IRDS.2002.1041446},
  abstract = {Exploration involving mapping and concurrent localization in an unknown environment is a pervasive task in mobile robotics. In general, the accuracy of the mapping process depends directly on the accuracy of the localization process. This paper address the problem of maximizing the accuracy of the map building process during exploration by adaptively selecting control actions that maximize localisation accuracy. The map building and exploration task is modeled using an Occupancy Grid (OG) with concurrent localisation performed using a feature-based Simultaneous Localisation And Mapping (SLAM) algorithm . Adaptive sensing aims at maximizing the map information by simultaneously maximizing the expected Shannon information gain (Mutual Information) on the OG map and minimizing the uncertainty of the vehicle pose and map feature uncertainty in the SLAM process. The resulting map building system is demonstrated in an indoor environment using data from a laser scanner mounted on a mobile platform.},
  file = {Bourgault et al. - 2002 - Information based adaptive robotic exploration.pdf},
  isbn = {978-0-7803-7398-3},
  language = {en}
}

@article{Bowling,
  title = {Rational and {{Convergent Learning}} in {{Stochastic Games}}},
  author = {Bowling, Michael and Veloso, Manuela},
  pages = {6},
  abstract = {This paper investigates the problem of policy learning in multiagent environments using the stochastic game framework, which we briefly overview. We introduce two properties as desirable for a learning agent when in the presence of other learning agents, namely rationality and convergence. We examine existing reinforcement learning algorithms according to these two properties and notice that they fail to simultaneously meet both criteria. We then contribute a new learning algorithm, WoLF policy hillclimbing, that is based on a simple principle: ``learn quickly while losing, slowly while winning.'' The algorithm is proven to be rational and we present empirical results for a number of stochastic games showing the algorithm converges.},
  file = {2001 - Bowling, Veloso - Rational and convergent learning in stochastic games.pdf},
  language = {en}
}

@article{Bowling2002,
  title = {Multiagent Learning Using a Variable Learning Rate},
  author = {Bowling, Michael and Veloso, Manuela},
  year = {2002},
  month = apr,
  volume = {136},
  pages = {215--250},
  issn = {00043702},
  doi = {10.1016/S0004-3702(02)00121-2},
  abstract = {Learning to act in a multiagent environment is a difficult problem since the normal definition of an optimal policy no longer applies. The optimal policy at any moment depends on the policies of the other agents. This creates a situation of learning a moving target. Previous learning algorithms have one of two shortcomings depending on their approach. They either converge to a policy that may not be optimal against the specific opponents' policies, or they may not converge at all. In this article we examine this learning problem in the framework of stochastic games. We look at a number of previous learning algorithms showing how they fail at one of the above criteria. We then contribute a new reinforcement learning technique using a variable learning rate to overcome these shortcomings. Specifically, we introduce the WoLF principle, ``Win or Learn Fast,'' for varying the learning rate. We examine this technique theoretically, proving convergence in self-play on a restricted class of iterated matrix games. We also present empirical results on a variety of more general stochastic games, in situations of self-play and otherwise, demonstrating the wide applicability of this method.},
  file = {2002 - Bowling, Veloso - Multiagent learning using a variable learning rate.pdf},
  journal = {Artificial Intelligence},
  language = {en},
  number = {2}
}

@article{Boyer2004,
  title = {Modeling the Searching Behavior of Social Monkeys},
  author = {Boyer, D. and Miramontes, O. and {Ramos-Fern{\'a}ndez}, G. and Mateos, J.L. and Cocho, G.},
  year = {2004},
  month = oct,
  volume = {342},
  pages = {329--335},
  issn = {03784371},
  doi = {10.1016/j.physa.2004.04.091},
  abstract = {We discuss various features of the trajectories of spider monkeys looking for food in a tropical forest, as observed recently in an extensive in situ study. Some of the features observed can be interpreted as the result of social interactions. In addition, a simple model of deterministic walk in a random environment reproduces the observed angular correlations between successive steps, and in some cases, the emergence of Le\~Avy distributions for the length of the steps.},
  file = {Boyer et al. - 2004 - Modeling the searching behavior of social monkeys.pdf},
  journal = {Physica A: Statistical Mechanics and its Applications},
  language = {en},
  number = {1-2}
}

@article{Boyer2006,
  title = {Scale-Free Foraging by Primates Emerges from Their Interaction with a Complex Environment},
  author = {Boyer, Denis and {Ramos-Fern{\'a}ndez}, Gabriel and Miramontes, Octavio and Mateos, Jos{\'e} L and Cocho, Germinal and Larralde, Hern{\'a}n and Ramos, Humberto and Rojas, Fernando},
  year = {2006},
  month = jul,
  volume = {273},
  pages = {1743--1750},
  issn = {0962-8452, 1471-2954},
  doi = {10.1098/rspb.2005.3462},
  abstract = {Scale-free foraging patterns are widespread among animals. These may be the outcome of an optimal searching strategy to find scarce, randomly distributed resources, but a less explored alternative is that this behaviour may result from the interaction of foraging animals with a particular distribution of resources. We introduce a simple foraging model where individual primates follow mental maps and choose their displacements according to a maximum efficiency criterion, in a spatially disordered environment containing many trees with a heterogeneous size distribution. We show that a particular tree-size frequency distribution induces non-Gaussian movement patterns with multiple spatial scales (L\'evy walks). These results are consistent with field observations of tree-size variation and spider monkey (               Ateles geoffroyi               ) foraging patterns. We discuss the consequences that our results may have for the patterns of seed dispersal by foraging primates.},
  file = {Boyer et al. - 2006 - Scale-free foraging by primates emerges from their.pdf},
  journal = {Proc. R. Soc. B.},
  language = {en},
  number = {1595}
}

@article{Brafman,
  title = {R-Max \textendash{} {{A General Polynomial Time Algorithm}} for {{Near}}-{{Optimal Reinforcement Learning}}},
  author = {Brafman, Ronen I and Tennenholtz, Moshe},
  pages = {19},
  abstract = {R-max is a very simple model-based reinforcement learning algorithm which can attain near-optimal average reward in polynomial time. In R-max, the agent always maintains a complete, but possibly inaccurate model of its environment and acts based on the optimal policy derived from this model. The model is initialized in an optimistic fashion: all actions in all states return the maximal possible reward (hence the name). During execution, it is updated based on the agent's observations. R-max improves upon several previous algorithms: (1) It is simpler and more general than Kearns and Singh's E3 algorithm, covering zero-sum stochastic games. (2) It has a built-in mechanism for resolving the exploration vs. exploitation dilemma. (3) It formally justifies the ``optimism under uncertainty'' bias used in many RL algorithms. (4) It is simpler, more general, and more efficient than Brafman and Tennenholtz's LSG algorithm for learning in single controller stochastic games. (5) It generalizes the algorithm by Monderer and Tennenholtz for learning in repeated games. (6) It is the only algorithm for learning in repeated games, to date, which is provably efficient, considerably improving and simplifying previous algorithms by Banos and by Megiddo.},
  file = {Brafman and Tennenholtz - R-max – A General Polynomial Time Algorithm for Ne 2.pdf},
  language = {en}
}

@article{Brafman2002,
  title = {R-Max \textendash{} {{A General Polynomial Time Algorithm}} for {{Near}}-{{Optimal Reinforcement Learning}}},
  author = {Brafman, Ronen I and Tennenholtz, Moshe},
  year = {2002},
  volume = {2},
  pages = {19},
  abstract = {R-max is a very simple model-based reinforcement learning algorithm which can attain near-optimal average reward in polynomial time. In R-max, the agent always maintains a complete, but possibly inaccurate model of its environment and acts based on the optimal policy derived from this model. The model is initialized in an optimistic fashion: all actions in all states return the maximal possible reward (hence the name). During execution, it is updated based on the agent's observations. R-max improves upon several previous algorithms: (1) It is simpler and more general than Kearns and Singh's E3 algorithm, covering zero-sum stochastic games. (2) It has a built-in mechanism for resolving the exploration vs. exploitation dilemma. (3) It formally justifies the ``optimism under uncertainty'' bias used in many RL algorithms. (4) It is simpler, more general, and more efficient than Brafman and Tennenholtz's LSG algorithm for learning in single controller stochastic games. (5) It generalizes the algorithm by Monderer and Tennenholtz for learning in repeated games. (6) It is the only algorithm for learning in repeated games, to date, which is provably efficient, considerably improving and simplifying previous algorithms by Banos and by Megiddo.},
  file = {Brafman and Tennenholtz - R-max – A General Polynomial Time Algorithm for Ne.pdf},
  journal = {Journal of Machine Learning Research},
  language = {en}
}

@article{Brafmana,
  title = {R-Max \textendash{} {{A General Polynomial Time Algorithm}} for {{Near}}-{{Optimal Reinforcement Learning}}},
  author = {Brafman, Ronen I and Tennenholtz, Moshe},
  pages = {19},
  abstract = {R-max is a very simple model-based reinforcement learning algorithm which can attain near-optimal average reward in polynomial time. In R-max, the agent always maintains a complete, but possibly inaccurate model of its environment and acts based on the optimal policy derived from this model. The model is initialized in an optimistic fashion: all actions in all states return the maximal possible reward (hence the name). During execution, it is updated based on the agent's observations. R-max improves upon several previous algorithms: (1) It is simpler and more general than Kearns and Singh's E3 algorithm, covering zero-sum stochastic games. (2) It has a built-in mechanism for resolving the exploration vs. exploitation dilemma. (3) It formally justifies the ``optimism under uncertainty'' bias used in many RL algorithms. (4) It is simpler, more general, and more efficient than Brafman and Tennenholtz's LSG algorithm for learning in single controller stochastic games. (5) It generalizes the algorithm by Monderer and Tennenholtz for learning in repeated games. (6) It is the only algorithm for learning in repeated games, to date, which is provably efficient, considerably improving and simplifying previous algorithms by Banos and by Megiddo.},
  file = {Brafman and Tennenholtz - R-max – A General Polynomial Time Algorithm for Ne 3.pdf},
  language = {en}
}

@book{Braun1983,
  title = {Differential {{Equation Models}}},
  author = {Braun, Martin and Coleman, Courtney S and Drew, Donald A},
  year = {1983},
  publisher = {{Springer New York}},
  address = {{New York, NY}},
  abstract = {The purpose of this four volume series is to make available for college teachers and students samples of important and realistic applications of mathematics which can be covered in undergraduate programs. The goal is to provide illustrations of how modem mathematics is actually employed to solve relevant contemporary problems. Although these independent chapters were prepared primarily for teachers in the general mathematical sciences, they should prove valuable to students, teachers, and research scientists in many of the fields of application as well. Prerequisites for each chapter and suggestions for the teacher are provided. Several of these chapters have been tested in a variety of classroom settings, and all have undergone extensive peer review and revision. Illustrations and exercises are included in most chapters. Some units can be covered in one class, whereas others provide sufficient material for a few weeks of class time. Volume 1 contains 23 chapters and deals with differential equations and, in the last four chapters, problems leading to partial differential equations. Applications are taken from medicine, biology, traffic systems and several other fields. The 14 chapters in Volume 2 are devoted mostly to problems arising in political science, but they also address questions appearing in sociology and ecology. Topics covered include voting systems, weighted voting, proportional representation, coalitional values, and committees. The 14 chapters in Volume 3 emphasize discrete mathematical methods such as those which arise in graph theory, combinatorics, and networks.},
  annotation = {OCLC: 840280008},
  file = {1983 - Lucas - Differential Equation Models.pdf;Braun et al. - 1983 - Differential Equation Models.pdf},
  isbn = {978-1-4612-5427-0 978-1-4612-5429-4},
  language = {en}
}

@article{Brazier1952,
  title = {Crosscorrelation and Autocorrelation Studies of Electroencephalographic Potentials},
  author = {Brazier, Mary A.B. and Casby, James U.},
  year = {1952},
  month = may,
  volume = {4},
  pages = {201--211},
  issn = {00134694},
  doi = {10.1016/0013-4694(52)90010-2},
  file = {Brazier and Casby - 1952 - Crosscorrelation and autocorrelation studies of el.pdf},
  journal = {Electroencephalography and Clinical Neurophysiology},
  language = {en},
  number = {2}
}

@article{Breakspear2003,
  title = {Modulation of Excitatory Synaptic Coupling Facilitates Synchronization and Complex Dynamics in a Nonlinear Model of Neuronal Dynamics},
  author = {Breakspear, Michael and R. Terry, John and J. Friston, Karl},
  year = {2003},
  month = jun,
  volume = {52-54},
  pages = {151--158},
  issn = {09252312},
  doi = {10.1016/S0925-2312(02)00740-3},
  abstract = {We study dynamical synchronization in a model of a neural system constituted by local networks of densely interconnected excitatory and inhibitory neurons. Neural dynamics are determined by voltage- and ligand-gated ion channels. Coupling between the local networks is introduced via sparse excitatory connectivity. With modulation of this long-range synaptic coupling the system undergoes a transition from independent oscillations to chaotic synchronization. Between these states exists a 'weakly' stable state with epochs of synchronization and complex intermittent desynchronization. This may facilitate adaptive brain function by engendering a diverse repertoire of dynamics and contribute to the genesis of complexity in the EEG.},
  file = {2003 - Breakspear, Terry, Friston - Modulation of excitatory synaptic coupling facilitates synchronization and complex dynamics in a bio.pdf},
  journal = {Neurocomputing},
  language = {en}
}

@article{Bressler2003,
  title = {Cortical {{Coordination Dynamics}} and the {{Disorganization Syndrome}} in {{Schizophrenia}}},
  author = {Bressler, Steven L},
  year = {2003},
  month = jul,
  volume = {28},
  pages = {S35-S39},
  issn = {0893-133X, 1740-634X},
  doi = {10.1038/sj.npp.1300145},
  file = {2003 - Bressler - Cortical Coordination Dynamics and the Disorganization Syndrome in Schizophrenia.pdf},
  journal = {Neuropsychopharmacology},
  language = {en},
  number = {S1}
}

@article{Brette2007,
  title = {Simulation of Networks of Spiking Neurons: {{A}} Review of Tools and Strategies},
  shorttitle = {Simulation of Networks of Spiking Neurons},
  author = {Brette, Romain and Rudolph, Michelle and Carnevale, Ted and Hines, Michael and Beeman, David and Bower, James M. and Diesmann, Markus and Morrison, Abigail and Goodman, Philip H. and Harris, Frederick C. and Zirpe, Milind and Natschl{\"a}ger, Thomas and Pecevski, Dejan and Ermentrout, Bard and Djurfeldt, Mikael and Lansner, Anders and Rochel, Olivier and Vieville, Thierry and Muller, Eilif and Davison, Andrew P. and El Boustani, Sami and Destexhe, Alain},
  year = {2007},
  month = dec,
  volume = {23},
  pages = {349--398},
  issn = {1573-6873},
  doi = {10.1007/s10827-007-0038-6},
  file = {2007 - Brette et al. - Simulation of networks of spiking neurons a review of tools and strategies.pdf},
  journal = {Journal of Computational Neuroscience},
  language = {en},
  number = {3}
}

@article{Brette2013,
  title = {An Ecological Approach to Neural Computation},
  author = {Brette, Romain},
  year = {2013},
  month = jul,
  volume = {14},
  issn = {1471-2202},
  doi = {10.1186/1471-2202-14-S1-P40},
  file = {2013 - Brette - An ecological approach to neural computation.pdf},
  journal = {BMC Neuroscience},
  language = {en},
  number = {S1}
}

@article{Brette2015,
  title = {What {{Is}} the {{Most Realistic Single}}-{{Compartment Model}} of {{Spike Initiation}}?},
  author = {Brette, Romain},
  editor = {Pillow, Jonathan W.},
  year = {2015},
  month = apr,
  volume = {11},
  pages = {e1004114},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1004114},
  file = {Brette - 2015 - What Is the Most Realistic Single-Compartment Mode.pdf},
  journal = {PLOS Computational Biology},
  language = {en},
  number = {4}
}

@techreport{Brette2017,
  title = {Is Coding a Relevant Metaphor for the Brain?},
  author = {Brette, Romain},
  year = {2017},
  month = jul,
  institution = {{Neuroscience}},
  doi = {10.1101/168237},
  abstract = {"Neural coding" is a popular metaphor in neuroscience, where objective properties of the world are communicated to the brain in the form of spikes. Here I argue that this metaphor is often inappropriate and misleading. First, when neurons are said to encode experimental parameters, the neural code depends on experimental details that are not carried by the coding variable. Thus, the representational power of neural codes is much more limited than generally implied. Second, neural codes carry information only by reference to things with known meaning. In contrast, perceptual systems must build information from relations between sensory signals and actions, forming a structured internal model. Neural codes are inadequate for this purpose because they are unstructured. Third, coding variables are observables tied to the temporality of experiments, while spikes are timed actions that mediate coupling in a distributed dynamical system. The coding metaphor tries to fit the dynamic, circular and distributed causal structure of the brain into a linear chain of transformations between observables, but the two causal structures are incongruent. I conclude that the neural coding metaphor cannot provide a basis for theories of brain function, because it is incompatible with both the causal structure of the brain and the informational requirements of cognition.},
  file = {Brette - 2017 - Is coding a relevant metaphor for the brain.pdf},
  language = {en},
  type = {Preprint}
}

@article{Briggs2010,
  title = {Organizing Principles of Cortical Layer 6},
  author = {{Briggs}},
  year = {2010},
  issn = {16625110},
  doi = {10.3389/neuro.04.003.2010},
  file = {2010 - Briggs - Organizing principles of cortical layer 6.pdf},
  journal = {Frontiers in Neural Circuits},
  language = {en}
}

@article{Brittain2014,
  title = {The Highs and Lows of Beta Activity in Cortico-Basal Ganglia Loops},
  author = {Brittain, John-Stuart and Sharott, Andrew and Brown, Peter},
  year = {2014},
  month = jun,
  volume = {39},
  pages = {1951--1959},
  issn = {0953816X},
  doi = {10.1111/ejn.12574},
  abstract = {Oscillatory activity in the beta (13\textendash 30 Hz) frequency band is widespread in cortico-basal ganglia circuits, and becomes prominent in Parkinson's disease (PD). Here we develop the hypothesis that the degree of synchronization in this frequency band is a critical factor in gating computation across a population of neurons, with increases in beta band synchrony entailing a loss of information-coding space and hence computational capacity. Task and context drive this dynamic gating, so that for each state there will be an optimal level of network synchrony, and levels lower or higher than this will impair behavioural performance. Thus, both the pathological exaggeration of synchrony, as observed in PD, and the ability of interventions like deep brain stimulation (DBS) to excessively suppress synchrony can potentially lead to impairments in behavioural performance. Indeed, under physiological conditions, the manipulation of computational capacity by beta activity may itself present a mechanism of action selection and maintenance.},
  file = {2014 - Brittain, Sharott, Brown - The highs and lows of beta activity in cortico-basal ganglia loops.pdf;2014 - Brittain, Sharott, Brown - The highs and lows of beta activity in cortico-basal ganglia loops(2).pdf;Brittain et al. - 2014 - The highs and lows of beta activity in cortico-bas 2.pdf;Brittain et al. - 2014 - The highs and lows of beta activity in cortico-bas.pdf},
  journal = {European Journal of Neuroscience},
  language = {en},
  number = {11}
}

@article{Brittin2021,
  title = {A Multi-Scale Brain Map Derived from Whole-Brain Volumetric Reconstructions},
  author = {Brittin, Christopher A. and Cook, Steven J. and Hall, David H. and Emmons, Scott W. and Cohen, Netta},
  year = {2021},
  month = mar,
  volume = {591},
  pages = {105--110},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/s41586-021-03284-x},
  file = {Brittin et al. - 2021 - A multi-scale brain map derived from whole-brain v.pdf},
  journal = {Nature},
  language = {en},
  number = {7848}
}

@article{Brockwell2004,
  title = {Recursive {{Bayesian Decoding}} of {{Motor Cortical Signals}} by {{Particle Filtering}}},
  author = {Brockwell, A. E. and Rojas, A. L. and Kass, R. E.},
  year = {2004},
  month = apr,
  volume = {91},
  pages = {1899--1907},
  issn = {0022-3077, 1522-1598},
  doi = {10.1152/jn.00438.2003},
  file = {2004 - Brockwell, Rojas, Kass - Recursive Bayesian Decoding of Motor Cortical Signals by Particle Filtering.pdf},
  journal = {Journal of Neurophysiology},
  language = {en},
  number = {4}
}

@article{Brogden1939,
  title = {Higher {{Order Conditioning}}},
  author = {Brogden, W. J.},
  year = {1939},
  month = oct,
  volume = {52},
  pages = {579},
  issn = {00029556},
  doi = {10.2307/1416470},
  file = {2012 - Press - Higher Order Conditioning Author ( s ) W . J . Brogden Reviewed work ( s ) Source The American Journal of Psychology , V.pdf},
  journal = {The American Journal of Psychology},
  language = {en},
  number = {4}
}

@article{Broom2005,
  title = {You Can Run\textemdash or You Can Hide: Optimal Strategies for Cryptic Prey against Pursuit Predators},
  shorttitle = {You Can Run\textemdash or You Can Hide},
  author = {Broom, Mark and Ruxton, Graeme D.},
  year = {2005},
  month = may,
  volume = {16},
  pages = {534--540},
  issn = {1465-7279, 1045-2249},
  doi = {10.1093/beheco/ari024},
  file = {Broom and Ruxton - 2005 - You can run—or you can hide optimal strategies fo.pdf},
  journal = {Behavioral Ecology},
  language = {en},
  number = {3}
}

@article{Brown2005,
  title = {A {{Ballistic Model}} of {{Choice Response Time}}.},
  author = {Brown, Scott and Heathcote, Andrew},
  year = {2005},
  volume = {112},
  pages = {117--128},
  issn = {1939-1471, 0033-295X},
  doi = {10.1037/0033-295X.112.1.117},
  file = {2005 - Brown, Heathcote - A ballistic model of choice response time.pdf},
  journal = {Psychological Review},
  language = {en},
  number = {1}
}

@article{Brozovic2008,
  title = {Mechanism of Gain Modulation at Single Neuron and Network Levels},
  author = {Brozovi{\'c}, M. and Abbott, L. F. and Andersen, R. A.},
  year = {2008},
  month = aug,
  volume = {25},
  pages = {158--168},
  issn = {0929-5313, 1573-6873},
  doi = {10.1007/s10827-007-0070-6},
  abstract = {Gain modulation, in which the sensitivity of a neural response to one input is modified by a second input, is studied at single-neuron and network levels. At the single neuron level, gain modulation can arise if the two inputs are subject to a direct multiplicative interaction. Alternatively, these inputs can be summed in a linear manner by the neuron and gain modulation can arise, instead, from a nonlinear input\textendash output relationship. We derive a mathematical constraint that can distinguish these two mechanisms even though they can look very similar, provided sufficient data of the appropriate type are available. Previously, it has been shown in coordinate transformation studies that artificial neurons with sigmoid transfer functions can acquire a nonlinear additive form of gain modulation through learning-driven adjustment of synaptic weights. We use the constraint derived for single-neuron studies to compare responses in this network with those of another network model based on a biologically inspired transfer function that can support approximately multiplicative interactions.},
  file = {2008 - Brozovi, Abbott, Andersen - Mechanism of gain modulation at single neuron and network levels.pdf},
  journal = {Journal of Computational Neuroscience},
  language = {en},
  number = {1}
}

@article{Brunel2003,
  title = {What {{Determines}} the {{Frequency}} of {{Fast Network Oscillations With Irregular Neural Discharges}}? {{I}}. {{Synaptic Dynamics}} and {{Excitation}}-{{Inhibition Balance}}},
  shorttitle = {What {{Determines}} the {{Frequency}} of {{Fast Network Oscillations With Irregular Neural Discharges}}?},
  author = {Brunel, Nicolas and Wang, Xiao-Jing},
  year = {2003},
  month = jul,
  volume = {90},
  pages = {415--430},
  issn = {0022-3077, 1522-1598},
  doi = {10.1152/jn.01095.2002},
  file = {2003 - Brunel, Wang - What determines the frequency of fast network oscillations with irregular neural discharges I. Synaptic dynamics a.pdf},
  journal = {Journal of Neurophysiology},
  language = {en},
  number = {1}
}

@article{Brunet2014,
  title = {Gamma or No Gamma, That Is the Question},
  author = {Brunet, Nicolas and Vinck, Martin and Bosman, Conrado A. and Singer, Wolf and Fries, Pascal},
  year = {2014},
  month = oct,
  volume = {18},
  pages = {507--509},
  issn = {13646613},
  doi = {10.1016/j.tics.2014.08.006},
  file = {Brunet et al. - 2014 - Gamma or no gamma, that is the question.pdf},
  journal = {Trends in Cognitive Sciences},
  language = {en},
  number = {10}
}

@article{Brunner2018,
  title = {Using {{State Predictions}} for {{Value Regularization}} in {{Curiosity Driven Deep Reinforcement Learning}}},
  author = {Brunner, Gino and Fritsche, Manuel and Richter, Oliver and Wattenhofer, Roger},
  year = {2018},
  month = sep,
  abstract = {Learning in sparse reward settings remains a challenge in Reinforcement Learning, which is often addressed by using intrinsic rewards. One promising strategy is inspired by human curiosity, requiring the agent to learn to predict the future. In this paper a curiosity-driven agent is extended to use these predictions directly for training. To achieve this, the agent predicts the value function of the next state at any point in time. Subsequently, the consistency of this prediction with the current value function is measured, which is then used as a regularization term in the loss function of the algorithm. Experiments were made on grid-world environments as well as on a 3D navigation task, both with sparse rewards. In the first case the extended agent is able to learn significantly faster than the baselines.},
  archiveprefix = {arXiv},
  eprint = {1810.00361},
  eprinttype = {arxiv},
  file = {Brunner et al. - 2018 - Using State Predictions for Value Regularization i.pdf},
  journal = {arXiv:1810.00361 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryclass = {cs, stat}
}

@article{Bruno2006,
  title = {Cortex {{Is Driven}} by {{Weak}} but {{Synchronously Active Thalamocortical Synapses}}},
  author = {Bruno, R. M.},
  year = {2006},
  month = jun,
  volume = {312},
  pages = {1622--1627},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1124593},
  file = {2006 - Bruno, Sakmann - Cortex is driven by weak but synchronously active thalamocortical synapses.pdf},
  journal = {Science},
  language = {en},
  number = {5780}
}

@article{Bshouty2005,
  title = {Exploring Learnability between Exact and {{PAC}}},
  author = {Bshouty, Nader H. and Jackson, Jeffrey C. and Tamon, Christino},
  year = {2005},
  month = jun,
  volume = {70},
  pages = {471--484},
  issn = {00220000},
  doi = {10.1016/j.jcss.2004.10.002},
  abstract = {We study a model of probably exactly correct (PExact) learning that can be viewed either as the Exact model (learning from equivalence queries only) relaxed so that counterexamples to equivalence queries are distributionally drawn rather than adversarially chosen or as the probably approximately correct (PAC) model strengthened to require a perfect hypothesis. We also introduce a model of probably almost exactly correct (PAExact) learning that requires a hypothesis with negligible error and thus lies between the PExact and PAC models. Unlike the Exact and PExact models, PAExact learning is applicable to classes of functions defined over infinite instance spaces. We obtain a number of separation results between these models. Of particular note are some positive results for efficient parallel learning in the PAExact model, which stand in stark contrast to earlier negative results for efficient parallel Exact learning.},
  file = {Bshouty et al. - 2005 - Exploring learnability between exact and PAC.pdf},
  journal = {Journal of Computer and System Sciences},
  language = {en},
  number = {4}
}

@article{Bubeck,
  title = {Bounded Regret in Stochastic Multi-Armed Bandits},
  author = {Bubeck, Sebastien and Perchet, Vianney and Rigollet, Philippe},
  pages = {13},
  abstract = {We study the stochastic multi-armed bandit problem when one knows the value \textmu ({$\star$}) of an optimal arm, as a well as a positive lower bound on the smallest positive gap {$\increment$}. We propose a new randomized policy that attains a regret uniformly bounded over time in this setting. We also prove several lower bounds, which show in particular that bounded regret is not possible if one only knows {$\increment$}, and bounded regret of order 1/{$\increment$} is not possible if one only knows \textmu ({$\star$}).},
  file = {Bubeck et al. - Bounded regret in stochastic multi-armed bandits.pdf},
  language = {en}
}

@article{Bubeck2010,
  title = {Pure {{Exploration}} for {{Multi}}-{{Armed Bandit Problems}}},
  author = {Bubeck, S{\'e}bastien and Munos, R{\'e}mi and Stoltz, Gilles},
  year = {2010},
  month = jun,
  abstract = {We consider the framework of stochastic multi-armed bandit problems and study the possibilities and limitations of forecasters that perform an on-line exploration of the arms. These forecasters are assessed in terms of their simple regret, a regret notion that captures the fact that exploration is only constrained by the number of available rounds (not necessarily known in advance), in contrast to the case when the cumulative regret is considered and when exploitation needs to be performed at the same time. We believe that this performance criterion is suited to situations when the cost of pulling an arm is expressed in terms of resources rather than rewards. We discuss the links between the simple and the cumulative regret. One of the main results in the case of a finite number of arms is a general lower bound on the simple regret of a forecaster in terms of its cumulative regret: the smaller the latter, the larger the former. Keeping this result in mind, we then exhibit upper bounds on the simple regret of some forecasters. The paper ends with a study devoted to continuous-armed bandit problems; we show that the simple regret can be minimized with respect to a family of probability distributions if and only if the cumulative regret can be minimized for it. Based on this equivalence, we are able to prove that the separable metric spaces are exactly the metric spaces on which these regrets can be minimized with respect to the family of all probability distributions with continuous mean-payoff functions.},
  archiveprefix = {arXiv},
  eprint = {0802.2655},
  eprinttype = {arxiv},
  file = {Bubeck et al. - 2010 - Pure Exploration for Multi-Armed Bandit Problems.pdf},
  journal = {arXiv:0802.2655 [cs, math, stat]},
  keywords = {Computer Science - Machine Learning,Mathematics - Statistics Theory},
  language = {en},
  primaryclass = {cs, math, stat}
}

@article{Bubeck2011,
  title = {Pure Exploration in Finitely-Armed and Continuous-Armed Bandits},
  author = {Bubeck, S{\'e}bastien and Munos, R{\'e}mi and Stoltz, Gilles},
  year = {2011},
  month = apr,
  volume = {412},
  pages = {1832--1852},
  issn = {03043975},
  doi = {10.1016/j.tcs.2010.12.059},
  abstract = {We consider the framework of stochastic multi-armed bandit problems and study the possibilities and limitations of forecasters that perform an on-line exploration of the arms. These forecasters are assessed in terms of their simple regret, a regret notion that captures the fact that exploration is only constrained by the number of available rounds (not necessarily known in advance), in contrast to the case when the cumulative regret is considered and when exploitation needs to be performed at the same time. We believe that this performance criterion is suited to situations when the cost of pulling an arm is expressed in terms of resources rather than rewards. We discuss the links between the simple and the cumulative regret. One of the main results in the case of a finite number of arms is a general lower bound on the simple regret of a forecaster in terms of its cumulative regret: the smaller the latter, the larger the former. Keeping this result in mind, we then exhibit upper bounds on the simple regret of some forecasters. The paper ends with a study devoted to continuous-armed bandit problems; we show that the simple regret can be minimized with respect to a family of probability distributions if and only if the cumulative regret can be minimized for it. Based on this equivalence, we are able to prove that the separable metric spaces are exactly the metric spaces on which these regrets can be minimized with respect to the family of all probability distributions with continuous mean-payoff functions.},
  file = {Bubeck et al. - 2011 - Pure exploration in finitely-armed and continuous-.pdf},
  journal = {Theoretical Computer Science},
  language = {en},
  number = {19}
}

@article{Buehlmann2010,
  title = {Optimal {{Information Transfer}} in the {{Cortex}} through {{Synchronization}}},
  author = {Buehlmann, Andres and Deco, Gustavo},
  editor = {Friston, Karl J.},
  year = {2010},
  month = sep,
  volume = {6},
  pages = {e1000934},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1000934},
  abstract = {In recent experimental work it has been shown that neuronal interactions are modulated by neuronal synchronization and that this modulation depends on phase shifts in neuronal oscillations. This result suggests that connections in a network can be shaped through synchronization. Here, we test and expand this hypothesis using a model network. We use transfer entropy, an information theoretical measure, to quantify the exchanged information. We show that transferred information depends on the phase relation of the signal, that the amount of exchanged information increases as a function of oscillations in the signal and that the speed of the information transfer increases as a function of synchronization. This implies that synchronization makes information transport more efficient. In summary, our results reinforce the hypothesis that synchronization modulates neuronal interactions and provide further evidence that gamma band synchronization has behavioral relevance.},
  file = {Buehlmann and Deco - 2010 - Optimal Information Transfer in the Cortex through.PDF},
  journal = {PLoS Comput Biol},
  language = {en},
  number = {9}
}

@article{Buesing2010,
  title = {A {{Spiking Neuron}} as {{Information Bottleneck}}},
  author = {Buesing, Lars and Maass, Wolfgang},
  year = {2010},
  month = aug,
  volume = {22},
  pages = {1961--1992},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/neco.2010.08-09-1084},
  file = {2010 - Buesing, Maass - A Spiking Neuron as Information Bottleneck.pdf},
  journal = {Neural Computation},
  language = {en},
  number = {8}
}

@article{Bunimovich2004,
  title = {Deterministic Walks in Random Environments},
  author = {Bunimovich, Leonid A},
  year = {2004},
  month = jan,
  volume = {187},
  pages = {20--29},
  issn = {01672789},
  doi = {10.1016/j.physd.2003.09.028},
  abstract = {Deterministic walks in random environments (DWRE) occupy an intermediate position between purely random (generated by random trials) and purely deterministic (generated by deterministic dynamical systems, e.g., by maps) models of diffusion. These models combine deterministic and probabilistic features. We review general properties of DWRE and demonstrate that, to a large extent, their dynamics and their statistics can be analyzed consecutively and separately. We also show that orbits of one-dimensional walks in rigid environments with non-constant rigidity almost surely visit each site infinitely many times.},
  file = {Bunimovich - 2004 - Deterministic walks in random environments.pdf},
  journal = {Physica D: Nonlinear Phenomena},
  language = {en},
  number = {1-4}
}

@article{Bunzeck2006,
  title = {Absolute {{Coding}} of {{Stimulus Novelty}} in the {{Human Substantia Nigra}}/{{VTA}}},
  author = {Bunzeck, Nico and D{\"u}zel, Emrah},
  year = {2006},
  month = aug,
  volume = {51},
  pages = {369--379},
  issn = {08966273},
  doi = {10.1016/j.neuron.2006.06.021},
  abstract = {Novelty exploration can enhance hippocampal plasticity in animals through dopaminergic neuromodulation arising in the substantia nigra/ventral tegmental area (SN/VTA). This enhancement can outlast the exploration phase by several minutes. Currently, little is known about dopaminergic novelty processing and its relationship to hippocampal function in humans. In two functional magnetic resonance imaging (fMRI) studies, SN/VTA activations in humans were indeed driven by stimulus novelty rather than other forms of stimulus salience such as rareness, negative emotional valence, or targetness of familiar stimuli, whereas hippocampal responses were less selective. SN/VTA novelty responses were scaled according to absolute rather than relative novelty in a given context, unlike adaptive SN/VTA responses recently reported for reward outcome in animal studies. Finally, novelty enhanced learning and perirhinal/parahippocampal processing of familiar items presented in the same context. Thus, the human SN/VTA can code absolute stimulus novelty and might contribute to enhancing learning in the context of novelty.},
  file = {2006 - Bunzeck, Düzel - Absolute Coding of Stimulus Novelty in the Human Substantia NigraVTA.pdf},
  journal = {Neuron},
  language = {en},
  number = {3}
}

@article{Buonomano1995,
  title = {Temporal Information Transformed into a Spatial Code by a Neural Network with Realistic Properties},
  author = {Buonomano, D. and Merzenich, M.},
  year = {1995},
  month = feb,
  volume = {267},
  pages = {1028--1030},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.7863330},
  file = {1995 - Buonomano, Mauk - Temporal information transformed into a spatial code by a neural network with realistic properties.pdf},
  journal = {Science},
  language = {en},
  number = {5200}
}

@article{Buonomano1998,
  title = {Net {{Interaction Between Different Forms}} of {{Short}}-{{Term Synaptic Plasticity}} and {{Slow}}-{{IPSPs}} in the {{Hippocampus}} and {{Auditory Cortex}}},
  author = {Buonomano, Dean V. and Merzenich, Michael M.},
  year = {1998},
  month = oct,
  volume = {80},
  pages = {1765--1774},
  issn = {0022-3077, 1522-1598},
  doi = {10.1152/jn.1998.80.4.1765},
  file = {1998 - Buonomano, Merzenich - Net interaction between different forms of short-term synaptic plasticity and slow-IPSPs in the hippocampu.pdf},
  journal = {Journal of Neurophysiology},
  language = {en},
  number = {4}
}

@article{Buonomano1999,
  title = {A {{Neural Network Model}} of {{Temporal Code Generation}} and {{Position}}-{{Invariant Pattern Recognition}}},
  author = {Buonomano, Dean V. and Merzenich, Michael},
  year = {1999},
  month = jan,
  volume = {11},
  pages = {103--116},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/089976699300016836},
  file = {1999 - Buonomano, Merzenich - A neural network model of temporal code generation and position- invariant pattern recognition.pdf},
  journal = {Neural Computation},
  language = {en},
  number = {1}
}

@article{Buonomano2009,
  title = {Harnessing {{Chaos}} in {{Recurrent Neural Networks}}},
  author = {Buonomano, Dean V.},
  year = {2009},
  month = aug,
  volume = {63},
  pages = {423--425},
  issn = {08966273},
  doi = {10.1016/j.neuron.2009.08.003},
  file = {2009 - Buonomano - Harnessing Chaos in Recurrent Neural Networks.pdf},
  journal = {Neuron},
  language = {en},
  number = {4}
}

@article{Burch1959,
  title = {Automatic Analysis of the Electroencephalogram: A Review and Classification of Systems},
  shorttitle = {Automatic Analysis of the Electroencephalogram},
  author = {Burch, Neil R},
  year = {1959},
  month = nov,
  volume = {11},
  pages = {827--834},
  issn = {00134694},
  doi = {10.1016/0013-4694(59)90133-6},
  file = {1958 - Burch - ANALYSIS OF THE ELECTROENCEPHALOGRAM.pdf;Burch - 1959 - Automatic analysis of the electroencephalogram a .pdf},
  journal = {Electroencephalography and Clinical Neurophysiology},
  language = {en},
  number = {4}
}

@article{Burda2018,
  title = {Large-{{Scale Study}} of {{Curiosity}}-{{Driven Learning}}},
  author = {Burda, Yuri and Edwards, Harri and Pathak, Deepak and Storkey, Amos and Darrell, Trevor and Efros, Alexei A.},
  year = {2018},
  month = aug,
  abstract = {Reinforcement learning algorithms rely on carefully engineering environment rewards that are extrinsic to the agent. However, annotating each environment with hand-designed, dense rewards is not scalable, motivating the need for developing reward functions that are intrinsic to the agent. Curiosity is a type of intrinsic reward function which uses prediction error as reward signal. In this paper: (a) We perform the first large-scale study of purely curiosity-driven learning, i.e. without any extrinsic rewards, across 54 standard benchmark environments, including the Atari game suite. Our results show surprisingly good performance, and a high degree of alignment between the intrinsic curiosity objective and the handdesigned extrinsic rewards of many game environments. (b) We investigate the effect of using different feature spaces for computing prediction error and show that random features are sufficient for many popular RL game benchmarks, but learned features appear to generalize better (e.g. to novel game levels in Super Mario Bros.). (c) We demonstrate limitations of the prediction-based rewards in stochastic setups. Game-play videos and code are at https://pathak22.github. io/large-scale-curiosity/.},
  archiveprefix = {arXiv},
  eprint = {1808.04355},
  eprinttype = {arxiv},
  file = {Burda et al. - 2018 - Large-Scale Study of Curiosity-Driven Learning.pdf},
  journal = {arXiv:1808.04355 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},
  language = {en},
  primaryclass = {cs, stat}
}

@article{Burda2018a,
  title = {Exploration by {{Random Network Distillation}}},
  author = {Burda, Yuri and Edwards, Harrison and Storkey, Amos and Klimov, Oleg},
  year = {2018},
  month = oct,
  abstract = {We introduce an exploration bonus for deep reinforcement learning methods that is easy to implement and adds minimal overhead to the computation performed. The bonus is the error of a neural network predicting features of the observations given by a fixed randomly initialized neural network. We also introduce a method to flexibly combine intrinsic and extrinsic rewards. We find that the random network distillation (RND) bonus combined with this increased flexibility enables significant progress on several hard exploration Atari games. In particular we establish state of the art performance on Montezuma's Revenge, a game famously difficult for deep reinforcement learning methods. To the best of our knowledge, this is the first method that achieves better than average human performance on this game without using demonstrations or having access to the underlying state of the game, and occasionally completes the first level.},
  archiveprefix = {arXiv},
  eprint = {1810.12894},
  eprinttype = {arxiv},
  file = {Burda et al. - 2018 - Exploration by Random Network Distillation.pdf},
  journal = {arXiv:1810.12894 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryclass = {cs, stat}
}

@article{Burgelman2002,
  title = {Strategy as {{Vector}} and the {{Inertia}} of {{Coevolutionary Lock}}-In},
  author = {Burgelman, Robert A.},
  year = {2002},
  month = jun,
  volume = {47},
  pages = {325},
  issn = {00018392},
  doi = {10.2307/3094808},
  file = {Burgelman - 2002 - Strategy as Vector and the Inertia of Coevolutiona.pdf},
  journal = {Administrative Science Quarterly},
  language = {en},
  number = {2}
}

@article{Burke1956,
  title = {The Electrical Properties of the Slow Muscle Fibre Membrane},
  author = {Burke, W. and Ginsborg, B. L.},
  year = {1956},
  month = jun,
  volume = {132},
  pages = {586--598},
  issn = {00223751},
  doi = {10.1113/jphysiol.1956.sp005551},
  file = {1956 - Burke, Ginsborg - THlE ELECTRICAL PROPERTIES OF THE SLOW MUSCLE FIBRE MEMBRANE From the Biophysics Department , University Colleg.pdf;Burke and Ginsborg - 1956 - The electrical properties of the slow muscle fibre.pdf},
  journal = {The Journal of Physiology},
  language = {en},
  number = {3}
}

@article{Burke2015,
  title = {Human Intracranial High-Frequency Activity during Memory Processing: Neural Oscillations or Stochastic Volatility?},
  shorttitle = {Human Intracranial High-Frequency Activity during Memory Processing},
  author = {Burke, John F and Ramayya, Ashwin G and Kahana, Michael J},
  year = {2015},
  month = apr,
  volume = {31},
  pages = {104--110},
  issn = {09594388},
  doi = {10.1016/j.conb.2014.09.003},
  file = {Burke et al. - 2015 - Human intracranial high-frequency activity during .pdf},
  journal = {Current Opinion in Neurobiology},
  language = {en}
}

@article{Burnham2004,
  title = {Multimodel {{Inference}}: {{Understanding AIC}} and {{BIC}} in {{Model Selection}}},
  shorttitle = {Multimodel {{Inference}}},
  author = {Burnham, Kenneth P. and Anderson, David R.},
  year = {2004},
  month = nov,
  volume = {33},
  pages = {261--304},
  issn = {0049-1241, 1552-8294},
  doi = {10.1177/0049124104268644},
  file = {2004 - Burnham - Multimodel Inference Understanding AIC and BIC in Model Selection.pdf},
  journal = {Sociological Methods \& Research},
  language = {en},
  number = {2}
}

@article{Burns2010,
  title = {Comparisons of the {{Dynamics}} of {{Local Field Potential}} and {{Multiunit Activity Signals}} in {{Macaque Visual Cortex}}},
  author = {Burns, S. P. and Xing, D. and Shapley, R. M.},
  year = {2010},
  month = oct,
  volume = {30},
  pages = {13739--13749},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.0743-10.2010},
  abstract = {The local field potential (LFP) and multi-unit activity (MUA) are extracellularly recorded signals that describe local neuronal network dynamics. In our experiments, the LFP and MUA, recorded from the same electrode in macaque V1 in response to drifting grating visual stimuli, were evaluated on coarse time-scales (\textasciitilde 1-5s) and fine time-scales ({$<$} 0.1s) . On coarse time-scales, MUA and the LFP both produced sustained visual responses to optimal and nonoptimal oriented visual stimuli. The sustainedness of the two signals across the population of recording sites was correlated (correlation coefficient \textasciitilde 0.4). At most recording sites the MUA was at least as sustained as the LFP and significantly more sustained for optimal orientations. In previous literature the BOLD (blood oxygen level dependent) signal of fMRI (functional magnetic resonance imaging) studies was found to be more strongly correlated with the LFP than with the MUA due to the lack of sustained response in the MUA signal. Since we found that MUA was as sustained as the LFP, MUA may also be correlated with BOLD. On fine time-scales, we computed the coherence between the LFP and MUA over the frequency range 10-150Hz. The LFP and MUA were weakly but significantly coherent (\textasciitilde{} 0.14) in the gamma-band (20-90Hz). The amount of gamma-band coherence was correlated with the power in the gamma-band of the LFP. The data were consistent with the proposal that the LFP and MUA are generated in a noisy, resonant cortical network.},
  file = {2012 - Manuscript - NIH Public Access.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {41}
}

@article{Burns2011,
  title = {Is {{Gamma}}-{{Band Activity}} in the {{Local Field Potential}} of {{V1 Cortex}} a "{{Clock}}" or {{Filtered Noise}}?},
  author = {Burns, S. P. and Xing, D. and Shapley, R. M.},
  year = {2011},
  month = jun,
  volume = {31},
  pages = {9658--9664},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.0660-11.2011},
  abstract = {Gamma-band (25\textendash 90Hz) peaks in local field potential (LFP) power spectra are present throughout the cerebral cortex and have been related to perception, attention, memory, and disorders e.g. schizophrenia and autism. It has been theorized gamma oscillations provide a `clock' for precise temporal encoding and `binding' of signals about stimulus features across brain regions. For gamma to function as a `clock' it must be autocoherent: phase and frequency conserved over a period of time. We computed phase and frequency trajectories of gamma-band bursts, using timefrequency analysis of LFPs recorded in macaque primary visual cortex (V1) during visual stimulation. The data were compared with simulations of random networks and clock signals in noise. Gamma-band bursts in LFP data were statistically indistinguishable from those found in filtered broadband noise. Therefore, V1 LFP data did not contain `clock'-like gamma-band signals. We consider possible functions for stochastic gamma-band activity, such as a synchronizing pulse signal.},
  file = {2012 - Manuscript - NIH Public Access(2).pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {26}
}

@article{Busemeyer2006,
  title = {Quantum Dynamics of Human Decision-Making},
  author = {Busemeyer, Jerome R. and Wang, Zheng and Townsend, James T.},
  year = {2006},
  month = jun,
  volume = {50},
  pages = {220--241},
  issn = {00222496},
  doi = {10.1016/j.jmp.2006.01.003},
  abstract = {A quantum dynamic model of decision-making is presented, and it is compared with a previously established Markov model. Both the quantum and the Markov models are formulated as random walk decision processes, but the probabilistic principles differ between the two approaches. Quantum dynamics describe the evolution of complex valued probability amplitudes over time, whereas Markov models describe the evolution of real valued probabilities over time. Quantum dynamics generate interference effects, which are not possible with Markov models. An interference effect occurs when the probability of the union of two possible paths is smaller than each individual path alone. The choice probabilities and distribution of choice response time for the quantum model are derived, and the predictions are contrasted with the Markov model.},
  file = {2006 - Busemeyer, Wang, Townsend - Quantum dynamics of human decision-making.pdf},
  journal = {Journal of Mathematical Psychology},
  language = {en},
  number = {3}
}

@article{Busemeyer2015,
  title = {What {{Is Quantum Cognition}}, and {{How Is It Applied}} to {{Psychology}}?},
  author = {Busemeyer, Jerome R. and Wang, Zheng},
  year = {2015},
  month = jun,
  volume = {24},
  pages = {163--169},
  issn = {0963-7214, 1467-8721},
  doi = {10.1177/0963721414568663},
  abstract = {Quantum cognition is a new research program that uses mathematical principles from quantum theory as a framework to explain human cognition, including judgment and decision making, concepts, reasoning, memory, and perception. This research is not concerned with whether the brain is a quantum computer. Instead, it uses quantum theory as a fresh conceptual framework and a coherent set of formal tools for explaining puzzling empirical findings in psychology. In this introduction, we focus on two quantum principles as examples to show why quantum cognition is an appealing new theoretical direction for psychology: complementarity, which suggests that some psychological measures have to be made sequentially and that the context generated by the first measure can influence responses to the next one, producing measurement order effects, and superposition, which suggests that some psychological states cannot be defined with respect to definite values but, instead, that all possible values within the superposition have some potential for being expressed. We present evidence showing how these two principles work together to provide a coherent explanation for many divergent and puzzling phenomena in psychology.},
  file = {2015 - Busemeyer, Wang - What Is Quantum Cognition, and How Is It Applied to Psychology.pdf;Busemeyer and Wang - 2015 - What Is Quantum Cognition, and How Is It Applied t.pdf},
  journal = {Current Directions in Psychological Science},
  language = {en},
  number = {3}
}

@article{Butson2007,
  title = {Patient-Specific Analysis of the Volume of Tissue Activated during Deep Brain Stimulation},
  author = {Butson, Christopher R. and Cooper, Scott E. and Henderson, Jaimie M. and McIntyre, Cameron C.},
  year = {2007},
  month = jan,
  volume = {34},
  pages = {661--670},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2006.09.034},
  file = {2007 - Butson et al. - Patient-specific analysis of the volume of tissue activated during deep brain stimulation.pdf},
  journal = {NeuroImage},
  language = {en},
  number = {2}
}

@article{Buzsaki1983,
  title = {Cellular Bases of Hippocampal {{EEG}} in the Behaving Rat},
  author = {Buzs{\'a}ki, Gy{\"o}rgy and {Lai-Wo S.}, Leung and Vanderwolf, Cornelius H.},
  year = {1983},
  month = oct,
  volume = {6},
  pages = {139--171},
  issn = {01650173},
  doi = {10.1016/0165-0173(83)90037-1},
  file = {1983 - Buzsáki, Leung, Vanderwolf - Cellular Bases of Hippocampal EEG in the Behaving Rat.pdf},
  journal = {Brain Research Reviews},
  language = {en},
  number = {2}
}

@article{Buzsaki2012,
  title = {The Origin of Extracellular Fields and Currents \textemdash{} {{EEG}}, {{ECoG}}, {{LFP}} and Spikes},
  author = {Buzs{\'a}ki, Gy{\"o}rgy and Anastassiou, Costas A. and Koch, Christof},
  year = {2012},
  month = jun,
  volume = {13},
  pages = {407--420},
  issn = {1471-003X, 1471-0048},
  doi = {10.1038/nrn3241},
  abstract = {Neuronal activity in the brain gives rise to transmembrane currents that can be measured in the extracellular medium. Although the major contributor of the extracellular signal is the synaptic transmembrane current, other sources \textemdash{} including Na+ and Ca2+ spikes, ionic fluxes through voltage- and ligand-gated channels, and intrinsic membrane oscillations \textemdash{} can substantially shape the extracellular field. High-density recordings of field activity in animals and subdural grid recordings in humans, combined with recently developed data processing tools and computational modelling, can provide insight into the cooperative behaviour of neurons, their average synaptic input and their spiking output, and can increase our understanding of how these processes contribute to the extracellular signal.},
  file = {2012 - Buzsáki, Anastassiou, Koch - The origin of extracellular fields and currents--EEG, ECoG, LFP and spikes.pdf},
  journal = {Nature Reviews Neuroscience},
  language = {en},
  number = {6}
}

@article{Buzsaki2014,
  title = {The Log-Dynamic Brain: How Skewed Distributions Affect Network Operations},
  shorttitle = {The Log-Dynamic Brain},
  author = {Buzs{\'a}ki, Gy{\"o}rgy and Mizuseki, Kenji},
  year = {2014},
  month = apr,
  volume = {15},
  pages = {264--278},
  issn = {1471-003X, 1471-0048},
  doi = {10.1038/nrn3687},
  abstract = {We often assume that the variables of functional and structural brain parameters \textemdash{} such as synaptic weights, the firing rates of individual neurons, the synchronous discharge of neural populations, the number of synaptic contacts between neurons and the size of dendritic boutons \textemdash{} have a bell-shaped distribution. However, at many physiological and anatomical levels in the brain, the distribution of numerous parameters is in fact strongly skewed with a heavy tail, suggesting that skewed (typically lognormal) distributions are fundamental to structural and functional brain organization. This insight not only has implications for how we should collect and analyse data, it may also help us to understand how the different levels of skewed distributions \textemdash{} from synapses to cognition \textemdash{} are related to each other.},
  file = {Buzsáki and Mizuseki - 2014 - The log-dynamic brain how skewed distributions af.pdf},
  journal = {Nat Rev Neurosci},
  language = {en},
  number = {4}
}

@article{Buzsaki2015,
  title = {What Does Gamma Coherence Tell Us about Inter-Regional Neural Communication?},
  author = {Buzs{\'a}ki, Gy{\"o}rgy and Schomburg, Erik W},
  year = {2015},
  month = apr,
  volume = {18},
  pages = {484--489},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.3952},
  file = {Buzsáki and Schomburg - 2015 - What does gamma coherence tell us about inter-regi.pdf},
  journal = {Nature Neuroscience},
  language = {en},
  number = {4}
}

@article{Buzsaki2015a,
  title = {Hippocampal Sharp Wave-Ripple: {{A}} Cognitive Biomarker for Episodic Memory and Planning: {{HIPPOCAMPAL SHARP WAVE}}-{{RIPPLE}}},
  shorttitle = {Hippocampal Sharp Wave-Ripple},
  author = {Buzs{\'a}ki, Gy{\"o}rgy},
  year = {2015},
  month = oct,
  volume = {25},
  pages = {1073--1188},
  issn = {10509631},
  doi = {10.1002/hipo.22488},
  abstract = {Sharp wave ripples (SPW-Rs) represent the most synchronous population pattern in the mammalian brain. Their excitatory output affects a wide area of the cortex and several subcortical nuclei. SPW-Rs occur during ``off-line'' states of the brain, associated with consummatory behaviors and non-REM sleep, and are influenced by numerous neurotransmitters and neuromodulators. They arise from the excitatory recurrent system of the CA3 region and the SPW-induced excitation brings about a fast network oscillation (ripple) in CA1. The spike content of SPW-Rs is temporally and spatially coordinated by a consortium of interneurons to replay fragments of waking neuronal sequences in a compressed format. SPW-Rs assist in transferring this compressed hippocampal representation to distributed circuits to support memory consolidation; selective disruption of SPW-Rs interferes with memory. Recently acquired and pre-existing information are combined during SPW-R replay to influence decisions, plan actions and, potentially, allow for creative thoughts. In addition to the widely studied contribution to memory, SPW-Rs may also affect endocrine function via activation of hypothalamic circuits. Alteration of the physiological mechanisms supporting SPW-Rs leads to their pathological conversion, ``p-ripples,'' which are a marker of epileptogenic tissue and can be observed in rodent models of schizophrenia and Alzheimer's Disease. Mechanisms for SPW-R genesis and function are discussed in this review. VC 2015 The Authors Hippocampus Published by Wiley Periodicals, Inc.},
  file = {Buzsáki - 2015 - Hippocampal sharp wave-ripple A cognitive biomark.pdf},
  journal = {Hippocampus},
  language = {en},
  number = {10}
}

@inproceedings{Bylander1997,
  title = {A Perceptron-like Online Algorithm for Tracking the Median},
  booktitle = {Proceedings of {{International Conference}} on {{Neural Networks}} ({{ICNN}}'97)},
  author = {Bylander, T. and Rosen, B.},
  year = {1997},
  volume = {4},
  pages = {2219--2224},
  publisher = {{IEEE}},
  address = {{Houston, TX, USA}},
  doi = {10.1109/ICNN.1997.614292},
  abstract = {We present an online algorithm for tracking the median of a series of values. The algorithm updates its current estimate of the median by incrementing or decrementing a fixed value, which is analogous to perceptron updating. The median value of a sequence minimizes the absolute loss, i.e., the sum of absolute deviations. Our analysis shows that the worst-case absolute loss of our algorithm is comparable to the absolute loss of any sequence of target medians, given restrictions on how much the target can change per trial.},
  file = {1997 - Bylander, Rosen - A perceptron-like online algorithm for tracking the median.pdf},
  isbn = {978-0-7803-4122-7},
  language = {en}
}

@article{Caballero2018,
  title = {A Probabilistic, Distributed, Recursive Mechanism for Decision-Making in the Brain},
  author = {Caballero, Javier A. and Humphries, Mark D. and Gurney, Kevin N.},
  editor = {Daunizeau, Jean},
  year = {2018},
  month = apr,
  volume = {14},
  pages = {e1006033},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1006033},
  abstract = {Decision formation recruits many brain regions, but the procedure they jointly execute is unknown. Here we characterize its essential composition, using as a framework a novel recursive Bayesian algorithm that makes decisions based on spike-trains with the statistics of those in sensory cortex (MT). Using it to simulate the random-dot-motion task, we demonstrate it quantitatively replicates the choice behaviour of monkeys, whilst predicting losses of otherwise usable information from MT. Its architecture maps to the recurrent corticobasal-ganglia-thalamo-cortical loops, whose components are all implicated in decision-making. We show that the dynamics of its mapped computations match those of neural activity in the sensorimotor cortex and striatum during decisions, and forecast those of basal ganglia output and thalamus. This also predicts which aspects of neural dynamics are and are not part of inference. Our single-equation algorithm is probabilistic, distributed, recursive, and parallel. Its success at capturing anatomy, behaviour, and electrophysiology suggests that the mechanism implemented by the brain has these same characteristics. Accepted: February 12, 2018 Published: April 3, 2018 Copyright: \textcopyright{} 2018 Caballero et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.},
  file = {Caballero et al. - 2018 - A probabilistic, distributed, recursive mechanism .pdf},
  journal = {PLOS Computational Biology},
  language = {en},
  number = {4}
}

@article{Caceres,
  title = {Towards a Realistic {{NNLIF}} Model: {{Analysis}} and Numerical Solver for Excitatory-Inhibitory Networks with Delay and Refractory Periods},
  author = {Caceres, Mar{\i}a J and Schneider, Ricarda},
  pages = {28},
  abstract = {The Network of Noisy Leaky Integrate and Fire (NNLIF) model describes the behavior of a neural network at mesoscopic level. It is one of the simplest self-contained mean-field models considered for that purpose. Even so, to study the mathematical properties of the model some simplifications were necessary [4, 5, 6], which disregard crucial phenomena. In this work we deal with the general NNLIF model without simplifications. It involves a network with two populations (excitatory and inhibitory), with transmission delays between the neurons and where the neurons remain in a refractory state for a certain time. We have studied the number of steady states in terms of the model parameters, the long time behaviour via the entropy method and Poincar\textasciiacute e's inequality, blow-up phenomena, and the importance of transmission delays between excitatory neurons to prevent blow-up and to give rise to synchronous solutions. Besides analytical results, we have presented a numerical resolutor for this model, based on high order flux-splitting WENO schemes and an explicit third order TVD Runge-Kutta method, in order to describe the wide range of phenomena exhibited by the network: blow-up, asynchronous/synchronous solutions and instability/stability of the steady states; the solver also allows us to observe the time evolution of the firing rates, refractory states and the probability distributions of the excitatory and inhibitory populations.},
  file = {Caceres and Schneider - Towards a realistic NNLIF model Analysis and nume.pdf},
  language = {en}
}

@article{Cachola2020,
  title = {{{TLDR}}: {{Extreme Summarization}} of {{Scientific Documents}}},
  shorttitle = {{{TLDR}}},
  author = {Cachola, Isabel and Lo, Kyle and Cohan, Arman and Weld, Daniel S.},
  year = {2020},
  month = may,
  abstract = {We introduce TLDR generation for scientific papers, a new automatic summarization task with high source compression, requiring expert background knowledge and complex language understanding. To facilitate research on this task, we introduce SciTLDR, a dataset of 3.9K TLDRs. Furthermore, we introduce a novel annotation protocol for scalably curating additional gold summaries by rewriting peer review comments. We use this protocol to augment our test set, yielding multiple gold TLDRs for evaluation, which is unlike most recent summarization datasets that assume only one valid gold summary. We present a training strategy for adapting pretrained language models that exploits similarities between TLDR generation and the related task of title generation, which outperforms strong extractive and abstractive summarization baselines.},
  archiveprefix = {arXiv},
  eprint = {2004.15011},
  eprinttype = {arxiv},
  file = {Cachola et al. - 2020 - TLDR Extreme Summarization of Scientific Document.pdf},
  journal = {arXiv:2004.15011 [cs]},
  keywords = {Computer Science - Computation and Language},
  language = {en},
  primaryclass = {cs}
}

@article{Calabresi2014,
  title = {Direct and Indirect Pathways of Basal Ganglia: A Critical Reappraisal},
  shorttitle = {Direct and Indirect Pathways of Basal Ganglia},
  author = {Calabresi, Paolo and Picconi, Barbara and Tozzi, Alessandro and Ghiglieri, Veronica and Di Filippo, Massimiliano},
  year = {2014},
  month = aug,
  volume = {17},
  pages = {1022--1030},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.3743},
  file = {2014 - Calabresi et al. - Direct and indirect pathways of basal ganglia a critical reappraisal.pdf;Calabresi et al. - 2014 - Direct and indirect pathways of basal ganglia a c.pdf},
  journal = {Nature Neuroscience},
  language = {en},
  number = {8}
}

@article{Calhoun2014,
  title = {Maximally Informative Foraging by {{Caenorhabditis}} Elegans},
  author = {Calhoun, Adam J and Chalasani, Sreekanth H and Sharpee, Tatyana O},
  year = {2014},
  month = dec,
  volume = {3},
  pages = {e04220},
  issn = {2050-084X},
  doi = {10.7554/eLife.04220},
  abstract = {Animals have evolved intricate search strategies to find new sources of food. Here, we analyze a complex food seeking behavior in the nematode Caenorhabditis elegans (C. elegans) to derive a general theory describing different searches. We show that C. elegans, like many other animals, uses a multi-stage search for food, where they initially explore a small area intensively (`local search') before switching to explore a much larger area (`global search'). We demonstrate that these search strategies as well as the transition between them can be quantitatively explained by a maximally informative search strategy, where the searcher seeks to continuously maximize information about the target. Although performing maximally informative search is computationally demanding, we show that a drift-diffusion model can approximate it successfully with just three neurons. Our study reveals how the maximally informative search strategy can be implemented and adopted to different search conditions.           ,              How an animal forages for food can make the difference between life and death, and there are several different searching strategies that may be adopted. Foraging could be more productive if animals could take into account any of the patterns with which food is distributed in their environment, but how much could they measure and memorize? Calhoun et al. show that a tiny worm called Caenorhabditis elegans can keep track of how its previous food finds were spread out, and uses this knowledge to optimize future searches for food.             When C. elegans forages, it begins by performing an intensive search of where it believes food is likely to be found. This strategy, called `local search', is characterised by the worm making numerous sharp turns that keep it in its target search area. If the worm has not found food after 15 min, it abruptly switches its behavior to a so-called `global search' strategy, which features fewer sharp turns and more forays into the surrounding area.             C. elegans is often thought to follow the smell of a food source in order to locate it. While reliable on small scale, this strategy can prove problematic when the distribution of food is patchy. Calhoun et al. show that in extreme conditions, such as when food is completely removed, the animals determine where and for how long to persist with their search based on their knowledge of what was typical of their environment. Such a strategy is called infotaxis, which literally means `guided by information'. While the neural circuits underpinning these behaviors remain to be found, Calhoun et al. propose a model that suggests that these circuits could be relatively simple, and made up of as few as three neurons.},
  file = {Calhoun et al. - 2014 - Maximally informative foraging by Caenorhabditis e.pdf},
  journal = {eLife},
  language = {en}
}

@article{Calhoun2015,
  title = {Neural {{Mechanisms}} for {{Evaluating Environmental Variability}} in {{Caenorhabditis}} Elegans},
  author = {Calhoun, Adam J. and Tong, Ada and Pokala, Navin and Fitzpatrick, James A.J. and Sharpee, Tatyana O. and Chalasani, Sreekanth H.},
  year = {2015},
  month = apr,
  volume = {86},
  pages = {428--441},
  issn = {08966273},
  doi = {10.1016/j.neuron.2015.03.026},
  abstract = {The ability to evaluate variability in the environment is vital for making optimal behavioral decisions. Here we show that Caenorhabditis elegans evaluates variability in its food environment and modifies its future behavior accordingly. We derive a behavioral model that reveals a critical period over which information about the food environment is acquired and predicts future search behavior. We also identify a pair of high-threshold sensory neurons that encode variability in food concentration and the downstream dopamine-dependent circuit that generates appropriate search behavior upon removal from food. Further, we show that CREB is required in a subset of interneurons and determines the timescale over which the variability is integrated. Interestingly, the variability circuit is a subset of a larger circuit driving search behavior, showing that learning directly modifies the very same neurons driving behavior. Our study reveals how a neural circuit decodes environmental variability to generate contextually appropriate decisions.},
  file = {Calhoun et al. - 2015 - Neural Mechanisms for Evaluating Environmental Var.pdf},
  journal = {Neuron},
  language = {en},
  number = {2}
}

@article{Callaway2000,
  title = {Network {{Robustness}} and {{Fragility}}: {{Percolation}} on {{Random Graphs}}},
  author = {Callaway, Duncan S and Newman, M E J and Strogatz, Steven H and Watts, Duncan J},
  year = {2000},
  volume = {85},
  pages = {4},
  file = {2000 - Callaway et al. - Network robustness and fragility percolation on random graphs.pdf},
  journal = {PHYSICAL REVIEW LETTERS},
  language = {en},
  number = {25}
}

@article{Callaway2001,
  title = {Are Randomly Grown Graphs Really Random?},
  author = {Callaway, Duncan S. and Hopcroft, John E. and Kleinberg, Jon M. and Newman, M. E. J. and Strogatz, Steven H.},
  year = {2001},
  month = sep,
  volume = {64},
  issn = {1063-651X, 1095-3787},
  doi = {10.1103/PhysRevE.64.041902},
  file = {2001 - Callaway et al. - Are randomly grown graphs really random.pdf},
  journal = {Physical Review E},
  language = {en},
  number = {4}
}

@article{Callaway2021,
  title = {Fixation Patterns in Simple Choice Reflect Optimal Information Sampling},
  author = {Callaway, Frederick and Rangel, Antonio and Griffiths, Thomas L.},
  editor = {Palminteri, Stefano},
  year = {2021},
  month = mar,
  volume = {17},
  pages = {e1008863},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1008863},
  abstract = {Simple choices (e.g., eating an apple vs. an orange) are made by integrating noisy evidence that is sampled over time and influenced by visual attention; as a result, fluctuations in visual attention can affect choices. But what determines what is fixated and when? To address this question, we model the decision process for simple choice as an information sampling problem, and approximate the optimal sampling policy. We find that it is optimal to sample from options whose value estimates are both high and uncertain. Furthermore, the optimal policy provides a reasonable account of fixations and choices in binary and trinary simple choice, as well as the differences between the two cases. Overall, the results show that the fixation process during simple choice is influenced dynamically by the value estimates computed during the decision process, in a manner consistent with optimal information sampling.},
  file = {Callaway et al. - 2021 - Fixation patterns in simple choice reflect optimal.pdf},
  journal = {PLoS Comput Biol},
  language = {en},
  number = {3}
}

@article{Callaway2021a,
  title = {Fixation Patterns in Simple Choice Reflect Optimal Information Sampling},
  author = {Callaway, Frederick and Rangel, Antonio and Griffiths, Thomas L.},
  editor = {Palminteri, Stefano},
  year = {2021},
  month = mar,
  volume = {17},
  pages = {e1008863},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1008863},
  abstract = {Simple choices (e.g., eating an apple vs. an orange) are made by integrating noisy evidence that is sampled over time and influenced by visual attention; as a result, fluctuations in visual attention can affect choices. But what determines what is fixated and when? To address this question, we model the decision process for simple choice as an information sampling problem, and approximate the optimal sampling policy. We find that it is optimal to sample from options whose value estimates are both high and uncertain. Furthermore, the optimal policy provides a reasonable account of fixations and choices in binary and trinary simple choice, as well as the differences between the two cases. Overall, the results show that the fixation process during simple choice is influenced dynamically by the value estimates computed during the decision process, in a manner consistent with optimal information sampling.},
  file = {Callaway et al. - 2021 - Fixation patterns in simple choice reflect optimal 2.pdf},
  journal = {PLoS Comput Biol},
  language = {en},
  number = {3}
}

@article{Camerer1998,
  title = {Experience-{{Weighted Attraction Learning}} in {{Coordination Games}}: {{Probability Rules}}, {{Heterogeneity}}, and {{Time}}-{{Variation}}},
  shorttitle = {Experience-{{Weighted Attraction Learning}} in {{Coordination Games}}},
  author = {Camerer, Colin and Ho, Teck-Hua},
  year = {1998},
  month = jun,
  volume = {42},
  pages = {305--326},
  issn = {00222496},
  doi = {10.1006/jmps.1998.1217},
  file = {1998 - Camerer, Ho - Experience-Weighted Attraction Learning in Coordination Games Probability Rules, Heterogeneity, and Time-Variation.pdf},
  journal = {Journal of Mathematical Psychology},
  language = {en},
  number = {2-3}
}

@article{Camerer1999,
  title = {Experience-Weighted {{Attraction Learning}} in {{Normal Form Games}}},
  author = {Camerer, Colin and Hua Ho, Teck},
  year = {1999},
  month = jul,
  volume = {67},
  pages = {827--874},
  issn = {0012-9682, 1468-0262},
  doi = {10.1111/1468-0262.00054},
  abstract = {In `experience-weighted attraction' \v{Z}EWA. learning, strategies have attractions that reflect initial predispositions, are updated based on payoff experience, and determine choice probabilities according to some rule \v{Z}e.g., logit.. A key feature is a parameter ? that weights the strength of hypothetical reinforcement of strategies that were not chosen according to the payoff they would have yielded, relative to reinforcement of chosen strategies according to received payoffs. The other key features are two discount rates, ␾ and ␳, which separately discount previous attractions, and an experience weight. EWA includes reinforcement learning and weighted fictitious play \v{Z}belief learning. as special cases, and hybridizes their key elements. When ? s 0 and ␳ s 0, cumulative choice reinforcement results. When ? s 1 and ␳ s ␾, levels of reinforcement of strategies are exactly the same as expected payoffs given weighted fictitious play beliefs. Using three sets of experimental data, parameter estimates of the model were calibrated on part of the data and used to predict a holdout sample. Estimates of ? are generally around .50, ␾ around .8᎐1, and ␳ varies from 0 to ␾. Reinforcement and belief-learning special cases are generally rejected in favor of EWA, though belief models do better in some constant-sum games. EWA is able to combine the best features of previous approaches, allowing attractions to begin and grow flexibly as choice reinforcement does, but reinforcing unchosen strategies substantially as belief-based models implicitly do.},
  file = {2003 - Camerer, Ho - Experience‐weighted Attraction Learning in Normal Form Games.pdf},
  journal = {Econometrica},
  language = {en},
  number = {4}
}

@article{Camerer2002,
  title = {Sophisticated {{Experience}}-{{Weighted Attraction Learning}} and {{Strategic Teaching}} in {{Repeated Games}}},
  author = {Camerer, Colin F. and Ho, Teck-Hua and Chong, Juin-Kuan},
  year = {2002},
  month = may,
  volume = {104},
  pages = {137--188},
  issn = {00220531},
  doi = {10.1006/jeth.2002.2927},
  file = {2002 - Camerer, Ho, Chong - Sophisticated experience-weighted attraction learning and strategic teaching in repeated games.pdf},
  journal = {Journal of Economic Theory},
  language = {en},
  number = {1}
}

@article{Camerer2003,
  title = {Behavioural Studies of Strategic Thinking in Games},
  author = {Camerer, Colin F.},
  year = {2003},
  month = may,
  volume = {7},
  pages = {225--231},
  issn = {13646613},
  doi = {10.1016/S1364-6613(03)00094-9},
  file = {2003 - Camerer - Behavioural studies of strategic thinking in games.pdf},
  journal = {Trends in Cognitive Sciences},
  language = {en},
  number = {5}
}

@article{Camley2018,
  title = {Collective Gradient Sensing and Chemotaxis: Modeling and Recent Developments},
  shorttitle = {Collective Gradient Sensing and Chemotaxis},
  author = {Camley, Brian A},
  year = {2018},
  month = jun,
  volume = {30},
  pages = {223001},
  issn = {0953-8984, 1361-648X},
  doi = {10.1088/1361-648X/aabd9f},
  file = {Camley - 2018 - Collective gradient sensing and chemotaxis modeli.pdf},
  journal = {Journal of Physics: Condensed Matter},
  language = {en},
  number = {22}
}

@article{Campbell2004,
  title = {Delayed {{Coupling Between Two Neural Network Loops}}},
  author = {Campbell, Sue Ann and Edwards, R. and {van den Driessche}, P.},
  year = {2004},
  month = jan,
  volume = {65},
  pages = {316--335},
  issn = {0036-1399, 1095-712X},
  doi = {10.1137/S0036139903434833},
  abstract = {Coupled loops with time delays are common in physiological systems such as neural networks. We study a Hopfield-type network that consists of a pair of one-way loops each with three neurons and two-way coupling (of either excitatory or inhibitory type) between a single neuron of each loop. Time delays are introduced in the connections between loops, and the effects of coupling strengths and delays on the network dynamics are investigated. These effects depend strongly on whether the coupling is symmetric (of the same type in both directions) or asymmetric (inhibitory in one direction and excitatory in the other). The network of six delay differential equations is studied by linear stability analysis and bifurcation theory. Loops having inherently stable zero solutions cannot be destabilized by weak coupling, regardless of the delay. Asymmetric coupling is weakly stabilizing but easily upset by delays. Symmetric coupling (if not too weak) can destabilize an inherently stable zero solution, leading to nontrivial fixed points if the gain of the neuron response function is not too negative or to oscillation otherwise. In the oscillation case, intermediate delays can restabilize the zero solution. At the borderline of the weak coupling region (symmetric or asymmetric), stability can change with delay ranges. When the coupling strengths are of the same magnitude, the oscillations of corresponding neurons in the two loops can be in phase, antiphase (symmetric coupling), or one quarter period out of phase (asymmetric coupling) depending on the delay.},
  file = {2015 - Campbell, Edwards, van den Driessche - Delayed Coupling between Two Neural Network Loops.pdf;Campbell et al. - 2004 - Delayed Coupling Between Two Neural Network Loops.pdf},
  journal = {SIAM Journal on Applied Mathematics},
  language = {en},
  number = {1}
}

@article{Canavier2015,
  title = {Phase-Resetting as a Tool of Information Transmission},
  author = {Canavier, Carmen C},
  year = {2015},
  month = apr,
  volume = {31},
  pages = {206--213},
  issn = {09594388},
  doi = {10.1016/j.conb.2014.12.003},
  file = {Canavier - 2015 - Phase-resetting as a tool of information transmiss.pdf},
  journal = {Current Opinion in Neurobiology},
  language = {en}
}

@article{Candes2008,
  title = {An {{Introduction To Compressive Sampling}}},
  author = {Candes, E.J. and Wakin, M.B.},
  year = {2008},
  month = mar,
  volume = {25},
  pages = {21--30},
  issn = {1053-5888},
  doi = {10.1109/MSP.2007.914731},
  file = {2008 - Candes, Wakin - An Introduction To Compressive Sampling.pdf},
  journal = {IEEE Signal Processing Magazine},
  language = {en},
  number = {2}
}

@article{Cannon2014,
  title = {{{LEMS}}: A Language for Expressing Complex Biological Models in Concise and Hierarchical Form and Its Use in Underpinning {{NeuroML}} 2},
  shorttitle = {{{LEMS}}},
  author = {Cannon, Robert C. and Gleeson, Padraig and Crook, Sharon and Ganapathy, Gautham and Marin, Boris and Piasini, Eugenio and Silver, R. Angus},
  year = {2014},
  month = sep,
  volume = {8},
  issn = {1662-5196},
  doi = {10.3389/fninf.2014.00079},
  abstract = {Computational models are increasingly important for studying complex neurophysiological systems. As scientific tools, it is essential that such models can be reproduced and critically evaluated by a range of scientists. However, published models are currently implemented using a diverse set of modeling approaches, simulation tools, and computer languages making them inaccessible and difficult to reproduce. Models also typically contain concepts that are tightly linked to domain-specific simulators, or depend on knowledge that is described exclusively in text-based documentation. To address these issues we have developed a compact, hierarchical, XML-based language called LEMS (Low Entropy Model Specification), that can define the structure and dynamics of a wide range of biological models in a fully machine readable format. We describe how LEMS underpins the latest version of NeuroML and show that this framework can define models of ion channels, synapses, neurons and networks. Unit handling, often a source of error when reusing models, is built into the core of the language by specifying physical quantities in models in terms of the base dimensions. We show how LEMS, together with the open source Java and Python based libraries we have developed, facilitates the generation of scripts for multiple neuronal simulators and provides a route for simulator free code generation. We establish that LEMS can be used to define models from systems biology and map them to neuroscience-domain specific simulators, enabling models to be shared between these traditionally separate disciplines. LEMS and NeuroML 2 provide a new, comprehensive framework for defining computational models of neuronal and other biological systems in a machine readable format, making them more reproducible and increasing the transparency and accessibility of their underlying structure and properties.},
  file = {2014 - Cannon et al. - LEMS a language for expressing complex biological models in concise and hierarchical form and its use in underpi.pdf;Cannon et al. - 2014 - LEMS a language for expressing complex biological.pdf},
  journal = {Frontiers in Neuroinformatics},
  language = {en}
}

@article{Cannon2015,
  title = {Neural {{Sequence Generation Using Spatiotemporal Patterns}} of {{Inhibition}}},
  author = {Cannon, Jonathan and Kopell, Nancy and Gardner, Timothy and Markowitz, Jeffrey},
  editor = {Sporns, Olaf},
  year = {2015},
  month = nov,
  volume = {11},
  pages = {e1004581},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1004581},
  file = {2015 - Cannon et al. - Neural Sequence Generation Using Spatiotemporal Patterns of Inhibition.pdf;Cannon et al. - 2015 - Neural Sequence Generation Using Spatiotemporal Pa.pdf},
  journal = {PLOS Computational Biology},
  language = {en},
  number = {11}
}

@article{Cannon2016,
  title = {Synaptic and Intrinsic Homeostasis Cooperate to Optimize Single Neuron Response Properties and Tune Integrator Circuits},
  author = {Cannon, Jonathan and Miller, Paul},
  year = {2016},
  month = nov,
  volume = {116},
  pages = {2004--2022},
  issn = {0022-3077, 1522-1598},
  doi = {10.1152/jn.00253.2016},
  file = {Cannon and Miller - 2016 - Synaptic and intrinsic homeostasis cooperate to op.pdf},
  journal = {Journal of Neurophysiology},
  language = {en},
  number = {5}
}

@article{Cannon2017,
  title = {Stable {{Control}} of {{Firing Rate Mean}} and {{Variance}} by {{Dual Homeostatic Mechanisms}}},
  author = {Cannon, Jonathan and Miller, Paul},
  year = {2017},
  month = dec,
  volume = {7},
  issn = {2190-8567},
  doi = {10.1186/s13408-017-0043-7},
  abstract = {Homeostatic processes that provide negative feedback to regulate neuronal firing rates are essential for normal brain function. Indeed, multiple parameters of individual neurons, including the scale of afferent synapse strengths and the densities of specific ion channels, have been observed to change on homeostatic time scales to oppose the effects of chronic changes in synaptic input. This raises the question of whether these processes are controlled by a single slow feedback variable or multiple slow variables. A single homeostatic process providing negative feedback to a neuron's firing rate naturally maintains a stable homeostatic equilibrium with a characteristic mean firing rate; but the conditions under which multiple slow feedbacks produce a stable homeostatic equilibrium have not yet been explored. Here we study a highly general model of homeostatic firing rate control in which two slow variables provide negative feedback to drive a firing rate toward two different target rates. Using dynamical systems techniques, we show that such a control system can be used to stably maintain a neuron's characteristic firing rate mean and variance in the face of perturbations, and we derive conditions under which this happens. We also derive expressions that clarify the relationship between the homeostatic firing rate targets and the resulting stable firing rate mean and variance. We provide specific examples of neuronal systems that can be effectively regulated by dual homeostasis. One of these examples is a recurrent excitatory network, which a dual feedback system can robustly tune to serve as an integrator.},
  file = {Cannon and Miller - 2017 - Stable Control of Firing Rate Mean and Variance by.pdf},
  journal = {The Journal of Mathematical Neuroscience},
  language = {en},
  number = {1}
}

@article{Canolty2006,
  title = {High {{Gamma Power Is Phase}}-{{Locked}} to {{Theta Oscillations}} in {{Human Neocortex}}},
  author = {Canolty, R. T. and Edwards, E. and Dalal, S. S. and Soltani, M. and Nagarajan, S. S. and Kirsch, H. E. and Berger, M. S. and Barbaro, N. M. and Knight, R. T.},
  year = {2006},
  month = sep,
  volume = {313},
  pages = {1626--1628},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1128115},
  file = {2006 - Canolty et al. - High gamma power is phase-locked to theta oscillations in human neocortex(2).pdf;2006 - Canolty et al. - High gamma power is phase-locked to theta oscillations in human neocortex(3).pdf;2009 - Canolty et al. - NIH Public Access.pdf},
  journal = {Science},
  language = {en},
  number = {5793}
}

@article{Canteras1990,
  title = {Afferent Connections of the Subthalamic Nucleus: A Combined Retrograde and Anterograde Horseradish Peroxidase Study in the Rat},
  shorttitle = {Afferent Connections of the Subthalamic Nucleus},
  author = {Canteras, Newton S. and {Shammah-Lagnado}, Sara J. and Silva, Bomfim A. and Ricardo, Juarez A.},
  year = {1990},
  month = apr,
  volume = {513},
  pages = {43--59},
  issn = {00068993},
  doi = {10.1016/0006-8993(90)91087-W},
  file = {1988 - Canteras et al. - Somatosensory Inputs To the Subthalamic Nucleus - a Combined Retrograde and Anterograde Horseradish-Peroxidase.pdf;Canteras et al. - 1990 - Afferent connections of the subthalamic nucleus a.pdf},
  journal = {Brain Research},
  language = {en},
  number = {1}
}

@article{Cantero2018,
  title = {Bundles of {{Brain Microtubules Generate Electrical Oscillations}}},
  author = {Cantero, Mar{\'i}a del Roc{\'i}o and Villa Etchegoyen, Cecilia and Perez, Paula L. and Scarinci, Noelia and Cantiello, Horacio F.},
  year = {2018},
  month = dec,
  volume = {8},
  issn = {2045-2322},
  doi = {10.1038/s41598-018-30453-2},
  file = {Cantero et al. - 2018 - Bundles of Brain Microtubules Generate Electrical .pdf},
  journal = {Scientific Reports},
  language = {en},
  number = {1}
}

@article{Cao2016,
  title = {Collective {{Activity}} of {{Many Bistable Assemblies Reproduces Characteristic Dynamics}} of {{Multistable Perception}}},
  author = {Cao, R. and Pastukhov, A. and Mattia, M. and Braun, J.},
  year = {2016},
  month = jun,
  volume = {36},
  pages = {6957--6972},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.4626-15.2016},
  file = {Cao et al. - 2016 - Collective Activity of Many Bistable Assemblies Re.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {26}
}

@article{Cao2016a,
  title = {Collective {{Activity}} of {{Many Bistable Assemblies Reproduces Characteristic Dynamics}} of {{Multistable Perception}}},
  author = {Cao, R. and Pastukhov, A. and Mattia, M. and Braun, J.},
  year = {2016},
  month = jun,
  volume = {36},
  pages = {6957--6972},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.4626-15.2016},
  file = {Cao et al. - 2016 - Collective Activity of Many Bistable Assemblies Re 2.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {26}
}

@article{Caraballo2001,
  title = {Attractors for {{Differential Equations}} with {{Variable Delays}}},
  author = {Caraballo, Tom{\'a}s and Langa, Jos{\'e} A. and Robinson, James C.},
  year = {2001},
  month = aug,
  volume = {260},
  pages = {421--438},
  issn = {0022247X},
  doi = {10.1006/jmaa.2000.7464},
  file = {2001 - Caraballo, Langa, Robinson - Attractors for Differential Equations with Variable Delays.pdf},
  journal = {Journal of Mathematical Analysis and Applications},
  language = {en},
  number = {2}
}

@article{Carandini2004,
  title = {Amplification of {{Trial}}-to-{{Trial Response Variability}} by {{Neurons}} in {{Visual Cortex}}},
  author = {Carandini, Matteo},
  editor = {{Charles Stevens}},
  year = {2004},
  month = aug,
  volume = {2},
  pages = {e264},
  issn = {1545-7885},
  doi = {10.1371/journal.pbio.0020264},
  file = {2004 - Carandini - Amplification of trial-to-trial response variability by neurons in visual cortex.pdf},
  journal = {PLoS Biology},
  language = {en},
  number = {9}
}

@article{Carandini2007,
  title = {Melting the {{Iceberg}}: {{Contrast Invariance}} in {{Visual Cortex}}},
  shorttitle = {Melting the {{Iceberg}}},
  author = {Carandini, Matteo},
  year = {2007},
  month = apr,
  volume = {54},
  pages = {11--13},
  issn = {08966273},
  doi = {10.1016/j.neuron.2007.03.019},
  file = {2007 - Carandini - Melting the Iceberg Contrast Invariance in Visual Cortex.pdf},
  journal = {Neuron},
  language = {en},
  number = {1}
}

@article{Cardin2005,
  title = {Stimulus-{{Dependent}} (30-50 {{Hz}}) {{Oscillations}} in {{Simple}} and {{Complex Fast Rhythmic Bursting Cells}} in {{Primary Visual Cortex}}},
  author = {Cardin, J. A.},
  year = {2005},
  month = jun,
  volume = {25},
  pages = {5339--5350},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.0374-05.2005},
  file = {2005 - Cardin, Palmer, Contreras - Stimulus-Dependent gamma (30-50 Hz) Oscillations in Simple and Complex Fast Rhythmic Bursting Cell.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {22}
}

@article{Cardin2016,
  title = {Snapshots of the {{Brain}} in {{Action}}: {{Local Circuit Operations}} through the {{Lens}} of {{Oscillations}}},
  shorttitle = {Snapshots of the {{Brain}} in {{Action}}},
  author = {Cardin, J. A.},
  year = {2016},
  month = oct,
  volume = {36},
  pages = {10496--10504},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.1021-16.2016},
  file = {Cardin - 2016 - Snapshots of the Brain in Action Local Circuit Op.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {41}
}

@article{Carlsson2020,
  title = {Topological Methods for Data Modelling},
  author = {Carlsson, Gunnar},
  year = {2020},
  month = dec,
  volume = {2},
  pages = {697--708},
  issn = {2522-5820},
  doi = {10.1038/s42254-020-00249-3},
  abstract = {The analysis of large and complex data sets is one of the most important problems facing the scientific community, and physics in particular. One response to this challenge has been the development of topological data analysis (TDA), which models data by graphs or networks rather than by linear algebraic (matrix) methods or cluster analysis. TDA represents the shape of the data (suitably defined) in a combinatorial fashion. Methods for measuring shape have been developed within mathematics, providing a toolkit referred to as homology. In working with data, one can use this kind of modelling to obtain an understanding of the overall structure of the data set. There is a suite of methods for constructing vector representations of various kinds of unstructured data. In this Review, we sketch the basics of TDA and provide examples where this kind of analysis has been carried out.},
  file = {Carlsson - 2020 - Topological methods for data modelling.pdf},
  journal = {Nat Rev Phys},
  language = {en},
  number = {12}
}

@article{Carp2010,
  title = {Age {{Differences}} in the {{Neural Representation}} of {{Working Memory Revealed}} by {{Multi}}-{{Voxel Pattern Analysis}}},
  author = {Carp, Joshua and Gmeindl, Leon and {Reuter-Lorenz}, Patricia A.},
  year = {2010},
  volume = {4},
  issn = {1662-5161},
  doi = {10.3389/fnhum.2010.00217},
  abstract = {Working memory function declines across the lifespan. Computational models of aging attribute such memory impairments to reduced distinctiveness between neural representations of different mental states in old age, a phenomenon termed dedifferentiation. These models predict that neural distinctiveness should be reduced uniformly across experimental conditions in older adults. In contrast, the Compensation-Related Utilization of Neural Circuits Hypothesis (CRUNCH) model predicts that the distinctiveness of neural representations should be increased in older adults (relative to young adults) at low levels of task demand but reduced at high levels of demand. The present study used multi-voxel pattern analysis to measure the effects of age and task demands on the distinctiveness of the neural representations of verbal and visuospatial working memory. Neural distinctiveness was estimated separately for memory encoding, maintenance, and retrieval, and for low, medium, and high memory loads. Results from sensory cortex during encoding and retrieval were consistent with the dedifferentiation hypothesis: distinctiveness of visual cortical representations during these phases was uniformly reduced in older adults, irrespective of memory load. However, maintenance-related responses in prefrontal and parietal regions yielded a strikingly different pattern of results. At low loads, older adults showed higher distinctiveness than younger adults; at high loads, this pattern reversed, such that distinctiveness was higher in young adults. This interaction between age group and memory load is at odds with the dedifferentiation hypothesis but consistent with CRUNCH. In sum, our results provide partial support for both dedifferentiation- and compensation-based models; we argue that comprehensive theories of cognitive aging must incorporate aspects of both models to fully explain complex patterns of age-related neuro-cognitive change.},
  file = {2010 - Carp, Gmeindl, Reuter-Lorenz - Age differences in the neural representation of working memory revealed by multi-voxel pattern ana.pdf},
  journal = {Frontiers in Human Neuroscience},
  language = {en}
}

@article{Carroll2019,
  title = {Mutual {{Information}} and the {{Edge}} of {{Chaos}} in {{Reservoir Computers}}},
  author = {Carroll, Thomas L.},
  year = {2019},
  month = jun,
  abstract = {A reservoir computer is a dynamical system that may be used to perform computations. A reservoir computer usually consists of a set of nonlinear nodes coupled together in a network so that there are feedback paths. Training the reservoir computer consists of inputing a signal of interest and fitting the time series signals of the reservoir computer nodes to a training signal that is related to the input signal. It is believed that dynamical systems function most efficiently as computers at the "edge of chaos", the point at which the largest Lyapunov exponent of the dynamical system transitions from negative to positive. In this work I simulate several different reservoir computers and ask if the best performance really does come at this edge of chaos. I find that while it is possible to get optimum performance at the edge of chaos, there may also be parameter values where the edge of chaos regime produces poor performance. This ambiguous parameter dependance has implications for building reservoir computers from analog physical systems, where the parameter range is restricted.},
  archiveprefix = {arXiv},
  eprint = {1906.03186},
  eprinttype = {arxiv},
  file = {Carroll - 2019 - Mutual Information and the Edge of Chaos in Reserv.pdf},
  journal = {arXiv:1906.03186 [nlin]},
  keywords = {Computer Science - Neural and Evolutionary Computing,Nonlinear Sciences - Adaptation and Self-Organizing Systems},
  language = {en},
  primaryclass = {nlin}
}

@article{Carthey2011,
  title = {Negotiating a Noisy, Information-Rich Environment in Search of Cryptic Prey: Olfactory Predators Need Patchiness in Prey Cues: {{Olfactory}} Predators Need Patchiness in Prey Cues},
  shorttitle = {Negotiating a Noisy, Information-Rich Environment in Search of Cryptic Prey},
  author = {Carthey, Alexandra J. R. and Bytheway, Jenna P. and Banks, Peter B.},
  year = {2011},
  month = jul,
  volume = {80},
  pages = {742--752},
  issn = {00218790},
  doi = {10.1111/j.1365-2656.2011.01817.x},
  file = {Carthey et al. - 2011 - Negotiating a noisy, information-rich environment .pdf},
  journal = {Journal of Animal Ecology},
  language = {en},
  number = {4}
}

@article{Carthey2011a,
  title = {Negotiating a Noisy, Information-Rich Environment in Search of Cryptic Prey: Olfactory Predators Need Patchiness in Prey Cues: {{Olfactory}} Predators Need Patchiness in Prey Cues},
  shorttitle = {Negotiating a Noisy, Information-Rich Environment in Search of Cryptic Prey},
  author = {Carthey, Alexandra J. R. and Bytheway, Jenna P. and Banks, Peter B.},
  year = {2011},
  month = jul,
  volume = {80},
  pages = {742--752},
  issn = {00218790},
  doi = {10.1111/j.1365-2656.2011.01817.x},
  file = {Carthey et al. - 2011 - Negotiating a noisy, information-rich environment  2.pdf},
  journal = {Journal of Animal Ecology},
  language = {en},
  number = {4}
}

@article{Carvalho2009,
  title = {Differential {{Effects}} of {{Excitatory}} and {{Inhibitory Plasticity}} on {{Synaptically Driven Neuronal Input}}-{{Output Functions}}},
  author = {Carvalho, Tiago P. and Buonomano, Dean V.},
  year = {2009},
  month = mar,
  volume = {61},
  pages = {774--785},
  issn = {08966273},
  doi = {10.1016/j.neuron.2009.01.013},
  abstract = {Ultimately, whether or not a neuron produces a spike determines its contribution to local computations. In response to brief stimuli the probability a neuron will fire can be described by its input-output function, which depends on the net balance and timing of excitatory and inhibitory currents. While excitatory and inhibitory synapses are plastic, most studies examine plasticity of subthreshold events. Thus, the effects of concerted regulation of excitatory and inhibitory synaptic strength on neuronal inputoutput functions are not well understood. Here, theoretical analyses reveal that excitatory synaptic strength controls the threshold of the neuronal input-output function, while inhibitory plasticity alters the threshold and gain. Experimentally, changes in the balance of excitation and inhibition in CA1 pyramidal neurons also altered their inputoutput function as predicted by the model. These results support the existence of two functional modes of plasticity that can be used to optimize information processing: threshold and gain plasticity.},
  file = {2009 - Carvalho, Buonomano - Differential Effects of Excitatory and Inhibitory Plasticity on Synaptically Driven Neuronal Input-Output F.pdf},
  journal = {Neuron},
  language = {en},
  number = {5}
}

@article{Casanova2008,
  title = {The Impact of Temporal Regularization on Estimates of the {{BOLD}} Hemodynamic Response Function: {{A}} Comparative Analysis},
  shorttitle = {The Impact of Temporal Regularization on Estimates of the {{BOLD}} Hemodynamic Response Function},
  author = {Casanova, Ramon and Ryali, Srikanth and Serences, John and Yang, Lucie and Kraft, Robert and Laurienti, Paul J. and Maldjian, Joseph A.},
  year = {2008},
  month = may,
  volume = {40},
  pages = {1606--1618},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2008.01.011},
  file = {2008 - Casanova et al. - The impact of temporal regularization on estimates of the BOLD hemodynamic response function a comparative anal.pdf},
  journal = {NeuroImage},
  language = {en},
  number = {4}
}

@article{Cataltepe1999,
  title = {No {{Free Lunch}} for {{Early Stopping}}},
  author = {Cataltepe, Zehra and {Abu-Mostafa}, Yaser S. and {Magdon-Ismail}, Malik},
  year = {1999},
  month = may,
  volume = {11},
  pages = {995--1009},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/089976699300016557},
  abstract = {We show that with a uniform prior on models having the same training error, early stopping at some fixed training error above the training error minimum results in an increase in the expected generalization error.},
  file = {Cataltepe et al. - 1999 - No Free Lunch for Early Stopping.pdf},
  journal = {Neural Computation},
  language = {en},
  number = {4}
}

@article{Cates2015,
  title = {Testing the Foundations of Classical Entropy: Colloid Experiments},
  shorttitle = {Celebrating {{{\emph{Soft Matter}}}} 's 10th Anniversary},
  author = {Cates, Michael E. and Manoharan, Vinothan N.},
  year = {2015},
  volume = {11},
  pages = {6538--6546},
  issn = {1744-683X, 1744-6848},
  doi = {10.1039/C5SM01014D},
  file = {2015 - Cates, Manoharan - Celebrating Soft Matter's 10th anniversary Testing the foundations of classical entropy colloid experiments(2).pdf;Cates and Manoharan - 2015 - Celebrating iSoft Matteri 's 10th anniversary 2.pdf;Cates and Manoharan - 2015 - Celebrating iSoft Matteri 's 10th anniversary.pdf},
  journal = {Soft Matter},
  language = {en},
  number = {33}
}

@article{Caze,
  title = {Dendrites Enable a Robust Mechanism for Neuronal Stimulus Selectivity},
  author = {Caze, Romain D and Jarvis, Sarah and Foust, Amanda J and Schultz, Simon R},
  pages = {16},
  abstract = {Hearing, vision, touch \textendash{} underlying all of these senses is stimulus selectivity, a robust information processing operation in which cortical neurons respond more to some stimuli than to others. Previous models assume that these neurons receive the highest weighted input from an ensemble encoding the preferred stimulus, but dendrites enable other possibilities. Non-linear dendritic processing can produce stimulus selectivity based on the spatial distribution of synapses, even if the total preferred stimulus weight does not exceed that of non-preferred stimuli. Using a multi-subunit non-linear model, we demonstrate that selectivity can arise from the spatial distribution of synapses.},
  file = {Caze et al. - Dendrites enable a robust mechanism for neuronal s.pdf},
  language = {en}
}

@article{Caze2013,
  title = {Passive {{Dendrites Enable Single Neurons}} to {{Compute Linearly Non}}-Separable {{Functions}}},
  author = {Caz{\'e}, Romain Daniel and Humphries, Mark and Gutkin, Boris},
  editor = {Sporns, Olaf},
  year = {2013},
  month = feb,
  volume = {9},
  pages = {e1002867},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1002867},
  abstract = {Local supra-linear summation of excitatory inputs occurring in pyramidal cell dendrites, the so-called dendritic spikes, results in independent spiking dendritic sub-units, which turn pyramidal neurons into two-layer neural networks capable of computing linearly non-separable functions, such as the exclusive OR. Other neuron classes, such as interneurons, may possess only a few independent dendritic sub-units, or only passive dendrites where input summation is purely sub-linear, and where dendritic sub-units are only saturating. To determine if such neurons can also compute linearly non-separable functions, we enumerate, for a given parameter range, the Boolean functions implementable by a binary neuron model with a linear sub-unit and either a single spiking or a saturating dendritic sub-unit. We then analytically generalize these numerical results to an arbitrary number of non-linear sub-units. First, we show that a single non-linear dendritic sub-unit, in addition to the somatic non-linearity, is sufficient to compute linearly non-separable functions. Second, we analytically prove that, with a sufficient number of saturating dendritic sub-units, a neuron can compute all functions computable with purely excitatory inputs. Third, we show that these linearly non-separable functions can be implemented with at least two strategies: one where a dendritic sub-unit is sufficient to trigger a somatic spike; another where somatic spiking requires the cooperation of multiple dendritic sub-units. We formally prove that implementing the latter architecture is possible with both types of dendritic sub-units whereas the former is only possible with spiking dendrites. Finally, we show how linearly non-separable functions can be computed by a generic two-compartment biophysical model and a realistic neuron model of the cerebellar stellate cell interneuron. Taken together our results demonstrate that passive dendrites are sufficient to enable neurons to compute linearly non-separable functions.},
  file = {Cazé et al. - 2013 - Passive Dendrites Enable Single Neurons to Compute.PDF},
  journal = {PLoS Computational Biology},
  language = {en},
  number = {2}
}

@techreport{Cazettes2021,
  title = {Reservoir of Decision Strategies in the Mouse Brain},
  author = {Cazettes, Fanny and Murakami, Masayoshi and Renart, Alfonso and Mainen, Zachary F.},
  year = {2021},
  month = apr,
  institution = {{Neuroscience}},
  doi = {10.1101/2021.04.01.438090},
  abstract = {Decision making strategies guided by observable stimuli and those that also require inferences about unobserved states have been linked to distinct computational requirements and neural substrates. Here, we formulate a model based on temporal integration and reset that incorporates both strategies into a unified family of decision algorithms. We show, using recordings from the frontal cortex of mice performing a foraging task, that the entire family of algorithms can be simultaneously decoded from the same neural ensemble, regardless of the one concurrently executed by the mice. Thus, using multiplexed integration, the cortex may avoid premature commitment to a single algorithm and maintain multiple decision strategies in parallel.},
  file = {Cazettes et al. - 2021 - Reservoir of decision strategies in the mouse brai.pdf},
  language = {en},
  type = {Preprint}
}

@article{Cervera2020,
  title = {Systems Neuroscience of Curiosity},
  author = {Cervera, Roberto Lopez and Wang, Maya Zhe and Hayden, Benjamin Y},
  year = {2020},
  month = oct,
  volume = {35},
  pages = {48--55},
  issn = {23521546},
  doi = {10.1016/j.cobeha.2020.06.011},
  file = {Cervera et al. - 2020 - Systems neuroscience of curiosity.pdf},
  journal = {Current Opinion in Behavioral Sciences},
  language = {en}
}

@article{Cesa-Bianchi,
  title = {Boltzmann {{Exploration Done Right}}},
  author = {{Cesa-Bianchi}, Nicol{\`o} and Lugosi, G{\'a}bor and Gentile, Claudio},
  pages = {15},
  file = {Cesa-Bianchi et al. - Boltzmann Exploration Done Right.pdf},
  language = {en}
}

@techreport{Cesario2019,
  title = {Your {{Brain Is Not}} an {{Onion}} with a {{Tiny Reptile Inside}}},
  author = {Cesario, Joseph and Johnson, David Jeffrey and Eisthen, Heather},
  year = {2019},
  month = dec,
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/x83dq},
  abstract = {A widespread misconception in much of psychology holds that (1) as vertebrate animals evolved, "newer" brain structures were added over existing "older" brain structures and (2) these newer, more complex structures endowed animals with newer and more complex psychological functions, behavioral flexibility, and language. This belief, though widely shared in our introductory textbooks, has long been discredited among neurobiologists and stands in contrast to the clear and unanimous agreement on these issues among those studying nervous system evolution. We bring psychologists up to date on this issue by describing the more accurate model of neural evolution, and we provide examples of how this inaccurate view may have impeded progress in psychology. We urge psychologists to abandon this mistaken view of human brains.},
  file = {Cesario et al. - 2019 - Your Brain Is Not an Onion with a Tiny Reptile Ins.pdf},
  language = {en},
  type = {Preprint}
}

@article{Chadwick2010,
  title = {Decoding {{Individual Episodic Memory Traces}} in the {{Human Hippocampus}}},
  author = {Chadwick, Martin J. and Hassabis, Demis and Weiskopf, Nikolaus and Maguire, Eleanor A.},
  year = {2010},
  month = mar,
  volume = {20},
  pages = {544--547},
  issn = {09609822},
  doi = {10.1016/j.cub.2010.01.053},
  abstract = {In recent years, multivariate pattern analyses have been performed on functional magnetic resonance imaging (fMRI) data, permitting prediction of mental states from local patterns of blood oxygen-level-dependent (BOLD) signal across voxels [1, 2]. We previously demonstrated that it is possible to predict the position of individuals in a virtualreality environment from the pattern of activity across voxels in the hippocampus [3]. Although this shows that spatial memories can be decoded, substantially more challenging, and arguably only possible to investigate in humans [4], is whether it is feasible to predict which complex everyday experience, or episodic memory, a person is recalling. Here we document for the first time that traces of individual rich episodic memories are detectable and distinguishable solely from the pattern of fMRI BOLD signals across voxels in the human hippocampus. In so doing, we uncovered a possible functional topography in the hippocampus, with preferential episodic processing by some hippocampal regions over others. Moreover, our results imply that the neuronal traces of episodic memories are stable (and thus predictable) even over many re-activations. Finally, our data provide further evidence for functional differentiation within the medial temporal lobe, in that we show the hippocampus contains significantly more episodic information than adjacent structures.},
  file = {2010 - Chadwick et al. - Decoding individual episodic memory traces in the human hippocampus.pdf},
  journal = {Current Biology},
  language = {en},
  number = {6}
}

@article{Chakravorty,
  title = {Information {{Space Receding Horizon Control}}},
  author = {Chakravorty, Suman and Erwin, R Scott},
  pages = {8},
  abstract = {In this paper, we present a receding horizon solution to the problem of optimal sensor scheduling problem. The optimal sensor scheduling problem can be posed as a Partially Observed Markov Decision Process (POMDP) whose solution is given by an Information Space (I-space) Dynamic Programming (DP) problem. We present a simulation based stochastic optimization technique that, combined with a receding horizon approach, obviates the need to solve the computationally intractable I-space DP problem. The technique is tested on a simple sensor scheduling problem where a sensor has to choose among the measurements of N dynamical systems such that the information regarding the aggregate system is maximized over an infinite horizon.},
  file = {Chakravorty and Erwin - Information Space Receding Horizon Control.pdf},
  language = {en}
}

@incollection{Chakravorty2014,
  title = {Multi-{{Armed Bandits}}, {{Gittins Index}}, and Its {{Calculation}}},
  booktitle = {Methods and {{Applications}} of {{Statistics}} in {{Clinical Trials}}},
  author = {Chakravorty, Jhelum and Mahajan, Aditya},
  editor = {Balakrishnan, N.},
  year = {2014},
  month = jun,
  pages = {416--435},
  publisher = {{John Wiley \& Sons, Inc.}},
  address = {{Hoboken, NJ, USA}},
  doi = {10.1002/9781118596333.ch24},
  file = {Chakravorty and Mahajan - 2014 - Multi-Armed Bandits, Gittins Index, and its Calcul.pdf},
  isbn = {978-1-118-59633-3 978-1-118-30476-1},
  language = {en}
}

@article{Chambers2012,
  title = {Parametric Computation Predicts a Multiplicative Interaction between Synaptic Strength Parameters That Control Gamma Oscillations},
  author = {Chambers, Jordan D. and Bethwaite, Blair and Diamond, Neil T. and Peachey, Tom and Abramson, David and Petrou, Steve and Thomas, Evan A.},
  year = {2012},
  volume = {6},
  issn = {1662-5188},
  doi = {10.3389/fncom.2012.00053},
  abstract = {Gamma oscillations are thought to be critical for a number of behavioral functions, they occur in many regions of the brain and through a variety of mechanisms. Fast repetitive bursting (FRB) neurons in layer 2 of the cortex are able to drive gamma oscillations over long periods of time. Even though the oscillation is driven by FRB neurons, strong feedback within the rest of the cortex must modulate properties of the oscillation such as frequency and power. We used a highly detailed model of the cortex to determine how a cohort of 33 parameters controlling synaptic drive might modulate gamma oscillation properties. We were interested in determining not just the effects of parameters individually, but we also wanted to reveal interactions between parameters beyond additive effects. To prevent a combinatorial explosion in parameter combinations that might need to be simulated, we used a fractional factorial design (FFD) that estimated the effects of individual parameters and two parameter interactions. This experiment required only 4096 model runs. We found that the largest effects on both gamma power and frequency came from a complex interaction between efficacy of synaptic connections from layer 2 inhibitory neurons to layer 2 excitatory neurons and the parameter for the reciprocal connection. As well as the effect of the individual parameters determining synaptic efficacy, there was an interaction between these parameters beyond the additive effects of the parameters alone. The magnitude of this effect was similar to that of the individual parameters, predicting that it is physiologically important in setting gamma oscillation properties.},
  file = {2012 - Chambers et al. - Parametric computation predicts a multiplicative interaction between synaptic strength parameters that control.pdf},
  journal = {Frontiers in Computational Neuroscience},
  language = {en}
}

@article{Chance,
  title = {Divisive Inhibition in Recurrent Networks},
  author = {Chance, Frances S and Abbott, L F},
  pages = {11},
  abstract = {Models of visual cortex suggest that response selectivity can arise from recurrent networks operating at high gain. However, such networks have a number of problematic features: (i) they operate perilously close to a point of instability, (ii) small changes in synaptic strength can dramatically modify the degree of amplification, and (iii) they respond slowly to rapidly changing stimuli. Divisive inhibition, acting through interneurons that are themselves divisively inhibited, can solve these problems without degrading the selectivity of a recurrent network.},
  file = {2000 - Chance, Abbott - Divisive inhibition in recurrent networks.pdf},
  language = {en}
}

@article{ChandranKS2016,
  title = {Comparison of {{Matching Pursuit Algorithm}} with {{Other Signal Processing Techniques}} for {{Computation}} of the {{Time}}-{{Frequency Power Spectrum}} of {{Brain Signals}}},
  author = {Chandran KS, S. and Mishra, A. and Shirhatti, V. and Ray, S.},
  year = {2016},
  month = mar,
  volume = {36},
  pages = {3399--3408},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.3633-15.2016},
  file = {Chandran KS et al. - 2016 - Comparison of Matching Pursuit Algorithm with Othe.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {12}
}

@article{Chandrasekaran2018,
  title = {Brittleness in Model Selection Analysis of Single Neuron Firing Rates},
  author = {Chandrasekaran, Chandramouli and {Soldado-Magraner}, Joana and Peixoto, Diogo and Newsome, William T and Shenoy, Krishna and Sahani, Maneesh},
  year = {2018},
  month = sep,
  doi = {10.1101/430710},
  abstract = {Models of complex heterogeneous systems like the brain are inescapably incomplete, and thus always falsified with enough data. As neural data grow in volume and complexity, absolute measures of adequacy are being replaced by model selection methods that rank the relative accuracy of competing theories. Selection still depends on incomplete mathematical instantiations, but the implicit expectation is that ranking is robust to their details. Here we highlight a contrary finding of ``brittleness,'' where data matching one theory conceptually are ranked closer to an instance of another. In particular, selection between recent models of decision making is conceptually misleading when data are simulated with minor distributional mismatch, with mixed secondary signals, or with non-stationary parameters; and decision-related responses in macaque cortex show features suggesting that these effects may impact empirical results. We conclude with recommendations to mitigate such brittleness when using model selection to study neural signals.},
  file = {Chandrasekaran et al. - 2018 - Brittleness in model selection analysis of single .pdf},
  journal = {bioRxiv},
  language = {en}
}

@article{Chang2017,
  title = {The {{Code}} for {{Facial Identity}} in the {{Primate Brain}}},
  author = {Chang, Le and Tsao, Doris Y.},
  year = {2017},
  month = jun,
  volume = {169},
  pages = {1013-1028.e14},
  issn = {00928674},
  doi = {10.1016/j.cell.2017.05.011},
  abstract = {Primates recognize complex objects such as faces with remarkable speed and reliability. Here, we reveal the brain's code for facial identity. Experiments in macaques demonstrate an extraordinarily simple transformation between faces and responses of cells in face patches. By formatting faces as points in a high-dimensional linear space, we discovered that each face cell's firing rate is proportional to the projection of an incoming face stimulus onto a single axis in this space, allowing a face cell ensemble to encode the location of any face in the space. Using this code, we could precisely decode faces from neural population responses and predict neural firing rates to faces. Furthermore, this code disavows the long-standing assumption that face cells encode specific facial identities, confirmed by engineering faces with drastically different appearance that elicited identical responses in single face cells. Our work suggests that other objects could be encoded by analogous metric coordinate systems.},
  file = {Chang and Tsao - 2017 - The Code for Facial Identity in the Primate Brain.pdf},
  journal = {Cell},
  language = {en},
  number = {6}
}

@article{Charness,
  title = {When {{Optimal Choices Feel Wrong}}: {{A Laboratory Study}} of {{Bayesian Updating}}, {{Complexity}}, and {{Affect}}},
  author = {Charness, Gary and Levin, Dan},
  pages = {41},
  abstract = {We examine decision-making under risk and uncertainty in a laboratory experiment. The heart of our design examines how one's propensity to use Bayes' rule is affected by whether this rule is aligned with reinforcement or clashes with it. In some cases, we create environments where Bayesian updating after a successful outcome should lead a decision-maker to make a change, while no change should be made after observing an unsuccessful outcome.},
  file = {2005 - Charness, Levin - When optimal choices feel wrong A laboratory study of bayesian updating, complexity, and affect.pdf},
  language = {en}
}

@article{Chaslot,
  title = {Monte-{{Carlo Tree Search}}: {{A New Framework}} for {{Game AI}}},
  author = {Chaslot, Guillaume and Bakkes, Sander and Szita, Istvan and Spronck, Pieter},
  pages = {2},
  abstract = {Classic approaches to game AI require either a high quality of domain knowledge, or a long time to generate effective AI behaviour. These two characteristics hamper the goal of establishing challenging game AI. In this paper, we put forward Monte-Carlo Tree Search as a novel, unified framework to game AI. In the framework, randomized explorations of the search space are used to predict the most promising game actions. We will demonstrate that Monte-Carlo Tree Search can be applied effectively to (1) classic board-games, (2) modern board-games, and (3) video games.},
  file = {Chaslot et al. - Monte-Carlo Tree Search A New Framework for Game .pdf},
  language = {en}
}

@article{Chaturvedi2012,
  title = {Current Steering to Activate Targeted Neural Pathways during Deep Brain Stimulation of the Subthalamic Region},
  author = {Chaturvedi, Ashutosh and Foutz, Thomas J. and McIntyre, Cameron C.},
  year = {2012},
  month = jul,
  volume = {5},
  pages = {369--377},
  issn = {1935861X},
  doi = {10.1016/j.brs.2011.05.002},
  file = {2012 - Chaturvedi, Foutz, McIntyre - Current steering to activate targeted neural pathways during deep brain stimulation of the subthala.pdf},
  journal = {Brain Stimulation},
  language = {en},
  number = {3}
}

@article{Chaturvedi2013,
  title = {Artificial Neural Network Based Characterization of the Volume of Tissue Activated during Deep Brain Stimulation},
  author = {Chaturvedi, Ashutosh and Luj{\'a}n, J Luis and McIntyre, Cameron C},
  year = {2013},
  month = oct,
  volume = {10},
  pages = {056023},
  issn = {1741-2560, 1741-2552},
  doi = {10.1088/1741-2560/10/5/056023},
  abstract = {Objective. Clinical deep brain stimulation (DBS) systems can be programmed with thousands of different stimulation parameter combinations (e.g. electrode contact(s), voltage, pulse width, frequency). Our goal was to develop novel computational tools to characterize the effects of stimulation parameter adjustment for DBS. Approach. The volume of tissue activated (VTA) represents a metric used to estimate the spatial extent of DBS for a given parameter setting. Traditional methods for calculating the VTA rely on activation function (AF)-based approaches and tend to overestimate the neural response when stimulation is applied through multiple electrode contacts. Therefore, we created a new method for VTA calculation that relied on artificial neural networks (ANNs). Main results. The ANN-based predictor provides more accurate descriptions of the spatial spread of activation compared to AF-based approaches for monopolar stimulation. In addition, the ANN was able to accurately estimate the VTA in response to multi-contact electrode configurations. Significance. The ANN-based approach may represent a useful method for fast computation of the VTA in situations with limited computational resources, such as a clinical DBS programming application on a tablet computer.},
  file = {2013 - Chaturvedi, Luján, McIntyre - Artificial neural network based characterization of the volume of tissue activated during deep bra.pdf},
  journal = {Journal of Neural Engineering},
  language = {en},
  number = {5}
}

@article{Chaudhuri2014,
  title = {A Diversity of Localized Timescales in Network Activity},
  author = {Chaudhuri, Rishidev and Bernacchia, Alberto and Wang, Xiao-Jing},
  year = {2014},
  month = jan,
  volume = {3},
  issn = {2050-084X},
  doi = {10.7554/eLife.01239},
  abstract = {Neurons show diverse timescales, so that different parts of a network respond with disparate temporal dynamics. Such diversity is observed both when comparing timescales across brain areas and among cells within local populations; the underlying circuit mechanism remains unknown. We examine conditions under which spatially local connectivity can produce such diverse temporal behavior.},
  file = {2014 - Chaudhuri, Bernacchia, Wang - A diversity of localized timescales in network activity.pdf;2014 - Chaudhuri, Bernacchia, Wang - A diversity of localized timescales in network activity(2).pdf;Chaudhuri et al. - 2014 - A diversity of localized timescales in network act 2.pdf;Chaudhuri et al. - 2014 - A diversity of localized timescales in network act.pdf},
  journal = {eLife},
  language = {en}
}

@article{Chaudhuri2015,
  title = {A {{Large}}-{{Scale Circuit Mechanism}} for {{Hierarchical Dynamical Processing}} in the {{Primate Cortex}}},
  author = {Chaudhuri, Rishidev and Knoblauch, Kenneth and Gariel, Marie-Alice and Kennedy, Henry and Wang, Xiao-Jing},
  year = {2015},
  month = oct,
  volume = {88},
  pages = {419--431},
  issn = {08966273},
  doi = {10.1016/j.neuron.2015.09.008},
  abstract = {We developed a large-scale dynamical model of the macaque neocortex, which is based on recently acquired directed- and weighted-connectivity data from tract-tracing experiments, and which incorporates heterogeneity across areas. A hierarchy of timescales naturally emerges from this system: sensory areas show brief, transient responses to input (appropriate for sensory processing), whereas association areas integrate inputs over time and exhibit persistent activity (suitable for decision-making and working memory). The model displays multiple temporal hierarchies, as evidenced by contrasting responses to visual versus somatosensory stimulation. Moreover, slower prefrontal and temporal areas have a disproportionate impact on global brain dynamics. These findings establish a circuit mechanism for ``temporal receptive windows'' that are progressively enlarged along the cortical hierarchy, suggest an extension of time integration in decision making from local to large circuits, and should prompt a reevaluation of the analysis of functional connectivity (measured by fMRI or electroencephalography/magnetoencephalography) by taking into account interareal heterogeneity.},
  file = {2015 - Chaudhuri et al. - A Large-Scale Circuit Mechanism for Hierarchical Dynamical Processing in the Primate Cortex.pdf;Chaudhuri et al. - 2015 - A Large-Scale Circuit Mechanism for Hierarchical D 2.pdf;Chaudhuri et al. - 2015 - A Large-Scale Circuit Mechanism for Hierarchical D.pdf},
  journal = {Neuron},
  language = {en},
  number = {2}
}

@article{Chaudhuri2016,
  title = {Random Recurrent Networks near Criticality Capture the Broadband Power Distribution of Human {{ECoG}} Dynamics},
  author = {Chaudhuri, Rishidev and He, Biyu and Wang, Xiao-Jing},
  year = {2016},
  month = jan,
  doi = {10.1101/036228},
  abstract = {The power spectrum of brain electric field potential recordings is dominated by an arrhythmic broadband signal but a mechanistic account of its underlying neural network dynamics is lacking. Here we show how the broadband power spectrum of field potential recordings can be explained by a simple random network of nodes near criticality. Such a recurrent network produces activity with a combination of a fast and a slow autocorrelation time constant, with the fast mode corresponding to local dynamics and the slow mode resulting from recurrent excitatory connections across the network. These modes are combined to produce a power spectrum similar to that observed in human intracranial EEG (i.e., electrocorticography, ECoG) recordings. Moreover, such a network naturally converts input correlations across nodes into temporal autocorrelation of the network activity. Consequently, increased independence between nodes results in a reduction in low-frequency power, which offers a possible explanation for observed changes in ECoG power spectra during task performance. Lastly, changes in network coupling produce changes in network activity power spectra reminiscent of those seen in human ECoG recordings across different arousal states. This model thus links macroscopic features of the empirical ECoG power spectrum to a parsimonious underlying network structure and proposes potential mechanisms for changes in ECoG power spectra observed across behavioral and arousal states. This provides a computational framework within which to generate and test hypotheses about the cellular and network mechanisms underlying whole brain electrical dynamics, their variations across behavioral states as well as abnormalities associated with brain diseases.},
  file = {Chaudhuri et al. - 2016 - Random recurrent networks near criticality capture.pdf},
  journal = {bioRxiv},
  language = {en}
}

@article{Chehelcheraghi2016,
  title = {A Neural Mass Model of Phase\textendash Amplitude Coupling},
  author = {Chehelcheraghi, Mojtaba and Nakatani, Chie and Steur, Erik and {van Leeuwen}, Cees},
  year = {2016},
  month = jun,
  volume = {110},
  pages = {171--192},
  issn = {0340-1200, 1432-0770},
  doi = {10.1007/s00422-016-0687-5},
  abstract = {Brain activity shows phase\textendash amplitude coupling between its slow and fast oscillatory components. We study phase\textendash amplitude coupling as recorded at individual sites, using a modified version of the well-known Wendling neural mass model. To the population of fast inhibitory interneurons of this model, we added external modulatory input and dynamic self-feedback. These two modifications together are sufficient to let the inhibitory population serve as a limit-cycle oscillator, with frequency characteristics comparable to the beta and gamma bands. The frequency and power of these oscillations can be tuned through the time constant of the dynamic and modulatory input. Alpha band activity is generated, as is usual in such models, as a result of interactions of pyramidal neurons and a population of slow inhibitory interneurons. The slow inhibitory population activity directly influences the fast oscillations via the synaptic gain between slow and fast inhibitory populations. As a result, the amplitude envelope of the fast oscillation is coupled to the phase of the slow activity; this result is consistent with the notion that phase\textendash amplitude coupling is effectuated by interactions between inhibitory interneurons.},
  file = {Chehelcheraghi et al. - 2016 - A neural mass model of phase–amplitude coupling.pdf},
  journal = {Biological Cybernetics},
  language = {en},
  number = {2-3}
}

@article{Chelaru2008,
  title = {Efficient Coding in Heterogeneous Neuronal Populations},
  author = {Chelaru, M. I. and Dragoi, V.},
  year = {2008},
  month = oct,
  volume = {105},
  pages = {16344--16349},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.0807744105},
  file = {Chelaru and Dragoi - 2008 - Efficient coding in heterogeneous neuronal populat.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {42}
}

@article{Chen,
  title = {Neural {{Ordinary Differential Equations}}},
  author = {Chen, Tian Qi and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David K},
  pages = {12},
  abstract = {We introduce a new family of deep neural network models. Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of the hidden state using a neural network. The output of the network is computed using a blackbox differential equation solver. These continuous-depth models have constant memory cost, adapt their evaluation strategy to each input, and can explicitly trade numerical precision for speed. We demonstrate these properties in continuous-depth residual networks and continuous-time latent variable models. We also construct continuous normalizing flows, a generative model that can train by maximum likelihood, without partitioning or ordering the data dimensions. For training, we show how to scalably backpropagate through any ODE solver, without access to its internal operations. This allows end-to-end training of ODEs within larger models.},
  file = {Chen et al. - Neural Ordinary Differential Equations.pdf},
  language = {en}
}

@article{Chen2013,
  title = {The {{Role}} of {{Coincidence}}-{{Detector Neurons}} in the {{Reliability}} and {{Precision}} of {{Subthreshold Signal Detection}} in {{Noise}}},
  author = {Chen, Yueling and Zhang, Hui and Wang, Hengtong and Yu, Lianchun and Chen, Yong},
  editor = {Chacron, Maurice J.},
  year = {2013},
  month = feb,
  volume = {8},
  pages = {e56822},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0056822},
  abstract = {Subthreshold signal detection is an important task for animal survival in complex environments, where noise increases both the external signal response and the spontaneous spiking of neurons. The mechanism by which neurons process the coding of signals is not well understood. Here, we propose that coincidence detection, one of the ways to describe the functionality of a single neural cell, can improve the reliability and the precision of signal detection through detection of presynaptic input synchrony. Using a simplified neuronal network model composed of dozens of integrate-and-fire neurons and a single coincidence-detector neuron, we show how the network reads out the subthreshold noisy signals reliably and precisely. We find suitable pairing parameters, the threshold and the detection time window of the coincidence-detector neuron, that optimize the precision and reliability of the neuron. Furthermore, it is observed that the refractory period induces an oscillation in the spontaneous firing, but the neuron can inhibit this activity and improve the reliability and precision further. In the case of intermediate intrinsic states of the input neuron, the network responds to the input more efficiently. These results present the critical link between spiking synchrony and noisy signal transfer, which is utilized in coincidence detection, resulting in enhancement of temporally sensitive coding scheme.},
  file = {2013 - Chen et al. - The Role of Coincidence-Detector Neurons in the Reliability and Precision of Subthreshold Signal Detection in Noise.pdf},
  journal = {PLoS ONE},
  language = {en},
  number = {2}
}

@article{Chen2018,
  title = {This {{Looks Like That}}: {{Deep Learning}} for {{Interpretable Image Recognition}}},
  shorttitle = {This {{Looks Like That}}},
  author = {Chen, Chaofan and Li, Oscar and Tao, Chaofan and Barnett, Alina Jade and Su, Jonathan and Rudin, Cynthia},
  year = {2018},
  month = jun,
  abstract = {When we are faced with challenging image classification tasks, we often explain our reasoning by dissecting the image, and pointing out prototypical aspects of one class or another. The mounting evidence for each of the classes helps us make our final decision. In this work, we introduce a deep network architecture that reasons in a similar way: the network dissects the image by finding prototypical parts, and combines evidence from the prototypes to make a final classification. The algorithm thus reasons in a way that is qualitatively similar to the way ornithologists, physicians, geologists, architects, and others would explain to people on how to solve challenging image classification tasks. The network uses only image-level labels for training, meaning that there are no labels for parts of images. We demonstrate the method on the CIFAR-10 dataset and 10 classes from the CUB200-2011 dataset.},
  archiveprefix = {arXiv},
  eprint = {1806.10574},
  eprinttype = {arxiv},
  file = {Chen et al. - 2018 - This Looks Like That Deep Learning for Interpreta.pdf},
  journal = {arXiv:1806.10574 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryclass = {cs, stat}
}

@article{Chen2018a,
  title = {Neural {{Ordinary Differential Equations}}},
  author = {Chen, Ricky T. Q. and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David},
  year = {2018},
  month = jun,
  abstract = {We introduce a new family of deep neural network models. Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of the hidden state using a neural network. The output of the network is computed using a blackbox differential equation solver. These continuous-depth models have constant memory cost, adapt their evaluation strategy to each input, and can explicitly trade numerical precision for speed. We demonstrate these properties in continuous-depth residual networks and continuous-time latent variable models. We also construct continuous normalizing flows, a generative model that can train by maximum likelihood, without partitioning or ordering the data dimensions. For training, we show how to scalably backpropagate through any ODE solver, without access to its internal operations. This allows end-to-end training of ODEs within larger models.},
  archiveprefix = {arXiv},
  eprint = {1806.07366},
  eprinttype = {arxiv},
  file = {Chen et al. - 2018 - Neural Ordinary Differential Equations.pdf},
  journal = {arXiv:1806.07366 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryclass = {cs, stat}
}

@article{Chen2020,
  title = {Tuning Movement for Sensing in an Uncertain World},
  author = {Chen, Chen and Murphey, Todd D and MacIver, Malcolm A},
  year = {2020},
  month = sep,
  volume = {9},
  pages = {e52371},
  issn = {2050-084X},
  doi = {10.7554/eLife.52371},
  abstract = {While animals track or search for targets, sensory organs make small unexplained movements on top of the primary task-related motions. While multiple theories for these movements exist\textemdash in that they support infotaxis, gain adaptation, spectral whitening, and high-pass filtering\textemdash predicted trajectories show poor fit to measured trajectories. We propose a new theory for these movements called energy-constrained proportional betting, where the probability of moving to a location is proportional to an expectation of how informative it will be balanced against the movement's predicted energetic cost. Trajectories generated in this way show good agreement with measured trajectories of fish tracking an object using electrosense, a mammal and an insect localizing an odor source, and a moth tracking a flower using vision. Our theory unifies the metabolic cost of motion with information theory. It predicts sense organ movements in animals and can prescribe sensor motion for robots to enhance performance.},
  file = {Chen et al. - 2020 - Tuning movement for sensing in an uncertain world.pdf},
  journal = {eLife},
  language = {en}
}

@article{Chen2020a,
  title = {Tuning Movement for Sensing in an Uncertain World},
  author = {Chen, Chen and Murphey, Todd D and MacIver, Malcolm A},
  year = {2020},
  month = sep,
  volume = {9},
  pages = {e52371},
  issn = {2050-084X},
  doi = {10.7554/eLife.52371},
  abstract = {While animals track or search for targets, sensory organs make small unexplained movements on top of the primary task-related motions. While multiple theories for these movements exist\textemdash in that they support infotaxis, gain adaptation, spectral whitening, and high-pass filtering\textemdash predicted trajectories show poor fit to measured trajectories. We propose a new theory for these movements called energy-constrained proportional betting, where the probability of moving to a location is proportional to an expectation of how informative it will be balanced against the movement's predicted energetic cost. Trajectories generated in this way show good agreement with measured trajectories of fish tracking an object using electrosense, a mammal and an insect localizing an odor source, and a moth tracking a flower using vision. Our theory unifies the metabolic cost of motion with information theory. It predicts sense organ movements in animals and can prescribe sensor motion for robots to enhance performance.},
  file = {Chen et al. - 2020 - Tuning movement for sensing in an uncertain world 2.pdf},
  journal = {eLife},
  language = {en}
}

@article{Cheng,
  title = {Polynomial {{Regression As}} an {{Alternative}} to {{Neural Nets}}},
  author = {Cheng, Xi and Khomtchouk, Bohdan and Matloff, Norman and Mohanty, Pete},
  pages = {23},
  abstract = {Despite the success of neural networks (NNs), there is still a concern among many over their ``black box'' nature. Why do they work? Here we present a simple analytic argument that NNs are in fact essentially polynomial regression models. This view will have various implications for NNs, e.g. providing an explanation for why convergence problems arise in NNs, and it gives rough guidance on avoiding overfitting. In addition, we use this phenomenon to predict and confirm a multicollinearity property of NNs not previously reported in the literature. Most importantly, given this loose correspondence, one may choose to routinely use polynomial models instead of NNs, thus avoiding some major problems of the latter, such as having to set many tuning parameters and dealing with convergence issues. We present a number of empirical results; in each case, the accuracy of the polynomial approach matches or exceeds that of NN approaches. A many-featured, open-source software package, polyreg, is available.},
  file = {Cheng et al. - Polynomial Regression As an Alternative to Neural .pdf},
  language = {en}
}

@article{Chernoff1986,
  title = {Numerical {{Solutions}} for {{Bayes Sequential Decision Problems}}},
  author = {Chernoff, Herman and Petkau, A. John},
  year = {1986},
  month = jan,
  volume = {7},
  pages = {46--59},
  issn = {0196-5204, 2168-3417},
  doi = {10.1137/0907003},
  abstract = {Certain sequential decision problems involving normal random variables reduce to optimal stopping problems which can be related to the solution of corresponding free boundary problems for the heat equation. The numerical solution of these free boundary problems can then be approximated by calculating the solution of simpler optimal stopping problems by backward induction. This approach is not well adapted for very precise results but is surprisingly effective for rough approximations. An estimate of the difference between the solutions of the related problems permits one to make continuity corrections which provide considerably improved accuracy. Further reductions in the necessary computational effort are possible by considering truncated procedures for one-sided boundaries and by exploiting monotone and symmetric boundaries.},
  file = {1986 - Chernoff, Petkaut - Numerical solutions for bayes sequential decision.pdf;Chernoff and Petkau - 1986 - Numerical Solutions for Bayes Sequential Decision .pdf},
  journal = {SIAM Journal on Scientific and Statistical Computing},
  language = {en},
  number = {1}
}

@article{Chersi2011,
  title = {Neuronal {{Chains}} for {{Actions}} in the {{Parietal Lobe}}: {{A Computational Model}}},
  shorttitle = {Neuronal {{Chains}} for {{Actions}} in the {{Parietal Lobe}}},
  author = {Chersi, Fabian and Ferrari, Pier Francesco and Fogassi, Leonardo},
  editor = {Meck, Warren H.},
  year = {2011},
  month = nov,
  volume = {6},
  pages = {e27652},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0027652},
  abstract = {The inferior part of the parietal lobe (IPL) is known to play a very important role in sensorimotor integration. Neurons in this region code goal-related motor acts performed with the mouth, with the hand and with the arm. It has been demonstrated that most IPL motor neurons coding a specific motor act (e.g., grasping) show markedly different activation patterns according to the final goal of the action sequence in which the act is embedded (grasping for eating or grasping for placing). Some of these neurons (parietal mirror neurons) show a similar selectivity also during the observation of the same action sequences when executed by others. Thus, it appears that the neuronal response occurring during the execution and the observation of a specific grasping act codes not only the executed motor act, but also the agent's final goal (intention). In this work we present a biologically inspired neural network architecture that models mechanisms of motor sequences execution and recognition. In this network, pools composed of motor and mirror neurons that encode motor acts of a sequence are arranged in form of action goal-specific neuronal chains. The execution and the recognition of actions is achieved through the propagation of activity bursts along specific chains modulated by visual and somatosensory inputs. The implemented spiking neuron network is able to reproduce the results found in neurophysiological recordings of parietal neurons during task performance and provides a biologically plausible implementation of the action selection and recognition process. Finally, the present paper proposes a mechanism for the formation of new neural chains by linking together in a sequential manner neurons that represent subsequent motor acts, thus producing goal-directed sequences.},
  file = {2011 - Chersi, Ferrari, Fogassi - Neuronal chains for actions in the parietal lobe A computational model.pdf},
  journal = {PLoS ONE},
  language = {en},
  number = {11}
}

@article{Cheung2019,
  title = {Superposition of Many Models into One},
  author = {Cheung, Brian and Terekhov, Alex and Chen, Yubei and Agrawal, Pulkit and Olshausen, Bruno},
  year = {2019},
  month = jun,
  abstract = {We present a method for storing multiple models within a single set of parameters. Models can coexist in superposition and still be retrieved individually. In experiments with neural networks, we show that a surprisingly large number of models can be effectively stored within a single parameter instance. Furthermore, each of these models can undergo thousands of training steps without significantly interfering with other models within the superposition. This approach may be viewed as the online complement of compression: rather than reducing the size of a network after training, we make use of the unrealized capacity of a network during training.},
  archiveprefix = {arXiv},
  eprint = {1902.05522},
  eprinttype = {arxiv},
  file = {Cheung et al. - 2019 - Superposition of many models into one.pdf},
  journal = {arXiv:1902.05522 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  language = {en},
  primaryclass = {cs}
}

@article{Childs2008,
  title = {Stability Diagram for the Forced {{Kuramoto}} Model},
  author = {Childs, Lauren M. and Strogatz, Steven H.},
  year = {2008},
  month = dec,
  volume = {18},
  pages = {043128},
  issn = {1054-1500, 1089-7682},
  doi = {10.1063/1.3049136},
  file = {2008 - Childs, Strogatz - Stability diagram for the forced Kuramoto model.pdf},
  journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
  language = {en},
  number = {4}
}

@article{Childs2011,
  title = {From {{Inflammation}} to {{Wound Healing}}: {{Using}} a {{Simple Model}} to {{Understand}} the {{Functional Versatility}} of {{Murine Macrophages}}},
  shorttitle = {From {{Inflammation}} to {{Wound Healing}}},
  author = {Childs, Lauren M. and Paskow, Michael and Morris, Sidney M. and Hesse, Matthias and Strogatz, Steven},
  year = {2011},
  month = nov,
  volume = {73},
  pages = {2575--2604},
  issn = {0092-8240, 1522-9602},
  doi = {10.1007/s11538-011-9637-5},
  file = {2011 - Childs et al. - From Inflammation to Wound Healing Using a Simple Model to Understand the Functional Versatility of Murine Macrop.pdf},
  journal = {Bulletin of Mathematical Biology},
  language = {en},
  number = {11}
}

@article{Chklovskii2002,
  title = {Wiring {{Optimization}} in {{Cortical Circuits}}},
  author = {Chklovskii, Dmitri B. and Schikorski, Thomas and Stevens, Charles F.},
  year = {2002},
  month = apr,
  volume = {34},
  pages = {341--347},
  issn = {08966273},
  doi = {10.1016/S0896-6273(02)00679-7},
  abstract = {Wiring a brain presents a formidable problem because neural circuits require an enormous number of fast and durable connections. We propose that evolution was likely to have optimized neural circuits to minimize conduction delays in axons, passive cable attenuation in dendrites, and the length of ``wire'' used to construct circuits, and to have maximized the density of synapses. Here we ask the question: ``What fraction of the volume should be taken up by axons and dendrites (i.e., wire) when these variables are at their optimal values?'' The biophysical properties of axons and dendrites dictate that wire should occupy 3/5 of the volume in an optimally wired gray matter. We have measured the fraction of the volume occupied by each cellular component and find that the volume of wire is close to the predicted optimal value.},
  file = {Chklovskii et al. - 2002 - Wiring Optimization in Cortical Circuits.pdf},
  journal = {Neuron},
  language = {en},
  number = {3}
}

@article{Cho2015,
  title = {Gamma {{Rhythms Link Prefrontal Interneuron Dysfunction}} with {{Cognitive Inflexibility}} in {{Dlx5}}/6+/- {{Mice}}},
  author = {Cho, Kathleen K.A. and Hoch, Renee and Lee, Anthony T. and Patel, Tosha and Rubenstein, John L.R. and Sohal, Vikaas S.},
  year = {2015},
  month = mar,
  volume = {85},
  pages = {1332--1343},
  issn = {08966273},
  doi = {10.1016/j.neuron.2015.02.019},
  abstract = {Abnormalities in GABAergic interneurons, particularly fast-spiking interneurons (FSINs) that generate gamma (g; \$30\textendash 120 Hz) oscillations, are hypothesized to disrupt prefrontal cortex (PFC)-dependent cognition in schizophrenia. Although g rhythms are abnormal in schizophrenia, it remains unclear whether they directly influence cognition. Mechanisms underlying schizophrenia's typical postadolescent onset also remain elusive. We addressed these issues using mice heterozygous for Dlx5/6, which regulate GABAergic interneuron development. In Dlx5/6+/\`A mice, FSINs become abnormal following adolescence, coinciding with the onset of cognitive inflexibility and deficient task-evoked g oscillations. Inhibiting PFC interneurons in control mice reproduced these deficits, whereas stimulating them at g-frequencies restored cognitive flexibility in adult Dlx5/6+/\`A mice. These pro-cognitive effects were frequency specific and persistent. These findings elucidate a mechanism whereby abnormal FSIN development may contribute to the post-adolescent onset of schizophrenia endophenotypes. Furthermore, they demonstrate a causal, potentially therapeutic, role for PFC interneuron-driven g oscillations in cognitive domains at the core of schizophrenia.},
  file = {Cho et al. - 2015 - Gamma Rhythms Link Prefrontal Interneuron Dysfunct.pdf},
  journal = {Neuron},
  language = {en},
  number = {6}
}

@article{Choi,
  title = {Inverse {{Reinforcement Learning}} in {{Partially Observable Environments}}},
  author = {Choi, Jaedeug and Kim, Kee-Eung and Kr, Ai Kaist Ac and Kr, Cs Kaist Ac},
  pages = {40},
  abstract = {Inverse reinforcement learning (IRL) is the problem of recovering the underlying reward function from the behavior of an expert. Most of the existing IRL algorithms assume that the environment is modeled as a Markov decision process (MDP), although it is desirable to handle partially observable settings in order to handle more realistic scenarios. In this paper, we present IRL algorithms for partially observable environments that can be modeled as a partially observable Markov decision process (POMDP). We deal with two cases according to the representation of the given expert's behavior, namely the case in which the expert's policy is explicitly given, and the case in which the expert's trajectories are available instead. The IRL in POMDPs poses a greater challenge than in MDPs since it is not only ill-posed due to the nature of IRL, but also computationally intractable due to the hardness in solving POMDPs. To overcome these obstacles, we present algorithms that exploit some of the classical results from the POMDP literature. Experimental results on several benchmark POMDP domains show that our work is useful for partially observable settings.},
  file = {2011 - Kim - Inverse Reinforcement Learning in Partially Observable Environments.pdf},
  language = {en}
}

@article{Choudhary2018,
  title = {Weak-Winner Phase Synchronization},
  author = {Choudhary, Anshul and Saha, Arindam and Krueger, Samuel and Finke, Christian and Rosa, Jr and Freund, Jan A. and Feudel, Ulrike},
  year = {2018},
  month = dec,
  abstract = {We report the observation of a novel and non-trivial synchronization state in a system consisting of three oscillators coupled in a linear chain. For certain ranges of coupling strength the weakly coupled oscillator pair exhibits phase synchronization while the strongly coupled oscillator pair does not. This intriguing "weak-winner" synchronization phenomenon can be explained by the interplay between non-isochronicity and natural frequency of the oscillator, as coupling strength is varied. Further, we present sufficient conditions under which the weak-winner phase synchronization can occur for limit cycle as well as chaotic oscillators. Employing model system from ecology as well as a paradigmatic model from physics, we demonstrate that this phenomenon is a generic feature for a large class of coupled oscillator systems. The realization of this peculiar yet quite generic weak-winner dynamics can have far reaching consequences in a wide range of scientific disciplines that deal with the phenomenon of phase synchronization. Our results also highlight the role of non-isochronicity (shear) as a fundamental feature of an oscillator in shaping the emergent dynamics.},
  archiveprefix = {arXiv},
  eprint = {1812.02642},
  eprinttype = {arxiv},
  file = {Choudhary et al. - 2018 - Weak-winner phase synchronization.pdf},
  journal = {arXiv:1812.02642 [nlin]},
  keywords = {Nonlinear Sciences - Chaotic Dynamics},
  language = {en},
  primaryclass = {nlin}
}

@article{Chung2014,
  title = {Empirical {{Evaluation}} of {{Gated Recurrent Neural Networks}} on {{Sequence Modeling}}},
  author = {Chung, Junyoung and Gulcehre, Caglar and Cho, KyungHyun and Bengio, Yoshua},
  year = {2014},
  month = dec,
  abstract = {In this paper we compare different types of recurrent units in recurrent neural networks (RNNs). Especially, we focus on more sophisticated units that implement a gating mechanism, such as a long short-term memory (LSTM) unit and a recently proposed gated recurrent unit (GRU). We evaluate these recurrent units on the tasks of polyphonic music modeling and speech signal modeling. Our experiments revealed that these advanced recurrent units are indeed better than more traditional recurrent units such as tanh units. Also, we found GRU to be comparable to LSTM.},
  archiveprefix = {arXiv},
  eprint = {1412.3555},
  eprinttype = {arxiv},
  file = {2014 - Chung et al. - Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling.pdf;Chung et al. - 2014 - Empirical Evaluation of Gated Recurrent Neural Net.pdf},
  journal = {arXiv:1412.3555 [cs]},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  language = {en},
  primaryclass = {cs}
}

@article{Chung2018,
  title = {Classification and {{Geometry}} of {{General Perceptual Manifolds}}},
  author = {Chung, SueYeon and Lee, Daniel D. and Sompolinsky, Haim},
  year = {2018},
  month = jul,
  volume = {8},
  pages = {031003},
  issn = {2160-3308},
  doi = {10.1103/PhysRevX.8.031003},
  file = {Chung et al. - 2018 - Classification and Geometry of General Perceptual .pdf},
  journal = {Phys. Rev. X},
  language = {en},
  number = {3}
}

@article{Churchland2006,
  title = {A {{Central Source}} of {{Movement Variability}}},
  author = {Churchland, Mark M. and Afshar, Afsheen and Shenoy, Krishna V.},
  year = {2006},
  month = dec,
  volume = {52},
  pages = {1085--1096},
  issn = {08966273},
  doi = {10.1016/j.neuron.2006.10.034},
  abstract = {Movements are universally, sometimes frustratingly, variable. When such variability causes error, we typically assume that something went wrong during the movement. The same assumption is made by recent and influential models of motor control. These posit that the principal limit on repeatable performance is neuromuscular noise that corrupts movement as it occurs. An alternative hypothesis is that movement variability arises before movements begin, during motor preparation. We examined this possibility directly by recording the preparatory activity of single cortical neurons during a highly practiced reach task. Small variations in preparatory neural activity were predictive of small variations in the upcoming reach. Effect magnitudes were such that at least half of the observed movement variability likely had its source during motor preparation. Thus, even for a highly practiced task, the ability to repeatedly plan the same movement limits our ability to repeatedly execute the same movement.},
  file = {2006 - Baudouin-Cornu, Bragg - Analyzing proteomic, genomic and transcriptomic elemental compositions to uncover the intimate evolution.pdf},
  journal = {Neuron},
  language = {en},
  number = {6}
}

@article{Churchland2007,
  title = {Delay of {{Movement Caused}} by {{Disruption}} of {{Cortical Preparatory Activity}}},
  author = {Churchland, Mark M. and Shenoy, Krishna V.},
  year = {2007},
  month = jan,
  volume = {97},
  pages = {348--359},
  issn = {0022-3077, 1522-1598},
  doi = {10.1152/jn.00808.2006},
  file = {2007 - Churchland, Shenoy - Delay of movement caused by disruption of cortical preparatory activity.pdf},
  journal = {Journal of Neurophysiology},
  language = {en},
  number = {1}
}

@article{Churchland2007a,
  title = {Techniques for Extracting Single-Trial Activity Patterns from Large-Scale Neural Recordings},
  author = {Churchland, Mark M and Yu, Byron M and Sahani, Maneesh and Shenoy, Krishna V},
  year = {2007},
  month = oct,
  volume = {17},
  pages = {609--618},
  issn = {09594388},
  doi = {10.1016/j.conb.2007.11.001},
  file = {2007 - Churchland et al. - Techniques for extracting single-trial activity patterns from large-scale neural recordings.pdf},
  journal = {Current Opinion in Neurobiology},
  language = {en},
  number = {5}
}

@article{Churchland2010,
  title = {Cortical {{Preparatory Activity}}: {{Representation}} of {{Movement}} or {{First Cog}} in a {{Dynamical Machine}}?},
  shorttitle = {Cortical {{Preparatory Activity}}},
  author = {Churchland, Mark M. and Cunningham, John P. and Kaufman, Matthew T. and Ryu, Stephen I. and Shenoy, Krishna V.},
  year = {2010},
  month = nov,
  volume = {68},
  pages = {387--400},
  issn = {08966273},
  doi = {10.1016/j.neuron.2010.09.015},
  abstract = {The motor cortices are active during both movement and movement preparation. A common assumption is that preparatory activity constitutes a subthreshold form of movement activity: a neuron active during rightward movements becomes modestly active during preparation of a rightward movement. We asked whether this pattern of activity is, in fact, observed. We found that it was not: at the level of a single neuron, preparatory tuning was weakly correlated with movement-period tuning. Yet, somewhat paradoxically, preparatory tuning could be captured by a preferred direction in an abstract ``space'' that described the population-level pattern of movement activity. In fact, this relationship accounted for preparatory responses better than did traditional tuning models. These results are expected if preparatory activity provides the initial state of a dynamical system whose evolution produces movement activity. Our results thus suggest that preparatory activity may not represent specific factors, and may instead play a more mechanistic role.},
  file = {2010 - Churchland et al. - Cortical Preparatory Activity Representation of Movement or First Cog in a Dynamical Machine.pdf},
  journal = {Neuron},
  language = {en},
  number = {3}
}

@article{Churchland2010a,
  title = {Stimulus Onset Quenches Neural Variability: A Widespread Cortical Phenomenon},
  shorttitle = {Stimulus Onset Quenches Neural Variability},
  author = {Churchland, Mark M and Yu, Byron M and Cunningham, John P and Sugrue, Leo P and Cohen, Marlene R and Corrado, Greg S and Newsome, William T and Clark, Andrew M and Hosseini, Paymon and Scott, Benjamin B and Bradley, David C and Smith, Matthew A and Kohn, Adam and Movshon, J Anthony and Armstrong, Katherine M and Moore, Tirin and Chang, Steve W and Snyder, Lawrence H and Lisberger, Stephen G and Priebe, Nicholas J and Finn, Ian M and Ferster, David and Ryu, Stephen I and Santhanam, Gopal and Sahani, Maneesh and Shenoy, Krishna V},
  year = {2010},
  month = mar,
  volume = {13},
  pages = {369--378},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.2501},
  file = {2010 - Churchland et al. - Stimulus onset quenches neural variability a widespread cortical phenomenon.pdf;2010 - Churchland et al. - Stimulus onset quenches neural variability a widespread cortical phenomenon(2).pdf},
  journal = {Nature Neuroscience},
  language = {en},
  number = {3}
}

@article{Churchland2012,
  title = {Neural Population Dynamics during Reaching},
  author = {Churchland, Mark M. and Cunningham, John P. and Kaufman, Matthew T. and Foster, Justin D. and Nuyujukian, Paul and Ryu, Stephen I. and Shenoy, Krishna V.},
  year = {2012},
  month = jul,
  volume = {487},
  pages = {51--56},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature11129},
  file = {2012 - Churchland et al. - Neural population dynamics during reaching.pdf},
  journal = {Nature},
  language = {en},
  number = {7405}
}

@article{Churchland2012a,
  title = {Two Layers of Neural Variability},
  author = {Churchland, Mark M and Abbott, L F},
  year = {2012},
  month = nov,
  volume = {15},
  pages = {1472--1474},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.3247},
  file = {2012 - Churchland, Abbott - Two layers of neural variability(2).pdf},
  journal = {Nature Neuroscience},
  language = {en},
  number = {11}
}

@article{Chvykov2021,
  title = {Low Rattling: {{A}} Predictive Principle for Self-Organization in Active Collectives},
  shorttitle = {Low Rattling},
  author = {Chvykov, Pavel and Berrueta, Thomas A. and Vardhan, Akash and Savoie, William and Samland, Alexander and Murphey, Todd D. and Wiesenfeld, Kurt and Goldman, Daniel I. and England, Jeremy L.},
  year = {2021},
  month = jan,
  volume = {371},
  pages = {90--95},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.abc6182},
  abstract = {Self-organization is frequently observed in active collectives, from ant rafts to molecular motor assemblies. General principles describing self-organization away from equilibrium have been challenging to identify. We offer a unifying framework that models the behavior of complex systems as largely random, while capturing their configuration-dependent response to external forcing. This allows derivation of a Boltzmann-like principle for understanding and manipulating driven self-organization. We validate our predictions experimentally in shape-changing robotic active matter, and outline a methodology for controlling collective behavior. Our findings highlight how emergent order depends sensitively on the matching between external patterns of forcing and internal dynamical response properties, pointing towards future approaches for design and control of active particle mixtures and metamaterials.},
  archiveprefix = {arXiv},
  eprint = {2101.00683},
  eprinttype = {arxiv},
  file = {Chvykov et al. - 2021 - Low rattling A predictive principle for self-orga.pdf},
  journal = {Science},
  keywords = {Computer Science - Robotics,Condensed Matter - Statistical Mechanics},
  language = {en},
  number = {6524}
}

@article{Chvykov2021a,
  title = {Low Rattling: {{A}} Predictive Principle for Self-Organization in Active Collectives},
  shorttitle = {Low Rattling},
  author = {Chvykov, Pavel and Berrueta, Thomas A. and Vardhan, Akash and Savoie, William and Samland, Alexander and Murphey, Todd D. and Wiesenfeld, Kurt and Goldman, Daniel I. and England, Jeremy L.},
  year = {2021},
  month = jan,
  volume = {371},
  pages = {90--95},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.abc6182},
  abstract = {Self-organization is frequently observed in active collectives as varied as ant rafts and molecular motor assemblies. General principles describing self-organization away from equilibrium have been challenging to identify. We offer a unifying framework that models the behavior of complex systems as largely random while capturing their configuration-dependent response to external forcing. This allows derivation of a Boltzmann-like principle for understanding and manipulating driven self-organization. We validate our predictions experimentally, with the use of shape-changing robotic active matter, and outline a methodology for controlling collective behavior. Our findings highlight how emergent order depends sensitively on the matching between external patterns of forcing and internal dynamical response properties, pointing toward future approaches for the design and control of active particle mixtures and metamaterials.},
  file = {Chvykov et al. - 2021 - Low rattling A predictive principle for self-orga 2.pdf},
  journal = {Science},
  language = {en},
  number = {6524}
}

@article{Cisek2009,
  title = {Decisions in {{Changing Conditions}}: {{The Urgency}}-{{Gating Model}}},
  shorttitle = {Decisions in {{Changing Conditions}}},
  author = {Cisek, P. and Puskas, G. A. and {El-Murr}, S.},
  year = {2009},
  month = sep,
  volume = {29},
  pages = {11560--11571},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.1844-09.2009},
  file = {2009 - Cisek, Puskas, El-Murr - Decisions in changing conditions the urgency-gating model.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {37}
}

@article{Cisek2010,
  title = {Neural {{Mechanisms}} for {{Interacting}} with a {{World Full}} of {{Action Choices}}},
  author = {Cisek, Paul and Kalaska, John F.},
  year = {2010},
  month = jun,
  volume = {33},
  pages = {269--298},
  issn = {0147-006X, 1545-4126},
  doi = {10.1146/annurev.neuro.051508.135409},
  abstract = {The neural bases of behavior are often discussed in terms of perceptual, cognitive, and motor stages, defined within an information processing framework that was originally inspired by models of human abstract problem solving. Here, we review a growing body of neurophysiological data that is difficult to reconcile with this influential theoretical perspective. As an alternative foundation for interpreting neural data, we consider frameworks borrowed from ethology, which emphasize the kinds of real-time interactive behaviors that animals have engaged in for millions of years. In particular, we discuss an ethologically-inspired view of interactive behavior as simultaneous processes that specify potential motor actions and select between them. We review how recent neurophysiological data from diverse cortical and subcortical regions appear more compatible with this parallel view than with the classical view of serial information processing stages.},
  file = {Cisek and Kalaska - 2010 - Neural Mechanisms for Interacting with a World Ful.pdf},
  journal = {Annu. Rev. Neurosci.},
  language = {en},
  number = {1}
}

@article{Cisek2019,
  title = {Resynthesizing Behavior through Phylogenetic Refinement},
  author = {Cisek, Paul},
  year = {2019},
  month = jun,
  issn = {1943-3921, 1943-393X},
  doi = {10.3758/s13414-019-01760-1},
  abstract = {This article proposes that biologically plausible theories of behavior can be constructed by following a method of Bphylogenetic refinement,\^ whereby they are progressively elaborated from simple to complex according to phylogenetic data on the sequence of changes that occurred over the course of evolution. It is argued that sufficient data exist to make this approach possible, and that the result can more effectively delineate the true biological categories of neurophysiological mechanisms than do approaches based on definitions of putative functions inherited from psychological traditions. As an example, the approach is used to sketch a theoretical framework of how basic feedback control of interaction with the world was elaborated during vertebrate evolution, to give rise to the functional architecture of the mammalian brain. The results provide a conceptual taxonomy of mechanisms that naturally map to neurophysiological and neuroanatomical data and that offer a context for defining putative functions that, it is argued, are better grounded in biology than are some of the traditional concepts of cognitive science.},
  file = {Cisek - 2019 - Resynthesizing behavior through phylogenetic refin.pdf},
  journal = {Atten Percept Psychophys},
  language = {en}
}

@article{Claus,
  title = {The {{Dynamics}} of {{Reinforcement Learning}} in {{Cooperative Multiagent Systems}}},
  author = {Claus, Caroline and Boutilier, Craig},
  pages = {7},
  abstract = {Reinforcement learning can provide a robust and natural means for agents to learn how to coordinate their action choices in multiagent systems. We examine some of the factors that can influence the dynamics of the learning process in such a setting. We first distinguish reinforcement learners that are unaware of (or ignore) the presence of other agents from those that explicitly attempt to learn the value of joint actions and the strategies of their counterparts. We study (a simple form of) Q-learning in cooperative multiagent systems under these two perspectives, focusing on the influence of that game structure and exploration strategies on convergence to (optimal and suboptimal) Nash equilibria. We then propose alternative optimistic exploration strategies that increase the likelihood of convergence to an optimal equilibrium.},
  file = {1998 - Claus, Boutilier - The Dynamics of Reinforcement Learning in Cooperative Multiagent Systems.pdf},
  language = {en}
}

@article{Clithero2011,
  title = {Within- and Cross-Participant Classifiers Reveal Different Neural Coding of Information},
  author = {Clithero, John A. and Smith, David V. and Carter, R. McKell and Huettel, Scott A.},
  year = {2011},
  month = may,
  volume = {56},
  pages = {699--708},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2010.03.057},
  abstract = {Analyzing distributed patterns of brain activation using multivariate pattern analysis (MVPA) has become a popular approach for using functional magnetic resonance imaging (fMRI) data to predict mental states. While the majority of studies currently build separate classifiers for each participant in the sample, in principle a single classifier can be derived from and tested on data from all participants. These two approaches, within- and cross-participant classification, rely on potentially different sources of variability and thus may provide distinct information about brain function. Here, we used both approaches to identify brain regions that contain information about passively received monetary rewards (i.e., images of currency that influenced participant payment) and social rewards (i.e., images of human faces). Our withinparticipant analyses implicated regions in the ventral visual processing stream\textemdash including fusiform gyrus and primary visual cortex\textemdash and ventromedial prefrontal cortex (VMPFC). Two key results indicate these regions may contain statistically discriminable patterns that contain different informational representations. First, cross-participant analyses implicated additional brain regions, including striatum and anterior insula. The cross-participant analyses also revealed systematic changes in predictive power across brain regions, with the pattern of change consistent with the functional properties of regions. Second, individual differences in classifier performance in VMPFC were related to individual differences in preferences between our two reward modalities. We interpret these results as reflecting a distinction between patterns showing participant-specific functional organization and those indicating aspects of brain organization that generalize across individuals.},
  file = {2011 - Clithero et al. - Within- and cross-participant classifiers reveal different neural coding of information.pdf},
  journal = {NeuroImage},
  language = {en},
  number = {2}
}

@article{Clune,
  title = {Evolving {{Three}}-{{Dimensional Objects}} with a {{Generative Encoding Inspired}} by {{Developmental Biology}}},
  author = {Clune, Jeff and Lipson, Hod},
  pages = {8},
  abstract = {This paper introduces an algorithm for evolving 3D objects with a generative encoding that abstracts how biological morphologies are produced. Evolving interesting 3D objects is useful in many disciplines, including artistic design (e.g. sculpture), engineering (e.g. robotics, architecture, or product design), and biology (e.g. for investigating morphological evolution). A critical element in evolving 3D objects is the representation, which strongly influences the types of objects produced. In 2007 a representation was introduced called Compositional Pattern Producing Networks (CPPN), which abstracts how natural phenotypes are generated. To date, however, the ability of CPPNs to create 3D objects has barely been explored. Here we present a new way to create 3D objects with CPPNs. Experiments with both interactive and target-based evolution demonstrate that CPPNs show potential in generating interesting, complex, 3D objects. We further show that changing the information provided to CPPNs and the functions allowed in their genomes biases the types of objects produced. Finally, we validate that the objects transfer well from simulation to the real-world by printing them with a 3D printer. Overall, this paper shows that evolving objects with encodings based on concepts from biological development can be a powerful way to evolve complex, interesting objects, which should be of use in fields as diverse as art, engineering, and biology.},
  file = {Clune and Lipson - Evolving Three-Dimensional Objects with a Generati.pdf},
  language = {en}
}

@article{Clune2012,
  title = {Ontogeny {{Tends}} to {{Recapitulate Phylogeny}} in {{Digital Organisms}}},
  author = {Clune, Jeff and Pennock, Robert T. and Ofria, Charles and Lenski, Richard E.},
  year = {2012},
  month = sep,
  volume = {180},
  pages = {E54-E63},
  issn = {0003-0147, 1537-5323},
  doi = {10.1086/666984},
  abstract = {Biologists have long debated whether ontogeny recapitulates phylogeny and, if so, why. Two plausible explanations are that (i) changes to early developmental stages are selected against because they tend to disrupt later development and (ii) simpler structures often precede more complex ones in both ontogeny and phylogeny if the former serve as building blocks for the latter. It is difficult to test these hypotheses experimentally in natural systems, so we used a computational system that exhibits evolutionary dynamics. We observed that ontogeny does indeed recapitulate phylogeny; traits that arose earlier in a lineage's history also tended to be expressed earlier in the development of individuals. The relative complexity of traits contributed substantially to this correlation, but a significant tendency toward recapitulation remained even after accounting for trait complexity. This additional effect provides evidence that selection against developmental disruption also contributed to the conservation of early stages in development.},
  file = {Clune et al. - 2012 - Ontogeny Tends to Recapitulate Phylogeny in Digita.pdf},
  journal = {The American Naturalist},
  language = {en},
  number = {3}
}

@article{Clune2012a,
  title = {Ontogeny {{Tends}} to {{Recapitulate Phylogeny}} in {{Digital Organisms}}},
  author = {Clune, Jeff and Pennock, Robert T. and Ofria, Charles and Lenski, Richard E.},
  year = {2012},
  month = sep,
  volume = {180},
  pages = {E54-E63},
  issn = {0003-0147, 1537-5323},
  doi = {10.1086/666984},
  abstract = {Biologists have long debated whether ontogeny recapitulates phylogeny and, if so, why. Two plausible explanations are that (i) changes to early developmental stages are selected against because they tend to disrupt later development and (ii) simpler structures often precede more complex ones in both ontogeny and phylogeny if the former serve as building blocks for the latter. It is difficult to test these hypotheses experimentally in natural systems, so we used a computational system that exhibits evolutionary dynamics. We observed that ontogeny does indeed recapitulate phylogeny; traits that arose earlier in a lineage's history also tended to be expressed earlier in the development of individuals. The relative complexity of traits contributed substantially to this correlation, but a significant tendency toward recapitulation remained even after accounting for trait complexity. This additional effect provides evidence that selection against developmental disruption also contributed to the conservation of early stages in development.},
  file = {Clune et al. - 2012 - Ontogeny Tends to Recapitulate Phylogeny in Digita 2.pdf},
  journal = {The American Naturalist},
  language = {en},
  number = {3}
}

@article{Clune2013,
  title = {The Evolutionary Origins of Modularity},
  author = {Clune, Jeff and Mouret, Jean-Baptiste and Lipson, Hod},
  year = {2013},
  month = mar,
  volume = {280},
  pages = {20122863},
  issn = {0962-8452, 1471-2954},
  doi = {10.1098/rspb.2012.2863},
  file = {Clune et al. - 2013 - The evolutionary origins of modularity.pdf},
  journal = {Proc. R. Soc. B},
  language = {en},
  number = {1755}
}

@article{Clune2013a,
  title = {The Evolutionary Origins of Modularity},
  author = {Clune, Jeff and Mouret, Jean-Baptiste and Lipson, Hod},
  year = {2013},
  month = mar,
  volume = {280},
  pages = {20122863},
  issn = {0962-8452, 1471-2954},
  doi = {10.1098/rspb.2012.2863},
  abstract = {A central biological question is how natural organisms are so evolvable (capable of quickly adapting to new environments). A key driver of evolvability is the widespread modularity of biological networks\textemdash their organization as functional, sparsely connected subunits\textemdash but there is no consensus regarding why modularity itself evolved. Although most hypotheses assume indirect selection for evolvability, here we demonstrate that the ubiquitous, direct selection pressure to reduce the cost of connections between network nodes causes the emergence of modular networks. Computational evolution experiments with selection pressures to maximize network performance and minimize connection costs yield networks that are significantly more modular and more evolvable than control experiments that only select for performance. These results will catalyse research in numerous disciplines, such as neuroscience and genetics, and enhance our ability to harness evolution for engineering purposes.},
  file = {Clune et al. - 2013 - The evolutionary origins of modularity 2.pdf},
  journal = {Proc. R. Soc. B.},
  language = {en},
  number = {1755}
}

@article{Cocchi2016,
  title = {A Hierarchy of Timescales Explains Distinct Effects of Local Inhibition of Primary Visual Cortex and Frontal Eye Fields},
  author = {Cocchi, Luca and Sale, Martin V and L Gollo, Leonardo and Bell, Peter T and Nguyen, Vinh T and Zalesky, Andrew and Breakspear, Michael and Mattingley, Jason B},
  year = {2016},
  month = sep,
  volume = {5},
  issn = {2050-084X},
  doi = {10.7554/eLife.15252},
  file = {Cocchi et al. - 2016 - A hierarchy of timescales explains distinct effect.pdf},
  journal = {eLife},
  language = {en}
}

@article{CogliatiDezza2017,
  title = {Learning the Value of Information and Reward over Time When Solving Exploration-Exploitation Problems},
  author = {Cogliati Dezza, Irene and Yu, Angela J. and Cleeremans, Axel and Alexander, William},
  year = {2017},
  month = dec,
  volume = {7},
  issn = {2045-2322},
  doi = {10.1038/s41598-017-17237-w},
  file = {Cogliati Dezza et al. - 2017 - Learning the value of information and reward over .pdf},
  journal = {Scientific Reports},
  language = {en},
  number = {1}
}

@article{Cohen-KashiMalina2016,
  title = {Local and Thalamic Origins of Ongoing and Sensory Evoked Cortical Correlations},
  author = {{Cohen-Kashi Malina}, Katayun and Mohar, Boaz and Rappaport, Akiva N and Lampl, Ilan},
  year = {2016},
  month = jun,
  doi = {10.1101/058727},
  abstract = {Thalamic inputs of layer 4 (L4) cells in sensory cortices are outnumbered by local connections. Thus, it was suggested that robust sensory response in L4 emerges due to synchronized thalamic activity. In order to investigate the role of both inputs in generation of cortical synchronization, we isolated the thalamic excitatory inputs of cortical cells by optogenetically silencing cortical firing. In anesthetized mice, we measured the correlation between isolated thalamic synaptic inputs of simultaneously patched nearby L4 cells of the barrel cortex. In contrast to correlated activity of excitatory synaptic inputs in the intact cortex, isolated thalamic inputs exhibit lower variability and asynchronous spontaneous and sensory evoked inputs. These results were further supported in awake mice when we recorded the excitatory inputs of individual cortical cells simultaneously with the local field potential (LFP) in a nearby site. Our results therefore indicate that cortical synchronization emerges by intracortical coupling.},
  file = {Cohen-Kashi Malina et al. - 2016 - Local and thalamic origins of ongoing and sensory .pdf},
  journal = {bioRxiv},
  language = {en}
}

@article{Cohen2014,
  title = {Fluctuations in {{Oscillation Frequency Control Spike Timing}} and {{Coordinate Neural Networks}}},
  author = {Cohen, M. X.},
  year = {2014},
  month = jul,
  volume = {34},
  pages = {8988--8998},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.0261-14.2014},
  file = {2014 - Cohen - Fluctuations in Oscillation Frequency Control Spike Timing.pdf;Cohen - 2014 - Fluctuations in Oscillation Frequency Control Spik.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {27}
}

@article{Cohen2015,
  title = {Serotonergic Neurons Signal Reward and Punishment on Multiple Timescales},
  author = {Cohen, Jeremiah Y and Amoroso, Mackenzie W and Uchida, Naoshige},
  year = {2015},
  month = feb,
  volume = {4},
  issn = {2050-084X},
  doi = {10.7554/eLife.06346},
  file = {2015 - Cohen, Amoroso, Uchida - Serotonergic neurons signal reward and punishment on multiple timescales.pdf;Cohen et al. - 2015 - Serotonergic neurons signal reward and punishment .pdf},
  journal = {eLife},
  language = {en}
}

@article{Cohen2017,
  title = {Computational Approaches to {{fMRI}} Analysis},
  author = {Cohen, Jonathan D and Daw, Nathaniel and Engelhardt, Barbara and Hasson, Uri and Li, Kai and Niv, Yael and Norman, Kenneth A and Pillow, Jonathan and Ramadge, Peter J and {Turk-Browne}, Nicholas B and Willke, Theodore L},
  year = {2017},
  month = mar,
  volume = {20},
  pages = {304--313},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.4499},
  file = {Cohen et al. - 2017 - Computational approaches to fMRI analysis.pdf},
  journal = {Nature Neuroscience},
  language = {en},
  number = {3}
}

@article{Colas,
  title = {{{GEP}}-{{PG}}: {{Decoupling Exploration}} and {{Exploitation}} in {{Deep Reinforcement Learning Algorithms}}},
  author = {Colas, Cedric and Sigaud, Olivier and Oudeyer, Pierre-Yves},
  pages = {10},
  abstract = {In continuous action domains, standard deep reinforcement learning algorithms like DDPG suffer from inefficient exploration when facing sparse or deceptive reward problems. Conversely, evolutionary and developmental methods focusing on exploration like Novelty Search, QualityDiversity or Goal Exploration Processes explore more robustly but are less efficient at fine-tuning policies using gradient-descent. In this paper, we present the GEP-PG approach, taking the best of both worlds by sequentially combining a Goal Exploration Process and two variants of DDPG. We study the learning performance of these components and their combination on a low dimensional deceptive reward problem and on the larger HalfCheetah benchmark. We show that DDPG fails on the former and that GEP-PG improves over the best DDPG variant in both environments.},
  file = {Colas et al. - GEP-PG Decoupling Exploration and Exploitation in.pdf},
  language = {en}
}

@article{Colas2019,
  title = {{{CURIOUS}}: {{Intrinsically Motivated Multi}}-{{Task}}  {{Multi}}-{{Goal Reinforcement Learning}}},
  author = {Colas, C{\'e}dric and Fournier, Pierre and Sigaud, Olivier and Chetouani, Mohamed and Oudeyer, Pierre-Yves},
  year = {2019},
  volume = {1810.06284v3},
  pages = {1--15},
  abstract = {In open-ended and changing environments, agents face a wide range of potential tasks that might not come with associated reward functions. Such autonomous learning agents must set their own tasks and build their own curriculum through an intrinsically motivated exploration. Because some tasks might prove easy and some impossible, agents must actively select which task to practice at any given moment, to maximize their overall mastery on the set of learnable tasks. This paper proposes CURIOUS, an algorithm that leverages: 1) an extension of Universal Value Function Approximators to achieve, within a unique policy, multiple tasks, each parameterized by multiple goals and 2) an automated curriculum learning mechanism that biases the attention of the agent towards tasks maximizing the absolute learning progress. Agents focus on achievable tasks first, and focus back on tasks that are being forgotten. Experiments conducted in a new multi-task multigoal robotic environment show that our algorithm benefits from these two ideas and demonstrate properties of robustness to distracting tasks, forgetting and changes in body properties.},
  file = {Colas et al. - CURIOUS Intrinsically Motivated Multi-Task  Multi.pdf},
  journal = {Arxiv},
  language = {en}
}

@article{Colas2020,
  title = {Scaling {{MAP}}-{{Elites}} to {{Deep Neuroevolution}}},
  author = {Colas, C{\'e}dric and Huizinga, Joost and Madhavan, Vashisht and Clune, Jeff},
  year = {2020},
  month = jun,
  pages = {67--75},
  doi = {10.1145/3377930.3390217},
  abstract = {Quality-Diversity (QD) algorithms, and MAP-Elites (ME) in particular, have proven very useful for a broad range of applications including enabling real robots to recover quickly from joint damage, solving strongly deceptive maze tasks or evolving robot morphologies to discover new gaits. However, present implementations of ME and other QD algorithms seem to be limited to low-dimensional controllers with far fewer parameters than modern deep neural network models. In this paper, we propose to leverage the efficiency of Evolution Strategies (ES) to scale MAP-Elites to high-dimensional controllers parameterized by large neural networks. We design and evaluate a new hybrid algorithm called MAP-Elites with Evolution Strategies (ME-ES) for post-damage recovery in a difficult highdimensional control task where traditional ME fails. Additionally, we show that ME-ES performs efficient exploration, on par with state-of-the-art exploration algorithms in high-dimensional control tasks with strongly deceptive rewards.},
  archiveprefix = {arXiv},
  eprint = {2003.01825},
  eprinttype = {arxiv},
  file = {Colas et al. - 2020 - Scaling MAP-Elites to Deep Neuroevolution.pdf},
  journal = {Proceedings of the 2020 Genetic and Evolutionary Computation Conference},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  language = {en}
}

@article{Colas2020a,
  title = {Language as a {{Cognitive Tool}} to {{Imagine Goals}} in {{Curiosity}}-{{Driven Exploration}}},
  author = {Colas, C{\'e}dric and Karch, Tristan and Lair, Nicolas and Dussoux, Jean-Michel and {Moulin-Frier}, Cl{\'e}ment and Dominey, Peter Ford and Oudeyer, Pierre-Yves},
  year = {2020},
  month = jun,
  abstract = {Developmental machine learning studies how artificial agents can model the way children learn open-ended repertoires of skills. Such agents need to create and represent goals, select which ones to pursue and learn to achieve them. Recent approaches have considered goal spaces that were either fixed and hand-defined or learned using generative models of states. This limited agents to sample goals within the distribution of known effects. We argue that the ability to imagine out-of-distribution goals is key to enable creative discoveries and open-ended learning. Children do so by leveraging the compositionality of language as a tool to imagine descriptions of outcomes they never experienced before, targeting them as goals during play. We introduce IMAGINE, an intrinsically motivated deep reinforcement learning architecture that models this ability. Such imaginative agents, like children, benefit from the guidance of a social peer who provides language descriptions. To take advantage of goal imagination, agents must be able to leverage these descriptions to interpret their imagined out-of-distribution goals. This generalization is made possible by modularity: a decomposition between learned goal-achievement reward function and policy relying on deep sets, gated attention and object-centered representations. We introduce the Playground environment and study how this form of goal imagination improves generalization and exploration over agents lacking this capacity. In addition, we identify the properties of goal imagination that enable these results and study the impacts of modularity and social interactions.},
  archiveprefix = {arXiv},
  eprint = {2002.09253},
  eprinttype = {arxiv},
  file = {Colas et al. - 2020 - Language as a Cognitive Tool to Imagine Goals in C.pdf},
  journal = {arXiv:2002.09253 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  language = {en},
  primaryclass = {cs}
}

@article{Colas2020b,
  title = {Language as a {{Cognitive Tool}} to {{Imagine Goals}} in {{Curiosity}}-{{Driven Exploration}}},
  author = {Colas, C{\'e}dric and Karch, Tristan and Lair, Nicolas and Dussoux, Jean-Michel and {Moulin-Frier}, Cl{\'e}ment and Dominey, Peter Ford and Oudeyer, Pierre-Yves},
  year = {2020},
  month = oct,
  abstract = {Developmental machine learning studies how artificial agents can model the way children learn open-ended repertoires of skills. Such agents need to create and represent goals, select which ones to pursue and learn to achieve them. Recent approaches have considered goal spaces that were either fixed and hand-defined or learned using generative models of states. This limited agents to sample goals within the distribution of known effects. We argue that the ability to imagine out-of-distribution goals is key to enable creative discoveries and open-ended learning. Children do so by leveraging the compositionality of language as a tool to imagine descriptions of outcomes they never experienced before, targeting them as goals during play. We introduce IMAGINE, an intrinsically motivated deep reinforcement learning architecture that models this ability. Such imaginative agents, like children, benefit from the guidance of a social peer who provides language descriptions. To take advantage of goal imagination, agents must be able to leverage these descriptions to interpret their imagined out-of-distribution goals. This generalization is made possible by modularity: a decomposition between learned goal-achievement reward function and policy relying on deep sets, gated attention and object-centered representations. We introduce the Playground environment and study how this form of goal imagination improves generalization and exploration over agents lacking this capacity. In addition, we identify the properties of goal imagination that enable these results and study the impacts of modularity and social interactions.},
  archiveprefix = {arXiv},
  eprint = {2002.09253},
  eprinttype = {arxiv},
  file = {Colas et al. - 2020 - Language as a Cognitive Tool to Imagine Goals in C 2.pdf},
  journal = {arXiv:2002.09253 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  language = {en},
  primaryclass = {cs}
}

@article{Colasa,
  title = {{{GEP}}-{{PG}}: {{Decoupling Exploration}} and {{Exploitation}} in {{Deep Reinforcement Learning Algorithms}}},
  author = {Colas, Cedric and Sigaud, Olivier and Oudeyer, Pierre-Yves},
  pages = {10},
  abstract = {In continuous action domains, standard deep reinforcement learning algorithms like DDPG suffer from inefficient exploration when facing sparse or deceptive reward problems. Conversely, evolutionary and developmental methods focusing on exploration like Novelty Search, QualityDiversity or Goal Exploration Processes explore more robustly but are less efficient at fine-tuning policies using gradient-descent. In this paper, we present the GEP-PG approach, taking the best of both worlds by sequentially combining a Goal Exploration Process and two variants of DDPG. We study the learning performance of these components and their combination on a low dimensional deceptive reward problem and on the larger HalfCheetah benchmark. We show that DDPG fails on the former and that GEP-PG improves over the best DDPG variant in both environments.},
  file = {Colas et al. - GEP-PG Decoupling Exploration and Exploitation in 2.pdf},
  language = {en}
}

@techreport{Cole2016,
  title = {Nonsinusoidal Oscillations Underlie Pathological Phase-Amplitude Coupling in the Motor Cortex in {{Parkinson}}'s Disease},
  author = {Cole, Scott R. and Peterson, Erik J. and {van der Meij}, Roemer and {de Hemptinne}, Coralie and Starr, Philip A. and Voytek, Bradley},
  year = {2016},
  month = apr,
  institution = {{Neuroscience}},
  doi = {10.1101/049304},
  abstract = {Parkinson's disease (PD) is associated with abnormal beta oscillations (13-30 Hz) in the basal ganglia and motor cortex (M1). Recent reports show that M1 beta-high gamma (50-200 Hz) phase-amplitude coupling (PAC) is exaggerated in PD and is reduced following acute deep brain stimulation (DBS). Here we analyze invasive M1 electrocorticography recordings in PD patients on and off DBS, and in isolated cervical dystonia patients, and show that M1 beta oscillations are nonsinusoidal, having sharp and asymmetric features. These sharp oscillatory beta features underlie the previously reported PAC, providing an alternative to the standard interpretation of PAC as an interaction between two distinct frequency components. Specifically, the ratio between peak and trough sharpness is nearly perfectly correlated with beta-high gamma PAC (r = 0.96) and predicts PD-related motor deficit. Using a simulation of the local field potential, we demonstrate that sharp oscillatory waves can arise from synchronous synaptic activity. We propose that exaggerated beta-high gamma PAC may actually reflect such synchronous synaptic activity, manifesting as sharp beta oscillations that are ``smoothed out'' with DBS. These results support the ``desynchronization'' hypothesis of DBS wherein DBS counteracts pathological synchronization throughout the basal ganglia-thalamocortical loop. We argue that PAC can be influenced by more than one mechanism. In this case synaptic synchrony, rather than the often assumed spike-field coherence, may underlie exaggerated PAC. These often overlooked temporal features of the oscillatory waveform carry critical physiological information about neural processes and dynamics that may lead to better understanding of underlying neuropathology.},
  file = {Cole et al. - 2016 - Nonsinusoidal oscillations underlie pathological p.pdf},
  language = {en},
  type = {Preprint}
}

@article{Cole2017,
  title = {Brain {{Oscillations}} and the {{Importance}} of {{Waveform Shape}}},
  author = {Cole, Scott R. and Voytek, Bradley},
  year = {2017},
  month = feb,
  volume = {21},
  pages = {137--149},
  issn = {13646613},
  doi = {10.1016/j.tics.2016.12.008},
  file = {Cole and Voytek - 2017 - Brain Oscillations and the Importance of Waveform  2.pdf;Cole and Voytek - 2017 - Brain Oscillations and the Importance of Waveform .pdf},
  journal = {Trends in Cognitive Sciences},
  language = {en},
  number = {2}
}

@article{Cole2017a,
  title = {Nonsinusoidal {{Beta Oscillations Reflect Cortical Pathophysiology}} in {{Parkinson}}'s {{Disease}}},
  author = {Cole, Scott R. and {van der Meij}, Roemer and Peterson, Erik J. and {de Hemptinne}, Coralie and Starr, Philip A. and Voytek, Bradley},
  year = {2017},
  month = may,
  volume = {37},
  pages = {4830--4840},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.2208-16.2017},
  file = {Cole et al. - 2017 - Nonsinusoidal Beta Oscillations Reflect Cortical P.pdf},
  journal = {The Journal of Neuroscience},
  language = {en},
  number = {18}
}

@techreport{Cole2018,
  title = {Hippocampal Theta Bursting and Waveform Shape Reflect {{CA1}} Spiking Patterns},
  author = {Cole, Scott and Voytek, Bradley},
  year = {2018},
  month = oct,
  institution = {{Neuroscience}},
  doi = {10.1101/452987},
  abstract = {Brain rhythms are nearly always analyzed in the spectral domain in terms of their power, phase, and frequency. While this conventional approach has uncovered spike-field coupling, as well as correlations to normal behaviors and pathological states, emerging work has highlighted the physiological and behavioral importance of multiple novel oscillation features. Oscillatory bursts, for example, uniquely index a variety of cognitive states, and the nonsinusoidal shape of oscillations relate to physiological changes, including Parkinson's disease. Open questions remain regarding how bursts and nonsinusoidal features relate to circuit-level processes, and how they interrelate. By analyzing unit and local field recordings in the rodent hippocampus, we uncover a number of significant relationships between oscillatory bursts, nonsinusoidal waveforms, and local inhibitory and excitatory spiking patterns. Bursts of theta oscillations are surprisingly related to a decrease in pyramidal neuron synchrony, and have no detectable effect on firing sequences, despite significant increases in neuronal firing rates during periods of theta bursting. Theta burst duration is predicted by the asymmetries of its first cycle, and cycle asymmetries relate to firing rate, synchrony, and sequences of pyramidal neurons and interneurons. These results provide compelling physiological evidence that time-domain features, of both nonsinusoidal hippocampal theta waveform and the theta bursting state, reflects local circuit properties. These results point to the possibility of inferring circuit states from local field potential features in the hippocampus and perhaps other brain regions with other rhythms.},
  file = {Cole and Voytek - 2018 - Hippocampal theta bursting and waveform shape refl.pdf},
  language = {en},
  type = {Preprint}
}

@article{Cole2019,
  title = {Cycle-by-Cycle Analysis of Neural Oscillations},
  author = {Cole, Scott and Voytek, Bradley},
  year = {2019},
  volume = {122},
  pages = {849--861},
  file = {Cole and Voytek - Cycle-by-cycle analysis of neural oscillations.pdf},
  journal = {Journal of Neurophysiology},
  language = {en},
  number = {2}
}

@article{Colgin2009,
  title = {Frequency of Gamma Oscillations Routes Flow of Information in the Hippocampus},
  author = {Colgin, Laura Lee and Denninger, Tobias and Fyhn, Marianne and Hafting, Torkel and Bonnevie, Tora and Jensen, Ole and Moser, May-Britt and Moser, Edvard I.},
  year = {2009},
  month = nov,
  volume = {462},
  pages = {353--357},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature08573},
  file = {Colgin et al. - 2009 - Frequency of gamma oscillations routes flow of inf.pdf},
  journal = {Nature},
  language = {en},
  number = {7271}
}

@article{Colgin2009a,
  title = {Frequency of Gamma Oscillations Routes Flow of Information in the Hippocampus},
  author = {Colgin, Laura Lee and Denninger, Tobias and Fyhn, Marianne and Hafting, Torkel and Bonnevie, Tora and Jensen, Ole and Moser, May-Britt and Moser, Edvard I.},
  year = {2009},
  month = nov,
  volume = {462},
  pages = {353--357},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature08573},
  file = {Colgin et al. - 2009 - Frequency of gamma oscillations routes flow of inf 2.pdf},
  journal = {Nature},
  language = {en},
  number = {7271}
}

@article{Colgin2016,
  title = {Rhythms of the Hippocampal Network},
  author = {Colgin, Laura Lee},
  year = {2016},
  month = apr,
  volume = {17},
  pages = {239--249},
  issn = {1471-003X, 1471-0048},
  doi = {10.1038/nrn.2016.21},
  abstract = {The hippocampal local field potential (LFP) shows three major types of rhythms: theta, sharp wave\textendash ripples and gamma. These rhythms are defined by their frequencies, they have behavioural correlates in several species including rats and humans, and they have been proposed to carry out distinct functions in hippocampal memory processing. However, recent findings have challenged traditional views on these behavioural functions. In this Review, I discuss our current understanding of the origins and the mnemonic functions of hippocampal theta, sharp wave\textendash ripples and gamma rhythms on the basis of findings from rodent studies. In addition, I present an updated synthesis of their roles and interactions within the hippocampal network.},
  file = {Colgin - 2016 - Rhythms of the hippocampal network.pdf},
  journal = {Nature Reviews Neuroscience},
  language = {en},
  number = {4}
}

@article{Colgin2016a,
  title = {Rhythms of the Hippocampal Network},
  author = {Colgin, Laura Lee},
  year = {2016},
  month = apr,
  volume = {17},
  pages = {239--249},
  issn = {1471-003X, 1471-0048},
  doi = {10.1038/nrn.2016.21},
  abstract = {The hippocampal local field potential (LFP) exhibits three major types of rhythms, theta, sharp wave-ripples and gamma. These rhythms are defined by their frequencies, have behavioral correlates in several species including rats and humans, and have been proposed to perform distinct functions in hippocampal memory processing. However, recent findings have challenged traditional views on these behavioral functions. Here I review our current understanding of the origins and mnemonic functions of hippocampal theta, sharp-wave ripples and gamma rhythms based on findings from rodent studies, and present an updated, synthesized view of their roles and interactions within the hippocampal network.},
  file = {Colgin - 2016 - Rhythms of the hippocampal network 2.pdf},
  journal = {Nat Rev Neurosci},
  language = {en},
  number = {4}
}

@article{Collins2017,
  title = {{{CAPACITY AND TRAINABILITY IN RECURRENT NEURAL NETWORKS}}},
  author = {Collins, Jasmine and {Sohl-Dickstein}, Jascha and Sussillo, David},
  year = {2017},
  pages = {16},
  abstract = {Two potential bottlenecks on the expressiveness of recurrent neural networks (RNNs) are their ability to store information about the task in their parameters, and to store information about the input history in their units. We show experimentally that all common RNN architectures achieve nearly the same per-task and per-unit capacity bounds with careful training, for a variety of tasks and stacking depths. They can store an amount of task information which is linear in the number of parameters, and is approximately 5 bits per parameter. They can additionally store approximately one real number from their input history per hidden unit. We further find that for several tasks it is the per-task parameter capacity bound that determines performance. These results suggest that many previous results comparing RNN architectures are driven primarily by differences in training effectiveness, rather than differences in capacity. Supporting this observation, we compare training difficulty for several architectures, and show that vanilla RNNs are far more difficult to train, yet have higher capacity. Finally, we propose two novel RNN architectures, one of which is easier to train than the LSTM or GRU.},
  file = {Collins et al. - 2017 - CAPACITY AND TRAINABILITY IN RECURRENT NEURAL NETW.pdf},
  language = {en}
}

@article{Collins2020,
  title = {Beyond Dichotomies in Reinforcement Learning},
  author = {Collins, Anne G. E. and Cockburn, Jeffrey},
  year = {2020},
  month = sep,
  issn = {1471-003X, 1471-0048},
  doi = {10.1038/s41583-020-0355-6},
  abstract = {Reinforcement learning (RL) is a framework of particular importance to psychology, neuroscience and machine learning. Interactions between these fields, as promoted through the common hub of RL, has facilitated paradigm shifts that relate multiple levels of analysis in a singular framework (for example, relating dopamine function to a computationally defined RL signal). Recently, more sophisticated RL algorithms have been proposed to better account for human learning, and in particular its oft-documented reliance on two separable systems: a model-based (MB) system and a model-free (MF) system. However, along with many benefits, this dichotomous lens can distort questions, and may contribute to an unnecessarily narrow perspective on learning and decision-making. Here, we outline some of the consequences that come from overconfidently mapping algorithms, such as MB versus MF RL, with putative cognitive processes. We argue that the field is well positioned to move beyond simplistic dichotomies, and we propose a means of refocusing research questions towards the rich and complex components that comprise learning and decision-making.},
  file = {Collins and Cockburn - 2020 - Beyond dichotomies in reinforcement learning.pdf},
  journal = {Nat Rev Neurosci},
  language = {en}
}

@article{Collins2020a,
  title = {Beyond Dichotomies in Reinforcement Learning},
  author = {Collins, Anne G. E. and Cockburn, Jeffrey},
  year = {2020},
  month = oct,
  volume = {21},
  pages = {576--586},
  issn = {1471-003X, 1471-0048},
  doi = {10.1038/s41583-020-0355-6},
  abstract = {Reinforcement learning (RL) is a framework of particular importance to psychology, neuroscience and machine learning. Interactions between these fields, as promoted through the common hub of RL, has facilitated paradigm shifts that relate multiple levels of analysis in a singular framework (for example, relating dopamine function to a computationally defined RL signal). Recently, more sophisticated RL algorithms have been proposed to better account for human learning, and in particular its oft-documented reliance on two separable systems: a model-based (MB) system and a model-free (MF) system. However, along with many benefits, this dichotomous lens can distort questions, and may contribute to an unnecessarily narrow perspective on learning and decision-making. Here, we outline some of the consequences that come from overconfidently mapping algorithms, such as MB versus MF RL, with putative cognitive processes. We argue that the field is well positioned to move beyond simplistic dichotomies, and we propose a means of refocusing research questions towards the rich and complex components that comprise learning and decision-making.},
  file = {Collins and Cockburn - 2020 - Beyond dichotomies in reinforcement learning 2.pdf},
  journal = {Nat Rev Neurosci},
  language = {en},
  number = {10}
}

@article{Collins2021,
  title = {A {{Review}} of {{Physics Simulators}} for {{Robotic Applications}}},
  author = {Collins, Jack and Chand, Shelvin and Vanderkop, Anthony and Howard, David},
  year = {2021},
  volume = {9},
  pages = {51416--51431},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2021.3068769},
  abstract = {The use of simulators in robotics research is widespread, underpinning the majority of recent advances in the field. There are now more options available to researchers than ever before, however navigating through the plethora of choices in search of the right simulator is often non-trivial. Depending on the field of research and the scenario to be simulated there will often be a range of suitable physics simulators from which it is difficult to ascertain the most relevant one. We have compiled a broad review of physics simulators for use within the major fields of robotics research. More specifically, we navigate through key sub-domains and discuss the features, benefits, applications and use-cases of the different simulators categorised by the respective research communities. Our review provides an extensive index of the leading physics simulators applicable to robotics researchers and aims to assist them in choosing the best simulator for their use case.},
  file = {Collins et al. - 2021 - A Review of Physics Simulators for Robotic Applica.pdf},
  journal = {IEEE Access},
  language = {en}
}

@article{Comins1976,
  title = {Predation in Multi-Prey Communities},
  author = {Comins, H.N. and Hassell, M.P.},
  year = {1976},
  month = oct,
  volume = {62},
  pages = {93--114},
  issn = {00225193},
  doi = {10.1016/0022-5193(76)90053-9},
  file = {Comins and Hassell - 1976 - Predation in multi-prey communities.pdf},
  journal = {Journal of Theoretical Biology},
  language = {en},
  number = {1}
}

@article{Cona2014,
  title = {A Thalamo-Cortical Neural Mass Model for the Simulation of Brain Rhythms during Sleep},
  author = {Cona, F. and Lacanna, M. and Ursino, M.},
  year = {2014},
  month = aug,
  volume = {37},
  pages = {125--148},
  issn = {0929-5313, 1573-6873},
  doi = {10.1007/s10827-013-0493-1},
  file = {2014 - Cona, Lacanna, Ursino - A thalamo-cortical neural mass model for the simulation of brain rhythms during sleep.pdf;Cona et al. - 2014 - A thalamo-cortical neural mass model for the simul.pdf},
  journal = {Journal of Computational Neuroscience},
  language = {en},
  number = {1}
}

@article{Conaway2017,
  title = {Solving {{Nonlinearly Separable Classifications}} in a {{Single}}-{{Layer Neural Network}}},
  author = {Conaway, Nolan and Kurtz, Kenneth J.},
  year = {2017},
  month = mar,
  volume = {29},
  pages = {861--866},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/NECO_a_00931},
  file = {Conaway and Kurtz - 2017 - Solving Nonlinearly Separable Classifications in a.pdf},
  journal = {Neural Computation},
  language = {en},
  number = {3}
}

@article{Connolly2012,
  title = {The {{Representation}} of {{Biological Classes}} in the {{Human Brain}}},
  author = {Connolly, A. C. and Guntupalli, J. S. and Gors, J. and Hanke, M. and Halchenko, Y. O. and Wu, Y.-C. and Abdi, H. and Haxby, J. V.},
  year = {2012},
  month = feb,
  volume = {32},
  pages = {2608--2618},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.5547-11.2012},
  file = {2012 - Connolly et al. - The representation of biological classes in the human brain.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {8}
}

@inproceedings{Connolly2015,
  title = {Guiding Deep Brain Stimulation Contact Selection Using Local Field Potentials Sensed by a Chronically Implanted Device in {{Parkinson}}'s Disease Patients},
  booktitle = {2015 7th {{International IEEE}}/{{EMBS Conference}} on {{Neural Engineering}} ({{NER}})},
  author = {Connolly, Allison T. and Kaemmerer, William F. and Dani, Siddharth and Stanslaski, Scott R. and Panken, Eric and Johnson, Matthew D. and Denison, Timothy},
  year = {2015},
  month = apr,
  pages = {840--843},
  publisher = {{IEEE}},
  address = {{Montpellier, France}},
  doi = {10.1109/NER.2015.7146754},
  abstract = {We have found that a set of support vector machines operating upon local field potentials sensed from an implanted DBS lead can identify the contact chosen by the physician for the patient's STN DBS therapy with 91\% accuracy. The finding is based on a small data set and thus subject to change with further data collection and crossvalidation. Nevertheless, the results suggest that an algorithm for selecting an effective contact for STN DBS based on the signals sensed from the DBS lead may be feasible.},
  file = {2015 - Connolly et al. - Guiding Deep Brain Stimulation Contact Selection Using Local Field Potentials Sensed by a Chronically Implant.pdf;Connolly et al. - 2015 - Guiding deep brain stimulation contact selection u.pdf},
  isbn = {978-1-4673-6389-1},
  language = {en}
}

@article{Connolly2015a,
  title = {Modulations in {{Oscillatory Frequency}} and {{Coupling}} in {{Globus Pallidus}} with {{Increasing Parkinsonian Severity}}},
  author = {Connolly, Allison T. and Jensen, Alicia L. and Bello, Edward M. and Netoff, Theoden I. and Baker, Kenneth B. and Johnson, Matthew D. and Vitek, Jerrold L.},
  year = {2015},
  month = apr,
  volume = {35},
  pages = {6231--6240},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.4137-14.2015},
  file = {../../Zotero/storage/65572R6R/Connolly et al. - 2015 - Modulations in Oscillatory Frequency and Coupling .pdf;Connolly et al. - 2015 - Modulations in Oscillatory Frequency and Coupling .pdf},
  journal = {The Journal of Neuroscience},
  language = {en},
  number = {15}
}

@article{Connor1977,
  title = {Neural Repetitive Firing: Modifications of the {{Hodgkin}}-{{Huxley}} Axon Suggested by Experimental Results from Crustacean Axons},
  shorttitle = {Neural Repetitive Firing},
  author = {Connor, J.A. and Walter, D. and McKown, R.},
  year = {1977},
  month = apr,
  volume = {18},
  pages = {81--102},
  issn = {00063495},
  doi = {10.1016/S0006-3495(77)85598-7},
  abstract = {The Hodgkin-Huxley equations for space-clamped squid axon (18'C) have been modified to approximate voltage clamp data from repetitive-firing crustacean walking leg axons and activity in response to constant current stimulation has been computed. The ino and h. parameters of the sodium conductance system were shifted along the voltage axis in opposite directions so that their relative overlap was increased approximately 7 mV. Time constants, Tm and Th, were moved in a similar manner. Voltage-dependent parameters of delayed potassium conductance, n,O and T, were shifted 4.3 mV in the positive direction and Tr was uniformly increased by a factor of 2. Leakage conductance and capacitance were unchanged. Repetitive activity of this modified circuit was qualitatively similar to that of the standard model. A fifth branch was added to the circuit representing a transient potassium conductance system present in the repetitive walking leg axons and in other repetitive neurons. This model, with various parameter choices, fired repetitively down to approximately 2 spikes/s and up to 350/s. The frequency vs. stimulus current plot could be fit well by a straight line over a decade of the low frequency range and the general appearance of the spike trains was similar to that of other repetitive neurons. Stimulus intensities were of the same order as those which produce repetitive activity in the standard Hodgkin-Huxley axon. The repetitive firing rate and first spike latency (utilization time) were found to be most strongly influenced by the inactivation time constant of the transient potassium conductance (TB), the delayed potassium conductance (Tn), and the value of leakage conductance (ga. The model presents a mechanism by which stable low frequency discharge can be generated by millisecondorder membrane conductance changes.},
  file = {1977 - The - Modifications of the Hodgkin-Huxley Axon Suggested.pdf},
  journal = {Biophysical Journal},
  language = {en},
  number = {1}
}

@article{Conroy,
  title = {{{fMRI}}-{{Based Inter}}-{{Subject Cortical Alignment Using Functional Connectivity}}},
  author = {Conroy, Bryan R and Singer, Benjamin D and Haxby, James V and Ramadge, Peter J},
  pages = {9},
  abstract = {The inter-subject alignment of functional MRI (fMRI) data is important for improving the statistical power of fMRI group analyses. In contrast to existing anatomically-based methods, we propose a novel multi-subject algorithm that derives a functional correspondence by aligning spatial patterns of functional connectivity across a set of subjects. We test our method on fMRI data collected during a movie viewing experiment. By cross-validating the results of our algorithm, we show that the correspondence successfully generalizes to a secondary movie dataset not used to derive the alignment.},
  file = {2009 - Conroy, Singer - fMRI-based inter-subject cortical alignment using functional connectivity.pdf},
  language = {en}
}

@article{Contia,
  title = {Improving {{Exploration}} in {{Evolution Strategies}} for {{Deep Reinforcement Learning}} via a {{Population}} of {{Novelty}}-{{Seeking Agents}}},
  author = {Conti, Edoardo and Madhavan, Vashisht and Such, Felipe Petroski and Lehman, Joel and Stanley, Kenneth and Clune, Jeff},
  pages = {12},
  abstract = {Evolution strategies (ES) are a family of black-box optimization algorithms able to train deep neural networks roughly as well as Q-learning and policy gradient methods on challenging deep reinforcement learning (RL) problems, but are much faster (e.g. hours vs. days) because they parallelize better. However, many RL problems require directed exploration because they have reward functions that are sparse or deceptive (i.e. contain local optima), and it is unknown how to encourage such exploration with ES. Here we show that algorithms that have been invented to promote directed exploration in small-scale evolved neural networks via populations of exploring agents, specifically novelty search (NS) and quality diversity (QD) algorithms, can be hybridized with ES to improve its performance on sparse or deceptive deep RL tasks, while retaining scalability. Our experiments confirm that the resultant new algorithms, NS-ES and two QD algorithms, NSR-ES and NSRA-ES, avoid local optima encountered by ES to achieve higher performance on Atari and simulated robots learning to walk around a deceptive trap. This paper thus introduces a family of fast, scalable algorithms for reinforcement learning that are capable of directed exploration. It also adds this new family of exploration algorithms to the RL toolbox and raises the interesting possibility that analogous algorithms with multiple simultaneous paths of exploration might also combine well with existing RL algorithms outside ES.},
  file = {Conti et al. - Improving Exploration in Evolution Strategies for  2.pdf},
  language = {en}
}

@article{Contin2010,
  title = {Pharmacokinetics of Levodopa},
  author = {Contin, Manuela and Martinelli, Paolo},
  year = {2010},
  month = nov,
  volume = {257},
  pages = {253--261},
  issn = {0340-5354, 1432-1459},
  doi = {10.1007/s00415-010-5728-8},
  file = {2010 - Contin, Martinelli - Pharmacokinetics of levodopa.pdf},
  journal = {Journal of Neurology},
  language = {en},
  number = {S2}
}

@article{Contractor2015,
  title = {Altered {{Neuronal}} and {{Circuit Excitability}} in {{Fragile X Syndrome}}},
  author = {Contractor, Anis and Klyachko, Vitaly A. and {Portera-Cailliau}, Carlos},
  year = {2015},
  month = aug,
  volume = {87},
  pages = {699--715},
  issn = {08966273},
  doi = {10.1016/j.neuron.2015.06.017},
  file = {Contractor et al. - 2015 - Altered Neuronal and Circuit Excitability in Fragi.pdf},
  journal = {Neuron},
  language = {en},
  number = {4}
}

@article{Cook,
  title = {It {{Takes Two Neurons To Ride}} a {{Bicycle}}},
  author = {Cook, Matthew},
  pages = {8},
  abstract = {Past attempts to get computers to ride bicycles have required an inordinate amount of learning time (1700 practice rides for a reinforcement learning approach [1], while still failing to be able to ride in a straight line), or have required an algebraic analysis of the exact equations of motion for the specific bicycle to be controlled [2, 3]. Mysteriously, humans do not need to do either of these when learning to ride a bicycle.},
  file = {2004 - Cook - It takes two neurons to ride a bicycle.pdf},
  language = {en}
}

@article{Cools2006,
  title = {Dopaminergic Modulation of Cognitive Function-Implications for l-{{DOPA}} Treatment in {{Parkinson}}'s Disease},
  author = {Cools, Roshan},
  year = {2006},
  month = jan,
  volume = {30},
  pages = {1--23},
  issn = {01497634},
  doi = {10.1016/j.neubiorev.2005.03.024},
  abstract = {It is well recognised that patients with Parkinson's disease exhibit cognitive deficits, even in the earliest disease stages. Whereas, L-DOPA therapy in early Parkinson's disease is accepted to improve the motor symptoms, the effects on cognitive performance are more complex: both positive and negative effects have been observed. The purpose of the present article is to review the effects of L-DOPA medication in Parkinson's disease on cognitive functions in the broad domains of cognitive flexibility and working memory. The review places the effects in Parkinson's disease within a framework of evidence from studies with healthy human volunteers, rodents and non-human primates as well as computational modeling work. It is suggested that beneficial or detrimental effects of L-DOPA are observed depending on task demands and basal dopamine levels in distinct parts of the striatum. The study of the beneficial and detrimental cognitive effects of L-DOPA in Parkinson's disease has substantial implications for the understanding and treatment development of cognitive abnormalities in Parkinson's disease as well as normal health.},
  file = {2006 - Cools - Dopaminergic modulation of cognitive function-implications for L-DOPA treatment in Parkinson's disease.pdf},
  journal = {Neuroscience \& Biobehavioral Reviews},
  language = {en},
  number = {1}
}

@article{Coombes2005,
  title = {Waves, Bumps, and Patterns in Neural Field Theories},
  author = {Coombes, S.},
  year = {2005},
  month = aug,
  volume = {93},
  pages = {91--108},
  issn = {0340-1200, 1432-0770},
  doi = {10.1007/s00422-005-0574-y},
  abstract = {Neural field models of firing rate activity have had a major impact in helping to develop an understanding of the dynamics seen in brain slice preparations. These models typically take the form of integrodifferential equations. Their non-local nature has led to the development of a set of analytical and numerical tools for the study of waves, bumps and patterns, based around natural extensions of those used for local differential equation models. In this paper we present a review of such techniques and show how recent advances have opened the way for future studies of neural fields in both one and two dimensions that can incorporate realistic forms of axo-dendritic interactions and the slow intrinsic currents that underlie bursting behaviour in single neurons.},
  file = {2005 - Coombes - Waves, bumps, and patterns in neural field theories.pdf},
  journal = {Biological Cybernetics},
  language = {en},
  number = {2}
}

@article{Coon2016,
  title = {Oscillatory Phase Modulates the Timing of Neuronal Activations and Resulting Behavior},
  author = {Coon, W.G. and Gunduz, A. and Brunner, P. and Ritaccio, A.L. and Pesaran, B. and Schalk, G.},
  year = {2016},
  month = jun,
  volume = {133},
  pages = {294--301},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2016.02.080},
  abstract = {Human behavioral response timing is highly variable from trial to trial. While it is generally understood that behavioral variability must be due to trial-by-trial variations in brain function, it is still largely unknown which physiological mechanisms govern the timing of neural activity as it travels through networks of neuronal populations, and how variations in the timing of neural activity relate to variations in the timing of behavior. In our study, we submitted recordings from the cortical surface to novel analytic techniques to chart the trajectory of neuronal population activity across the human cortex in single trials, and found joint modulation of the timing of this activity and of consequent behavior by neuronal oscillations in the alpha band (8\textendash 12 Hz). Specifically, we established that the onset of population activity tends to occur during the trough of oscillatory activity, and that deviations from this preferred relationship are related to changes in the timing of population activity and the speed of the resulting behavioral response. These results indicate that neuronal activity incurs variable delays as it propagates across neuronal populations, and that the duration of each delay is a function of the instantaneous phase of oscillatory activity. We conclude that the results presented in this paper are supportive of a general model for variability in the effective speed of information transmission in the human brain and for variability in the timing of human behavior.},
  file = {Coon et al. - 2016 - Oscillatory phase modulates the timing of neuronal 2.pdf;Coon et al. - 2016 - Oscillatory phase modulates the timing of neuronal.pdf},
  journal = {NeuroImage},
  language = {en}
}

@article{Cooper2003,
  title = {Paradox Lost? {{Exploring}} the Role of Alpha Oscillations during Externally vs. Internally Directed Attention and the Implications for Idling and Inhibition Hypotheses},
  shorttitle = {Paradox Lost?},
  author = {Cooper, Nicholas R and Croft, Rodney J and Dominey, Samuel J.J and Burgess, Adrian P and Gruzelier, John H},
  year = {2003},
  month = jan,
  volume = {47},
  pages = {65--74},
  issn = {01678760},
  doi = {10.1016/S0167-8760(02)00107-1},
  abstract = {Although slow waves of the electroencephalogram (EEG) have been associated with attentional processes, the functional significance of the alpha component in the EEG (8.1\textendash 12 Hz) remains uncertain. Conventionally, synchronisation in the alpha frequency range is taken to be a marker of cognitive inactivity, i.e. `cortical idling'. However, it has been suggested that alpha may index the active inhibition of sensory information during internally directed attentional tasks such as mental imagery. More recently, this idea has been amended to encompass the notion of alpha synchronisation as a means of inhibition of non-task relevant cortical areas irrespective of the direction of attention. Here we test the adequacy of the one idling and two inhibition hypotheses about alpha. In two experiments we investigated the relation between alpha and internally vs. externally directed attention using mental imagery vs. sensory-intake paradigms. Results from both experiments showed a clear relationship between alpha and both attentional factors and increased task demands. At various scalp sites alpha amplitudes were greater during internally directed attention and during increased load, results incompatible with alpha reflecting cortical idling and more in keeping with suggestions of active inhibition necessary for internally driven mental operations.},
  file = {2003 - Cooper et al. - Paradox lost Exploring the role of alpha oscillations during externally vs. internally directed attention and the.pdf},
  journal = {International Journal of Psychophysiology},
  language = {en},
  number = {1}
}

@article{Corbit2016,
  title = {Pallidostriatal {{Projections Promote Oscillations}} in a {{Dopamine}}-{{Depleted Biophysical Network Model}}},
  author = {Corbit, V. L. and Whalen, T. C. and Zitelli, K. T. and Crilly, S. Y. and Rubin, J. E. and Gittis, A. H.},
  year = {2016},
  month = may,
  volume = {36},
  pages = {5556--5571},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.0339-16.2016},
  file = {Corbit et al. - 2016 - Pallidostriatal Projections Promote Oscillations i.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {20}
}

@incollection{Corne2003,
  title = {No {{Free Lunch}} and {{Free Leftovers Theorems}} for {{Multiobjective Optimisation Problems}}},
  booktitle = {Evolutionary {{Multi}}-{{Criterion Optimization}}},
  author = {Corne, David W. and Knowles, Joshua D.},
  editor = {Goos, Gerhard and Hartmanis, Juris and {van Leeuwen}, Jan and Fonseca, Carlos M. and Fleming, Peter J. and Zitzler, Eckart and Thiele, Lothar and Deb, Kalyanmoy},
  year = {2003},
  volume = {2632},
  pages = {327--341},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/3-540-36970-8_23},
  abstract = {The classic NFL theorems are invariably cast in terms of single objective optimization problems. We confirm that the classic NFL theorem holds for general multiobjective fitness spaces, and show how this follows from a `single-objective' NFL theorem. We also show that, given any particular Pareto Front, an NFL theorem holds for the set of all multiobjective problems which have that Pareto Front. It follows that, given any `shape' or class of Pareto fronts, an NFL theorem holds for the set of all multiobjective problems in that class. These findings have salience in test function design. Such NFL results are cast in the typical context of absolute performance, assuming a performance metric which returns a value based on the result produced by a single algorithm. But, in multiobjective search we commonly use comparative metrics, which return performance measures based non-trivially on the results from two (or more) algorithms. Closely related to but extending the observations in the seminal NFL work concerning minimax distinctions between algorithms, we provide a `Free Leftovers' theorem for comparative performance of algorithms over permutation functions; in words: over the space of permutation problems, every algorithm has some companion algorithm(s) which it outperforms, according to a certain well-behaved metric, when comparative performance is summed over all problems in the space.},
  file = {Corne and Knowles - 2003 - No Free Lunch and Free Leftovers Theorems for Mult.pdf},
  isbn = {978-3-540-01869-8 978-3-540-36970-7},
  language = {en}
}

@incollection{Corne2003a,
  title = {No {{Free Lunch}} and {{Free Leftovers Theorems}} for {{Multiobjective Optimisation Problems}}},
  booktitle = {Evolutionary {{Multi}}-{{Criterion Optimization}}},
  author = {Corne, David W. and Knowles, Joshua D.},
  editor = {Goos, Gerhard and Hartmanis, Juris and {van Leeuwen}, Jan and Fonseca, Carlos M. and Fleming, Peter J. and Zitzler, Eckart and Thiele, Lothar and Deb, Kalyanmoy},
  year = {2003},
  volume = {2632},
  pages = {327--341},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/3-540-36970-8_23},
  abstract = {The classic NFL theorems are invariably cast in terms of single objective optimization problems. We confirm that the classic NFL theorem holds for general multiobjective fitness spaces, and show how this follows from a `single-objective' NFL theorem. We also show that, given any particular Pareto Front, an NFL theorem holds for the set of all multiobjective problems which have that Pareto Front. It follows that, given any `shape' or class of Pareto fronts, an NFL theorem holds for the set of all multiobjective problems in that class. These findings have salience in test function design. Such NFL results are cast in the typical context of absolute performance, assuming a performance metric which returns a value based on the result produced by a single algorithm. But, in multiobjective search we commonly use comparative metrics, which return performance measures based non-trivially on the results from two (or more) algorithms. Closely related to but extending the observations in the seminal NFL work concerning minimax distinctions between algorithms, we provide a `Free Leftovers' theorem for comparative performance of algorithms over permutation functions; in words: over the space of permutation problems, every algorithm has some companion algorithm(s) which it outperforms, according to a certain well-behaved metric, when comparative performance is summed over all problems in the space.},
  file = {Corne and Knowles - 2003 - No Free Lunch and Free Leftovers Theorems for Mult.pdf},
  isbn = {978-3-540-01869-8 978-3-540-36970-7},
  language = {en}
}

@article{Cornell-Bell1990,
  title = {Glutamate Induces Calcium Waves in Cultured Astrocytes: Long-Range Glial Signaling},
  shorttitle = {Glutamate Induces Calcium Waves in Cultured Astrocytes},
  author = {{Cornell-Bell}, A. and Finkbeiner, S. and Cooper, M. and Smith, S.},
  year = {1990},
  month = jan,
  volume = {247},
  pages = {470--473},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1967852},
  file = {Cornell-Bell et al. - 1990 - Glutamate induces calcium waves in cultured astroc.pdf},
  journal = {Science},
  language = {en},
  number = {4941}
}

@article{Cornell-Bell1990a,
  title = {Glutamate Induces Calcium Waves in Cultured Astrocytes: Long-Range Glial Signaling},
  shorttitle = {Glutamate Induces Calcium Waves in Cultured Astrocytes},
  author = {{Cornell-Bell}, A. and Finkbeiner, S. and Cooper, M. and Smith, S.},
  year = {1990},
  month = jan,
  volume = {247},
  pages = {470--473},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1967852},
  file = {Cornell-Bell et al. - 1990 - Glutamate induces calcium waves in cultured astroc 2.pdf},
  journal = {Science},
  language = {en},
  number = {4941}
}

@article{Cossell2015,
  title = {Functional Organization of Excitatory Synaptic Strength in Primary Visual Cortex},
  author = {Cossell, Lee and Iacaruso, Maria Florencia and Muir, Dylan R. and Houlton, Rachael and Sader, Elie N. and Ko, Ho and Hofer, Sonja B. and {Mrsic-Flogel}, Thomas D.},
  year = {2015},
  month = feb,
  volume = {518},
  pages = {399--403},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature14182},
  file = {2015 - Cossell et al. - Functional organization of excitatory synaptic strength in primary visual cortex.pdf;Cossell et al. - 2015 - Functional organization of excitatory synaptic str.pdf},
  journal = {Nature},
  language = {en},
  number = {7539}
}

@article{Costa2019,
  title = {Subcortical {{Substrates}} of {{Explore}}-{{Exploit Decisions}} in {{Primates}}},
  author = {Costa, Vincent D. and Mitz, Andrew R. and Averbeck, Bruno B.},
  year = {2019},
  month = aug,
  volume = {103},
  pages = {533-545.e5},
  issn = {08966273},
  doi = {10.1016/j.neuron.2019.05.017},
  abstract = {The explore-exploit dilemma refers to the challenge of deciding when to forego immediate rewards and explore new opportunities that could lead to greater rewards in the future. While motivational neural circuits facilitate learning based on past choices and outcomes, it is unclear whether they also support computations relevant for deciding when to explore. We recorded neural activity in the amygdala and ventral striatum of rhesus macaques as they solved a task that required them to balance novelty-driven exploration with exploitation of what they had already learned. Using a partially observable Markov decision process (POMDP) model to quantify explore-exploit trade-offs, we identified that the ventral striatum and amygdala differ in how they represent the immediate value of exploitative choices and the future value of exploratory choices. These findings show that subcortical motivational circuits are important in guiding explore-exploit decisions.},
  file = {Costa et al. - 2019 - Subcortical Substrates of Explore-Exploit Decision.pdf},
  journal = {Neuron},
  language = {en},
  number = {3}
}

@article{Courbariaux2015,
  title = {{{BinaryConnect}}: {{Training Deep Neural Networks}} with Binary Weights during Propagations},
  shorttitle = {{{BinaryConnect}}},
  author = {Courbariaux, Matthieu and Bengio, Yoshua and David, Jean-Pierre},
  year = {2015},
  month = nov,
  abstract = {Deep Neural Networks (DNN) have achieved state-of-the-art results in a wide range of tasks, with the best results obtained with large training sets and large models. In the past, GPUs enabled these breakthroughs because of their greater computational speed. In the future, faster computation at both training and test time is likely to be crucial for further progress and for consumer applications on low-power devices. As a result, there is much interest in research and development of dedicated hardware for Deep Learning (DL). Binary weights, i.e., weights which are constrained to only two possible values (e.g. -1 or 1), would bring great benefits to specialized DL hardware by replacing many multiply-accumulate operations by simple accumulations, as multipliers are the most space and powerhungry components of the digital implementation of neural networks. We introduce BinaryConnect, a method which consists in training a DNN with binary weights during the forward and backward propagations, while retaining precision of the stored weights in which gradients are accumulated. Like other dropout schemes, we show that BinaryConnect acts as regularizer and we obtain near state-of-the-art results with BinaryConnect on the permutation-invariant MNIST, CIFAR-10 and SVHN.},
  archiveprefix = {arXiv},
  eprint = {1511.00363},
  eprinttype = {arxiv},
  file = {2015 - Courbariaux, Bengio, David - BinaryConnect Training Deep Neural Networks with binary weights during propagations.pdf;Courbariaux et al. - 2015 - BinaryConnect Training Deep Neural Networks with .pdf},
  journal = {arXiv:1511.00363 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  language = {en},
  primaryclass = {cs}
}

@article{Courtiol2020,
  title = {Dynamical {{Mechanisms}} of {{Interictal Resting}}-{{State Functional Connectivity}} in {{Epilepsy}}},
  author = {Courtiol, Julie and Guye, Maxime and Bartolomei, Fabrice and Petkoski, Spase and Jirsa, Viktor K.},
  year = {2020},
  month = jul,
  volume = {40},
  pages = {5572--5588},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.0905-19.2020},
  file = {Courtiol et al. - 2020 - Dynamical Mechanisms of Interictal Resting-State F.pdf},
  journal = {J. Neurosci.},
  language = {en},
  number = {29}
}

@article{Coutanche2012,
  title = {The Advantage of Brief {{fMRI}} Acquisition Runs for Multi-Voxel Pattern Detection across Runs},
  author = {Coutanche, Marc N. and {Thompson-Schill}, Sharon L.},
  year = {2012},
  month = jul,
  volume = {61},
  pages = {1113--1119},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2012.03.076},
  abstract = {Functional magnetic resonance imaging (fMRI) studies are broken up into runs (or `sessions'), frequently selected to be long to minimize across-run signal variations. For investigations that use multi-voxel pattern analysis (MVPA), however, employing many short runs might improve a classifier's ability to generalize across irrelevant pattern variations and detect condition-related activity patterns. We directly tested this hypothesis by scanning participants with both long and short runs and comparing MVPA performance using data from each set of runs. Every run included presentations of faces, places, man-made objects and fruit in a blocked 1-back design. MVPA performance significantly improved from using a large number of short runs, compared to several long runs, in across-run classifications with identical amounts of data. Superior classification was found across variations in the classifier employed, feature selection procedure and region of interest. Performance improvements also extended to an information brain mapping `searchlight' procedure. These results suggest that investigators looking to maximize the detection of subtle multi-voxel patterns across runs might consider employing short fMRI runs.},
  file = {2013 - Coutanche, Thompson-Schill - The advantage of brief fMRI acquisition runs for multi-voxel pattern detection across runs.pdf},
  journal = {NeuroImage},
  language = {en},
  number = {4}
}

@article{Cowen2017,
  title = {Self-Report Captures 27 Distinct Categories of Emotion Bridged by Continuous Gradients},
  author = {Cowen, Alan S. and Keltner, Dacher},
  year = {2017},
  month = sep,
  volume = {114},
  pages = {E7900-E7909},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1702247114},
  file = {Cowen and Keltner - 2017 - Self-report captures 27 distinct categories of emo.pdf},
  journal = {Proc Natl Acad Sci USA},
  language = {en},
  number = {38}
}

@article{Cox2003,
  title = {Functional Magnetic Resonance Imaging ({{fMRI}}) ``Brain Reading'': Detecting and Classifying Distributed Patterns of {{fMRI}} Activity in Human Visual Cortex},
  shorttitle = {Functional Magnetic Resonance Imaging ({{fMRI}}) ``Brain Reading''},
  author = {Cox, David D and Savoy, Robert L},
  year = {2003},
  month = jun,
  volume = {19},
  pages = {261--270},
  issn = {10538119},
  doi = {10.1016/S1053-8119(03)00049-1},
  abstract = {Traditional (univariate) analysis of functional MRI (fMRI) data relies exclusively on the information contained in the time course of individual voxels. Multivariate analyses can take advantage of the information contained in activity patterns across space, from multiple voxels. Such analyses have the potential to greatly expand the amount of information extracted from fMRI data sets. In the present study, multivariate statistical pattern recognition methods, including linear discriminant analysis and support vector machines, were used to classify patterns of fMRI activation evoked by the visual presentation of various categories of objects. Classifiers were trained using data from voxels in predefined regions of interest during a subset of trials for each subject individually. Classification of subsequently collected fMRI data was attempted according to the similarity of activation patterns to prior training examples. Classification was done using only small amounts of data (20 s worth) at a time, so such a technique could, in principle, be used to extract information about a subject's percept on a near real-time basis. Classifiers trained on data acquired during one session were equally accurate in classifying data collected within the same session and across sessions separated by more than a week, in the same subject. Although the highest classification accuracies were obtained using patterns of activity including lower visual areas as input, classification accuracies well above chance were achieved using regions of interest restricted to higher-order object-selective visual areas. In contrast to typical fMRI data analysis, in which hours of data across many subjects are averaged to reveal slight differences in activation, the use of pattern recognition methods allows a subtle 10-way discrimination to be performed on an essentially trial-by-trial basis within individuals, demonstrating that fMRI data contain far more information than is typically appreciated.},
  file = {2003 - Cox, Savoy - Functional magnetic resonance imaging (fMRI) “brain reading” detecting and classifying distributed patterns of f.pdf},
  journal = {NeuroImage},
  language = {en},
  number = {2}
}

@article{Cox2016,
  title = {Variability and Stability of Large-Scale Cortical Oscillation Patterns},
  author = {Cox, Roy and Schapiro, Anna and Stickgold, Robert},
  year = {2016},
  month = dec,
  doi = {10.1101/093005},
  abstract = {Individual differences in brain organization exist at many spatial and temporal scales, contributing to the substantial heterogeneity underlying human thought and behavior. Oscillatory neural activity is crucial for these behaviors, but how such rhythms are expressed across the cortex within and across individuals has not been thoroughly characterized. Combining electroencephalography (EEG) with representational similarity and multivariate classification techniques, we provide a systematic characterization of brain-wide activity across frequency bands and oscillatory features during rest and task performance. Results indicate that oscillatory profiles exhibit sizable group-level correspondences, indicating the presence of common templates of oscillatory organization. At the same time, we observed well-defined subject-specific network profiles that were discernible above and beyond the structure shared across individuals. These individualized patterns were sufficiently stable over time to allow successful classification of individuals several months later. Finally, our findings indicate that the network structure of rhythmic activity varies considerably across distinct oscillatory frequencies and features, suggesting the existence of multiple, parallel information processing streams embedded in distributed electrophysiological activity. Together, these findings affirm the richness of spatiotemporal EEG signals and emphasize the utility of multivariate network analyses for understanding the role of brain oscillations in physiology and behavior.},
  file = {Cox et al. - 2016 - Variability and stability of large-scale cortical .pdf},
  journal = {bioRxiv},
  language = {en}
}

@article{Cragg2004,
  title = {Synaptic Release of Dopamine in the Subthalamic Nucleus},
  author = {Cragg, Stephanie J. and Baufreton, Jerome and Xue, Yi and Bolam, J. Paul and Bevan, Mark D.},
  year = {2004},
  month = oct,
  volume = {20},
  pages = {1788--1802},
  issn = {0953-816X, 1460-9568},
  doi = {10.1111/j.1460-9568.2004.03629.x},
  abstract = {The direct modulation of subthalamic nucleus (STN) neurons by dopamine (DA) neurons of the substantia nigra (SN) is controversial owing to the thick caliber and low density of DA axons in the STN. The abnormal activity of the STN in Parkinson's disease (PD), which is central to the appearance of symptoms, is therefore thought to result from the loss of DA in the striatum. We carried out three experiments in rats to explore the function of DA in the STN: (i) light and electron microscopic analysis of tyrosine hydroxylase (TH)-, dopamine b-hydroxylase (DbH)- and DA-immunoreactive structures to determine whether DA axons form synapses; (ii) fast-scan cyclic voltammetry (FCV) to determine whether DA axons release DA; and (iii) patch clamp recording to determine whether DA, at a concentration similar to that detected by FCV, can modulate activity and synaptic transmission {$\fracslash$} integration. TH- and DAimmunoreactive axons mostly formed symmetric synapses. Because DbH-immunoreactive axons were rare and formed asymmetric synapses, they comprised the minority of TH-immunoreactive synapses. Voltammetry demonstrated that DA release was sufficient for the activation of receptors and abolished by blockade of voltage-dependent Na+ channels or removal of extracellular Ca2+. The lifetime and concentration of extracellular DA was increased by blockade of the DA transporter. Dopamine application depolarized STN neurons, increased their frequency of activity and reduced the impact of c-aminobutyric acid (GABA)-ergic inputs. These findings suggest that SN DA neurons directly modulate the activity of STN neurons and their loss may contribute to the abnormal activity of STN neurons in PD.},
  file = {2004 - Cragg et al. - Synaptic release of dopamine in the subthalamic nucleus.pdf},
  journal = {European Journal of Neuroscience},
  language = {en},
  number = {7}
}

@book{Crank1975,
  title = {The Mathematics of Diffusion},
  author = {Crank, John},
  year = {1975},
  edition = {2d ed},
  publisher = {{Clarendon Press}},
  address = {{Oxford, [Eng]}},
  file = {1975 - Crank - THE MATHEMATICS OF DIFFUSION.pdf;Crank - 1975 - The mathematics of diffusion.pdf},
  isbn = {978-0-19-853344-3},
  keywords = {Diffusion},
  language = {en},
  lccn = {QD543 .C77 1975}
}

@article{Cremer2019,
  title = {Chemotaxis as a Navigation Strategy to Boost Range Expansion},
  author = {Cremer, Jonas and Honda, Tomoya and Tang, Ying and {Wong-Ng}, Jerome and Vergassola, Massimo and Hwa, Terence},
  year = {2019},
  month = nov,
  volume = {575},
  pages = {658--663},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/s41586-019-1733-y},
  file = {Cremer et al. - 2019 - Chemotaxis as a navigation strategy to boost range.pdf},
  journal = {Nature},
  language = {en},
  number = {7784}
}

@article{Crespo-Facorro2001,
  title = {Neural {{Mechanisms}} of {{Anhedonia}} in {{Schizophrenia}}: {{A PET Study}} of {{Response}} to {{Unpleasant}} and {{Pleasant Odors}}},
  shorttitle = {Neural {{Mechanisms}} of {{Anhedonia}} in {{Schizophrenia}}},
  author = {{Crespo-Facorro}, Benedicto and Paradiso, Sergio and Andreasen, Nancy C. and O'Leary, Daniel S. and Watkins, G. Leonard and Ponto, Laura L. B. and Hichwa, Richard D.},
  year = {2001},
  month = jul,
  volume = {286},
  pages = {427},
  issn = {0098-7484},
  doi = {10.1001/jama.286.4.427},
  abstract = {Objective To study the neural basis of emotional processing in schizophrenia by exploring the pattern of brain responses to olfactory stimuli in patients and healthy volunteers. Design Positron emission tomographic study of patients with schizophrenia and healthy volunteers. Positron emission tomographic data were collected between July 21, 1995, and September 11, 1997, and data analyses were conducted in 1999-2001. Setting The Mental Health Clinical Research Center at the University of Iowa, Iowa City. Participants Sixteen healthy volunteers with a mean age of 29.5 years and 18 patients with schizophrenia and a mean age of 30.0 years. Main Outcome Measure Areas of relative increase or decrease in regional cerebral blood flow, measured using positron emission tomography and the [15O]water method while participants performed an emotion-induction olfactory task to determine response to pleasant (vanillin) and unpleasant (4-methylvaleric acid) odors, compared between patients and healthy volunteers. Results Patients with schizophrenia subjectively experienced unpleasant odors in a manner similar to healthy volunteers but showed impairment in the experience of pleasant odors. The analysis of the regional cerebral blood flow revealed that patients failed to activate limbic/paralimbic regions (eg, insular cortex, nucleus accumbens, and parahippocampal gyrus) during the experience of unpleasant odors, recruiting a compensatory set of frontal cortical regions instead. Conclusion Abnormalities in the complex functional interactions between mesolimbic and frontal regions may underlie emotional disturbances in schizophrenia. JAMA. 2001;286:427-435},
  file = {2001 - Crespo-Facorro et al. - Neural Mechanisms of Anhedonia in Schizophrenia A PET Study of Response to Unpleasant and Pleasant Odors.pdf},
  journal = {JAMA},
  language = {en},
  number = {4}
}

@article{Creswell2018,
  title = {Generative {{Adversarial Networks}}: {{An Overview}}},
  shorttitle = {Generative {{Adversarial Networks}}},
  author = {Creswell, Antonia and White, Tom and Dumoulin, Vincent and Arulkumaran, Kai and Sengupta, Biswa and Bharath, Anil A.},
  year = {2018},
  month = jan,
  volume = {35},
  pages = {53--65},
  issn = {1053-5888},
  doi = {10.1109/MSP.2017.2765202},
  abstract = {Generative adversarial networks (GANs) provide a way to learn deep representations without extensively annotated training data. They achieve this through deriving backpropagation signals through a competitive process involving a pair of networks. The representations that can be learned by GANs may be used in a variety of applications, including image synthesis, semantic image editing, style transfer, image super-resolution and classification. The aim of this review paper is to provide an overview of GANs for the signal processing community, drawing on familiar analogies and concepts where possible. In addition to identifying different methods for training and constructing GANs, we also point to remaining challenges in their theory and application.},
  file = {Creswell et al. - 2018 - Generative Adversarial Networks An Overview.pdf},
  journal = {IEEE Signal Processing Magazine},
  language = {en},
  number = {1}
}

@article{Crossley2016,
  title = {A Two-Neuron System for Adaptive Goal-Directed Decision-Making in {{Lymnaea}}},
  author = {Crossley, Michael and Staras, Kevin and Kemenes, Gy{\"o}rgy},
  year = {2016},
  month = dec,
  volume = {7},
  issn = {2041-1723},
  doi = {10.1038/ncomms11793},
  file = {Crossley et al. - 2016 - A two-neuron system for adaptive goal-directed dec.pdf},
  journal = {Nature Communications},
  language = {en},
  number = {1}
}

@article{Csaba,
  title = {Perspectives of {{Using Oscillators}} for {{Computing}} and {{Signal Processing}}},
  author = {Csaba, Gyorgy and Porod, Wolfgang},
  pages = {12},
  abstract = {It is an intriguing concept to use oscillators as fundamental building blocks of electronic computers. The idea is not new, but is currently subject to intense research as a part of the quest for 'beyond Moore' electronic devices. In this paper we give an engineering-minded survey of oscillator-based computing architectures, with the goal of understanding their promise and limitations for next-generation computing. We will mostly discuss non-Boolean, neurally-inspired computing concepts and put the emphasis on hardware and on circuits where the oscillators are realized from emerging, nanoscale building blocks. Despite all the promise that oscillatory computing holds, existing literature gives very few clear-cut arguments about the possible benefits of using oscillators in place of other analog nonlinear circuit elements. In this survey we will argue for finding the rationale of using oscillatory building blocks and call for benchmarking studies that compare oscillatory computing circuits to level-based (analog) implementations.},
  file = {Csaba and Porod - Perspectives of Using Oscillators for Computing an.pdf},
  language = {en}
}

@article{Csicsvari2003,
  title = {Mechanisms of {{Gamma Oscillations}} in the {{Hippocampus}} of the {{Behaving Rat}}},
  author = {Csicsvari, Jozsef and Jamieson, Brian and Wise, Kensall D. and Buzs{\'a}ki, Gy{\"o}rgy},
  year = {2003},
  month = jan,
  volume = {37},
  pages = {311--322},
  issn = {08966273},
  doi = {10.1016/S0896-6273(02)01169-8},
  abstract = {Gamma frequency oscillations (30\textendash 100 Hz) have been suggested to underlie various cognitive and motor functions. Here, we examine the generation of gamma oscillation currents in the hippocampus, using twodimensional, 96-site silicon probes. Two gamma generators were identified, one in the dentate gyrus and another in the CA3-CA1 regions. The coupling strength between the two oscillators varied during both theta and nontheta states. Both pyramidal cells and interneurons were phase-locked to gamma waves. Anatomical connectivity, rather than physical distance, determined the coupling strength of the oscillating neurons. CA3 pyramidal neurons discharged CA3 and CA1 interneurons at latencies indicative of monosynaptic connections. Intrahippocampal gamma oscillation emerges in the CA3 recurrent system, which entrains the CA1 region via its interneurons.},
  file = {Csicsvari et al. - 2003 - Mechanisms of Gamma Oscillations in the Hippocampu.pdf},
  journal = {Neuron},
  language = {en},
  number = {2}
}

@article{Cuccu2018,
  title = {Playing {{Atari}} with {{Six Neurons}}},
  author = {Cuccu, Giuseppe and Togelius, Julian and {Cudre-Mauroux}, Philippe},
  year = {2018},
  month = jun,
  abstract = {Deep reinforcement learning on Atari games maps pixel directly to actions; internally, the deep neural network bears the responsibility of both extracting useful information and making decisions based on it. Aiming at devoting entire deep networks to decision making alone, we propose a new method for learning policies and compact state representations separately but simultaneously for policy approximation in reinforcement learning. State representations are generated by a novel algorithm based on Vector Quantization and Sparse Coding, trained online along with the network, and capable of growing its dictionary size over time. We also introduce new techniques allowing both the neural network and the evolution strategy to cope with varying dimensions. This enables networks of only 6 to 18 neurons to learn to play a selection of Atari games with performance comparable\textemdash and occasionally superior\textemdash to state-of-the-art techniques using evolution strategies on deep networks two orders of magnitude larger.},
  archiveprefix = {arXiv},
  eprint = {1806.01363},
  eprinttype = {arxiv},
  file = {Cuccu et al. - 2018 - Playing Atari with Six Neurons.pdf},
  journal = {arXiv:1806.01363 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  language = {en},
  primaryclass = {cs, stat}
}

@article{Cugno2018,
  title = {Geometric Principles of Second Messenger Dynamics in Dendritic Spines},
  author = {Cugno, Andrea and Bartol, Thomas M. and Sejnowski, Terrence J. and Iyengar, Ravi and Rangamani, Padmini},
  year = {2018},
  month = oct,
  doi = {10.1101/444489},
  abstract = {Dendritic spines are small, bulbous protrusions along dendrites in neurons and play a critical role in synaptic transmission. Dendritic spines come in a variety of shapes that depend on their developmental state. Additionally, roughly 14{$\sim$}19\% of mature spines have a specialized endoplasmic reticulum called the spine apparatus. How do the shape of a postsynaptic spine and its internal organization affect the spatiotemporal dynamics of short timescale signaling? This question is central for understanding the beginnings of synaptic transmission, learning, and memory formation. In this work, we used mathematical modeling using reaction-diffusion equations in idealized geometries (ellipsoids and spheres) to characterize the effect of spine and spine apparatus geometries on the spatio-temporal dynamics of second messengers. Our analyses and simulations showed that in the short timescale, spine size and shape coupled with the spine apparatus geometries govern the spatiotemporal chemical dynamics of second messengers within the cell. We showed that the curvature of the geometries gives rise to pseudoharmonic functions, which predict the locations of maximum and minimum concentrations. Furthermore, we showed that the lifetime of the chemical gradient can be fine-tuned by localization of fluxes on the spine head and varying the relative curvatures and distances between the spine apparatus and the spine head. Thus, we identified some of the key geometric determinants of how spine head and spine apparatus may regulate the short timescale chemical dynamics of small molecules.},
  file = {Cugno et al. - 2018 - Geometric principles of second messenger dynamics .pdf},
  journal = {bioRxiv},
  language = {en}
}

@article{Cui2017,
  title = {The {{HTM Spatial Pooler}}: A Neocortical Algorithm for Online Sparse Distributed Coding},
  shorttitle = {The {{HTM Spatial Pooler}}},
  author = {Cui, Yuwei and Ahmad, Subutai and Hawkins, Jeff},
  year = {2017},
  month = feb,
  doi = {10.1101/085035},
  abstract = {Each region in the cortex receives input through millions of axons from sensory organs and from other cortical regions. It remains a mystery how cortical neurons learn to form specific connections from this large number of unlabeled inputs in order to support further computations. Hierarchical temporal memory (HTM) provides a theoretical framework for understanding the computational principles in the neocortex. In this paper we describe an important component of HTM, the HTM spatial pooler that models how neurons learn feedforward connections. The spatial pooler converts arbitrary binary input patterns into sparse distributed representations (SDRs) using competitive Hebbian learning rules and homeostasis excitability control mechanisms. Through a series of simulations, we demonstrate the key computational properties of HTM spatial pooler, including preserving semantic similarity among inputs, fast adaptation to changing statistics of the inputs, improved noise robustness over learning, efficient use of all cells and flexibility in the event of cell death or loss of input afferents. To quantify these properties, we developed a set of metrics that can be directly measured from the spatial pooler outputs. These metrics can be used as complementary performance indicators for any sparse coding algorithm. We discuss the relationship with neuroscience and previous studies of sparse coding and competitive learning. The HTM spatial pooler represents a neurally inspired algorithm for learning SDRs from noisy data streams online.},
  file = {Cui et al. - 2017 - The HTM Spatial Pooler a neocortical algorithm fo.pdf},
  journal = {bioRxiv},
  language = {en}
}

@inproceedings{Cully2013,
  title = {Behavioral Repertoire Learning in Robotics},
  booktitle = {Proceeding of the Fifteenth Annual Conference on {{Genetic}} and Evolutionary Computation Conference - {{GECCO}} '13},
  author = {Cully, Antoine and Mouret, Jean-Baptiste},
  year = {2013},
  pages = {175},
  publisher = {{ACM Press}},
  address = {{Amsterdam, The Netherlands}},
  doi = {10.1145/2463372.2463399},
  abstract = {Learning in robotics typically involves choosing a simple goal (e.g. walking) and assessing the performance of each controller with regard to this task (e.g. walking speed). However, learning advanced, input-driven controllers (e.g. walking in each direction) requires testing each controller on a large sample of the possible input signals. This costly process makes difficult to learn useful low-level controllers in robotics.},
  file = {Cully and Mouret - 2013 - Behavioral repertoire learning in robotics.pdf},
  isbn = {978-1-4503-1963-8},
  language = {en}
}

@article{Cully2015,
  title = {Robots That Can Adapt like Animals},
  author = {Cully, Antoine and Clune, Jeff and Tarapore, Danesh and Mouret, Jean-Baptiste},
  year = {2015},
  month = may,
  volume = {521},
  pages = {503--507},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature14422},
  abstract = {As robots leave the controlled environments of factories to autonomously function in more complex, natural environments, they will have to respond to the inevitable fact that they will become damaged. However, while animals can quickly adapt to a wide variety of injuries, current robots cannot "think outside the box" to find a compensatory behavior when damaged: they are limited to their pre-specified self-sensing abilities, can diagnose only anticipated failure modes, and require a pre-programmed contingency plan for every type of potential damage, an impracticality for complex robots. Here we introduce an intelligent trial and error algorithm that allows robots to adapt to damage in less than two minutes, without requiring self-diagnosis or pre-specified contingency plans. Before deployment, a robot exploits a novel algorithm to create a detailed map of the space of high-performing behaviors: This map represents the robot's intuitions about what behaviors it can perform and their value. If the robot is damaged, it uses these intuitions to guide a trial-and-error learning algorithm that conducts intelligent experiments to rapidly discover a compensatory behavior that works in spite of the damage. Experiments reveal successful adaptations for a legged robot injured in five different ways, including damaged, broken, and missing legs, and for a robotic arm with joints broken in 14 different ways. This new technique will enable more robust, effective, autonomous robots, and suggests principles that animals may use to adapt to injury.},
  archiveprefix = {arXiv},
  eprint = {1407.3501},
  eprinttype = {arxiv},
  file = {Cully et al. - 2015 - Robots that can adapt like animals.pdf},
  journal = {Nature},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Computer Science - Robotics,Quantitative Biology - Neurons and Cognition},
  language = {en},
  number = {7553}
}

@article{Cunningham2004,
  title = {A Role for Fast Rhythmic Bursting Neurons in Cortical Gamma Oscillations in Vitro},
  author = {Cunningham, M. O. and Whittington, M. A. and Bibbig, A. and Roopun, A. and LeBeau, F. E. N. and Vogt, A. and Monyer, H. and Buhl, E. H. and Traub, R. D.},
  year = {2004},
  month = may,
  volume = {101},
  pages = {7152--7157},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.0402060101},
  file = {2004 - Cunningham et al. - A role for fast rhythmic bursting neurons in cortical gamma oscillations in vitro.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {18}
}

@article{Curto2008,
  title = {Cell {{Groups Reveal Structure}} of {{Stimulus Space}}},
  author = {Curto, Carina and Itskov, Vladimir},
  editor = {Friston, Karl J.},
  year = {2008},
  month = oct,
  volume = {4},
  pages = {e1000205},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1000205},
  abstract = {An important task of the brain is to represent the outside world. It is unclear how the brain may do this, however, as it can only rely on neural responses and has no independent access to external stimuli in order to ``decode'' what those responses mean. We investigate what can be learned about a space of stimuli using only the action potentials (spikes) of cells with stereotyped\textemdash but unknown\textemdash receptive fields. Using hippocampal place cells as a model system, we show that one can (1) extract global features of the environment and (2) construct an accurate representation of space, up to an overall scale factor, that can be used to track the animal's position. Unlike previous approaches to reconstructing position from place cell activity, this information is derived without knowing place fields or any other functions relating neural responses to position. We find that simply knowing which groups of cells fire together reveals a surprising amount of structure in the underlying stimulus space; this may enable the brain to construct its own internal representations.},
  file = {2008 - Curto, Itskov - Cell groups reveal structure of stimulus space.pdf},
  journal = {PLoS Computational Biology},
  language = {en},
  number = {10}
}

@article{Curto2013,
  title = {Combinatorial {{Neural Codes}} from a {{Mathematical Coding Theory Perspective}}},
  author = {Curto, Carina and Itskov, Vladimir and Morrison, Katherine and Roth, Zachary and Walker, Judy L.},
  year = {2013},
  month = jul,
  volume = {25},
  pages = {1891--1925},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/NECO_a_00459},
  file = {2013 - Curto et al. - Combinatorial Neural Codes from a Mathematical Coding Theory Perspective.pdf},
  journal = {Neural Computation},
  language = {en},
  number = {7}
}

@article{Curto2013a,
  title = {Encoding {{Binary Neural Codes}} in {{Networks}} of {{Threshold}}-{{Linear Neurons}}},
  author = {Curto, Carina and Degeratu, Anda and Itskov, Vladimir},
  year = {2013},
  month = nov,
  volume = {25},
  pages = {2858--2903},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/NECO_a_00504},
  file = {2014 - Lehky et al. - Encoding Binary Neural Codes in Networks of Threshold-Linear Neurons.pdf;Curto et al. - 2013 - Encoding Binary Neural Codes in Networks of Thresh.pdf},
  journal = {Neural Computation},
  language = {en},
  number = {11}
}

@article{Curto2013b,
  title = {The {{Neural Ring}}: {{An Algebraic Tool}} for {{Analyzing}} the {{Intrinsic Structure}} of {{Neural Codes}}},
  shorttitle = {The {{Neural Ring}}},
  author = {Curto, Carina and Itskov, Vladimir and {Veliz-Cuba}, Alan and Youngs, Nora},
  year = {2013},
  month = sep,
  volume = {75},
  pages = {1571--1611},
  issn = {0092-8240, 1522-9602},
  doi = {10.1007/s11538-013-9860-3},
  abstract = {Neurons in the brain represent external stimuli via neural codes. These codes often arise from stereotyped stimulus-response maps, associating to each neuron a convex receptive field. An important problem confronted by the brain is to infer properties of a represented stimulus space without knowledge of the receptive fields, using only the intrinsic structure of the neural code. How does the brain do this? To address this question, it is important to determine what stimulus space features can\textemdash in principle\textemdash be extracted from neural codes. This motivates us to define the neural ring and a related neural ideal, algebraic objects that encode the full combinatorial data of a neural code. Our main finding is that these objects can be expressed in a ``canonical form'' that directly translates to a minimal description of the receptive field structure intrinsic to the code. We also find connections to Stanley\textendash Reisner rings, and use ideas similar to those in the theory of monomial ideals to obtain an algorithm for computing the primary decomposition of pseudo-monomial ideals. This allows us to algorithmically extract the canonical form associated to any neural code, providing the groundwork for inferring stimulus space features from neural activity alone.},
  file = {2013 - Curto et al. - The Neural Ring An Algebraic Tool for Analyzing the Intrinsic Structure of Neural Codes.pdf},
  journal = {Bulletin of Mathematical Biology},
  language = {en},
  number = {9}
}

@article{Curto2015,
  title = {Neural Ring Homomorphisms and Maps between Neural Codes},
  author = {Curto, Carina and Youngs, Nora},
  year = {2015},
  month = nov,
  abstract = {Understanding how the brain stores and processes information is central to mathematical neuroscience. Neural data is often represented as a neural code: a set of binary firing patterns C {$\subset$} \{0, 1\}n. We have previously introduced the neural ring, an algebraic object which encodes combinatorial information, in order to analyze the structure of neural codes. We now relate maps between neural codes to notions of homomorphism between the corresponding neural rings. Using three natural operations on neural codes (permutation, inclusion, deletion) as motivation, we search for a restricted class of homomorphisms which correspond to these natural operations. We choose the framework of linear-monomial module homomorphisms, and find that the class of associated code maps neatly captures these three operations, and necessarily includes two others - repetition and adding trivial neurons - which are also meaningful in a neural coding context.},
  archiveprefix = {arXiv},
  eprint = {1511.00255},
  eprinttype = {arxiv},
  file = {2015 - Curto, Youngs - Neural ring homomorphisms and maps between neural codes.pdf;Curto and Youngs - 2015 - Neural ring homomorphisms and maps between neural .pdf},
  journal = {arXiv:1511.00255 [q-bio]},
  keywords = {Quantitative Biology - Neurons and Cognition},
  language = {en},
  primaryclass = {q-bio}
}

@article{Curto2015a,
  title = {Pattern Completion in Symmetric Threshold-Linear Networks},
  author = {Curto, Carina and Morrison, Katherine},
  year = {2015},
  month = dec,
  abstract = {Threshold-linear networks are a common class of firing rate models that describe recurrent interactions among neurons. Unlike their linear counterparts, these networks generically possess multiple stable fixed points (steady states), making them viable candidates for memory encoding and retrieval. In this work, we characterize stable fixed points of general threshold-linear networks with constant external drive, and discover constraints on the co-existence of fixed points involving different subsets of active neurons. In the case of symmetric networks, we prove the following antichain property: if a set of neurons {$\tau$} is the support of a stable fixed point, then no proper subset or superset of {$\tau$} can support a stable fixed point. Symmetric threshold-linear networks thus appear to be well suited for pattern completion, since the dynamics are guaranteed not to get ``stuck'' in a subset or superset of a stored pattern. We also show that for any graph G, we can construct a network whose stable fixed points correspond precisely to the maximal cliques of G. As an application, we design network decoders for place field codes, and demonstrate their efficacy for error correction and pattern completion. The proofs of our main results build on the theory of permitted sets in threshold-linear networks, including recently-developed connections to classical distance geometry.},
  archiveprefix = {arXiv},
  eprint = {1512.00897},
  eprinttype = {arxiv},
  file = {2015 - Curto, Morrison - Pattern completion in symmetric threshold-linear networks.pdf;Curto and Morrison - 2015 - Pattern completion in symmetric threshold-linear n.pdf},
  journal = {arXiv:1512.00897 [q-bio]},
  keywords = {Quantitative Biology - Neurons and Cognition},
  language = {en},
  primaryclass = {q-bio}
}

@article{Curto2015b,
  title = {What Makes a Neural Code Convex?},
  author = {Curto, Carina and Gross, Elizabeth and Jeffries, Jack and Morrison, Katherine and Omar, Mohamed and Rosen, Zvi and Shiu, Anne and Youngs, Nora},
  year = {2015},
  month = aug,
  abstract = {Neural codes allow the brain to represent, process, and store information about the world. Combinatorial codes, comprised of binary patterns of neural activity, encode information via the collective behavior of populations of neurons. A code is called convex if its codewords correspond to regions defined by an arrangement of convex open sets in Euclidean space. Convex codes have been observed experimentally in many brain areas, including sensory cortices and the hippocampus, where neurons exhibit convex receptive fields. What makes a neural code convex? That is, how can we tell from the intrinsic structure of a code if there exists a corresponding arrangement of convex open sets? Using tools from combinatorics and commutative algebra, we uncover key signatures of convex and non-convex codes. In many cases, these signatures are sufficient to determine convexity, and reveal bounds on the minimal dimension of the underlying Euclidean space.},
  archiveprefix = {arXiv},
  eprint = {1508.00150},
  eprinttype = {arxiv},
  file = {2015 - Curto et al. - What makes a neural code convex Introduction Convex neural codes.pdf;Curto et al. - 2015 - What makes a neural code convex.pdf},
  journal = {arXiv:1508.00150 [math, q-bio]},
  keywords = {Mathematics - Combinatorics,Quantitative Biology - Neurons and Cognition},
  language = {en},
  primaryclass = {math, q-bio}
}

@article{Curto2016,
  title = {What Can Topology Tell Us about the Neural Code?},
  author = {Curto, Carina},
  year = {2016},
  month = may,
  abstract = {Neuroscience is undergoing a period of rapid experimental progress and expansion. New mathematical tools, previously unknown in the neuroscience community, are now being used to tackle fundamental questions and analyze emerging data sets. Consistent with this trend, the last decade has seen an uptick in the use of topological ideas and methods in neuroscience. In this talk I will survey recent applications of topology in neuroscience, and explain why topology is an especially natural tool for understanding neural codes. Note: This is a write-up of my talk for the Current Events Bulletin, held at the 2016 Joint Math Meetings in Seattle, WA.},
  archiveprefix = {arXiv},
  eprint = {1605.01905},
  eprinttype = {arxiv},
  file = {2015 - Curto - What can topology tell us about the neural code.pdf;Curto - 2016 - What can topology tell us about the neural code.pdf},
  journal = {arXiv:1605.01905 [q-bio]},
  keywords = {Quantitative Biology - Neurons and Cognition},
  language = {en},
  primaryclass = {q-bio}
}

@article{Cybenkot,
  title = {Approximation by Superpositions of a Sigmoidal Function},
  author = {Cybenkot, G},
  pages = {12},
  file = {Cybenkot - Approximation by superpositions of a sigmoidal fun 2.pdf},
  language = {en}
}

@article{Cybenkot1989,
  title = {Approximation by Superpositions of a Sigmoidal Function},
  author = {Cybenkot, G},
  year = {1989},
  volume = {2},
  pages = {12},
  file = {Cybenkot - Approximation by superpositions of a sigmoidal fun.pdf},
  journal = {Math. Control Signals System},
  language = {en}
}

@article{Dabney2020,
  title = {A Distributional Code for Value in Dopamine-Based Reinforcement Learning},
  author = {Dabney, Will and {Kurth-Nelson}, Zeb and Uchida, Naoshige and Starkweather, Clara Kwon and Hassabis, Demis and Munos, R{\'e}mi and Botvinick, Matthew},
  year = {2020},
  month = jan,
  volume = {577},
  pages = {671--675},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/s41586-019-1924-6},
  file = {Dabney et al. - 2020 - A distributional code for value in dopamine-based .pdf},
  journal = {Nature},
  language = {en},
  number = {7792}
}

@article{Dai2012,
  title = {Generic {{Indicators}} for {{Loss}} of {{Resilience Before}} a {{Tipping Point Leading}} to {{Population Collapse}}},
  author = {Dai, L. and Vorselen, D. and Korolev, K. S. and Gore, J.},
  year = {2012},
  month = jun,
  volume = {336},
  pages = {1175--1177},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1219805},
  file = {Dai et al. - 2012 - Generic Indicators for Loss of Resilience Before a.pdf},
  journal = {Science},
  language = {en},
  number = {6085}
}

@article{Dale1997,
  title = {Selective Averaging of Rapidly Presented Individual Trials Using {{fMRI}}},
  author = {Dale, Anders M. and Buckner, Randy L.},
  year = {1997},
  volume = {5},
  pages = {329--340},
  issn = {10659471},
  doi = {10.1002/(SICI)1097-0193(1997)5:5<329::AID-HBM1>3.0.CO;2-5},
  abstract = {A major limitation in conducting functional neuroimaging studies, particularly for cognitive experiments, has been the use of blocked task paradigms. Here we explored whether selective averaging techniques similar to those applied in event-related potential (ERP) experiments could be used to demonstrate functional magnetic resonance imaging (fMRI) responses to rapidly intermixed trials. In the first two experiments, we observed that for 1-sec trials of full-field visual checkerboard stimulation, the fMRI blood oxygenation level-dependent (BOLD) signal summated in a roughly linear fashion across successive trials even at very short (2 sec and 5 sec) intertrial intervals, although subtle departures from linearity were observed. In experiments 3 and 4, we observed that it is possible to obtain robust activation maps for rapidly presented randomly mixed trial types (left- and right-hemifield visual checkerboard stimulation) spaced as little as 2 sec apart. Taken collectively, these results suggest that selective averaging may enable fMRI experimental designs identical to those used in typical behavioral and ERP studies. The ability to analyze closely spaced single-trial, or event-related, signals provides for a class of experiments which cannot be conducted using blocked designs. Trial types can be randomly intermixed, and selective averaging based upon trial type and/or subject performance is possible. Hum. Brain Mapping 5:329\textendash 340, 1997.},
  file = {1997 - Dale, Buckner - Selective averaging of rapidly presented individual trials using fMRI.pdf},
  journal = {Human Brain Mapping},
  language = {en},
  number = {5}
}

@article{daLuz2015,
  title = {And yet It Optimizes},
  author = {{da Luz}, M.G.E. and Raposo, E.P. and Viswanathan, G.M.},
  year = {2015},
  month = sep,
  volume = {14},
  pages = {94--98},
  issn = {15710645},
  doi = {10.1016/j.plrev.2015.07.007},
  file = {da Luz et al. - 2015 - And yet it optimizes.pdf},
  journal = {Physics of Life Reviews},
  language = {en}
}

@article{daLuz2016,
  title = {Subjective Expectation of Rewards Can Change the Behavior of Smart but Impatient Foragers},
  author = {{da Luz}, Marcos Gomes Eleuterio and Raposo, Ernesto P. and Viswanathan, Gandhimohan M.},
  year = {2016},
  month = aug,
  volume = {113},
  pages = {8571--8573},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1609369113},
  file = {da Luz et al. - 2016 - Subjective expectation of rewards can change the b.pdf},
  journal = {Proc Natl Acad Sci USA},
  language = {en},
  number = {31}
}

@article{Dam2009,
  title = {Exploration and {{Exploitation During Sequential Search}}},
  author = {Dam, Gregory and K{\"o}rding, Konrad},
  year = {2009},
  month = may,
  volume = {33},
  pages = {530--541},
  issn = {03640213, 15516709},
  doi = {10.1111/j.1551-6709.2009.01021.x},
  abstract = {When we learn how to throw darts we adjust how we throw based on where the darts stick. Much of skill learning is computationally similar in that we learn using feedback obtained after the completion of individual actions. We can formalize such tasks as a search problem; among the set of all possible actions, find the action that leads to the highest reward. In such cases our actions have two objectives: we want to best utilize what we already know (exploitation), but we also want to learn to be more successful in the future (exploration). Here we tested how participants learn movement trajectories where feedback is provided as a monetary reward that depends on the chosen trajectory. We mathematically derived the optimal search policy for our experiment using decision theory. The search behavior of participants is well predicted by an ideal searcher model that optimally combines exploration and exploitation.},
  file = {Dam and Körding - 2009 - Exploration and Exploitation During Sequential Sea.pdf},
  journal = {Cognitive Science},
  language = {en},
  number = {3}
}

@article{Dam2009a,
  title = {Exploration and {{Exploitation During Sequential Search}}},
  author = {Dam, Gregory and K{\"o}rding, Konrad},
  year = {2009},
  month = may,
  volume = {33},
  pages = {530--541},
  issn = {03640213, 15516709},
  doi = {10.1111/j.1551-6709.2009.01021.x},
  abstract = {When we learn how to throw darts we adjust how we throw based on where the darts stick. Much of skill learning is computationally similar in that we learn using feedback obtained after the completion of individual actions. We can formalize such tasks as a search problem; among the set of all possible actions, find the action that leads to the highest reward. In such cases our actions have two objectives: we want to best utilize what we already know (exploitation), but we also want to learn to be more successful in the future (exploration). Here we tested how participants learn movement trajectories where feedback is provided as a monetary reward that depends on the chosen trajectory. We mathematically derived the optimal search policy for our experiment using decision theory. The search behavior of participants is well predicted by an ideal searcher model that optimally combines exploration and exploitation.},
  file = {Dam and Körding - 2009 - Exploration and Exploitation During Sequential Sea 2.pdf},
  journal = {Cognitive Science},
  language = {en},
  number = {3}
}

@article{Dam2013,
  title = {Credit {{Assignment}} during {{Movement Reinforcement Learning}}},
  author = {Dam, Gregory and Kording, Konrad and Wei, Kunlin},
  editor = {Gribble, Paul L.},
  year = {2013},
  month = feb,
  volume = {8},
  pages = {e55352},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0055352},
  abstract = {We often need to learn how to move based on a single performance measure that reflects the overall success of our movements. However, movements have many properties, such as their trajectories, speeds and timing of end-points, thus the brain needs to decide which properties of movements should be improved; it needs to solve the credit assignment problem. Currently, little is known about how humans solve credit assignment problems in the context of reinforcement learning. Here we tested how human participants solve such problems during a trajectory-learning task. Without an explicitly-defined target movement, participants made hand reaches and received monetary rewards as feedback on a trialby-trial basis. The curvature and direction of the attempted reach trajectories determined the monetary rewards received in a manner that can be manipulated experimentally. Based on the history of action-reward pairs, participants quickly solved the credit assignment problem and learned the implicit payoff function. A Bayesian credit-assignment model with built-in forgetting accurately predicts their trial-by-trial learning.},
  file = {Dam et al. - 2013 - Credit Assignment during Movement Reinforcement Le.PDF},
  journal = {PLoS ONE},
  language = {en},
  number = {2}
}

@article{Damodaran2014,
  title = {Synchronized Firing of Fast-Spiking Interneurons Is Critical to Maintain Balanced Firing between Direct and Indirect Pathway Neurons of the Striatum},
  author = {Damodaran, Sriraman and Evans, Rebekah C. and Blackwell, Kim T.},
  year = {2014},
  month = feb,
  volume = {111},
  pages = {836--848},
  issn = {0022-3077, 1522-1598},
  doi = {10.1152/jn.00382.2013},
  file = {2014 - Damodaran, Evans, Blackwell - Synchronized firing of fast-spiking interneurons is critical to maintain balanced firing between di.pdf;Damodaran et al. - 2014 - Synchronized firing of fast-spiking interneurons i.pdf},
  journal = {Journal of Neurophysiology},
  language = {en},
  number = {4}
}

@article{Danziger2015,
  title = {A Neuron Model of Stochastic Resonance Using Rectangular Pulse Trains},
  author = {Danziger, Zachary and Grill, Warren M.},
  year = {2015},
  month = feb,
  volume = {38},
  pages = {53--66},
  issn = {0929-5313, 1573-6873},
  doi = {10.1007/s10827-014-0526-4},
  abstract = {Stochastic resonance (SR) is the enhanced representation of a weak input signal by the addition of an optimal level of broadband noise to a nonlinear (threshold) system. Since its discovery in the 1980s the domain of input signals shown to be applicable to SR has greatly expanded, from strictly periodic inputs to now nearly any aperiodic forcing function. The perturbations (noise) used to generate SR have also expanded, from white noise to now colored noise or vibrational forcing. This study demonstrates that a new class of perturbations can achieve SR, namely, series of stochastically generated biphasic pulse trains. Using these pulse trains as `noise' we show that a Hodgkin Huxley model neuron exhibits SR behavior when detecting weak input signals. This result is of particular interest to neuroscience because nearly all artificial neural stimulation is implemented with square current or voltage pulses rather than broadband noise, and this new method may facilitate the translation of the performance gains achievable through SR to neural prosthetics.},
  file = {2015 - Danzinger, Grill - A Neuron Model of Stochastic Resonance Using Rectangular Pulse Trains.pdf;Danziger and Grill - 2015 - A neuron model of stochastic resonance using recta.pdf},
  journal = {Journal of Computational Neuroscience},
  language = {en},
  number = {1}
}

@article{Dasgupta2017,
  title = {A Neural Algorithm for a Fundamental Computing Problem},
  author = {Dasgupta, Sanjoy and Stevens, Charles F and Navlakha, Saket},
  year = {2017},
  pages = {5},
  file = {Dasgupta et al. - 2017 - A neural algorithm for a fundamental computing pro.pdf},
  language = {en}
}

@article{Dauphin,
  title = {{{MetaInit}}: {{Initializing}} Learning by Learning to Initialize},
  author = {Dauphin, Yann N and Schoenholz, Samuel},
  pages = {13},
  abstract = {Deep learning models frequently trade handcrafted features for deep features learned with much less human intervention using gradient descent. While this paradigm has been enormously successful, deep networks are often difficult to train and performance can depend crucially on the initial choice of parameters. In this work, we introduce an algorithm called MetaInit as a step towards automating the search for good initializations using meta-learning. Our approach is based on a hypothesis that good initializations make gradient descent easier by starting in regions that look locally linear with minimal second order effects. We formalize this notion via a quantity that we call the gradient quotient, which can be computed with any architecture or dataset. MetaInit minimizes this quantity efficiently by using gradient descent to tune the norms of the initial weight matrices. We conduct experiments on plain and residual networks and show that the algorithm can automatically recover from a class of bad initializations. MetaInit allows us to train networks and achieve performance competitive with the state-of-the-art without batch normalization or residual connections. In particular, we find that this approach outperforms normalization for networks without skip connections on CIFAR-10 and can scale to Resnet-50 models on Imagenet.},
  file = {Dauphin and Schoenholz - MetaInit Initializing learning by learning to ini.pdf},
  language = {en}
}

@article{dAutume2019,
  title = {Episodic {{Memory}} in {{Lifelong Language Learning}}},
  author = {{d'Autume}, Cyprien de Masson and Ruder, Sebastian and Kong, Lingpeng and Yogatama, Dani},
  year = {2019},
  month = oct,
  abstract = {We introduce a lifelong language learning setup where a model needs to learn from a stream of text examples without any dataset identifier. We propose an episodic memory model that performs sparse experience replay and local adaptation to mitigate catastrophic forgetting in this setup. Experiments on text classification and question answering demonstrate the complementary benefits of sparse experience replay and local adaptation to allow the model to continuously learn from new datasets. We also show that the space complexity of the episodic memory module can be reduced significantly ({$\sim$}50-90\%) by randomly choosing which examples to store in memory with a minimal decrease in performance. We consider an episodic memory component as a crucial building block of general linguistic intelligence and see our model as a first step in that direction.},
  archiveprefix = {arXiv},
  eprint = {1906.01076},
  eprinttype = {arxiv},
  file = {d'Autume et al. - 2019 - Episodic Memory in Lifelong Language Learning.pdf},
  journal = {arXiv:1906.01076 [cs, stat]},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryclass = {cs, stat}
}

@article{David2003,
  title = {A Neural Mass Model for {{MEG}}/{{EEG}}:},
  shorttitle = {A Neural Mass Model for {{MEG}}/{{EEG}}},
  author = {David, Olivier and Friston, Karl J.},
  year = {2003},
  month = nov,
  volume = {20},
  pages = {1743--1755},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2003.07.015},
  abstract = {Although MEG/EEG signals are highly variable, systematic changes in distinct frequency bands are commonly encountered. These frequency-specific changes represent robust neural correlates of cognitive or perceptual processes (for example, alpha rhythms emerge on closing the eyes). However, their functional significance remains a matter of debate. Some of the mechanisms that generate these signals are known at the cellular level and rest on a balance of excitatory and inhibitory interactions within and between populations of neurons. The kinetics of the ensuing population dynamics determine the frequency of oscillations. In this work we extended the classical nonlinear lumped-parameter model of alpha rhythms, initially developed by Lopes da Silva and colleagues [Kybernetik 15 (1974) 27], to generate more complex dynamics. We show that the whole spectrum of MEG/EEG signals can be reproduced within the oscillatory regime of this model by simply changing the population kinetics. We used the model to examine the influence of coupling strength and propagation delay on the rhythms generated by coupled cortical areas. The main findings were that (1) coupling induces phase-locked activity, with a phase shift of 0 or ␲ when the coupling is bidirectional, and (2) both coupling and propagation delay are critical determinants of the MEG/EEG spectrum. In forthcoming articles, we will use this model to (1) estimate how neuronal interactions are expressed in MEG/EEG oscillations and establish the construct validity of various indices of nonlinear coupling, and (2) generate event-related transients to derive physiologically informed basis functions for statistical modelling of average evoked responses.},
  file = {2003 - David, Friston - A neural mass model for MEGEEG.pdf},
  journal = {NeuroImage},
  language = {en},
  number = {3}
}

@techreport{David2019,
  title = {Single {{Cortical Neurons}} as {{Deep Artificial Neural Networks}}},
  author = {David, Beniaguev and Idan, Segev and Michael, London},
  year = {2019},
  month = apr,
  institution = {{Neuroscience}},
  doi = {10.1101/613141},
  abstract = {We propose a novel approach based on modern deep artificial neural networks (DNNs) for understanding how the morpho-electrical complexity of neurons shapes their input/output (I/O) properties at the millisecond resolution in response to massive synaptic input. The I/O of integrate and fire point neuron is accurately captured by a DNN with a single unit and one hidden layer. A fully connected DNN with one hidden layer faithfully replicated the I/O relationship of a detailed model of Layer 5 cortical pyramidal cell (L5PC) receiving AMPA and GABAA synapses. However, when adding voltage-gated NMDA-conductances, a temporally-convolutional DNN with seven layers was required. Analysis of the DNN filters provides new insights into dendritic processing shaping the I/O properties of neurons. This work proposes a systematic approach for characterizing the functional ``depth'' of a biological neurons, suggesting that cortical pyramidal neurons and the networks they form are computationally much more powerful than previously assumed.},
  file = {David et al. - 2019 - Single Cortical Neurons as Deep Artificial Neural .pdf},
  language = {en},
  type = {Preprint}
}

@article{Davis2014,
  title = {What Do Differences between Multi-Voxel and Univariate Analysis Mean? {{How}} Subject-, Voxel-, and Trial-Level Variance Impact {{fMRI}} Analysis},
  shorttitle = {What Do Differences between Multi-Voxel and Univariate Analysis Mean?},
  author = {Davis, Tyler and LaRocque, Karen F. and Mumford, Jeanette A. and Norman, Kenneth A. and Wagner, Anthony D. and Poldrack, Russell A.},
  year = {2014},
  month = aug,
  volume = {97},
  pages = {271--283},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2014.04.037},
  abstract = {Multi-voxel pattern analysis (MVPA) has led to major changes in how fMRI data are analyzed and interpreted. Many studies now report both MVPA results and results from standard univariate voxel-wise analysis, often with the goal of drawing different conclusions from each. Because MVPA results can be sensitive to latent multidimensional representations and processes whereas univariate voxel-wise analysis cannot, one conclusion that is often drawn when MVPA and univariate results differ is that the activation patterns underlying MVPA results contain a multidimensional code. In the current study, we conducted simulations to formally test this assumption. Our findings reveal that MVPA tests are sensitive to the magnitude of voxel-level variability in the effect of a condition within subjects, even when the same linear relationship is coded in all voxels. We also find that MVPA is insensitive to subject-level variability in mean activation across an ROI, which is the primary variance component of interest in many standard univariate tests. Together, these results illustrate that differences between MVPA and univariate tests do not afford conclusions about the nature or dimensionality of the neural code. Instead, targeted tests of the informational content and/or dimensionality of activation patterns are critical for drawing strong conclusions about the representational codes that are indicated by significant MVPA results.},
  file = {2014 - Davis1 et al. - What Do Differences Between Multi-voxel and Univariate Analysis Mean How Subject-, Voxel-, and Trial-level Varian.pdf;Davis et al. - 2014 - What do differences between multi-voxel and univar.pdf},
  journal = {NeuroImage},
  language = {en}
}

@article{Davis2020,
  title = {Spontaneous Travelling Cortical Waves Gate Perception in Behaving Primates},
  author = {Davis, Zachary W. and Muller, Lyle and {Martinez-Trujillo}, Julio and Sejnowski, Terrence and Reynolds, John H.},
  year = {2020},
  month = oct,
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/s41586-020-2802-y},
  file = {Davis et al. - 2020 - Spontaneous travelling cortical waves gate percept.pdf},
  journal = {Nature},
  language = {en}
}

@article{Daw,
  title = {The Pigeon as Particle Filter},
  author = {Daw, Nathaniel D and Courville, Aaron C},
  pages = {8},
  abstract = {Although theorists have interpreted classical conditioning as a laboratory model of Bayesian belief updating, a recent reanalysis showed that the key features that theoretical models capture about learning are artifacts of averaging over subjects. Rather than learning smoothly to asymptote (reflecting, according to Bayesian models, the gradual tradeoff from prior to posterior as data accumulate), subjects learn suddenly, and their predictions fluctuate perpetually. We suggest that abrupt and unstable learning can be modeled by assuming subjects are conducting inference using sequential Monte Carlo sampling with a small number of samples \textemdash{} one, in our simulations. Ensemble behavior resembles exact Bayesian models since, as in particle filters, it averages over many samples. Further, the model is capable of exhibiting sophisticated behaviors like retrospective revaluation at the ensemble level, even given minimally sophisticated individuals that do not track uncertainty in their beliefs over trials.},
  file = {2008 - Daw, Courville - The pigeon as particle filter.pdf},
  language = {en}
}

@article{Daw2002,
  title = {Opponent Interactions between Serotonin and Dopamine},
  author = {Daw, Nathaniel D and Kakade, Sham and Dayan, Peter},
  year = {2002},
  month = jun,
  volume = {15},
  pages = {603--616},
  issn = {08936080},
  doi = {10.1016/S0893-6080(02)00052-7},
  abstract = {Anatomical and pharmacological evidence suggests that the dorsal raphe serotonin system and the ventral tegmental and substantia nigra dopamine system may act as mutual opponents. In the light of the temporal difference model of the involvement of the dopamine system in reward learning, we consider three aspects of motivational opponency involving dopamine and serotonin. We suggest that a tonic serotonergic signal reports the long-run average reward rate as part of an average-case reinforcement learning model; that a tonic dopaminergic signal reports the long-run average punishment rate in a similar context; and finally speculate that a phasic serotonin signal might report an ongoing prediction error for future punishment. q 2002 Elsevier Science Ltd. All rights reserved.},
  file = {2002 - Daw, Kakade, Dayan - Opponent interactions between serotonin and dopamine.pdf},
  journal = {Neural Networks},
  language = {en},
  number = {4-6}
}

@article{Daw2006,
  title = {Cortical Substrates for Exploratory Decisions in Humans},
  author = {Daw, Nathaniel D. and O'Doherty, John P. and Dayan, Peter and Seymour, Ben and Dolan, Raymond J.},
  year = {2006},
  month = jun,
  volume = {441},
  pages = {876--879},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature04766},
  file = {Daw et al. - 2006 - Cortical substrates for exploratory decisions in h.pdf},
  journal = {Nature},
  language = {en},
  number = {7095}
}

@article{Dawa,
  title = {Trial-by-Trial Data Analysis Using Computational Models},
  author = {Daw, Nathaniel D},
  pages = {26},
  file = {Daw - Trial-by-trial data analysis using computational m.pdf},
  language = {en}
}

@article{Dayan1993,
  title = {Improving {{Generalisation}} for {{Temporal Difference Learning}}: {{The Successor Representation}}},
  author = {Dayan, Peter},
  year = {1993},
  volume = {5},
  pages = {613--624},
  abstract = {Estimation of returns over time, the focus of temporal difference (TD) algorithms, imposes particular constraints on good function approximators or representations. Appropriate generalisation between states is determined by how similar their successors are, and representations should follow suit. This paper shows how TD machinery can be used to learn such representations, and illustrates, using a navigation task, the appropriately distributed nature of the result.},
  file = {1993 - Dayan - Improving Generalization for Temporal Difference Learning The Successor Representation.pdf},
  journal = {Neural Computation},
  language = {en},
  number = {4}
}

@article{Dayan1995,
  title = {The {{Helmholtz Machine}}},
  author = {Dayan, Peter and Hinton, Geoffrey E. and Neal, Radford M. and Zemel, Richard S.},
  year = {1995},
  month = sep,
  volume = {7},
  pages = {889--904},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/neco.1995.7.5.889},
  file = {1995 - Dayan et al. - The helmholtz machine.pdf},
  journal = {Neural Computation},
  language = {en},
  number = {5}
}

@article{Dayan1996,
  title = {Exploration Bonuses and Dual Control},
  author = {Dayan, Peter and Sejnowski, Terrence J.},
  year = {1996},
  month = oct,
  volume = {25},
  pages = {5--22},
  issn = {0885-6125, 1573-0565},
  doi = {10.1007/BF00115298},
  abstract = {Finding the Bayesian balance between exploration and exploitation in adaptive optimal control is in general intractable. This paper shows how to compute suboptimal estimates based on a certainty equivalence approximation (Cozzolino, Gonzalez-Zubieta \& Miller, 1965) arising from a form of dual control. This systematizes and extends existing uses of exploration bonuses in reinforcement learning (Sutton, 1990). The approach has two components: a statistical model of uncertainty in the world and a way of turning this into exploratory behavior. This general approach is applied to two-dimensional mazes with moveable barriers and its performance is compared with Sutton's DYNA system.},
  file = {Dayan and Sejnowski - 1996 - Exploration bonuses and dual control.pdf},
  journal = {Mach Learn},
  language = {en},
  number = {1}
}

@book{Dayan2005,
  title = {Theoretical {{Neuroscience}}},
  author = {Dayan, Peter and Abbott, L F},
  year = {2005},
  publisher = {{MIT press}}
}

@inproceedings{deAbril2018,
  title = {Curiosity-{{Driven Reinforcement Learning}} with {{Homeostatic Regulation}}},
  booktitle = {2018 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  author = {{de Abril}, Ildefons Magrans and Kanai, Ryota},
  year = {2018},
  month = jul,
  pages = {1--6},
  publisher = {{IEEE}},
  address = {{Rio de Janeiro}},
  doi = {10.1109/IJCNN.2018.8489075},
  abstract = {We propose a curiosity reward based on information theory principles and consistent with the animal instinct to maintain certain critical parameters within a bounded range. Our experimental validation shows the added value of the additional homeostatic drive to enhance the overall information gain of a reinforcement learning agent interacting with a complex environment using continuous actions. Our method builds upon two ideas: i) To take advantage of a new Bellman-like equation of information gain and ii) to simplify the computation of the local rewards by avoiding the approximation of complex distributions over continuous states and actions.},
  file = {de Abril and Kanai - 2018 - Curiosity-Driven Reinforcement Learning with Homeo.pdf},
  isbn = {978-1-5090-6014-6},
  language = {en}
}

@article{Deacon2008,
  title = {Shannon - {{Boltzmann}} - {{Darwin}}: {{Redefining}} Information ({{Part II}})},
  shorttitle = {Shannon - {{Boltzmann}} - {{Darwin}}},
  author = {Deacon, Terrence W.},
  year = {2008},
  month = jan,
  volume = {2008},
  pages = {169--196},
  issn = {1662-1425},
  doi = {10.3726/81605_169},
  abstract = {A scientifically adequate theory of semiotic processes must ultimately be founded on a theory of information that can unify the physical, biological, cognitive, and computational uses of the concept. Unfortunately, no such unification exists, and more importantly, the causal status of informational content remains ambiguous as a result. Lacking this grounding, semiotic theories have tended to be predominantly phenomenological taxonomies rather than dynamical explanations of the representational processes of natural systems. This paper argues that the problem of information that prevents the development of a scientific semiotic theory is the necessity of analyzing it as a negative relationship: defined with respect to absence. This is cryptically implicit in concepts of design and function in biology, acknowledged in psychological and philosophical accounts of intentionality and content, and is explicitly formulated in the mathematical theory of communication (aka ``information theory''). Beginning from the base established by Claude Shannon, which otherwise ignores issues of content, reference, and evaluation, this two part essay explores its relationship to two other higher-order theories that are also explicitly based on an analysis of absence: Boltzmann's theory of thermodynamic entropy (in Part 1) and Darwin's theory of natural selection (in Part 2). This comparison demonstrates that these theories are both formally homologous and hierarchically interdependent. Their synthesis into a general theory of entropy and information provides the necessary grounding for theories of function and semiosis.},
  file = {2015 - Deacon - Shannon - Boltzmann — Darwin Redefining information ( Part II ).pdf;Deacon - 2008 - Shannon - Boltzmann - Darwin Redefining informati.pdf},
  journal = {Cognitive Semiotics},
  language = {en},
  number = {2}
}

@article{Deb2002,
  title = {A Fast and Elitist Multiobjective Genetic Algorithm: {{NSGA}}-{{II}}},
  shorttitle = {A Fast and Elitist Multiobjective Genetic Algorithm},
  author = {Deb, K. and Pratap, A. and Agarwal, S. and Meyarivan, T.},
  year = {2002},
  month = apr,
  volume = {6},
  pages = {182--197},
  issn = {1089778X},
  doi = {10.1109/4235.996017},
  file = {2002 - Deb et al. - A fast and elitist multiobjective genetic algorithm NSGA-II.pdf},
  journal = {IEEE Transactions on Evolutionary Computation},
  language = {en},
  number = {2}
}

@article{DeDeo2011,
  title = {Effective Theories for Circuits and Automata},
  author = {DeDeo, Simon},
  year = {2011},
  month = sep,
  volume = {21},
  pages = {037106},
  issn = {1054-1500, 1089-7682},
  doi = {10.1063/1.3640747},
  file = {2011 - DeDeo - Effective theories for circuits and automata.pdf},
  journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
  language = {en},
  number = {3}
}

@article{Deemyad2018,
  title = {Astrocytes Integrate and Drive Action Potential Firing in Inhibitory Subnetworks},
  author = {Deemyad, Tara and L{\"u}thi, Joel and Spruston, Nelson},
  year = {2018},
  month = dec,
  volume = {9},
  pages = {4336},
  issn = {2041-1723},
  doi = {10.1038/s41467-018-06338-3},
  file = {Deemyad et al. - 2018 - Astrocytes integrate and drive action potential fi.pdf},
  journal = {Nat Commun},
  language = {en},
  number = {1}
}

@article{Deger2014,
  title = {Fluctuations and Information Filtering in Coupled Populations of Spiking Neurons with Adaptation},
  author = {Deger, Moritz and Schwalger, Tilo and Naud, Richard and Gerstner, Wulfram},
  year = {2014},
  month = dec,
  volume = {90},
  issn = {1539-3755, 1550-2376},
  doi = {10.1103/PhysRevE.90.062704},
  file = {2014 - Deger et al. - Dynamics of interacting finite-sized networks of spiking neurons with adaptation.pdf;2014 - Deger et al. - Dynamics of interacting finite-sized networks of spiking neurons with adaptation(2).pdf;Deger et al. - 2014 - Fluctuations and information filtering in coupled  2.pdf;Deger et al. - 2014 - Fluctuations and information filtering in coupled .pdf},
  journal = {Physical Review E},
  keywords = {Quantitative Biology - Neurons and Cognition},
  language = {en},
  number = {6}
}

@article{Degris2012,
  title = {Off-{{Policy Actor}}-{{Critic}}},
  author = {Degris, Thomas and White, Martha and Sutton, Richard S.},
  year = {2012},
  month = may,
  abstract = {This paper presents the first actor-critic algorithm for off-policy reinforcement learning. Our algorithm is online and incremental, and its per-time-step complexity scales linearly with the number of learned weights. Previous work on actor-critic algorithms is limited to the on-policy setting and does not take advantage of the recent advances in offpolicy gradient temporal-difference learning. Off-policy techniques, such as Greedy-GQ, enable a target policy to be learned while following and obtaining data from another (behavior) policy. For many problems, however, actor-critic methods are more practical than action value methods (like Greedy-GQ) because they explicitly represent the policy; consequently, the policy can be stochastic and utilize a large action space. In this paper, we illustrate how to practically combine the generality and learning potential of off-policy learning with the flexibility in action selection given by actor-critic methods. We derive an incremental, linear time and space complexity algorithm that includes eligibility traces, prove convergence under assumptions similar to previous off-policy algorithms1, and empirically show better or comparable performance to existing algorithms on standard reinforcement-learning benchmark problems.},
  archiveprefix = {arXiv},
  eprint = {1205.4839},
  eprinttype = {arxiv},
  file = {2012 - Degris, White, Sutton - Off-Policy Actor-Critic.pdf},
  journal = {arXiv:1205.4839 [cs]},
  keywords = {Computer Science - Machine Learning},
  language = {en},
  primaryclass = {cs}
}

@article{deHemptinne2015,
  title = {Therapeutic Deep Brain Stimulation Reduces Cortical Phase-Amplitude Coupling in {{Parkinson}}'s Disease},
  author = {{de Hemptinne}, Coralie and Swann, Nicole C and Ostrem, Jill L and {Ryapolova-Webb}, Elena S and San Luciano, Marta and Galifianakis, Nicholas B and Starr, Philip A},
  year = {2015},
  month = may,
  volume = {18},
  pages = {779--786},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.3997},
  file = {de Hemptinne et al. - 2015 - Therapeutic deep brain stimulation reduces cortica.pdf},
  journal = {Nature Neuroscience},
  language = {en},
  number = {5}
}

@article{Dehghani2016,
  title = {Dynamic {{Balance}} of {{Excitation}} and {{Inhibition}} in {{Human}} and {{Monkey Neocortex}}},
  author = {Dehghani, Nima and Peyrache, Adrien and Telenczuk, Bartosz and Le Van Quyen, Michel and Halgren, Eric and Cash, Sydney S. and Hatsopoulos, Nicholas G. and Destexhe, Alain},
  year = {2016},
  month = sep,
  volume = {6},
  issn = {2045-2322},
  doi = {10.1038/srep23176},
  file = {2014 - Dehghani et al. - Multiscale Balance of Excitation and Inhibition in Single-Unit ensemble Recordings in Human and Monkey Neocorte.pdf;Dehghani et al. - 2016 - Dynamic Balance of Excitation and Inhibition in Hu 2.pdf;Dehghani et al. - 2016 - Dynamic Balance of Excitation and Inhibition in Hu.pdf},
  journal = {Scientific Reports},
  language = {en},
  number = {1}
}

@article{Dehmamy2018,
  title = {A Structural Transition in Physical Networks},
  author = {Dehmamy, Nima and Milanlouei, Soodabeh and Barab{\'a}si, Albert-L{\'a}szl{\'o}},
  year = {2018},
  month = nov,
  volume = {563},
  pages = {676--680},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/s41586-018-0726-6},
  file = {Dehmamy et al. - 2018 - A structural transition in physical networks.pdf},
  journal = {Nature},
  language = {en},
  number = {7733}
}

@article{deJager2011,
  title = {Levy {{Walks Evolve Through Interaction Between Movement}} and {{Environmental Complexity}}},
  author = {{de Jager}, M. and Weissing, F. J. and Herman, P. M. J. and Nolet, B. A. and {van de Koppel}, J.},
  year = {2011},
  month = jun,
  volume = {332},
  pages = {1551--1553},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1201187},
  file = {de Jager et al. - 2011 - Levy Walks Evolve Through Interaction Between Move.pdf},
  journal = {Science},
  language = {en},
  number = {6037}
}

@article{deJager2011a,
  title = {Levy {{Walks Evolve Through Interaction Between Movement}} and {{Environmental Complexity}}},
  author = {{de Jager}, M. and Weissing, F. J. and Herman, P. M. J. and Nolet, B. A. and {van de Koppel}, J.},
  year = {2011},
  month = jun,
  volume = {332},
  pages = {1551--1553},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1201187},
  file = {de Jager et al. - 2011 - Levy Walks Evolve Through Interaction Between Move 2.pdf},
  journal = {Science},
  language = {en},
  number = {6037}
}

@article{deJager2012,
  title = {Response to {{Comment}} on "{{Levy Walks Evolve Through Interaction Between Movement}} and {{Environmental Complexity}}"},
  author = {{de Jager}, M. and Weissing, F. J. and Herman, P. M. J. and Nolet, B. A. and {van de Koppel}, J.},
  year = {2012},
  month = feb,
  volume = {335},
  pages = {918--918},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1215903},
  file = {de Jager et al. - 2012 - Response to Comment on Levy Walks Evolve Through .pdf},
  journal = {Science},
  language = {en},
  number = {6071}
}

@article{deJager2014,
  title = {How Superdiffusion Gets Arrested: Ecological Encounters Explain Shift from {{L\'evy}} to {{Brownian}} Movement},
  shorttitle = {How Superdiffusion Gets Arrested},
  author = {{de Jager}, Monique and Bartumeus, Frederic and K{\"o}lzsch, Andrea and Weissing, Franz J. and Hengeveld, Geerten M. and Nolet, Bart A. and Herman, Peter M. J. and {van de Koppel}, Johan},
  year = {2014},
  month = jan,
  volume = {281},
  pages = {20132605},
  issn = {0962-8452, 1471-2954},
  doi = {10.1098/rspb.2013.2605},
  abstract = {Ecological theory uses Brownian motion as a default template for describing ecological movement, despite limited mechanistic underpinning. The generality of Brownian motion has recently been challenged by empirical studies that highlight alternative movement patterns of animals, especially when foraging in resource-poor environments. Yet, empirical studies reveal animals moving in a Brownian fashion when resources are abundant. We demonstrate that Einstein's original theory of collision-induced Brownian motion in physics provides a parsimonious, mechanistic explanation for these observations. Here, Brownian motion results from frequent encounters between organisms in dense environments. In density-controlled experiments, movement patterns of mussels shifted from L\'evy towards Brownian motion with increasing density. When the analysis was restricted to moves not truncated by encounters, this shift did not occur. Using a theoretical argument, we explain that any movement pattern approximates Brownian motion at high-resource densities, provided that movement is interrupted upon encounters. Hence, the observed shift to Brownian motion does not indicate a density-dependent change in movement strategy but rather results from frequent collisions. Our results emphasize the need for a more mechanistic use of Brownian motion in ecology, highlighting that especially in rich environments, Brownian motion emerges from ecological interactions, rather than being a default movement pattern.},
  file = {de Jager et al. - 2014 - How superdiffusion gets arrested ecological encou.pdf},
  journal = {Proc. R. Soc. B.},
  language = {en},
  number = {1774}
}

@article{delaFuente-Fernandez2004,
  title = {Presynaptic Mechanisms of Motor Fluctuations in {{Parkinson}}'s Disease: A Probabilistic Model},
  shorttitle = {Presynaptic Mechanisms of Motor Fluctuations in {{Parkinson}}'s Disease},
  author = {{de la Fuente-Fern{\'a}ndez}, Ra{\'u}l and Schulzer, Michael and Mak, Edwin and Calne, Donald B. and Stoessl, A. Jon},
  year = {2004},
  month = apr,
  volume = {127},
  pages = {888--899},
  issn = {1460-2156, 0006-8950},
  doi = {10.1093/brain/awh102},
  file = {2004 - De La Fuente-Fernández et al. - Presynaptic mechanisms of motor fluctuations in Parkinson's disease A probabilistic model.pdf},
  journal = {Brain},
  language = {en},
  number = {4}
}

@incollection{DeLaPava2015,
  title = {A {{Gaussian Process Emulator}} for {{Estimating}} the {{Volume}} of {{Tissue Activated During Deep Brain Stimulation}}},
  booktitle = {Pattern {{Recognition}} and {{Image Analysis}}},
  author = {De La Pava, Iv{\'a}n and G{\'o}mez, Viviana and {\'A}lvarez, Mauricio A. and Henao, {\'O}scar A. and {Daza-Santacoloma}, Genaro and Orozco, {\'A}lvaro A.},
  editor = {Paredes, Roberto and Cardoso, Jaime S. and Pardo, Xos{\'e} M.},
  year = {2015},
  volume = {9117},
  pages = {691--699},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-19390-8_77},
  file = {2015 - Ángel et al. - A Gaussian Process Emulator for Estimating the Volume of Tissue Activated During Deep Brain Stimulation.pdf;De La Pava et al. - 2015 - A Gaussian Process Emulator for Estimating the Vol.pdf},
  isbn = {978-3-319-19389-2 978-3-319-19390-8},
  language = {en}
}

@article{Delevich2016,
  title = {Parvalbumin Interneuron Dysfunction in a Thalamus - Prefrontal Cortex Circuit in {{Disc1}} Deficiency Mice},
  author = {Delevich, Kristen and {Jaaro-Peled}, Hanna and Penzo, Mario and Sawa, Akira and Li, Bo},
  year = {2016},
  month = may,
  doi = {10.1101/054759},
  abstract = {Two of the most consistent findings across disrupted-in-schizophrenia-1 (DISC1) mouse models are impaired working memory and reduced number or function of parvalbumin interneurons within the prefrontal cortex. While these findings suggest parvalbumin interneuron dysfunction in DISC1-related pathophysiology, to date, cortical inhibitory circuit function has not been investigated in depth in DISC1 deficiency mouse models. Here we assessed the function of a feedforward circuit between the mediodorsal thalamus (MD) and the medial prefrontal cortex (mPFC) in mice harboring a deletion in one allele of the Disc1 gene. We found that the inhibitory drive onto layer 3 pyramidal neurons in the mPFC was significantly reduced in the Disc1 deficient mice. This reduced inhibition was accompanied by decreased GABA release from local parvalbumin, but not somatostatin, inhibitory interneurons, and by impaired feedforward inhibition in the MD-mPFC circuit. Our results reveal a cellular mechanism by which deficiency in DISC1 causes neural circuit dysfunction frequently implicated in psychiatric disorders.},
  file = {Delevich et al. - 2016 - Parvalbumin interneuron dysfunction in a thalamus .pdf},
  journal = {bioRxiv},
  language = {en}
}

@article{DeLuca2014,
  title = {Statistically Rigorous Calculations Do Not Support Common Input and Long-Term Synchronization of Motor-Unit Firings},
  author = {De Luca, Carlo J. and Kline, Joshua C.},
  year = {2014},
  month = dec,
  volume = {112},
  pages = {2729--2744},
  issn = {0022-3077, 1522-1598},
  doi = {10.1152/jn.00725.2013},
  file = {De Luca and Kline - 2014 - Statistically rigorous calculations do not support.pdf},
  journal = {Journal of Neurophysiology},
  language = {en},
  number = {11}
}

@article{DeMartino2008,
  title = {Combining Multivariate Voxel Selection and Support Vector Machines for Mapping and Classification of {{fMRI}} Spatial Patterns},
  author = {De Martino, Federico and Valente, Giancarlo and Staeren, No{\"e}l and Ashburner, John and Goebel, Rainer and Formisano, Elia},
  year = {2008},
  month = oct,
  volume = {43},
  pages = {44--58},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2008.06.037},
  abstract = {In functional brain mapping, pattern recognition methods allow detecting multivoxel patterns of brain activation which are informative with respect to a subject's perceptual or cognitive state. The sensitivity of these methods, however, is greatly reduced when the proportion of voxels that convey the discriminative information is small compared to the total number of measured voxels. To reduce this dimensionality problem, previous studies employed univariate voxel selection or region-of-interest-based strategies as a preceding step to the application of machine learning algorithms.},
  file = {2008 - De Martino et al. - Combining multivariate voxel selection and support vector machines for mapping and classification of fMRI spa.pdf},
  journal = {NeuroImage},
  language = {en},
  number = {1}
}

@article{Deneve,
  title = {Bayesian Inference in Spiking Neurons},
  author = {Deneve, Sophie},
  pages = {8},
  abstract = {We propose a new interpretation of spiking neurons as Bayesian integrators accumulating evidence over time about events in the external world or the body, and communicating to other neurons their certainties about these events. In this model, spikes signal the occurrence of new information, i.e. what cannot be predicted from the past activity. As a result, firing statistics are close to Poisson, albeit providing a deterministic representation of probabilities. We proceed to develop a theory of Bayesian inference in spiking neural networks, recurrent interactions implementing a variant of belief propagation.},
  file = {2005 - Deneve - Bayesian Inference in Spiking Neurons.pdf},
  language = {en}
}

@article{Deneve2016,
  title = {Efficient Codes and Balanced Networks},
  author = {Den{\`e}ve, Sophie and Machens, Christian K},
  year = {2016},
  month = mar,
  volume = {19},
  pages = {375--382},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.4243},
  file = {Denève and Machens - 2016 - Efficient codes and balanced networks.pdf},
  journal = {Nature Neuroscience},
  language = {en},
  number = {3}
}

@article{Denker2011,
  title = {The {{Local Field Potential Reflects Surplus Spike Synchrony}}},
  author = {Denker, Michael and Roux, S{\'e}bastien and Lind{\'e}n, Henrik and Diesmann, Markus and Riehle, Alexa and Gr{\"u}n, Sonja},
  year = {2011},
  month = dec,
  volume = {21},
  pages = {2681--2695},
  issn = {1460-2199, 1047-3211},
  doi = {10.1093/cercor/bhr040},
  file = {Denker et al. - 2011 - The Local Field Potential Reflects Surplus Spike S.pdf},
  journal = {Cerebral Cortex},
  language = {en},
  number = {12}
}

@article{DePasquale,
  title = {Using {{Firing}}-{{Rate Dynamics}} to {{Train Recurrent Networks}} of {{Spiking Model Neurons}}},
  author = {DePasquale, Brian and Churchland, Mark M and Abbott, L F},
  pages = {17},
  abstract = {Recurrent neural networks are powerful tools for understanding and modeling computation and representation by populations of neurons. Continuous-variable or ``rate'' model networks have been analyzed and applied extensively for these purposes. However, neurons fire action potentials, and the discrete nature of spiking is an important feature of neural circuit dynamics. Despite significant advances, training recurrently connected spiking neural networks remains a challenge. We present a procedure for training recurrently connected spiking networks to generate dynamical patterns autonomously, to produce complex temporal outputs based on integrating network input, and to model physiological data. Our procedure makes use of a continuous-variable network to identify targets for training the inputs to the spiking model neurons. Surprisingly, we are able to construct spiking networks that duplicate tasks performed by continuous-variable networks with only a relatively minor expansion in the number of neurons. Our approach provides a novel view of the significance and appropriate use of ``firing rate'' models, and it is a useful approach for building model spiking networks that can be used to address important questions about representation and computation in neural systems.},
  file = {DePasquale et al. - Using Firing-Rate Dynamics to Train Recurrent Netw.pdf},
  language = {en}
}

@article{DePasquale2018,
  title = {Full-{{FORCE}}: {{A}} Target-Based Method for Training Recurrent Networks},
  shorttitle = {Full-{{FORCE}}},
  author = {DePasquale, Brian and Cueva, Christopher J. and Rajan, Kanaka and Escola, G. Sean and Abbott, L. F.},
  editor = {Chacron, Maurice J.},
  year = {2018},
  month = feb,
  volume = {13},
  pages = {e0191527},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0191527},
  abstract = {Trained recurrent networks are powerful tools for modeling dynamic neural computations. We present a target-based method for modifying the full connectivity matrix of a recurrent network to train it to perform tasks involving temporally complex input/output transformations. The method introduces a second network during training to provide suitable ``target'' dynamics useful for performing the task. Because it exploits the full recurrent connectivity, the method produces networks that perform tasks with fewer neurons and greater noise robustness than traditional least-squares (FORCE) approaches. In addition, we show how introducing additional input signals into the target-generating network, which act as task hints, greatly extends the range of tasks that can be learned and provides control over the complexity and nature of the dynamics of the trained, task-performing network.},
  file = {DePasquale et al. - 2018 - full-FORCE A target-based method for training rec.pdf},
  journal = {PLOS ONE},
  language = {en},
  number = {2}
}

@article{Destexhe,
  title = {Kinetic {{Models}} of {{Synaptic Transmission}}},
  author = {Destexhe, A and Mainen, Z F and Sejnowski, T J and Koch, C (EDITOR) and Segev, I (EDITOR)},
  pages = {26},
  file = {1998 - Destexhe et al. - Kinetic models of synaptic transmission.pdf},
  language = {en}
}

@article{Destexhe2003,
  title = {The High-Conductance State of Neocortical Neurons in Vivo},
  author = {Destexhe, Alain and Rudolph, Michael and Par{\'e}, Denis},
  year = {2003},
  month = sep,
  volume = {4},
  pages = {739--751},
  issn = {1471-003X, 1471-0048},
  doi = {10.1038/nrn1198},
  file = {2003 - Destexhe, Rudolph, Paré - The high-conductance state of neocortical neurons in vivo.pdf},
  journal = {Nature Reviews Neuroscience},
  language = {en},
  number = {9}
}

@article{Deutsch,
  title = {Constructor {{Theory}} of {{Information}}},
  author = {Deutsch, David and Marletto, Chiara},
  pages = {30},
  file = {Deutsch and Marletto - Constructor Theory of Information.pdf},
  language = {en}
}

@article{Deutscha,
  title = {Constructor {{Theory}} of {{Information}}},
  author = {Deutsch, David and Marletto, Chiara},
  pages = {30},
  file = {Deutsch and Marletto - Constructor Theory of Information.pdf},
  language = {en}
}

@article{Dexter2019,
  title = {A {{Complex Hierarchy}} of {{Avoidance Behaviors}} in a {{Single}}-{{Cell Eukaryote}}},
  author = {Dexter, Joseph P. and Prabakaran, Sudhakaran and Gunawardena, Jeremy},
  year = {2019},
  month = dec,
  volume = {29},
  pages = {4323-4329.e2},
  issn = {09609822},
  doi = {10.1016/j.cub.2019.10.059},
  file = {Dexter et al. - 2019 - A Complex Hierarchy of Avoidance Behaviors in a Si.pdf},
  journal = {Current Biology},
  language = {en},
  number = {24}
}

@article{Dhawale,
  title = {1 {{Automated}} Long-Term Recording and Analysis of Neural Activity in Behaving Animals},
  author = {Dhawale, Ashesh K and Poddar, Rajesh and Wolff, Steffen B E and Normand, Valentin A and {\"O}lveczky, Bence P},
  pages = {108},
  file = {Dhawale et al. - 1 Automated long-term recording and analysis of ne.pdf},
  language = {en}
}

@article{Dhawale2017,
  title = {The {{Role}} of {{Variability}} in {{Motor Learning}}},
  author = {Dhawale, Ashesh K. and Smith, Maurice A. and {\"O}lveczky, Bence P.},
  year = {2017},
  month = jul,
  volume = {40},
  pages = {479--498},
  issn = {0147-006X, 1545-4126},
  doi = {10.1146/annurev-neuro-072116-031548},
  abstract = {Trial-to-trial variability in the execution of movements and motor skills is ubiquitous and widely considered to be the unwanted consequence of a noisy nervous system. However, recent studies have suggested that motor variability may also be a feature of how sensorimotor systems operate and learn. This view, rooted in reinforcement learning theory, equates motor variability with purposeful exploration of motor space that, when coupled with reinforcement, can drive motor learning. Here we review studies that explore the relationship between motor variability and motor learning in both humans and animal models. We discuss neural circuit mechanisms that underlie the generation and regulation of motor variability and consider the implications that this work has for our understanding of motor learning.},
  file = {Dhawale et al. - 2017 - The Role of Variability in Motor Learning.pdf},
  journal = {Annu. Rev. Neurosci.},
  language = {en},
  number = {1}
}

@article{Dhurandhar2020,
  title = {Enhancing {{Simple Models}} by {{Exploiting What They Already Know}}},
  author = {Dhurandhar, Amit and Shanmugam, Karthikeyan and Luss, Ronny},
  year = {2020},
  month = jun,
  abstract = {There has been recent interest in improving performance of simple models for multiple reasons such as interpretability, robust learning from small data, deployment in memory constrained settings as well as environmental considerations. In this paper, we propose a novel method SRatio that can utilize information from high performing complex models (viz. deep neural networks, boosted trees, random forests) to reweight a training dataset for a potentially low performing simple model of much lower complexity such as a decision tree or a shallow network enhancing its performance. Our method also leverages the per sample hardness estimate of the simple model which is not the case with the prior works which primarily consider the complex model's confidences/predictions and is thus conceptually novel. Moreover, we generalize and formalize the concept of attaching probes to intermediate layers of a neural network to other commonly used classifiers and incorporate this into our method. The benefit of these contributions is witnessed in the experiments where on 6 UCI datasets and CIFAR-10 we outperform competitors in a majority (16 out of 27) of the cases and tie for best performance in the remaining cases. In fact, in a couple of cases, we even approach the complex model's performance. We also conduct further experiments to validate assertions and intuitively understand why our method works. Theoretically, we motivate our approach by showing that the weighted loss minimized by simple models using our weighting upper bounds the loss of the complex model.},
  archiveprefix = {arXiv},
  eprint = {1905.13565},
  eprinttype = {arxiv},
  file = {Dhurandhar et al. - 2020 - Enhancing Simple Models by Exploiting What They Al.pdf},
  journal = {arXiv:1905.13565 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryclass = {cs, stat}
}

@article{Dhurandhar2020a,
  title = {Enhancing {{Simple Models}} by {{Exploiting What They Already Know}}},
  author = {Dhurandhar, Amit and Shanmugam, Karthikeyan and Luss, Ronny},
  year = {2020},
  month = jun,
  abstract = {There has been recent interest in improving performance of simple models for multiple reasons such as interpretability, robust learning from small data, deployment in memory constrained settings as well as environmental considerations. In this paper, we propose a novel method SRatio that can utilize information from high performing complex models (viz. deep neural networks, boosted trees, random forests) to reweight a training dataset for a potentially low performing simple model of much lower complexity such as a decision tree or a shallow network enhancing its performance. Our method also leverages the per sample hardness estimate of the simple model which is not the case with the prior works which primarily consider the complex model's confidences/predictions and is thus conceptually novel. Moreover, we generalize and formalize the concept of attaching probes to intermediate layers of a neural network to other commonly used classifiers and incorporate this into our method. The benefit of these contributions is witnessed in the experiments where on 6 UCI datasets and CIFAR-10 we outperform competitors in a majority (16 out of 27) of the cases and tie for best performance in the remaining cases. In fact, in a couple of cases, we even approach the complex model's performance. We also conduct further experiments to validate assertions and intuitively understand why our method works. Theoretically, we motivate our approach by showing that the weighted loss minimized by simple models using our weighting upper bounds the loss of the complex model.},
  archiveprefix = {arXiv},
  eprint = {1905.13565},
  eprinttype = {arxiv},
  file = {Dhurandhar et al. - 2020 - Enhancing Simple Models by Exploiting What They Al 2.pdf},
  journal = {arXiv:1905.13565 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryclass = {cs, stat}
}

@book{DiChio2011,
  title = {Applications of {{Evolutionary Computation}}: {{EvoApplications}} 2011: {{EvoCOMPLEX}}, {{EvoGAMES}}, {{EvoIASP}}, {{EvoINTELLIGENCE}}, {{EvoNUM}}, and {{EvoSTOC}}, {{Torino}}, {{Italy}}, {{April}} 27-29, 2011, {{Proceedings}}, {{Part I}}},
  shorttitle = {Applications of {{Evolutionary Computation}}},
  editor = {Di Chio, Cecilia and Cagnoni, Stefano and Cotta, Carlos and Ebner, Marc and Ek{\'a}rt, Anik{\'o} and {Esparcia-Alc{\'a}zar}, Anna I. and Merelo, Juan J. and Neri, Ferrante and Preuss, Mike and Richter, Hendrik and Togelius, Julian and Yannakakis, Georgios N.},
  year = {2011},
  volume = {6624},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-20525-5},
  file = {Di Chio et al. - 2011 - Applications of Evolutionary Computation EvoAppli.pdf},
  isbn = {978-3-642-20524-8 978-3-642-20525-5},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{Diedrichsen2011,
  title = {Comparing the Similarity and Spatial Structure of Neural Representations: {{A}} Pattern-Component Model},
  shorttitle = {Comparing the Similarity and Spatial Structure of Neural Representations},
  author = {Diedrichsen, J{\"o}rn and Ridgway, Gerard R. and Friston, Karl J. and Wiestler, Tobias},
  year = {2011},
  month = apr,
  volume = {55},
  pages = {1665--1678},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2011.01.044},
  abstract = {In recent years there has been growing interest in multivariate analyses of neuroimaging data, which can be used to detect distributed patterns of activity that encode an experimental factor of interest. In this setting, it has become common practice to study the correlations between patterns to make inferences about the way a brain region represents stimuli or tasks (known as representational similarity analysis). Although it would be of great interest to compare these correlations from different regions, direct comparisons are currently not possible. This is because sample correlations are strongly influenced by voxel-selection, fMRI noise, and nonspecific activation patterns, all of which can differ widely between regions. Here, we present a multivariate modeling framework in which the measured patterns are decomposed into their constituent parts. The model is based on a standard linear mixed model, in which pattern components are considered to be randomly distributed over voxels. The model allows one to estimate the true correlations of the underlying neuronal pattern components, thereby enabling comparisons between different regions or individuals. The pattern estimates also allow us to make inferences about the spatial structure of different response components. Thus, the new model provides a theoretical and analytical framework to study the structure of distributed neural representations.},
  file = {2011 - Diedrichsen et al. - Comparing the similarity and spatial structure of neural representations a pattern-component model.pdf},
  journal = {NeuroImage},
  language = {en},
  number = {4}
}

@inproceedings{Diehl2015,
  title = {Fast-Classifying, High-Accuracy Spiking Deep Networks through Weight and Threshold Balancing},
  booktitle = {2015 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  author = {Diehl, Peter U. and Neil, Daniel and Binas, Jonathan and Cook, Matthew and Liu, Shih-Chii and Pfeiffer, Michael},
  year = {2015},
  month = jul,
  pages = {1--8},
  publisher = {{IEEE}},
  address = {{Killarney, Ireland}},
  doi = {10.1109/IJCNN.2015.7280696},
  abstract = {Deep neural networks such as Convolutional Networks (ConvNets) and Deep Belief Networks (DBNs) represent the state-of-the-art for many machine learning and computer vision classification problems. To overcome the large computational cost of deep networks, spiking deep networks have recently been proposed, given the specialized hardware now available for spiking neural networks (SNNs). However, this has come at the cost of performance losses due to the conversion from analog neural networks (ANNs) without a notion of time, to sparsely firing, event-driven SNNs. Here we analyze the effects of converting deep ANNs into SNNs with respect to the choice of parameters for spiking neurons such as firing rates and thresholds. We present a set of optimization techniques to minimize performance loss in the conversion process for ConvNets and fully connected deep networks. These techniques yield networks that outperform all previous SNNs on the MNIST database to date, and many networks here are close to maximum performance after only 20 ms of simulated time. The techniques include using rectified linear units (ReLUs) with zero bias during training, and using a new weight normalization method to help regulate firing rates. Our method for converting an ANN into an SNN enables lowlatency classification with high accuracies already after the first output spike, and compared with previous SNN approaches it yields improved performance without increased training time. The presented analysis and optimization techniques boost the value of spiking deep networks as an attractive framework for neuromorphic computing platforms aimed at fast and efficient pattern recognition.},
  file = {2015 - Diehl et al. - Fast-classifying, high-accuracy spiking deep networks through weight and threshold balancing.pdf;Diehl et al. - 2015 - Fast-classifying, high-accuracy spiking deep netwo.pdf},
  isbn = {978-1-4799-1960-4},
  language = {en}
}

@article{Ding2016,
  title = {Cortical Tracking of Hierarchical Linguistic Structures in Connected Speech},
  author = {Ding, Nai and Melloni, Lucia and Zhang, Hang and Tian, Xing and Poeppel, David},
  year = {2016},
  month = jan,
  volume = {19},
  pages = {158--164},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.4186},
  file = {2015 - Ding et al. - Cortical tracking of hierarchical linguistic structures in connected speech.pdf;Ding et al. - 2016 - Cortical tracking of hierarchical linguistic struc.pdf},
  journal = {Nature Neuroscience},
  language = {en},
  number = {1}
}

@article{Doelling2019,
  title = {An Oscillator Model Better Predicts Cortical Entrainment to Music},
  author = {Doelling, Keith B. and Assaneo, M. Florencia and Bevilacqua, Dana and Pesaran, Bijan and Poeppel, David},
  year = {2019},
  month = apr,
  pages = {201816414},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1816414116},
  abstract = {A body of research demonstrates convincingly a role for synchronization of auditory cortex to rhythmic structure in sounds including speech and music. Some studies hypothesize that an oscillator in auditory cortex could underlie important temporal processes such as segmentation and prediction. An important critique of these findings raises the plausible concern that what is measured is perhaps not an oscillator but is instead a sequence of evoked responses. The two distinct mechanisms could look very similar in the case of rhythmic input, but an oscillator might better provide the computational roles mentioned above (i.e., segmentation and prediction). We advance an approach to adjudicate between the two models: analyzing the phase lag between stimulus and neural signal across different stimulation rates. We ran numerical simulations of evoked and oscillatory computational models, showing that in the evoked case,phase lag is heavily rate-dependent, while the oscillatory model displays marked phase concentration across stimulation rates. Next, we compared these model predictions with magnetoencephalography data recorded while participants listened to music of varying note rates. Our results show that the phase concentration of the experimental data is more in line with the oscillatory model than with the evoked model. This finding supports an auditory cortical signal that (               i               ) contains components of both bottom-up evoked responses and internal oscillatory synchronization whose strengths are weighted by their appropriateness for particular stimulus types and (               ii               ) cannot be explained by evoked responses alone.},
  file = {Doelling et al. - 2019 - An oscillator model better predicts cortical entra.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en}
}

@article{Doi2014,
  title = {A {{Simple Model}} of {{Optimal Population Coding}} for {{Sensory Systems}}},
  author = {Doi, Eizaburo and Lewicki, Michael S.},
  editor = {Bethge, Matthias},
  year = {2014},
  month = aug,
  volume = {10},
  pages = {e1003761},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1003761},
  abstract = {A fundamental task of a sensory system is to infer information about the environment. It has long been suggested that an important goal of the first stage of this process is to encode the raw sensory signal efficiently by reducing its redundancy in the neural representation. Some redundancy, however, would be expected because it can provide robustness to noise inherent in the system. Encoding the raw sensory signal itself is also problematic, because it contains distortion and noise. The optimal solution would be constrained further by limited biological resources. Here, we analyze a simple theoretical model that incorporates these key aspects of sensory coding, and apply it to conditions in the retina. The model specifies the optimal way to incorporate redundancy in a population of noisy neurons, while also optimally compensating for sensory distortion and noise. Importantly, it allows an arbitrary input-to-output cell ratio between sensory units (photoreceptors) and encoding units (retinal ganglion cells), providing predictions of retinal codes at different eccentricities. Compared to earlier models based on redundancy reduction, the proposed model conveys more information about the original signal. Interestingly, redundancy reduction can be near-optimal when the number of encoding units is limited, such as in the peripheral retina. We show that there exist multiple, equally-optimal solutions whose receptive field structure and organization vary significantly. Among these, the one which maximizes the spatial locality of the computation, but not the sparsity of either synaptic weights or neural responses, is consistent with known basic properties of retinal receptive fields. The model further predicts that receptive field structure changes less with light adaptation at higher input-to-output cell ratios, such as in the periphery.},
  file = {Doi and Lewicki - 2014 - A Simple Model of Optimal Population Coding for Se.pdf},
  journal = {PLoS Computational Biology},
  language = {en},
  number = {8}
}

@article{Doiron2001,
  title = {Subtractive and {{Divisive Inhibition}}: {{Effect}} of {{Voltage}}-{{Dependent Inhibitory Conductances}} and {{Noise}}},
  shorttitle = {Subtractive and {{Divisive Inhibition}}},
  author = {Doiron, Brent and Longtin, Andr{\'e} and Berman, Neil and Maler, Leonard},
  year = {2001},
  month = jan,
  volume = {13},
  pages = {227--248},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/089976601300014691},
  file = {2001 - Doiron et al. - Subtractive and divisive inhibition effect of voltage-dependent inhibitory conductances and noise.pdf},
  journal = {Neural Computation},
  language = {en},
  number = {1}
}

@article{Doll2015,
  title = {Model-Based Choices Involve Prospective Neural Activity},
  author = {Doll, Bradley B and Duncan, Katherine D and Simon, Dylan A and Shohamy, Daphna and Daw, Nathaniel D},
  year = {2015},
  month = may,
  volume = {18},
  pages = {767--772},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.3981},
  file = {2015 - Doll et al. - Model-based choices involve prospective neural activity.pdf;Doll et al. - 2015 - Model-based choices involve prospective neural act.pdf},
  journal = {Nature Neuroscience},
  language = {en},
  number = {5}
}

@article{Doncieux2020,
  title = {Novelty {{Search}} Makes {{Evolvability Inevitable}}},
  author = {Doncieux, Stephane and Paolo, Giuseppe and Laflaqui{\`e}re, Alban and Coninx, Alexandre},
  year = {2020},
  month = may,
  abstract = {Evolvability is an important feature that impacts the ability of evolutionary processes to find interesting novel solutions and to deal with changing conditions of the problem to solve. The estimation of evolvability is not straight-forward and is generally too expensive to be directly used as selective pressure in the evolutionary process. Indirectly promoting evolvability as a side effect of other easier and faster to compute selection pressures would thus be advantageous. In an unbounded behavior space, it has already been shown that evolvable individuals naturally appear and tend to be selected as they are more likely to invade empty behavior niches. Evolvability is thus a natural byproduct of the search in this context. However, practical agents and environments often impose limits on the reachable behavior space. How do these boundaries impact evolvability? In this context, can evolvability still be promoted without explicitly rewarding it? We show that Novelty Search implicitly creates a pressure for high evolvability even in bounded behavior spaces, and explore the reasons for such a behavior. More precisely we show that, throughout the search, the dynamic evaluation of novelty rewards individuals which are very mobile in the behavior space, which in turn promotes evolvability.},
  archiveprefix = {arXiv},
  eprint = {2005.06224},
  eprinttype = {arxiv},
  file = {Doncieux et al. - 2020 - Novelty Search makes Evolvability Inevitable.pdf},
  journal = {arXiv:2005.06224 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Computer Science - Robotics},
  language = {en},
  primaryclass = {cs}
}

@article{Donoghue2020,
  title = {Parameterizing Neural Power Spectra into Periodic and Aperiodic Components},
  author = {Donoghue, Thomas and Haller, Matar and Peterson, Erik J. and Varma, Paroma and Sebastian, Priyadarshini and Gao, Richard and Noto, Torben and Lara, Antonio H. and Wallis, Joni D. and Knight, Robert T. and Shestyuk, Avgusta and Voytek, Bradley},
  year = {2020},
  month = dec,
  volume = {23},
  pages = {1655--1665},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/s41593-020-00744-x},
  file = {Donoghue et al. - 2020 - Parameterizing neural power spectra into periodic .pdf},
  journal = {Nat Neurosci},
  language = {en},
  number = {12}
}

@book{Dorigo2012,
  title = {Swarm {{Intelligence}}},
  editor = {Dorigo, Marco and Birattari, Mauro and Blum, Christian and Christensen, Anders Lyhne and Engelbrecht, Andries P. and Gro{\ss}, Roderich and St{\"u}tzle, Thomas and Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard},
  year = {2012},
  volume = {7461},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-32650-9},
  file = {Dorigo et al. - 2012 - Swarm Intelligence.pdf},
  isbn = {978-3-642-32649-3 978-3-642-32650-9},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@incollection{Douglas2010,
  title = {Canonical {{Cortical Circuits}}},
  booktitle = {Handbook of {{Brain Microcircuits}}},
  author = {Douglas, Rodney J. and Martin, Kevan A. C.},
  editor = {Shepherd, MD, DPhil, Gordon and Grillner, MD, Sten},
  year = {2010},
  month = aug,
  pages = {15--21},
  publisher = {{Oxford University Press}},
  doi = {10.1093/med/9780195389883.003.0002},
  file = {2010 - Douglas - Canonical cortical circuits.pdf;2010 - Douglas - Canonical cortical circuits(2).pdf},
  isbn = {978-0-19-538988-3},
  language = {en}
}

@article{Doya2000,
  title = {Complementary Roles of Basal Ganglia and Cerebellum in Learning and Motor Control},
  author = {Doya, K},
  year = {2000},
  month = dec,
  volume = {10},
  pages = {732--739},
  issn = {09594388},
  doi = {10.1016/S0959-4388(00)00153-7},
  file = {Doya - 2000 - Complementary roles of basal ganglia and cerebellu.pdf},
  journal = {Current Opinion in Neurobiology},
  language = {en},
  number = {6}
}

@article{Dreyer2014,
  title = {Three {{Mechanisms}} by Which {{Striatal Denervation Causes Breakdown}} of {{Dopamine Signaling}}},
  author = {Dreyer, J. K.},
  year = {2014},
  month = sep,
  volume = {34},
  pages = {12444--12456},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.1458-14.2014},
  file = {2014 - Dreyer - Three Mechanisms by which Striatal Denervation Causes Breakdown of Dopamine Signaling.pdf;Dreyer - 2014 - Three Mechanisms by which Striatal Denervation Cau.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {37}
}

@article{Dreyfus2002,
  title = {Richard {{Bellman}} on the {{Birth}} of {{Dynamic Programming}}},
  author = {Dreyfus, Stuart},
  year = {2002},
  month = feb,
  volume = {50},
  pages = {48--51},
  issn = {0030-364X, 1526-5463},
  doi = {10.1287/opre.50.1.48.17791},
  file = {Dreyfus - 2002 - Richard Bellman on the Birth of Dynamic Programmin.pdf},
  journal = {Operations Research},
  language = {en},
  number = {1}
}

@article{Drion2015,
  title = {Ion Channel Degeneracy Enables Robust and Tunable Neuronal Firing Rates},
  author = {Drion, Guillaume and O'Leary, Timothy and Marder, Eve},
  year = {2015},
  month = sep,
  volume = {112},
  pages = {E5361-E5370},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1516400112},
  file = {2015 - Drion, O’Leary, Marder - Ion channel degeneracy enables robust and tunable neuronal firing rates.pdf;Drion et al. - 2015 - Ion channel degeneracy enables robust and tunable .pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {38}
}

@article{Druckmann2007,
  title = {A Novel Multiple Objective Optimization Framework for Constraining Conductance-Based Neuron Models by Experimental Data},
  author = {Druckmann, Shaul},
  year = {2007},
  month = nov,
  volume = {1},
  pages = {7--18},
  issn = {16624548},
  doi = {10.3389/neuro.01.1.1.001.2007},
  file = {2007 - Druckmann et al. - A novel multiple objective optimization framework for constraining conductance-based neuron models by experime.pdf},
  journal = {Frontiers in Neuroscience},
  language = {en},
  number = {1}
}

@article{Druckmann2008,
  title = {Evaluating Automated Parameter Constraining Procedures of Neuron Models by Experimental and Surrogate Data},
  author = {Druckmann, Shaul and Berger, Thomas K. and Hill, Sean and Sch{\"u}rmann, Felix and Markram, Henry and Segev, Idan},
  year = {2008},
  month = nov,
  volume = {99},
  pages = {371--379},
  issn = {0340-1200, 1432-0770},
  doi = {10.1007/s00422-008-0269-2},
  file = {2008 - Druckmann et al. - Evaluating automated parameter constraining procedures of neuron models by experimental and surrogate data.pdf},
  journal = {Biological Cybernetics},
  language = {en},
  number = {4-5}
}

@article{Druckmann2011,
  title = {Effective {{Stimuli}} for {{Constructing Reliable Neuron Models}}},
  author = {Druckmann, Shaul and Berger, Thomas K. and Sch{\"u}rmann, Felix and Hill, Sean and Markram, Henry and Segev, Idan},
  editor = {Graham, Lyle J.},
  year = {2011},
  month = aug,
  volume = {7},
  pages = {e1002133},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1002133},
  abstract = {The rich dynamical nature of neurons poses major conceptual and technical challenges for unraveling their nonlinear membrane properties. Traditionally, various current waveforms have been injected at the soma to probe neuron dynamics, but the rationale for selecting specific stimuli has never been rigorously justified. The present experimental and theoretical study proposes a novel framework, inspired by learning theory, for objectively selecting the stimuli that best unravel the neuron's dynamics. The efficacy of stimuli is assessed in terms of their ability to constrain the parameter space of biophysically detailed conductance-based models that faithfully replicate the neuron's dynamics as attested by their ability to generalize well to the neuron's response to novel experimental stimuli. We used this framework to evaluate a variety of stimuli in different types of cortical neurons, ages and animals. Despite their simplicity, a set of stimuli consisting of step and ramp current pulses outperforms synaptic-like noisy stimuli in revealing the dynamics of these neurons. The general framework that we propose paves a new way for defining, evaluating and standardizing effective electrical probing of neurons and will thus lay the foundation for a much deeper understanding of the electrical nature of these highly sophisticated and non-linear devices and of the neuronal networks that they compose.},
  file = {2011 - Druckmann et al. - Effective stimuli for constructing reliable neuron models.pdf},
  journal = {PLoS Computational Biology},
  language = {en},
  number = {8}
}

@article{Druckmann2012,
  title = {Neuronal {{Circuits Underlying Persistent Representations Despite Time Varying Activity}}},
  author = {Druckmann, Shaul and Chklovskii, Dmitri B.},
  year = {2012},
  month = nov,
  volume = {22},
  pages = {2095--2103},
  issn = {09609822},
  doi = {10.1016/j.cub.2012.08.058},
  abstract = {Background: Our brains are capable of remarkably stable stimulus representations despite time-varying neural activity. For instance, during delay periods in working memory tasks, while stimuli are represented in working memory, neurons in the prefrontal cortex, thought to support the memory representation, exhibit time-varying neuronal activity. Since neuronal activity encodes the stimulus, its time-varying dynamics appears to be paradoxical and incompatible with stable network stimulus representations. Indeed, this finding raises a fundamental question: can stable representations only be encoded with stable neural activity, or, its corollary, is every change in activity a sign of change in stimulus representation? Results: Here we explain how different time-varying representations offered by individual neurons can be woven together to form a coherent, time-invariant, representation. Motivated by two ubiquitous features of the neocortex\textemdash redundancy of neural representation and sparse intracortical connections\textemdash we derive a network architecture that resolves the apparent contradiction between representation stability and changing neural activity. Unexpectedly, this network architecture exhibits many structural properties that have been measured in cortical sensory areas. In particular, we can account for few-neuron motifs, synapse weight distribution, and the relations between neuronal functional properties and connection probability. Conclusions: We show that the intuition regarding network stimulus representation, typically derived from considering single neurons, may be misleading and that time-varying activity of distributed representation in cortical circuits does not necessarily imply that the network explicitly encodes time-varying properties.},
  file = {2012 - Druckmann, Chklovskii - Neuronal circuits underlying persistent representations despite time varying activity.pdf;2012 - Druckmann, Chklovskii - Neuronal circuits underlying persistent representations despite time varying activity(2).pdf},
  journal = {Current Biology},
  language = {en},
  number = {22}
}

@article{Duann2002,
  title = {Single-{{Trial Variability}} in {{Event}}-{{Related BOLD Signals}}},
  author = {Duann, Jeng-Ren and Jung, Tzyy-Ping and Kuo, Wen-Jui and Yeh, Tzu-Chen and Makeig, Scott and Hsieh, Jen-Chuen and Sejnowski, Terrence J.},
  year = {2002},
  month = apr,
  volume = {15},
  pages = {823--835},
  issn = {10538119},
  doi = {10.1006/nimg.2001.1049},
  file = {2002 - Duann et al. - Single-trial variability in event-related BOLD signals.pdf},
  journal = {NeuroImage},
  language = {en},
  number = {4}
}

@article{Duarte2017,
  title = {Synaptic Patterning and the Timescales of Cortical Dynamics},
  author = {Duarte, Renato and Seeholzer, Alexander and Zilles, Karl and Morrison, Abigail},
  year = {2017},
  month = apr,
  volume = {43},
  pages = {156--165},
  issn = {09594388},
  doi = {10.1016/j.conb.2017.02.007},
  file = {Duarte et al. - 2017 - Synaptic patterning and the timescales of cortical.pdf},
  journal = {Current Opinion in Neurobiology},
  language = {en}
}

@techreport{Dubois2020,
  title = {Human Complex Exploration Strategies Are Extended via Noradrenaline-Modulated Heuristics},
  author = {Dubois, M and Habicht, J and Michely, J and Moran, R and Dolan, Rj and Hauser, Tu},
  year = {2020},
  month = feb,
  institution = {{Neuroscience}},
  doi = {10.1101/2020.02.20.958025},
  abstract = {Abstract           An exploration-exploitation trade-off, the arbitration between sampling a lesser-known against a known rich option, is thought to be solved using computationally demanding exploration algorithms. Given known limitations in human cognitive resources, we hypothesised the presence of additional cheaper strategies. We examined for such heuristics in choice behaviour where we show this involves a value-free random exploration, that ignores all prior knowledge, and a novelty exploration that targets novel options alone. In a double-blind, placebo-controlled drug study, assessing contributions of dopamine (400mg amisulpride) and noradrenaline (40mg propranolol), we show that value-free random exploration is attenuated under the influence of propranolol, but not under amisulpride. Our findings demonstrate that humans deploy distinct computationally cheap exploration strategies and where value-free random exploration is under noradrenergic control.                        Data and materials availability             Data and code will be provided upon acceptance.},
  file = {Dubois et al. - 2020 - Human complex exploration strategies are extended .pdf},
  language = {en},
  type = {Preprint}
}

@article{Dudman2016,
  title = {The Basal Ganglia: From Motor Commands to the Control of Vigor},
  shorttitle = {The Basal Ganglia},
  author = {Dudman, Joshua T and Krakauer, John W},
  year = {2016},
  month = apr,
  volume = {37},
  pages = {158--166},
  issn = {09594388},
  doi = {10.1016/j.conb.2016.02.005},
  file = {Dudman and Krakauer - 2016 - The basal ganglia from motor commands to the cont.pdf},
  journal = {Current Opinion in Neurobiology},
  language = {en}
}

@article{Dudman2016a,
  title = {The Basal Ganglia: From Motor Commands to the Control of Vigor},
  shorttitle = {The Basal Ganglia},
  author = {Dudman, Joshua T and Krakauer, John W},
  year = {2016},
  month = apr,
  volume = {37},
  pages = {158--166},
  issn = {09594388},
  doi = {10.1016/j.conb.2016.02.005},
  file = {Dudman and Krakauer - 2016 - The basal ganglia from motor commands to the cont 2.pdf},
  journal = {Current Opinion in Neurobiology},
  language = {en}
}

@incollection{Duff1995,
  title = {Q-{{Learning}} for {{Bandit Problems}}},
  booktitle = {Machine {{Learning Proceedings}} 1995},
  author = {Duff, Michael O.},
  year = {1995},
  pages = {209--217},
  publisher = {{Elsevier}},
  doi = {10.1016/B978-1-55860-377-6.50034-7},
  abstract = {Multi-armed bandits may be viewed as decompositionally-structured Markov decision processes (MDP's) with potentially very large state sets. A particularly elegant methodology for computing optimal policies was developed over twenty ago by Gittins Gittins \& Jones, 1974]. Gittins' approach reduces the problem of nding optimal policies for the original MDP to a sequence of low-dimensional stopping problems whose solutions determine the optimal policy through the so-called \textbackslash Gittins indices." Katehakis and Veinott Katehakis \& Veinott, 1987] have shown that the Gittins index for a task in state i may be interpreted as a particular component of the maximum-value function associated with the \textbackslash restart-in-i" process, a simple MDP to which standard solution methods for computing optimal policies, such as successive approximation, apply. This paper explores the problem of learning the Gittins indices on-line without the aid of a process model; it suggests utilizing task-state-speci c Q-learning agents to solve their respective restart-in-state-i subproblems, and includes an example in which the online reinforcement learning approach is applied to a simple problem of stochastic scheduling|one instance drawn from a wide class of problems that may be formulated as bandit problems.},
  file = {Duff - 1995 - Q-Learning for Bandit Problems.pdf},
  isbn = {978-1-55860-377-6},
  language = {en}
}

@article{Dummer2014,
  title = {Self-Consistent Determination of the Spike-Train Power Spectrum in a Neural Network with Sparse Connectivity},
  author = {Dummer, Benjamin and Wieland, Stefan and Lindner, Benjamin},
  year = {2014},
  month = sep,
  volume = {8},
  issn = {1662-5188},
  doi = {10.3389/fncom.2014.00104},
  abstract = {A major source of random variability in cortical networks is the quasi-random arrival of presynaptic action potentials from many other cells. In network studies as well as in the study of the response properties of single cells embedded in a network, synaptic background input is often approximated by Poissonian spike trains. However, the output statistics of the cells is in most cases far from being Poisson. This is inconsistent with the assumption of similar spike-train statistics for pre- and postsynaptic cells in a recurrent network. Here we tackle this problem for the popular class of integrate-and-fire neurons and study a self-consistent statistics of input and output spectra of neural spike trains. Instead of actually using a large network, we use an iterative scheme, in which we simulate a single neuron over several generations. In each of these generations, the neuron is stimulated with surrogate stochastic input that has a similar statistics as the output of the previous generation. For the surrogate input, we employ two distinct approximations: (i) a superposition of renewal spike trains with the same interspike interval density as observed in the previous generation and (ii) a Gaussian current with a power spectrum proportional to that observed in the previous generation. For input parameters that correspond to balanced input in the network, both the renewal and the Gaussian iteration procedure converge quickly and yield comparable results for the self-consistent spike-train power spectrum. We compare our results to large-scale simulations of a random sparsely connected network of leaky integrate-and-fire neurons (Brunel, 2000) and show that in the asynchronous regime close to a state of balanced synaptic input from the network, our iterative schemes provide an excellent approximations to the autocorrelation of spike trains in the recurrent network.},
  file = {Dummer et al. - 2014 - Self-consistent determination of the spike-train p.pdf},
  journal = {Frontiers in Computational Neuroscience},
  language = {en}
}

@article{Dumoulin2018,
  title = {A Guide to Convolution Arithmetic for Deep Learning},
  author = {Dumoulin, Vincent and Visin, Francesco},
  year = {2018},
  month = jan,
  abstract = {We introduce a guide to help deep learning practitioners understand and manipulate convolutional neural network architectures. The guide clarifies the relationship between various properties (input shape, kernel shape, zero padding, strides and output shape) of convolutional, pooling and transposed convolutional layers, as well as the relationship between convolutional and transposed convolutional layers. Relationships are derived for various cases, and are illustrated in order to make them intuitive.},
  archiveprefix = {arXiv},
  eprint = {1603.07285},
  eprinttype = {arxiv},
  file = {Dumoulin and Visin - 2018 - A guide to convolution arithmetic for deep learnin.pdf},
  journal = {arXiv:1603.07285 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  language = {en},
  primaryclass = {cs, stat}
}

@article{Dunovan2016,
  title = {Believer-{{Skeptic Meets Actor}}-{{Critic}}: {{Rethinking}} the {{Role}} of {{Basal Ganglia Pathways}} during {{Decision}}-{{Making}} and {{Reinforcement Learning}}},
  shorttitle = {Believer-{{Skeptic Meets Actor}}-{{Critic}}},
  author = {Dunovan, Kyle and Verstynen, Timothy},
  year = {2016},
  month = mar,
  volume = {10},
  issn = {1662-453X},
  doi = {10.3389/fnins.2016.00106},
  abstract = {The flexibility of behavioral control is a testament to the brain's capacity for dynamically resolving uncertainty during goal-directed actions. This ability to select actions and learn from immediate feedback is driven by the dynamics of basal ganglia (BG) pathways. A growing body of empirical evidence conflicts with the traditional view that these pathways act as independent levers for facilitating (i.e., direct pathway) or suppressing (i.e., indirect pathway) motor output, suggesting instead that they engage in a dynamic competition during action decisions that computationally captures action uncertainty. Here we discuss the utility of encoding action uncertainty as a dynamic competition between opposing control pathways and provide evidence that this simple mechanism may have powerful implications for bridging neurocomputational theories of decision making and reinforcement learning.},
  file = {Dunovan and Verstynen - 2016 - Believer-Skeptic Meets Actor-Critic Rethinking th.pdf},
  journal = {Front. Neurosci.},
  language = {en}
}

@article{Dunovan2019,
  title = {Reward-Driven Changes in Striatal Pathway Competition Shape Evidence Evaluation in Decision-Making},
  author = {Dunovan, Kyle and Vich, Catalina and Clapp, Matthew and Verstynen, Timothy and Rubin, Jonathan},
  editor = {Bogacz, Rafal},
  year = {2019},
  month = may,
  volume = {15},
  pages = {e1006998},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1006998},
  file = {Dunovan et al. - 2019 - Reward-driven changes in striatal pathway competit.pdf},
  journal = {PLoS Comput Biol},
  language = {en},
  number = {5}
}

@article{Dunsmoor2015,
  title = {Categories, Concepts, and Conditioning: How Humans Generalize Fear},
  shorttitle = {Categories, Concepts, and Conditioning},
  author = {Dunsmoor, Joseph E. and Murphy, Gregory L.},
  year = {2015},
  month = feb,
  volume = {19},
  pages = {73--77},
  issn = {13646613},
  doi = {10.1016/j.tics.2014.12.003},
  file = {2015 - Dunsmoor, Murphy - Categories, concepts, and conditioning How humans generalize fear.pdf;Dunsmoor and Murphy - 2015 - Categories, concepts, and conditioning how humans.pdf},
  journal = {Trends in Cognitive Sciences},
  language = {en},
  number = {2}
}

@article{Duval2016,
  title = {A Brain Network Model Explaining Tremor in {{Parkinson}}'s Disease},
  author = {Duval, Christian and Daneault, Jean-Francois and Hutchison, William D. and Sadikot, Abbas F.},
  year = {2016},
  month = jan,
  volume = {85},
  pages = {49--59},
  issn = {09699961},
  doi = {10.1016/j.nbd.2015.10.009},
  abstract = {This paper presents a novel model of tremor in Parkinson's disease (PD) based on extensive literature review as well as novel results stemming from functional stereotactic neurosurgery for the alleviation of tremor in PD. Specifically, evidence that suggests the basal ganglia induces PD tremor via excessive inhibitory output to the thalamus and altered firing patterns which in turn generate rhythmic bursting activity of thalamic cells is presented. Then, evidence that the thalamus generates PD tremor by facilitating the generation and consolidation of rhythmic bursting activity of neurons within its nuclei is also offered. Finally, evidence that the cerebellum may modulate characteristics of PD tremor by treating it as if it was a voluntary motor behavior is presented. Accordingly, the current paper proposes that PD tremor is induced by abnormal basal ganglia activity; it is generated by the thalamus, and modulated or reinforced by the cerebellum.},
  file = {2015 - Duval et al. - A brain network model explaining tremor in Parkinson's disease.pdf;Duval et al. - 2016 - A brain network model explaining tremor in Parkins.pdf},
  journal = {Neurobiology of Disease},
  language = {en}
}

@article{Dwork2013,
  title = {The {{Algorithmic Foundations}} of {{Differential Privacy}}},
  author = {Dwork, Cynthia and Roth, Aaron},
  year = {2013},
  volume = {9},
  pages = {211--407},
  issn = {1551-305X, 1551-3068},
  doi = {10.1561/0400000042},
  file = {2013 - Dwork, Roth - The Algorithmic Foundations of Differential Privacy.pdf},
  journal = {Foundations and Trends\textregistered{} in Theoretical Computer Science},
  language = {en},
  number = {3-4}
}

@article{Ebert2014,
  title = {Coordinated Reset Stimulation in a Large-Scale Model of the {{STN}}-{{GPe}} Circuit},
  author = {Ebert, Martin and Hauptmann, Christian and Tass, Peter A.},
  year = {2014},
  month = nov,
  volume = {8},
  issn = {1662-5188},
  doi = {10.3389/fncom.2014.00154},
  abstract = {Synchronization of populations of neurons is a hallmark of several brain diseases. Coordinated reset (CR) stimulation is a model-based stimulation technique which specifically counteracts abnormal synchrony by desynchronization. Electrical CR stimulation, e.g., for the treatment of Parkinson's disease (PD), is administered via depth electrodes. In order to get a deeper understanding of this technique, we extended the top-down approach of previous studies and constructed a large-scale computational model of the respective brain areas. Furthermore, we took into account the spatial anatomical properties of the simulated brain structures and incorporated a detailed numerical representation of 2 {$\cdot$} 104 simulated neurons. We simulated the subthalamic nucleus (STN) and the globus pallidus externus (GPe). Connections within the STN were governed by spike-timing dependent plasticity (STDP). In this way, we modeled the physiological and pathological activity of the considered brain structures. In particular, we investigated how plasticity could be exploited and how the model could be shifted from strongly synchronized (pathological) activity to strongly desynchronized (healthy) activity of the neuronal populations via CR stimulation of the STN neurons. Furthermore, we investigated the impact of specific stimulation parameters especially the electrode position on the stimulation outcome. Our model provides a step forward toward a biophysically realistic model of the brain areas relevant to the emergence of pathological neuronal activity in PD. Furthermore, our model constitutes a test bench for the optimization of both stimulation parameters and novel electrode geometries for efficient CR stimulation.},
  file = {2014 - Ebert, Hauptmann, Tass - Coordinated reset stimulation in a large-scale model of the STN-GPe circuit.pdf;Ebert et al. - 2014 - Coordinated reset stimulation in a large-scale mod.pdf},
  journal = {Frontiers in Computational Neuroscience},
  language = {en}
}

@article{Eckhardt2007,
  title = {Modeling Walker Synchronization on the {{Millennium Bridge}}},
  author = {Eckhardt, Bruno and Ott, Edward and Strogatz, Steven H. and Abrams, Daniel M. and McRobie, Allan},
  year = {2007},
  month = feb,
  volume = {75},
  issn = {1539-3755, 1550-2376},
  doi = {10.1103/PhysRevE.75.021110},
  file = {2007 - Eckhardt et al. - Modeling walker synchronization on the millennium bridge.pdf},
  journal = {Physical Review E},
  language = {en},
  number = {2}
}

@article{Ecoffet2019,
  title = {Go-{{Explore}}: A {{New Approach}} for {{Hard}}-{{Exploration Problems}}},
  shorttitle = {Go-{{Explore}}},
  author = {Ecoffet, Adrien and Huizinga, Joost and Lehman, Joel and Stanley, Kenneth O. and Clune, Jeff},
  year = {2019},
  month = may,
  abstract = {A grand challenge in reinforcement learning is intelligent exploration, especially when rewards are sparse or deceptive. Two Atari games serve as benchmarks for such hard-exploration domains: Montezuma's Revenge and Pitfall. On both games, current RL algorithms perform poorly, even those with intrinsic motivation, which is the dominant method to encourage exploration and improve performance on hardexploration domains. To address this shortfall, we introduce a new algorithm called Go-Explore. It exploits the following principles: (1) remember states that have previously been visited, (2) first return to a promising state (without exploration), then explore from it, and (3) solve simulated environments through exploiting any available means (including by introducing determinism), then robustify (create a policy that can reliably perform the solution) via imitation learning. The combined effect of these principles generates dramatic performance improvements on hardexploration problems. On Montezuma's Revenge, without being provided any domain knowledge, Go-Explore scores over 43,000 points, almost 4 times the previous state of the art. Go-Explore can also easily harness human-provided domain knowledge, and when augmented with it Go-Explore scores a mean of over 650,000 points on Montezuma's Revenge. Its max performance of 18 million surpasses the human world record by an order of magnitude, thus meeting even the strictest definition of ``superhuman'' performance. On Pitfall, Go-Explore with domain knowledge is the first algorithm to score above zero. Its mean performance of almost 60,000 points also exceeds expert human performance. Because GoExplore can produce many high-performing demonstrations automatically and cheaply, it also outperforms previous imitation learning work in which the solution was provided in the form of a human demonstration. Go-Explore opens up many new research directions into improving it and weaving its insights into current RL algorithms. It may also enable progress on previously unsolvable hard-exploration problems in a variety of domains, especially the many that often harness a simulator during training (e.g. robotics).},
  archiveprefix = {arXiv},
  eprint = {1901.10995},
  eprinttype = {arxiv},
  file = {Ecoffet et al. - 2019 - Go-Explore a New Approach for Hard-Exploration Pr.pdf},
  journal = {arXiv:1901.10995 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryclass = {cs, stat}
}

@article{Ecoffet2020,
  title = {First Return Then Explore},
  author = {Ecoffet, Adrien and Huizinga, Joost and Lehman, Joel and Stanley, Kenneth O. and Clune, Jeff},
  year = {2020},
  month = may,
  abstract = {The promise of reinforcement learning is to solve complex sequential decision problems by specifying a high-level reward function only. However, RL algorithms struggle when, as is often the case, simple and intuitive rewards provide sparse and deceptive feedback. Avoiding these pitfalls requires thoroughly exploring the environment, but despite substantial investments by the community, creating algorithms that can do so remains one of the central challenges of the field. We hypothesize that the main impediment to effective exploration originates from algorithms forgetting how to reach previously visited states ("detachment") and from failing to first return to a state before exploring from it ("derailment"). We introduce Go-Explore, a family of algorithms that addresses these two challenges directly through the simple principles of explicitly remembering promising states and first returning to such states before exploring. Go-Explore solves all heretofore unsolved Atari games (those for which algorithms could not previously outperform humans when evaluated following current community standards) and surpasses the state of the art on all hard-exploration games, with orders of magnitude improvements on the grand challenges Montezuma's Revenge and Pitfall. We also demonstrate the practical potential of Go-Explore on a challenging and extremely sparse-reward robotics task. Additionally, we show that adding a goal-conditioned policy can further improve Go-Explore's exploration efficiency and enable it to handle stochasticity throughout training. The striking contrast between the substantial performance gains from Go-Explore and the simplicity of its mechanisms suggests that remembering promising states, returning to them, and exploring from them is a powerful and general approach to exploration, an insight that may prove critical to the creation of truly intelligent learning agents.},
  archiveprefix = {arXiv},
  eprint = {2004.12919},
  eprinttype = {arxiv},
  file = {Ecoffet et al. - 2020 - First return then explore.pdf},
  journal = {arXiv:2004.12919 [cs]},
  keywords = {Computer Science - Artificial Intelligence},
  language = {en},
  primaryclass = {cs}
}

@article{Economo2018,
  title = {Distinct Descending Motor Cortex Pathways and Their Roles in Movement},
  author = {Economo, Michael N. and Viswanathan, Sarada and Tasic, Bosiljka and Bas, Erhan and Winnubst, Johan and Menon, Vilas and Graybuck, Lucas T. and Nguyen, Thuc Nghi and Smith, Kimberly A. and Yao, Zizhen and Wang, Lihua and Gerfen, Charles R. and Chandrashekar, Jayaram and Zeng, Hongkui and Looger, Loren L. and Svoboda, Karel},
  year = {2018},
  month = nov,
  volume = {563},
  pages = {79--84},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/s41586-018-0642-9},
  file = {Economo et al. - 2018 - Distinct descending motor cortex pathways and thei.pdf},
  journal = {Nature},
  language = {en},
  number = {7729}
}

@article{Edwards2007,
  title = {Revisiting {{L\'evy}} Flight Search Patterns of Wandering Albatrosses, Bumblebees and Deer},
  author = {Edwards, Andrew M. and Phillips, Richard A. and Watkins, Nicholas W. and Freeman, Mervyn P. and Murphy, Eugene J. and Afanasyev, Vsevolod and Buldyrev, Sergey V. and {da Luz}, M. G. E. and Raposo, E. P. and Stanley, H. Eugene and Viswanathan, Gandhimohan M.},
  year = {2007},
  month = oct,
  volume = {449},
  pages = {1044--1048},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature06199},
  file = {Edwards et al. - 2007 - Revisiting Lévy flight search patterns of wanderin.pdf},
  journal = {Nature},
  language = {en},
  number = {7165}
}

@article{Edwards2007a,
  title = {Revisiting {{L\'evy}} Flight Search Patterns of Wandering Albatrosses, Bumblebees and Deer},
  author = {Edwards, Andrew M. and Phillips, Richard A. and Watkins, Nicholas W. and Freeman, Mervyn P. and Murphy, Eugene J. and Afanasyev, Vsevolod and Buldyrev, Sergey V. and {da Luz}, M. G. E. and Raposo, E. P. and Stanley, H. Eugene and Viswanathan, Gandhimohan M.},
  year = {2007},
  month = oct,
  volume = {449},
  pages = {1044--1048},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature06199},
  file = {Edwards et al. - 2007 - Revisiting Lévy flight search patterns of wanderin 2.pdf},
  journal = {Nature},
  language = {en},
  number = {7165}
}

@article{Edwards2007b,
  title = {Revisiting {{L\'evy}} Flight Search Patterns of Wandering Albatrosses, Bumblebees and Deer},
  author = {Edwards, Andrew M. and Phillips, Richard A. and Watkins, Nicholas W. and Freeman, Mervyn P. and Murphy, Eugene J. and Afanasyev, Vsevolod and Buldyrev, Sergey V. and {da Luz}, M. G. E. and Raposo, E. P. and Stanley, H. Eugene and Viswanathan, Gandhimohan M.},
  year = {2007},
  month = oct,
  volume = {449},
  pages = {1044--1048},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature06199},
  file = {Edwards et al. - 2007 - Revisiting Lévy flight search patterns of wanderin 3.pdf},
  journal = {Nature},
  language = {en},
  number = {7165}
}

@article{Edwards2007c,
  title = {Revisiting {{L\'evy}} Flight Search Patterns of Wandering Albatrosses, Bumblebees and Deer},
  author = {Edwards, Andrew M. and Phillips, Richard A. and Watkins, Nicholas W. and Freeman, Mervyn P. and Murphy, Eugene J. and Afanasyev, Vsevolod and Buldyrev, Sergey V. and {da Luz}, M. G. E. and Raposo, E. P. and Stanley, H. Eugene and Viswanathan, Gandhimohan M.},
  year = {2007},
  month = oct,
  volume = {449},
  pages = {1044--1048},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature06199},
  file = {Edwards et al. - 2007 - Revisiting Lévy flight search patterns of wanderin 4.pdf},
  journal = {Nature},
  language = {en},
  number = {7165}
}

@article{Edwards2007d,
  title = {Revisiting {{L\'evy}} Flight Search Patterns of Wandering Albatrosses, Bumblebees and Deer},
  author = {Edwards, Andrew M. and Phillips, Richard A. and Watkins, Nicholas W. and Freeman, Mervyn P. and Murphy, Eugene J. and Afanasyev, Vsevolod and Buldyrev, Sergey V. and {da Luz}, M. G. E. and Raposo, E. P. and Stanley, H. Eugene and Viswanathan, Gandhimohan M.},
  year = {2007},
  month = oct,
  volume = {449},
  pages = {1044--1048},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature06199},
  file = {Edwards et al. - 2007 - Revisiting Lévy flight search patterns of wanderin 5.pdf},
  journal = {Nature},
  language = {en},
  number = {7165}
}

@article{Effenberger2015,
  title = {Self-Organization in {{Balanced State Networks}} by {{STDP}} and {{Homeostatic Plasticity}}},
  author = {Effenberger, Felix and Jost, J{\"u}rgen and Levina, Anna},
  editor = {Morrison, Abigail},
  year = {2015},
  month = sep,
  volume = {11},
  pages = {e1004420},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1004420},
  file = {Effenberger et al. - 2015 - Self-organization in Balanced State Networks by ST.pdf},
  journal = {PLOS Computational Biology},
  language = {en},
  number = {9}
}

@article{Eichler2017,
  title = {The Complete Connectome of a Learning and Memory Centre in an Insect Brain},
  author = {Eichler, Katharina and Li, Feng and {Litwin-Kumar}, Ashok and Park, Youngser and Andrade, Ingrid and {Schneider-Mizell}, Casey M. and Saumweber, Timo and Huser, Annina and Eschbach, Claire and Gerber, Bertram and Fetter, Richard D. and Truman, James W. and Priebe, Carey E. and Abbott, L. F. and Thum, Andreas S. and Zlatic, Marta and Cardona, Albert},
  year = {2017},
  month = aug,
  volume = {548},
  pages = {175--182},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature23455},
  file = {Eichler et al. - 2017 - The complete connectome of a learning and memory c.pdf},
  journal = {Nature},
  language = {en},
  number = {7666}
}

@article{Eilam2005,
  title = {Die Hard: {{A}} Blend of Freezing and Fleeing as a Dynamic Defense\textemdash Implications for the Control of Defensive Behavior},
  shorttitle = {Die Hard},
  author = {Eilam, David},
  year = {2005},
  month = jan,
  volume = {29},
  pages = {1181--1191},
  issn = {01497634},
  doi = {10.1016/j.neubiorev.2005.03.027},
  abstract = {Freezing, fleeing or fighting back are general defensive responses in many taxa. These defenses are mutually exclusive, since a prey cannot simultaneously flee and fight, or freeze and flee. Each of these defenses by itself is rudimentary and probably cannot provide a completely effective means to elude predation. Freezing is efficient only if employed before the prey is spotted by the predator, otherwise the prey becomes a stationary, easy to catch target. In fleeing, the prey can move directly away and maximize its distance from the predator, move toward the predator to confine it to a single clashing point, or dodge sideways to evade the attack. Prey can also run in a straight path that is efficient against slow or distant predators, or in a zigzag path that is efficient when a raptor is close or fast. In all, freezing and fleeing constitute together a complex and flexible defensive response, and are probably controlled by different motor systems that are interconnected to allow fast switching between these behaviors, as required for an effective and versatile response.},
  file = {Eilam - 2005 - Die hard A blend of freezing and fleeing as a dyn.pdf},
  journal = {Neuroscience \& Biobehavioral Reviews},
  language = {en},
  number = {8}
}

@article{Eklund2016,
  title = {Cluster Failure: {{Why fMRI}} Inferences for Spatial Extent Have Inflated False-Positive Rates},
  shorttitle = {Cluster Failure},
  author = {Eklund, Anders and Nichols, Thomas E. and Knutsson, Hans},
  year = {2016},
  month = jul,
  volume = {113},
  pages = {7900--7905},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1602413113},
  file = {Eklund et al. - 2016 - Cluster failure Why fMRI inferences for spatial e 2.pdf;Eklund et al. - 2016 - Cluster failure Why fMRI inferences for spatial e.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {28}
}

@article{Ekstrom2005,
  title = {Human Hippocampal Theta Activity during Virtual Navigation},
  author = {Ekstrom, Arne D. and Caplan, Jeremy B. and Ho, Emily and Shattuck, Kirk and Fried, Itzhak and Kahana, Michael J.},
  year = {2005},
  volume = {15},
  pages = {881--889},
  issn = {1050-9631, 1098-1063},
  doi = {10.1002/hipo.20109},
  abstract = {This study examines whether 4\textendash 8-Hz theta oscillations can be seen in the human hippocampus, and whether these oscillations increase during virtual movement and searching, as they do in rodents. Recordings from both hippocampal and neocortical depth electrodes were analyzed while six epileptic patients played a virtual taxi-driver game. During the game, the patients alternated between searching for passengers, whose locations were random, and delivering them to stores, whose locations remained constant. In both hippocampus and neocortex, theta increased during virtual movement in all phases of the game. Hippocampal and neocortical theta activity were also significantly correlated with each other, but this correlation did not differ between neocortex and hippocampus and within disparate neocortical electrodes. Our findings demonstrate the existence of movement-related theta oscillations in human hippocampus, and suggest that both cortical and hippocampal oscillations play a role in attention and sensorimotor integration. VC 2005 Wiley-Liss, Inc.},
  file = {2005 - Ekstrom et al. - Human hippocampal theta activity during virtual navigation.pdf},
  journal = {Hippocampus},
  language = {en},
  number = {7}
}

@article{ElBoustani2007,
  title = {Activated Cortical States: {{Experiments}}, Analyses and Models},
  shorttitle = {Activated Cortical States},
  author = {El Boustani, Sami and Pospischil, Martin and {Rudolph-Lilith}, Michelle and Destexhe, Alain},
  year = {2007},
  month = jan,
  volume = {101},
  pages = {99--109},
  issn = {09284257},
  doi = {10.1016/j.jphysparis.2007.10.001},
  abstract = {In awake animals, the cerebral cortex displays an ``activated'' state, with distinct characteristics compared to other states like slow-wave sleep or anesthesia. These characteristics include a sustained depolarized membrane potential (Vm) and irregular firing activity. In the present paper, we evaluate our understanding of cortical activated states from a computational neuroscience point of view. We start by reviewing the electrophysiological characteristics of activated cortical states based on recordings and analysis performed in awake cat association cortex. These analyses show that cortical activity is characterized by an apparent Poisson-distributed stochastic dynamics, both at the single-cell and population levels, and that single cells display a high-conductance state dominated by inhibition. We next overview computational models of the ``awake'' cortex, and perform the same analyses as in the experiments. Many properties identified experimentally are indeed reproduced by models, such as depolarized Vm, irregular firing with apparent Poisson statistics, and the determinant role of inhibitory fluctuations on spiking. However, other features are not well reproduced, such as firing statistics and the conductance state of the membrane, suggesting that the network state displayed by models is not entirely correct. We also show how networks can approach a correct conductance state, suggesting ways by which future models will generate activity fully consistent with experimental data.},
  file = {2007 - El Boustani et al. - Activated cortical states Experiments, analyses and models.pdf},
  journal = {Journal of Physiology-Paris},
  language = {en},
  number = {1-3}
}

@article{Eliasmith2012,
  title = {A {{Large}}-{{Scale Model}} of the {{Functioning Brain}}},
  author = {Eliasmith, C. and Stewart, T. C. and Choo, X. and Bekolay, T. and DeWolf, T. and Tang, Y. and Rasmussen, D.},
  year = {2012},
  month = nov,
  volume = {338},
  pages = {1202--1205},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1225266},
  file = {Eliasmith et al. - 2012 - A Large-Scale Model of the Functioning Brain.pdf},
  journal = {Science},
  language = {en},
  number = {6111}
}

@article{Eliasmith2014,
  title = {The Use and Abuse of Large-Scale Brain Models},
  author = {Eliasmith, Chris and Trujillo, Oliver},
  year = {2014},
  month = apr,
  volume = {25},
  pages = {1--6},
  issn = {09594388},
  doi = {10.1016/j.conb.2013.09.009},
  file = {Eliasmith and Trujillo - 2014 - The use and abuse of large-scale brain models.pdf},
  journal = {Current Opinion in Neurobiology},
  language = {en}
}

@article{Endres2003,
  title = {A New Metric for Probability Distributions},
  author = {Endres, D.M. and Schindelin, J.E.},
  year = {2003},
  month = jul,
  volume = {49},
  pages = {1858--1860},
  issn = {0018-9448},
  doi = {10.1109/TIT.2003.813506},
  abstract = {We introduce a metric for probability distributions, which is bounded, information-theoretically motivated, and has a natural Bayesian interpretation. The square root of the well-known distance is an asymptotic approximation to it. Moreover, it is a close relative of the capacitory discrimination and Jensen\textendash Shannon divergence.},
  file = {Endres and Schindelin - 2003 - A new metric for probability distributions.pdf},
  journal = {IEEE Trans. Inform. Theory},
  language = {en},
  number = {7}
}

@article{Enel2016,
  title = {Reservoir {{Computing Properties}} of {{Neural Dynamics}} in {{Prefrontal Cortex}}},
  author = {Enel, Pierre and Procyk, Emmanuel and Quilodran, Ren{\'e} and Dominey, Peter Ford},
  editor = {O'Reilly, Jill X},
  year = {2016},
  month = jun,
  volume = {12},
  pages = {e1004967},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1004967},
  file = {Enel et al. - 2016 - Reservoir Computing Properties of Neural Dynamics .pdf},
  journal = {PLOS Computational Biology},
  language = {en},
  number = {6}
}

@article{Engelken2015,
  title = {Comment on ``{{Two}} Types of Asynchronous Activity in Networks of Excitatory and Inhibitory Spiking Neurons''},
  author = {Engelken, Rainer and Farkhooi, Farzad and Hansel, David and {van Vreeswijk}, Carl and Wolf, Fred},
  year = {2015},
  month = apr,
  doi = {10.1101/017798},
  abstract = {Slow neural dynamics are believed to be important for behavior, learning and memory. Rate models operating in the chaotic regime show a rich dynamics at the scale of hundreds of milliseconds and provide remarkable learning capabilities. However, neurons in the brain communicate via spikes and it is a major challenge in computational neuroscience to obtain similar slow rate dynamics in networks of spiking neuron models. This question was addressed in a recent paper by Ostojic. The central claim of that paper is the existence of two states of asynchronous activity separated by a phase transition in spiking networks with fast synapses. We found that the analysis presented in the paper is factually incorrect and conceptually misleading. We provide compelling evidence that there is no such phase transition.},
  file = {2014 - Unknown - Comment on Two types of asynchronous activity in networks of excitatory and inhibitory spiking neurons.pdf;Engelken et al. - 2015 - Comment on “Two types of asynchronous activity in .pdf},
  journal = {bioRxiv},
  language = {en}
}

@article{Engle-Warnick2006,
  title = {Learning to Trust in Indefinitely Repeated Games},
  author = {{Engle-Warnick}, J. and Slonim, Robert L.},
  year = {2006},
  month = jan,
  volume = {54},
  pages = {95--114},
  issn = {08998256},
  doi = {10.1016/j.geb.2004.10.009},
  abstract = {Although it is well known that trust and trustworthiness (i.e., the fulfillment of trust) are important behaviors for the fulfillment of incomplete contracts, less is known about how the economic environment influences them. In this paper we design an experiment to examine how exogenously determined (stochastic) past relationship lengths affect trust and trustworthiness in new relationships. We find that shorter-lasting relationships have an immediate negative impact on both behaviors in the relationships that immediately follow, while longer-lasting relationships have the opposite effect. The effect of stochastic end-points declines for trustworthiness but not for trust as subjects gain experience, indicating that trust is able to rebound when longer-lasting relationships follow shorter-lasting ones.},
  file = {2006 - Engle-Warnick, Slonim - Learning to trust in indefinitely repeated games.pdf},
  journal = {Games and Economic Behavior},
  language = {en},
  number = {1}
}

@article{Ennis1988,
  title = {A Multidimensional Stochastic Theory of Similarity},
  author = {Ennis, Daniel M. and Palen, Joseph J. and Mullen, Kenneth},
  year = {1988},
  month = dec,
  volume = {32},
  pages = {449--465},
  issn = {00222496},
  doi = {10.1016/0022-2496(88)90023-5},
  file = {1988 - Ennis - Theory of Similarity.pdf},
  journal = {Journal of Mathematical Psychology},
  language = {en},
  number = {4}
}

@inproceedings{Epshteyn2008,
  title = {Active Reinforcement Learning},
  booktitle = {Proceedings of the 25th International Conference on {{Machine}} Learning - {{ICML}} '08},
  author = {Epshteyn, Arkady and Vogel, Adam and DeJong, Gerald},
  year = {2008},
  pages = {296--303},
  publisher = {{ACM Press}},
  address = {{Helsinki, Finland}},
  doi = {10.1145/1390156.1390194},
  abstract = {When the transition probabilities and rewards of a Markov Decision Process (MDP) are known, an agent can obtain the optimal policy without any interaction with the environment. However, exact transition probabilities are difficult for experts to specify. One option left to an agent is a long and potentially costly exploration of the environment. In this paper, we propose another alternative: given initial (possibly inaccurate) specification of the MDP, the agent determines the sensitivity of the optimal policy to changes in transitions and rewards. It then focuses its exploration on the regions of space to which the optimal policy is most sensitive. We show that the proposed exploration strategy performs well on several control and planning problems.},
  file = {Epshteyn et al. - 2008 - Active reinforcement learning.pdf},
  isbn = {978-1-60558-205-4},
  language = {en}
}

@article{Erev1998,
  title = {Predicting {{How People Play Games}}: {{Reinforcement Learning}} in {{Experimental Games}} with {{Unique}} , {{Mixed Strategy Equilibria}}},
  author = {Erev, Ido and Roth, Alvin E.},
  year = {1998},
  volume = {88},
  pages = {848--881},
  file = {1998 - Erev, Roth - Predicting how people play games Reinforcement learning in games with unique strategy equilibrium.pdf},
  journal = {The American Economic Review},
  language = {en},
  number = {4}
}

@article{Erev1998a,
  title = {Signal Detection by Human Observers: {{A}} Cutoff Reinforcement Learning Model of Categorization Decisions under Uncertainty.},
  shorttitle = {Signal Detection by Human Observers},
  author = {Erev, Ido},
  year = {1998},
  volume = {105},
  pages = {280--298},
  issn = {0033-295X},
  doi = {10.1037//0033-295X.105.2.280},
  file = {Erev - 1998 - Signal detection by human observers A cutoff rein.pdf},
  journal = {Psychological Review},
  language = {en},
  number = {2}
}

@article{Eriksson2019,
  title = {Slow and Fast Cortical Dynamics Distinguish Motor Planning and Execution},
  author = {Eriksson, David},
  year = {2019},
  volume = {857300v1},
  pages = {31},
  file = {Eriksson - Slow and fast cortical dynamics distinguish motor .pdf},
  journal = {bioRxiv},
  language = {en}
}

@book{Ermentrout2010,
  title = {Mathematical Foundations of Neuroscience},
  author = {Ermentrout, Bard and Terman, David H.},
  year = {2010},
  publisher = {{Springer}},
  address = {{New York}},
  file = {2010 - Ermentrout, Terman - Mathematical foundations of neuroscience.pdf},
  isbn = {978-0-387-87707-5},
  keywords = {Computational neuroscience,Mathematics,Neurosciences},
  language = {en},
  lccn = {QP357.5 .E76 2010},
  number = {v. 35},
  series = {Interdisciplinary Applied Mathematics}
}

@article{Ester2015,
  title = {Parietal and {{Frontal Cortex Encode Stimulus}}-{{Specific Mnemonic Representations}} during {{Visual Working Memory}}},
  author = {Ester, Edward F. and Sprague, Thomas C. and Serences, John T.},
  year = {2015},
  month = aug,
  volume = {87},
  pages = {893--905},
  issn = {08966273},
  doi = {10.1016/j.neuron.2015.07.013},
  abstract = {Working memory (WM) enables the storage and manipulation of information in an active state. WM storage has long been associated with sustained increases in activation across a network of frontal and parietal cortical regions. However, recent evidence suggests that these regions primarily encode information related to general task goals rather than featureselective representations of specific memoranda. These goal-related representations are thought to provide top-down feedback that coordinates the representation of fine-grained details in early sensory areas. Here, we test this model using fMRI-based reconstructions of remembered visual details from region-level activation patterns. We could reconstruct high-fidelity representations of a remembered orientation based on activation patterns in occipital visual cortex and in several sub-regions of frontal and parietal cortex, independent of sustained increases in mean activation. These results challenge models of WM that postulate disjoint frontoparietal ``top-down control'' and posterior sensory ``feature storage'' networks.},
  file = {2015 - Ester, Sprague, Serences - Parietal and Frontal Cortex Encode Stimulus- Specific Mnemonic Representations during Visual Working M.pdf;Ester et al. - 2015 - Parietal and Frontal Cortex Encode Stimulus-Specif.pdf},
  journal = {Neuron},
  language = {en},
  number = {4}
}

@article{Estes1994TowardAS,
  title = {Toward a Statistical Theory of Learning.},
  author = {Estes, W.},
  year = {1994},
  volume = {101},
  pages = {94--107},
  file = {Estes - 1994 - Toward a statistical theory of learning..pdf},
  journal = {Psychological Review}
}

@article{Etcheverry2020,
  title = {Hierarchically {{Organized Latent Modules}} for {{Exploratory Search}} in {{Morphogenetic Systems}}},
  author = {Etcheverry, Mayalen and {Moulin-Frier}, Clement and Oudeyer, Pierre-Yves},
  year = {2020},
  month = oct,
  abstract = {Self-organization of complex morphological patterns from local interactions is a fascinating phenomenon in many natural and artificial systems. In the artificial world, typical examples of such morphogenetic systems are cellular automata. Yet, their mechanisms are often very hard to grasp and so far scientific discoveries of novel patterns have primarily been relying on manual tuning and ad hoc exploratory search. The problem of automated diversity-driven discovery in these systems was recently introduced [26, 62], highlighting that two key ingredients are autonomous exploration and unsupervised representation learning to describe ``relevant'' degrees of variations in the patterns. In this paper, we motivate the need for what we call Meta-diversity search, arguing that there is not a unique ground truth interesting diversity as it strongly depends on the final observer and its motives. Using a continuous game-of-life system for experiments, we provide empirical evidences that relying on monolithic architectures for the behavioral embedding design tends to bias the final discoveries (both for hand-defined and unsupervisedly-learned features) which are unlikely to be aligned with the interest of a final end-user. To address these issues, we introduce a novel dynamic and modular architecture that enables unsupervised learning of a hierarchy of diverse representations. Combined with intrinsically motivated goal exploration algorithms, we show that this system forms a discovery assistant that can efficiently adapt its diversity search towards preferences of a user using only a very small amount of user feedback.},
  archiveprefix = {arXiv},
  eprint = {2007.01195},
  eprinttype = {arxiv},
  file = {Etcheverry et al. - 2020 - Hierarchically Organized Latent Modules for Explor.pdf},
  journal = {arXiv:2007.01195 [nlin, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Nonlinear Sciences - Cellular Automata and Lattice Gases,Statistics - Machine Learning},
  language = {en},
  primaryclass = {nlin, stat}
}

@article{Ethofer2009,
  title = {Decoding of {{Emotional Information}} in {{Voice}}-{{Sensitive Cortices}}},
  author = {Ethofer, Thomas and Van De Ville, Dimitri and Scherer, Klaus and Vuilleumier, Patrik},
  year = {2009},
  month = jun,
  volume = {19},
  pages = {1028--1033},
  issn = {09609822},
  doi = {10.1016/j.cub.2009.04.054},
  abstract = {The ability to correctly interpret emotional signals from others is crucial for successful social interaction. Previous neuroimaging studies showed that voice-sensitive auditory areas [1\textendash 3] activate to a broad spectrum of vocally expressed emotions more than to neutral speech melody (prosody). However, this enhanced response occurs irrespective of the specific emotion category, making it impossible to distinguish different vocal emotions with conventional analyses [4\textendash 8]. Here, we presented pseudowords spoken in five prosodic categories (anger, sadness, neutral, relief, joy) during event-related functional magnetic resonance imaging (fMRI), then employed multivariate pattern analysis [9, 10] to discriminate between these categories on the basis of the spatial response pattern within the auditory cortex. Our results demonstrate successful decoding of vocal emotions from fMRI responses in bilateral voicesensitive areas, which could not be obtained by using averaged response amplitudes only. Pairwise comparisons showed that each category could be classified against all other alternatives, indicating for each emotion a specific spatial signature that generalized across speakers. These results demonstrate for the first time that emotional information is represented by distinct spatial patterns that can be decoded from brain activity in modality-specific cortical areas.},
  file = {2009 - Ethofer et al. - Decoding of emotional information in voice-sensitive cortices.pdf},
  journal = {Current Biology},
  language = {en},
  number = {12}
}

@article{Evci2019,
  title = {Rigging the {{Lottery}}: {{Making All Tickets Winners}}},
  shorttitle = {Rigging the {{Lottery}}},
  author = {Evci, Utku and Gale, Trevor and Menick, Jacob and Castro, Pablo Samuel and Elsen, Erich},
  year = {2019},
  month = nov,
  abstract = {Sparse neural networks have been shown to be more parameter and compute efficient compared to dense networks and in some cases are used to decrease wall clock inference times. There is a large body of work on training dense networks to yield sparse networks for inference (Molchanov et al., 2017; Zhu \& Gupta, 2018; Narang et al., 2017; Li et al., 2016; Guo et al., 2016). This limits the size of the largest trainable sparse model to that of the largest trainable dense model. In this paper we introduce a method to train sparse neural networks with a fixed parameter count and a fixed computational cost throughout training, without sacrificing accuracy relative to existing dense-to-sparse training methods. Our method updates the topology of the network during training by using parameter magnitudes and infrequent gradient calculations. We show that this approach requires fewer floating-point operations (FLOPs) to achieve a given level of accuracy compared to prior techniques. Importantly,by adjusting the topology it can start from any initialization \textendash{} not just ``lucky'' ones. We demonstrate state-of-the-art sparse training results with ResNet-50, MobileNet v1 and MobileNet v2 on the ImageNet-2012 dataset, WideResNets on the CIFAR-10 dataset and RNNs on the WikiText-103 dataset. Finally, we provide some insights into why allowing the topology to change during the optimization can overcome local minima encountered when the topology remains static.},
  archiveprefix = {arXiv},
  eprint = {1911.11134},
  eprinttype = {arxiv},
  file = {Evci et al. - 2019 - Rigging the Lottery Making All Tickets Winners.pdf},
  journal = {arXiv:1911.11134 [cs, stat]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryclass = {cs, stat}
}

@incollection{Even-Dar2001,
  title = {Learning {{Rates}} for {{Q}}-{{Learning}}},
  booktitle = {Computational {{Learning Theory}}},
  author = {{Even-Dar}, Eyal and Mansour, Yishay},
  editor = {Goos, G. and Hartmanis, J. and {van Leeuwen}, J. and Helmbold, David and Williamson, Bob},
  year = {2001},
  volume = {2111},
  pages = {589--604},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/3-540-44581-1_39},
  abstract = {In this paper we derive convergence rates for Q-learning. We show an interesting relationship between the convergence rate and the learning rate used in Q-learning. For a polynomial learning rate, one which is 1/t{$\omega$} at time t where {$\omega$} {$\in$} (1/2, 1), we show that the convergence rate is polynomial in 1/(1 - {$\gamma$}), where {$\gamma$} is the discount factor. In contrast we show that for a linear learning rate, one which is 1/t at time t, the convergence rate has an exponential dependence on 1/(1 - {$\gamma$}). In addition we show a simple example that proves this exponential behavior is inherent for linear learning rates.},
  file = {Even-Dar and Mansour - 2001 - Learning Rates for Q-Learning.pdf},
  isbn = {978-3-540-42343-0 978-3-540-44581-4},
  language = {en}
}

@incollection{Even-Dar2002,
  title = {{{PAC Bounds}} for {{Multi}}-Armed {{Bandit}} and {{Markov Decision Processes}}},
  booktitle = {Computational {{Learning Theory}}},
  author = {{Even-Dar}, Eyal and Mannor, Shie and Mansour, Yishay},
  editor = {Goos, G. and Hartmanis, J. and {van Leeuwen}, J. and Kivinen, Jyrki and Sloan, Robert H.},
  year = {2002},
  volume = {2375},
  pages = {255--270},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/3-540-45435-7_18},
  file = {Even-Dar et al. - 2002 - PAC Bounds for Multi-armed Bandit and Markov Decis.pdf},
  isbn = {978-3-540-43836-6 978-3-540-45435-9},
  language = {en}
}

@incollection{Even-Dar2002a,
  title = {{{PAC Bounds}} for {{Multi}}-Armed {{Bandit}} and {{Markov Decision Processes}}},
  booktitle = {Computational {{Learning Theory}}},
  author = {{Even-Dar}, Eyal and Mannor, Shie and Mansour, Yishay},
  editor = {Goos, G. and Hartmanis, J. and {van Leeuwen}, J. and Kivinen, Jyrki and Sloan, Robert H.},
  year = {2002},
  volume = {2375},
  pages = {255--270},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/3-540-45435-7_18},
  file = {Even-Dar et al. - 2002 - PAC Bounds for Multi-armed Bandit and Markov Decis 2.pdf},
  isbn = {978-3-540-43836-6 978-3-540-45435-9},
  language = {en}
}

@article{Even-Dar2006,
  title = {Action {{Elimination}} and {{Stopping Conditions}} for the {{Multi}}-{{Armed Bandit}} and {{Reinforcement Learning Problems}}},
  author = {{Even-Dar}, Eyal and Mannor, Shie and Mansour, Yishay},
  year = {2006},
  volume = {7},
  pages = {1--27},
  abstract = {We incorporate statistical confidence intervals in both the multi-armed bandit and the reinforcement learning problems. In the bandit problem we show that given n arms, it suffices to pull the arms a total of O (n/{$\epsilon$}2) log(1/{$\delta$}) times to find an {$\epsilon$}-optimal arm with probability of at least 1 - {$\delta$}. This bound matches the lower bound of Mannor and Tsitsiklis (2004) up to constants. We also devise action elimination procedures in reinforcement learning algorithms. We describe a framework that is based on learning the confidence interval around the value function or the Q-function and eliminating actions that are not optimal (with high probability). We provide a model-based and a model-free variants of the elimination method. We further derive stopping conditions guaranteeing that the learned policy is approximately optimal with high probability. Simulations demonstrate a considerable speedup and added robustness over {$\epsilon$}-greedy Q-learning.},
  file = {Even-Dar et al. - Action Elimination and Stopping Conditions for the.pdf},
  journal = {Journal of Machine Learning Research},
  language = {en}
}

@article{Evensen2004,
  title = {Sampling Strategies and Square Root Analysis Schemes for the {{EnKF}}},
  author = {Evensen, Geir},
  year = {2004},
  month = dec,
  volume = {54},
  pages = {539--560},
  issn = {1616-7341, 1616-7228},
  doi = {10.1007/s10236-004-0099-2},
  abstract = {The purpose of this paper is to examine how different sampling strategies and implementations of the analysis scheme influence the quality of the results in the EnKF. It is shown that by selecting the initial ensemble, the model noise and the measurement perturbations wisely, it is possible to achieve a significant improvement in the EnKF results, using the same number of members in the ensemble. The results are also compared with a square root implementation of the EnKF analysis scheme where the analyzed ensemble is computed without the perturbation of measurements. It is shown that the measurement perturbations introduce sampling errors which can be reduced using improved sampling schemes in the standard EnKF or fully eliminated when the square root analysis algorithm is used. Further, a new computationally efficient square root algorithm is proposed which allows for the use of a low-rank representation of the measurement error covariance matrix. It is shown that this algorithm in fact solves the full problem at a low cost without introducing any new approximations.},
  file = {2004 - Evensen - Sampling strategies and square root analysis schemes for the EnKF.pdf},
  journal = {Ocean Dynamics},
  language = {en},
  number = {6}
}

@article{Fadlallah,
  title = {Weighted-{{Permutation Entropy}}: {{An Improved Complexity Measure}} for {{Time Series}}},
  author = {Fadlallah, Bilal and Pr{\i}ncipe, Jose and Chen, Badong and Keil, Andreas},
  pages = {8},
  file = {2013 - Fadlallah, Keil - Weighted-Permutation Entropy An Improved Complexity Measure for Time Series.pdf},
  language = {en}
}

@techreport{Faghihi2019,
  title = {Toward {{One}}-{{Shot Learning}} in {{Neuroscience}}-{{Inspired Deep Spiking Neural Networks}}},
  author = {Faghihi, Faramarz and Molhem, Hossein and Moustafa, Ahmed A.},
  year = {2019},
  month = nov,
  institution = {{Neuroscience}},
  doi = {10.1101/829556},
  abstract = {Conventional deep neural networks capture essential information processing stages in perception. Deep neural networks often require very large volume of training examples, whereas children can learn concepts such as hand-written digits with few examples. The goal of this project is to develop a deep spiking neural network that can learn from few training trials. Using known neuronal mechanisms, a spiking neural network model is developed and trained to recognize hand-written digits with presenting one to four training examples for each digit taken from the MNIST database. The model detects and learns geometric features of the images from MNIST database. In this work, a novel biological back-propagation based learning rule is developed and used to a train the network to detect basic features of different digits. For this purpose, randomly initialized synaptic weights between the layers are being updated. By using a neuroscience inspired mechanism named `synaptic pruning' and a predefined threshold, some of the synapses through the training are deleted. Hence, information channels are constructed that are highly specific for each digit as matrix of synaptic connections between two layers of spiking neural networks. These connection matrixes named `information channels' are used in the test phase to assign a digit class to each test image. As similar to humans' abilities to learn from small training trials, the developed spiking neural network needs a very small dataset for training, compared to conventional deep learning methods checked on MNIST dataset.},
  file = {Faghihi et al. - 2019 - Toward One-Shot Learning in Neuroscience-Inspired .pdf},
  language = {en},
  type = {Preprint}
}

@article{Faisal2008,
  title = {Noise in the Nervous System},
  author = {Faisal, A. Aldo and Selen, Luc P. J. and Wolpert, Daniel M.},
  year = {2008},
  month = apr,
  volume = {9},
  pages = {292--303},
  issn = {1471-003X, 1471-0048},
  doi = {10.1038/nrn2258},
  abstract = {Random disturbances of signals, termed `noise', pose a fundamental problem for information processing and affect all aspects of nervous-system function. However, the nature, amount and impact of noise in the nervous system have only recently been addressed in a quantitative manner. Experimental and computational methods have shown that multiple noise sources contribute to cellular and behavioural trial-to-trial variability. We review the sources of noise in the nervous system, from the molecular to the behavioural level, and show how noise contributes to trial-totrial variability. We highlight how noise affects neuronal networks and the principles the nervous system applies to counter detrimental effects of noise, and briefly discuss noise's potential benefits.},
  file = {2008 - Faisal, Selen, Wolpert - Noise in the nervous system.pdf;2008 - Faisal, Selen, Wolpert - Noise in the nervous system(2).pdf},
  journal = {Nature Reviews Neuroscience},
  language = {en},
  number = {4}
}

@article{Falke2010,
  title = {{{THE TWO}}-{{COMPONENT SIGNALING PATHWAY OF BACTERIAL}}},
  author = {Falke, Joseph J and Bass, Randal B and Butler, Scott L and Chervitz, Stephen A and Danielson, Mark A},
  year = {2010},
  pages = {55},
  abstract = {The chemosensory pathway of bacterial chemotaxis has become a paradigm for the two-component superfamily of receptor-regulated phosphorylation pathways. This simple pathway illustrates many of the fundamental principles and unanswered questions in the field of signaling biology. A molecular description of pathway function has progressed rapidly because it is accessible to diverse structural, biochemical, and genetic approaches. As a result, structures are emerging for most of the pathway elements, biochemical studies are elucidating the mechanisms of key signaling events, and genetic methods are revealing the intermolecular interactions that transmit information between components. Recent advances include (a) the first molecular picture of a conformational transmembrane signal in a cell surface receptor, (b) four new structures of kinase domains and adaptation enzymes, and (c) significant new insights into the mechanisms of receptor-mediated kinase regulation, receptor adaptation, and the phospho-activation of signaling proteins. Overall, the chemosensory pathway and the propulsion system it regulates provide an ideal system in which to probe molecular principles underlying complex cellular signaling and behavior.},
  file = {Falke et al. - 2010 - THE TWO-COMPONENT SIGNALING PATHWAY OF BACTERIAL.pdf},
  language = {en}
}

@article{Fan,
  title = {{{LIBLINEAR}}: {{A Library}} for {{Large Linear Classification}}},
  author = {Fan, Rong-En and Chang, Kai-Wei and Hsieh, Cho-Jui and Wang, Xiang-Rui and Lin, Chih-Jen},
  pages = {29},
  abstract = {LIBLINEAR is an open source library for large-scale linear classification. It supports logistic regression and linear support vector machines. We provide easy-to-use command-line tools and library calls for users and developers. Comprehensive documents are available for both beginners and advanced users. Experiments demonstrate that LIBLINEAR is very efficient on large sparse data sets.},
  file = {Fan et al. - LIBLINEAR A Library for Large Linear Classiﬁcatio.pdf},
  language = {en}
}

@techreport{Fan2019,
  title = {All-Optical Electrophysiology Reveals Excitation, Inhibition, and Neuromodulation in Cortical Layer 1},
  author = {Fan, Linlin Z. and Kheifets, Simon and B{\"o}hm, Urs L. and Piatkevich, Kiryl D. and Wu, Hao and Parot, Vicente and Xie, Michael E. and Boyden, Edward S. and Takesian, Anne E. and Cohen, Adam E.},
  year = {2019},
  month = apr,
  institution = {{Neuroscience}},
  doi = {10.1101/614172},
  abstract = {Abstract                        The stability of neural dynamics arises through a tight coupling of excitatory (E) and inhibitory (I) signals. Genetically encoded voltage indicators (GEVIs) can report both spikes and subthreshold dynamics             in vivo             , but voltage alone only reveals the combined effects of E and I synaptic inputs, not their separate contributions individually. Here we combine optical recording of membrane voltage with simultaneous optogenetic manipulation to probe E and I individually in barrel cortex Layer 1 (L1) neurons in awake mice. Our studies show that L1 neurons integrate thalamocortical excitation and lateral inhibition to produce precisely timed responses to whisker stimuli. Top-down neuromodulatory inputs drive additional excitation in L1. Together, these results suggest a model for computation in L1 consistent with its hypothesized role in attentional gating of the underlying cortex.                                   One Sentence Summary             All-optical electrophysiology revealed the function in awake mice of an inhibitory microcircuit in barrel cortex Layer 1.},
  file = {Fan et al. - 2019 - All-optical electrophysiology reveals excitation, .pdf},
  language = {en},
  type = {Preprint}
}

@article{Fang1991,
  title = {A Method to Effect Physiological Recruitment Order in Electrically Activated Muscle},
  author = {Fang, Z.-P. and Mortimer, J.T.},
  year = {Feb./1991},
  volume = {38},
  pages = {175--179},
  issn = {00189294},
  doi = {10.1109/10.76384},
  abstract = {A new stimulation method has been utilized to achieve physiological recruitment order of small-to-large motor units in electrically activated muscles. The use of quasitrapezoidal-shaped pulses and a tripolar cuff electrode made selective activation of small motor axons possible, thus recruiting slow-twitch, fatigue-resistant muscle units before fast-twitch, fatigable units in a heterogeneous muscle. Isometric contraction force from the medial gastrocnemius muscle was measured in five cats. The physiological recruitment order was evidenced by larger twitch widths at lower force levels and smaller twitch widths at higher force levels in the muscles tested. In addition, force modulation process was more gradual and fused contractions were obtained at lower stimulation frequencies when the new stimulation method was employed. Furthermore, muscles activated by the new method were more fatigue-resistant under repetitive activation at low force levels. This stimulation method is simpler to implement and has fewer adverse effects on the neuromuscular system than previous blocking methods. Therefore, it may have applications in future functional neuromuscular stimulation systems.},
  file = {1991 - Fang, Mortimer - A Method to Effect Physiological Recruitment Order in Electrically Activated Muscle.pdf},
  journal = {IEEE Transactions on Biomedical Engineering},
  language = {en},
  number = {2}
}

@article{Fardet2020,
  title = {Simple Models Including Energy and Spike Constraints Reproduce Complex Activity Patterns and Metabolic Disruptions},
  author = {Fardet, Tanguy and Levina, Anna},
  editor = {Kumar, Arvind},
  year = {2020},
  month = dec,
  volume = {16},
  pages = {e1008503},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1008503},
  abstract = {In this work, we introduce new phenomenological neuronal models (eLIF and mAdExp) that account for energy supply and demand in the cell as well as the inactivation of spike generation how these interact with subthreshold and spiking dynamics. Including these constraints, the new models reproduce a broad range of biologically-relevant behaviors that are identified to be crucial in many neurological disorders, but were not captured by commonly used phenomenological models. Because of their low dimensionality eLIF and mAdExp open the possibility of future large-scale simulations for more realistic studies of brain circuits involved in neuronal disorders. The new models enable both more accurate modeling and the possibility to study energy-associated disorders over the whole time-course of disease progression instead of only comparing the initially healthy status with the final diseased state. These models, therefore, provide new theoretical and computational methods to assess the opportunities of early diagnostics and the potential of energy-centered approaches to improve therapies.},
  file = {Fardet and Levina - 2020 - Simple models including energy and spike constrain.pdf},
  journal = {PLoS Comput Biol},
  language = {en},
  number = {12}
}

@article{Farhoodi2019,
  title = {On Functions Computed on Trees},
  author = {Farhoodi, Roozbeh and Filom, Khashayar and Jones, Ilenna Simone and Kording, Konrad Paul},
  year = {2019},
  month = apr,
  abstract = {Any function can be constructed using a hierarchy of simpler functions through compositions. Such a hierarchy can be characterized by a binary rooted tree. Each node of this tree is associated with a function which takes as inputs two numbers from its children and produces one output. Since thinking about functions in terms of computation graphs is getting popular we may want to know which functions can be implemented on a given tree. Here, we describe a set of necessary constraints in the form of a system of non-linear partial differential equations that must be satisfied. Moreover, we prove that these conditions are sufficient in both contexts of analytic and bit-value functions. In the latter case, we explicitly enumerate discrete functions and observe that there are relatively few. Our point of view allows us to compare different neural network architectures in regard to their function spaces. Our work connects the structure of computation graphs with the functions they can implement and has potential applications to neuroscience and computer science.},
  archiveprefix = {arXiv},
  eprint = {1904.02309},
  eprinttype = {arxiv},
  file = {Farhoodi et al. - 2019 - On functions computed on trees.pdf},
  journal = {arXiv:1904.02309 [cs, math, q-bio, stat]},
  keywords = {Computer Science - Machine Learning,Mathematics - Combinatorics,Quantitative Biology - Neurons and Cognition,Statistics - Machine Learning},
  language = {en},
  primaryclass = {cs, math, q-bio, stat}
}

@article{Farmer1990,
  title = {A Rosetta Stone for Connectionism},
  author = {Farmer, Doyne},
  year = {1990},
  volume = {42},
  pages = {153--187},
  file = {Farmer - A ROSETrA STONE FOR CONNECTIONISM.pdf},
  journal = {Physica D},
  language = {en},
  number = {42}
}

@inproceedings{Favaretto2017,
  title = {Bode Meets {{Kuramoto}}: {{Synchronized}} Clusters in Oscillatory Networks},
  shorttitle = {Bode Meets {{Kuramoto}}},
  booktitle = {2017 {{American Control Conference}} ({{ACC}})},
  author = {Favaretto, Chiara and Bassett, Danielle S. and Cenedese, Angelo and Pasqualetti, Fabio},
  year = {2017},
  month = may,
  pages = {2799--2804},
  publisher = {{IEEE}},
  address = {{Seattle, WA, USA}},
  doi = {10.23919/ACC.2017.7963375},
  abstract = {In this paper we study cluster synchronization in a network of Kuramoto oscillators, where groups of oscillators evolve cohesively and at different frequencies from the neighboring oscillators. Synchronization is critical in a variety of systems, where it enables complex functionalities and behaviors. Synchronization over networks depends on the oscillators' dynamics, the interaction topology, and coupling strengths, and the relationship between these different factors can be quite intricate. In this work we formally show that three network properties enable the emergence of cluster synchronization. Specifically, weak inter-cluster connections, strong intra-cluster connections, and sufficiently diverse natural frequencies among oscillators belonging to different groups. Our approach relies on system-theoretic tools, and is validated with numerical studies.},
  file = {Favaretto et al. - 2017 - Bode meets Kuramoto Synchronized clusters in osci.pdf},
  isbn = {978-1-5090-5992-8},
  language = {en}
}

@article{Feingold2015,
  title = {Bursts of Beta Oscillation Differentiate Postperformance Activity in the Striatum and Motor Cortex of Monkeys Performing Movement Tasks},
  author = {Feingold, Joseph and Gibson, Daniel J. and DePasquale, Brian and Graybiel, Ann M.},
  year = {2015},
  month = nov,
  volume = {112},
  pages = {13687--13692},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1517629112},
  file = {2015 - Feingold et al. - Bursts of beta oscillation differentiate postperformance activity in the striatum and motor cortex of monkey(2).pdf;Feingold et al. - 2015 - Bursts of beta oscillation differentiate postperfo 2.pdf;Feingold et al. - 2015 - Bursts of beta oscillation differentiate postperfo.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {44}
}

@article{Fellous1998,
  title = {Computational {{Models}} of {{Neuromodulation}}},
  author = {Fellous, Jean-Marc and Linster, Christiane},
  year = {1998},
  pages = {771--805},
  file = {1998 - Fellous, Linser - Computational Models of Neuromodulation.pdf},
  journal = {Neural Computation},
  language = {en},
  number = {10}
}

@article{Feltovich,
  title = {John {{Du}} Y {{Department}} of {{Economics University}} of {{Pittsburgh Pittsburgh}}, {{PA}} 15260, {{USA}} Jdu Y+@pitt.Edu},
  author = {Feltovich, Nick},
  pages = {50},
  abstract = {How do individuals achieve good outcomes" in one shot strategic situations? One much explored possibility is that they engage in some kind of preplay communication|cheap talk|in which they endeavor to convince one another of the actions they intend to play. However, there may be no incentive for such communication to be truthful, or even informative. Another, less explored, possibility is that individuals take account of their knowledge of the past behavior of others when deciding which actions to play. While these two possibilities have been considered separately, there has been little research comparing the importance of these two devices as aids in achieving good outcomes. We design and run an experiment with human subjects that allows for such a comparison. The e ects of cheap talk and observation of past actions are compared with each other, and with the standard control case where neither cheap talk nor observation is allowed. We consider three di erent 2  2 games and explain why cheap talk or observation is likely to be the more e ective device for achieving good outcomes in each game. The experimental evidence suggests that both devices|cheap talk and observation| make cooperation and successful coordination more likely and increase payo s relative to the control. The relative success of cheap talk versus observation in achieving such good outcomes depends on the game played, in accordance with our predictions. We also nd that the signals players send are informative in the sense that they are cor related with their eventual actions, and that receivers of signals take this fact into account by conditioning their actions on the signal they receive. The results of this experiment can be used to extend game theoretic models of how individuals make use of the di erent types of information available in strategic environments. As a rst step in this direction, we construct a learning model in which individuals can condition their behavior on cheap talk or observed past actions, and we show that this model provides a good quantitative as well as qualitative t to the experimental data.},
  file = {2002 - Duffy, Feltovich - Do actions speak louder than words An experimental comparison of observation and cheap talk.pdf},
  language = {en}
}

@article{Ferrario2016,
  title = {Homeostasis {{Meets Motivation}} in the {{Battle}} to {{Control Food Intake}}},
  author = {Ferrario, Carrie R. and Labou{\`e}be, Gwena{\"e}l and Liu, Shuai and Nieh, Edward H. and Routh, Vanessa H. and Xu, Shengjin and O'Connor, Eoin C.},
  year = {2016},
  month = nov,
  volume = {36},
  pages = {11469--11481},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.2338-16.2016},
  file = {Ferrario et al. - 2016 - Homeostasis Meets Motivation in the Battle to Cont.pdf},
  journal = {J. Neurosci.},
  language = {en},
  number = {45}
}

@article{Ferre1997,
  title = {Adenosine\textendash Dopamine Receptor\textendash Receptor Interactions as an Integrative Mechanism in the Basal Ganglia},
  author = {Ferr{\'e}, Sergi and Fuxe, Kjell and B. Fredholm, Bertil and Morelli, Micaela and Popoli, Patrizia},
  year = {1997},
  month = oct,
  volume = {20},
  pages = {482--487},
  issn = {01662236},
  doi = {10.1016/S0166-2236(97)01096-5},
  file = {1997 - Ferré et al. - Adenosine-dopamine receptor-receptor interactions as an integrative mechanism in the basal ganglia.pdf},
  journal = {Trends in Neurosciences},
  language = {en},
  number = {10}
}

@inproceedings{Fiechter1997,
  title = {{{PAC}} Adaptive Control of Linear Systems},
  booktitle = {Proceedings of the Tenth Annual Conference on {{Computational}} Learning Theory  - {{COLT}} '97},
  author = {Fiechter, Claude-Nicolas},
  year = {1997},
  pages = {72--80},
  publisher = {{ACM Press}},
  address = {{Nashville, Tennessee, United States}},
  doi = {10.1145/267460.267481},
  file = {Fiechter - 1997 - PAC adaptive control of linear systems.pdf},
  isbn = {978-0-89791-891-6},
  language = {en}
}

@article{Fields2015,
  title = {A New Mechanism of Nervous System Plasticity: Activity-Dependent Myelination},
  shorttitle = {A New Mechanism of Nervous System Plasticity},
  author = {Fields, R. Douglas},
  year = {2015},
  month = dec,
  volume = {16},
  pages = {756--767},
  issn = {1471-003X, 1471-0048},
  doi = {10.1038/nrn4023},
  abstract = {The synapse is the focus of experimental research and theory on the cellular mechanisms of nervous system plasticity and learning, but recent research is expanding the consideration of plasticity into new mechanisms beyond the synapse, notably including the possibility that conduction velocity could be modifiable through changes in myelin to optimize the timing of information transmission through neural circuits. This concept emerges from a confluence of brain imaging that reveals changes in white matter in the human brain during learning, together with cellular studies showing that the process of myelination can be influenced by action potential firing in axons. This Opinion article summarizes the new research on activity-dependent myelination, explores the possible implications of these studies and outlines the potential for new research.},
  file = {2015 - Fields - A new mechanism of nervous system plasticity activity-dependent myelination.pdf;Fields - 2015 - A new mechanism of nervous system plasticity acti.pdf},
  journal = {Nature Reviews Neuroscience},
  language = {en},
  number = {12}
}

@article{Fiete2006,
  title = {Gradient {{Learning}} in {{Spiking Neural Networks}} by {{Dynamic Perturbation}} of {{Conductances}}},
  author = {Fiete, Ila R. and Seung, H. Sebastian},
  year = {2006},
  month = jul,
  volume = {97},
  issn = {0031-9007, 1079-7114},
  doi = {10.1103/PhysRevLett.97.048104},
  file = {2006 - Fiete, Seung - Gradient learning in spiking neural networks by dynamic perturbation of conductances.pdf},
  journal = {Physical Review Letters},
  language = {en},
  number = {4}
}

@article{Filatrella2007,
  title = {Generalized Coupling in the {{Kuramoto}} Model},
  author = {Filatrella, G. and Pedersen, N. F. and Wiesenfeld, K.},
  year = {2007},
  month = jan,
  volume = {75},
  issn = {1539-3755, 1550-2376},
  doi = {10.1103/PhysRevE.75.017201},
  file = {2007 - Filatrella, Pedersen, Wiesenfeld - Generalized coupling in the Kuramoto model.pdf},
  journal = {Physical Review E},
  language = {en},
  number = {1}
}

@techreport{Filipowicz2020,
  title = {The Complexity of Model-Free and Model-Based Learning Strategies},
  author = {Filipowicz, Alexandre L. S. and Levine, Jonathan and Piasini, Eugenio and Tavoni, Gaia and Kable, Joseph W. and Gold, Joshua I.},
  year = {2020},
  month = jan,
  institution = {{Neuroscience}},
  doi = {10.1101/2019.12.28.879965},
  abstract = {A proposed continuum of learning strategies, from model-free to model-based, is thought to progress systematically in complexity and therefore flexibility. Here we distinguish different forms of complexity to show that, contrary to this idea, strategies at both ends of this continuum can be equally flexible. Using a canonical learning task, we first simulated behavior to show that computational complexity, a measure of implementation demands, is higher for a standard modelbased versus model-free algorithm, but information complexity, a measure of flexibility, is not. We then analyzed human behavior to show that information complexity, which unlike computational complexity can be estimated from behavior, tended to increase for strategies that were increasingly either model-free or model-based, resulting in similar accuracy, suboptimal use of information, and increased response times. Thus, model-free and model-based strategies can have similar overall flexibility and instead are better distinguished by the specific task features from which they learn.},
  file = {Filipowicz et al. - 2020 - The complexity of model-free and model-based learn.pdf},
  language = {en},
  type = {Preprint}
}

@techreport{Findling2018,
  title = {Computational Noise in Reward-Guided Learning Drives Behavioral Variability in Volatile Environments},
  author = {Findling, Charles and Skvortsova, Vasilisa and Dromnelle, Remi and Palminteri, Stefano and Wyart, Valentin},
  year = {2018},
  month = oct,
  institution = {{Neuroscience}},
  doi = {10.1101/439885},
  abstract = {When learning the value of actions in volatile environments, humans often make seemingly irrational decisions which fail to maximize expected value. We reasoned that these 'non-greedy' decisions, instead of reflecting information seeking during choice, may be caused by computational noise in the learning of action values. Here, using reinforcement learning (RL) models of behavior and multimodal neurophysiological data, we show that the majority of non-greedy decisions stems from this learning noise. The trial-to-trial variability of sequential learning steps and their impact on behavior could be predicted both by BOLD responses to obtained rewards in the dorsal anterior cingulate cortex (dACC) and by phasic pupillary dilation - suggestive of neuromodulatory fluctuations driven by the locus coeruleus-norepinephrine (LC-NE) system. Together, these findings indicate that most of behavioral variability, rather than reflecting human exploration, is due to the limited computational precision of reward-guided learning.},
  file = {Findling et al. - 2018 - Computational noise in reward-guided learning driv.pdf},
  language = {en},
  type = {Preprint}
}

@article{Finkbeiner1992,
  title = {Calcium Waves in Astrocytes-Filling in the Gaps},
  author = {Finkbeiner, Steven},
  year = {1992},
  month = jun,
  volume = {8},
  pages = {1101--1108},
  issn = {08966273},
  doi = {10.1016/0896-6273(92)90131-V},
  file = {Finkbeiner - 1992 - Calcium waves in astrocytes-filling in the gaps.pdf},
  journal = {Neuron},
  language = {en},
  number = {6}
}

@article{Fisher2018,
  title = {Lack of Group-to-Individual Generalizability Is a Threat to Human Subjects Research},
  author = {Fisher, Aaron J. and Medaglia, John D. and Jeronimus, Bertus F.},
  year = {2018},
  month = jul,
  volume = {115},
  pages = {E6106-E6115},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1711978115},
  abstract = {Only for ergodic processes will inferences based on group-level data generalize to individual experience or behavior. Because human social and psychological processes typically have an individually variable and time-varying nature, they are unlikely to be ergodic. In this paper, six studies with a repeated-measure design were used for symmetric comparisons of interindividual and intraindividual variation. Our results delineate the potential scope and impact of nonergodic data in human subjects research. Analyses across six samples (with 87\textendash 94 participants and an equal number of assessments per participant) showed some degree of agreement in central tendency estimates (mean) between groups and individuals across constructs and data collection paradigms. However, the variance around the expected value was two to four times larger within individuals than within groups. This suggests that literatures in social and medical sciences may overestimate the accuracy of aggregated statistical estimates. This observation could have serious consequences for how we understand the consistency between group and individual correlations, and the generalizability of conclusions between domains. Researchers should explicitly test for equivalence of processes at the individual and group level across the social and medical sciences.},
  file = {Fisher et al. - 2018 - Lack of group-to-individual generalizability is a .pdf},
  journal = {Proc Natl Acad Sci USA},
  language = {en},
  number = {27}
}

@article{Fister2019,
  title = {Novelty Search for Global Optimization},
  author = {Fister, Iztok and Iglesias, Andres and Galvez, Akemi and Del Ser, Javier and Osaba, Eneko and Fister, Iztok and Perc, Matja{\v z} and Slavinec, Mitja},
  year = {2019},
  month = apr,
  volume = {347},
  pages = {865--881},
  issn = {00963003},
  doi = {10.1016/j.amc.2018.11.052},
  file = {Fister et al. - 2019 - Novelty search for global optimization.pdf},
  journal = {Applied Mathematics and Computation},
  language = {en}
}

@article{Flavell2013,
  title = {Serotonin and the {{Neuropeptide PDF Initiate}} and {{Extend Opposing Behavioral States}} in {{C}}. Elegans},
  author = {Flavell, Steven W. and Pokala, Navin and Macosko, Evan Z. and Albrecht, Dirk R. and Larsch, Johannes and Bargmann, Cornelia I.},
  year = {2013},
  month = aug,
  volume = {154},
  pages = {1023--1035},
  issn = {00928674},
  doi = {10.1016/j.cell.2013.08.001},
  abstract = {Foraging animals have distinct exploration and exploitation behaviors that are organized into discrete behavioral states. Here, we characterize a neuromodulatory circuit that generates long-lasting roaming and dwelling states in Caenorhabditis elegans. We find that two opposing neuromodulators, serotonin and the neuropeptide pigment dispersing factor (PDF), each initiate and extend one behavioral state. Serotonin promotes dwelling states through the MOD-1 serotonin-gated chloride channel. The spontaneous activity of serotonergic neurons correlates with dwelling behavior, and optogenetic modulation of the critical MOD-1-expressing targets induces prolonged dwelling states. PDF promotes roaming states through a Gas-coupled PDF receptor; optogenetic activation of cAMP production in PDF receptor-expressing cells induces prolonged roaming states. The neurons that produce and respond to each neuromodulator form a distributed circuit orthogonal to the classical wiring diagram, with several essential neurons that express each molecule. The slow temporal dynamics of this neuromodulatory circuit supplement fast motor circuits to organize long-lasting behavioral states.},
  file = {Flavell et al. - 2013 - Serotonin and the Neuropeptide PDF Initiate and Ex.pdf},
  journal = {Cell},
  language = {en},
  number = {5}
}

@article{Fletcher2019,
  title = {Neocortical {{Topology Governs}} the {{Dendritic Integrative Capacity}} of {{Layer}} 5 {{Pyramidal Neurons}}},
  author = {Fletcher, Lee N. and Williams, Stephen R.},
  year = {2019},
  month = jan,
  volume = {101},
  pages = {76-90.e4},
  issn = {08966273},
  doi = {10.1016/j.neuron.2018.10.048},
  abstract = {The structure of the neocortex varies across the neocortical mantle to govern the physical size of principal neurons. What impact such anatomical variation has on the computational operations of principal neurons remains unknown. Here, we demonstrate within a functionally defined area that neocortical thickness governs the anatomical, electrophysiological, and computational properties of the principal output neurons of the neocortex. We find that neocortical thickness and the size of layer 5B pyramidal neurons changes as a gradient across the rostro-caudal axis of the rat primary visual cortex. Simultaneous somato-dendritic whole-cell recordings and compartmental modeling revealed that the electrical architecture of principal neurons was not preserved; rather, primary visual cortex site-dependent differences in intracellular resistivity accentuated a gradient of the electrical behavior of layer 5B pyramidal neurons to influence the emergence of active dendritic computations. Our findings therefore reveal an exquisite relationship between neocortical structure and neuronal computation.},
  file = {Fletcher and Williams - 2019 - Neocortical Topology Governs the Dendritic Integra.pdf},
  journal = {Neuron},
  language = {en},
  number = {1}
}

@article{Florence2019,
  title = {Self-{{Supervised Correspondence}} in {{Visuomotor Policy Learning}}},
  author = {Florence, Peter and Manuelli, Lucas and Tedrake, Russ},
  year = {2019},
  month = sep,
  abstract = {In this paper we explore using self-supervised correspondence for improving the generalization performance and sample efficiency of visuomotor policy learning. Prior work has primarily used approaches such as autoencoding, pose-based losses, and end-to-end policy optimization in order to train the visual portion of visuomotor policies. We instead propose an approach using self-supervised dense visual correspondence training, and show this enables visuomotor policy learning with surprisingly high generalization performance with modest amounts of data: using imitation learning, we demonstrate extensive hardware validation on challenging manipulation tasks with as few as 50 demonstrations. Our learned policies can generalize across classes of objects, react to deformable object configurations, and manipulate textureless symmetrical objects in a variety of backgrounds, all with closedloop, real-time vision-based policies. Simulated imitation learning experiments suggest that correspondence training offers sample complexity and generalization benefits compared to autoencoding and end-to-end training.},
  archiveprefix = {arXiv},
  eprint = {1909.06933},
  eprinttype = {arxiv},
  file = {Florence et al. - 2019 - Self-Supervised Correspondence in Visuomotor Polic.pdf},
  journal = {arXiv:1909.06933 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  language = {en},
  primaryclass = {cs}
}

@article{Focardi2009,
  title = {Adaptive {{L\'evy Walks}} in {{Foraging Fallow Deer}}},
  author = {Focardi, Stefano and Montanaro, Paolo and Pecchioli, Elena},
  editor = {Adler, Frederick R.},
  year = {2009},
  month = aug,
  volume = {4},
  pages = {e6587},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0006587},
  abstract = {Background: Le\textasciiacute vy flights are random walks, the step lengths of which come from probability distributions with heavy power-law tails, such that clusters of short steps are connected by rare long steps. Le\textasciiacute vy walks maximise search efficiency of mobile foragers. Recently, several studies raised some concerns about the reliability of the statistical analysis used in previous analyses. Further, it is unclear whether Le\textasciiacute vy walks represent adaptive strategies or emergent properties determined by the interaction between foragers and resource distribution. Thus two fundamental questions still need to be addressed: the presence of Le\textasciiacute vy walks in the wild and whether or not they represent a form of adaptive behaviour. Methodology/Principal Findings: We studied 235 paths of solitary and clustered (i.e. foraging in group) fallow deer (Dama dama), exploiting the same pasture. We used maximum likelihood estimation for discriminating between a power-tailed distribution and the exponential alternative and rank/frequency plots to discriminate between Le\textasciiacute vy walks and composite Brownian walks. We showed that solitary deer perform Le\textasciiacute vy searches, while clustered animals did not adopt that strategy. Conclusion/Significance: Our demonstration of the presence of Le\textasciiacute vy walks is, at our knowledge, the first available which adopts up-to-date statistical methodologies in a terrestrial mammal. Comparing solitary and clustered deer, we concluded that the Le\textasciiacute vy walks of solitary deer represent an adaptation maximising encounter rates with forage resources and not an epiphenomenon induced by a peculiar food distribution.},
  file = {Focardi et al. - 2009 - Adaptive Lévy Walks in Foraging Fallow Deer.pdf},
  journal = {PLoS ONE},
  language = {en},
  number = {8}
}

@article{Focardi2014,
  title = {The {{L\'evy}} Flight Foraging Hypothesis in a Pelagic Seabird},
  author = {Focardi, Stefano and Cecere, Jacopo G.},
  editor = {Hays, Graeme},
  year = {2014},
  month = mar,
  volume = {83},
  pages = {353--364},
  issn = {00218790},
  doi = {10.1111/1365-2656.12147},
  file = {Focardi and Cecere - 2014 - The Lévy flight foraging hypothesis in a pelagic s.pdf},
  journal = {J Anim Ecol},
  language = {en},
  number = {2}
}

@book{Fonseca2003,
  title = {Evolutionary Multi-Criterion Optimization: Second International Conference, {{EMO}} 2003, {{Faro}}, {{Portugal}}, {{April}} 8-11, 2003: Proceedings},
  shorttitle = {Evolutionary Multi-Criterion Optimization},
  editor = {da Fonseca, Carlos M.},
  year = {2003},
  publisher = {{Springer}},
  address = {{Berlin ; New York}},
  file = {Fonseca - 2003 - Evolutionary multi-criterion optimization second .pdf},
  isbn = {978-3-540-01869-8},
  keywords = {Congresses,Mathematical optimization,Multiple criteria decision making},
  language = {en},
  lccn = {T57.95 .E48 2003},
  number = {2632},
  series = {Lecture Notes in Computer Science}
}

@article{Forger2011,
  title = {Optimal {{Stimulus Shapes}} for {{Neuronal Excitation}}},
  author = {Forger, Daniel B. and Paydarfar, David and Clay, John R.},
  editor = {Morrison, Abigail},
  year = {2011},
  month = jul,
  volume = {7},
  pages = {e1002089},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1002089},
  abstract = {An important problem in neuronal computation is to discern how features of stimuli control the timing of action potentials. One aspect of this problem is to determine how an action potential, or spike, can be elicited with the least energy cost, e.g., a minimal amount of applied current. Here we show in the Hodgkin \& Huxley model of the action potential and in experiments on squid giant axons that: 1) spike generation in a neuron can be highly discriminatory for stimulus shape and 2) the optimal stimulus shape is dependent upon inputs to the neuron. We show how polarity and time course of postsynaptic currents determine which of these optimal stimulus shapes best excites the neuron. These results are obtained mathematically using the calculus of variations and experimentally using a stochastic search methodology. Our findings reveal a surprising complexity of computation at the single cell level that may be relevant for understanding optimization of signaling in neurons and neuronal networks.},
  file = {2011 - Forger, Paydarfar, Clay - Optimal Stimulus Shapes for Neuronal Excitation.pdf},
  journal = {PLoS Computational Biology},
  language = {en},
  number = {7}
}

@article{Forkman1991b,
  title = {Some {{Problems}} with {{Current Patch}}-{{Choice Theory}}: {{A Study}} on the {{Mongolian Gerbil}}},
  author = {Forkman, Bj{\"o}rn},
  year = {1991},
  volume = {117},
  pages = {243--254},
  file = {Forkman - 1991 - Some Problems with Current Patch-Choice Theory A  2.pdf},
  journal = {Behaviour},
  language = {en},
  number = {3/4}
}

@article{Forkman1996,
  title = {The {{Foraging Behaviour}} of {{Mongolian Gerbils}}: {{A Behavioural Need}} or a {{Need}} to {{Know}}?},
  author = {Forkman, Bj{\"o}rn},
  year = {1996},
  volume = {133},
  pages = {129--143},
  abstract = {In the present paper Mongolian gerbils (Meriones unguiculatus) were shown to prefer to forage from an unprofitable food source when it contained hidden food, but not when the food was clearly visible. Four experiments were performed, in each experiment the animal could forage from either a food source with easily accessible food or from a food source which required more work. In the first experiment the animal could choose between seeds with husks and those without, and in the second experiment between seeds glued to a stick and seeds in a bowl. In both these experiments the animals could see the food of both food sources. The animals chose to forage from the most profitable food source, i.e. the seeds without husks and the seeds in a bowl respectively. In the third experiment the animals could choose between eating seeds hidden under lids or seeds in a bowl, and finally in the fourth experiment they could forage for seeds on a camouflaging surface or on a surface where the seeds were clearly visible. In these last two experiments it was impossible to see the food in the unprofitable food sources without working for it. In these situations the animals choose to forage from the unprofitable food source, i.e. from underneath the lids and on the camouflaging surface. These results are in accordance with exploration being the driving force behind contrafreeloading (learned industrioussness). The results cannot be explained by classical optimal foraging theory.},
  file = {Forkman - 1996 - The Foraging Behaviour of Mongolian Gerbils A Beh.pdf},
  journal = {Behaviour},
  language = {en},
  number = {1/2}
}

@article{Forkman1996a,
  title = {The {{Foraging Behaviour}} of {{Mongolian Gerbils}}: {{A Behavioural Need}} or a {{Need}} to {{Know}}?},
  author = {Forkman, Bj{\"o}rn},
  year = {1996},
  volume = {133},
  pages = {129--143},
  abstract = {In the present paper Mongolian gerbils (Meriones unguiculatus) were shown to prefer to forage from an unprofitable food source when it contained hidden food, but not when the food was clearly visible. Four experiments were performed, in each experiment the animal could forage from either a food source with easily accessible food or from a food source which required more work. In the first experiment the animal could choose between seeds with husks and those without, and in the second experiment between seeds glued to a stick and seeds in a bowl. In both these experiments the animals could see the food of both food sources. The animals chose to forage from the most profitable food source, i.e. the seeds without husks and the seeds in a bowl respectively. In the third experiment the animals could choose between eating seeds hidden under lids or seeds in a bowl, and finally in the fourth experiment they could forage for seeds on a camouflaging surface or on a surface where the seeds were clearly visible. In these last two experiments it was impossible to see the food in the unprofitable food sources without working for it. In these situations the animals choose to forage from the unprofitable food source, i.e. from underneath the lids and on the camouflaging surface. These results are in accordance with exploration being the driving force behind contrafreeloading (learned industrioussness). The results cannot be explained by classical optimal foraging theory.},
  file = {Forkman - 1996 - The Foraging Behaviour of Mongolian Gerbils A Beh 2.pdf},
  journal = {Behaviour},
  language = {en},
  number = {1/2}
}

@article{Formisano2008,
  title = {Multivariate Analysis of {{fMRI}} Time Series: Classification and Regression of Brain Responses Using Machine Learning},
  shorttitle = {Multivariate Analysis of {{fMRI}} Time Series},
  author = {Formisano, Elia and De Martino, Federico and Valente, Giancarlo},
  year = {2008},
  month = sep,
  volume = {26},
  pages = {921--934},
  issn = {0730725X},
  doi = {10.1016/j.mri.2008.01.052},
  abstract = {Machine learning and pattern recognition techniques are being increasingly employed in functional magnetic resonance imaging (fMRI) data analysis. By taking into account the full spatial pattern of brain activity measured simultaneously at many locations, these methods allow detecting subtle, non-strictly localized effects that may remain invisible to the conventional analysis with univariate statistical methods. In typical fMRI applications, pattern recognition algorithms ``learn'' a functional relationship between brain response patterns and a perceptual, cognitive or behavioral state of a subject expressed in terms of a label, which may assume discrete (classification) or continuous (regression) values. This learned functional relationship is then used to predict the unseen labels from a new data set (``brain reading''). In this article, we describe the mathematical foundations of machine learning applications in fMRI. We focus on two methods, support vector machines and relevance vector machines, which are respectively suited for the classification and regression of fMRI patterns. Furthermore, by means of several examples and applications, we illustrate and discuss the methodological challenges of using machine learning algorithms in the context of fMRI data analysis.},
  file = {2008 - Formisano, De Martino, Valente - Multivariate analysis of fMRI time series classification and regression of brain responses using.pdf},
  journal = {Magnetic Resonance Imaging},
  language = {en},
  number = {7}
}

@article{Forster2000,
  title = {Key {{Concepts}} in {{Model Selection}}: {{Performance}} and {{Generalizability}}},
  shorttitle = {Key {{Concepts}} in {{Model Selection}}},
  author = {Forster, Malcolm R},
  year = {2000},
  month = mar,
  volume = {44},
  pages = {205--231},
  issn = {00222496},
  doi = {10.1006/jmps.1999.1284},
  file = {2000 - Forster - Key Concepts in Model Selection Performance and Generalizability.pdf},
  journal = {Journal of Mathematical Psychology},
  language = {en},
  number = {1}
}

@article{Foster,
  title = {Regret {{Testing}}: {{Learning}} to {{Play Nash Equilibrium Without Knowing You Have}} an {{Opponent}}},
  author = {Foster, Dean P and Young, H Peyton},
  pages = {29},
  abstract = {A learning rule is uncoupled if a player does not condition his strategy on the opponent's payoffs. It is radically uncoupled if a player does not condition his strategy on the opponent's actions or payoffs. We demonstrate a family of simple, radically uncoupled learning rules whose period-by-period behavior comes arbitrarily close to Nash equilibrium behavior in any finite two-person game.},
  file = {2006 - Foster, Young - Regret testing learning to play Nash equilibrium without knowing you have an opponent.pdf},
  language = {en}
}

@article{Foster2016,
  title = {The Topography of Alpha-Band Activity Tracks the Content of Spatial Working Memory},
  author = {Foster, Joshua J. and Sutterer, David W. and Serences, John T. and Vogel, Edward K. and Awh, Edward},
  year = {2016},
  month = jan,
  volume = {115},
  pages = {168--177},
  issn = {0022-3077, 1522-1598},
  doi = {10.1152/jn.00860.2015},
  file = {2015 - Foster et al. - The topography of alpha-band activity tracks the content of spatial working memory.pdf;Foster et al. - 2016 - The topography of alpha-band activity tracks the c 2.pdf;Foster et al. - 2016 - The topography of alpha-band activity tracks the c.pdf},
  journal = {Journal of Neurophysiology},
  language = {en},
  number = {1}
}

@article{Fourcaud2002,
  title = {Dynamics of the {{Firing Probability}} of {{Noisy Integrate}}-and-{{Fire Neurons}}},
  author = {Fourcaud, Nicolas and Brunel, Nicolas},
  year = {2002},
  month = sep,
  volume = {14},
  pages = {2057--2110},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/089976602320264015},
  file = {2002 - Fourcaud, Brunel - Dynamics of the firing probability of noisy integrate-and-fire neurons.pdf},
  journal = {Neural Computation},
  language = {en},
  number = {9}
}

@article{Foutz2010,
  title = {Evaluation of Novel Stimulus Waveforms for Deep Brain Stimulation},
  author = {Foutz, Thomas J and McIntyre, Cameron C},
  year = {2010},
  month = dec,
  volume = {7},
  pages = {066008},
  issn = {1741-2560, 1741-2552},
  doi = {10.1088/1741-2560/7/6/066008},
  abstract = {Deep brain stimulation (DBS) is an established therapy for the treatment of a wide range of neurological disorders. Historically, DBS and other neurostimulation technologies have relied on rectangular stimulation waveforms to impose their effects on the nervous system. Recent work has suggested that non-rectangular waveforms may have advantages over the traditional rectangular pulse. Therefore, we used detailed computer models to compare a range of charge-balanced biphasic waveforms with rectangular, exponential, triangular, Gaussian and sinusoidal stimulus pulse shapes. We explored the neural activation energy of these waveforms for both intracellular and extracellular current-controlled stimulation conditions. In the context of extracellular stimulation, we compared their effects on both axonal fibers of passage and projection neurons. Finally, we evaluated the impact of delivering the waveforms through a clinical DBS electrode, as opposed to a theoretical point source. Our results suggest that DBS with a 1 ms centered-triangular pulse can decrease energy consumption by 64\% when compared with the standard 100 {$\mu$}s rectangular pulse (energy cost of 48 and 133 nJ, respectively, to stimulate 50\% of a distributed population of axons) and can decrease energy consumption by 10\% when compared with the most energy efficient rectangular pulse (1.25 ms duration). In turn, there may be measureable energy savings when using appropriately designed non-rectangular pulses in clinical DBS applications, thereby warranting further experimental investigation.},
  file = {2010 - Foutz, McIntyre - Evaluation of novel stimulus waveforms for deep brain stimulation.pdf},
  journal = {Journal of Neural Engineering},
  language = {en},
  number = {6}
}

@article{Fowler,
  title = {Data {{Assimilation}} Tutorial on the {{Kalman}} Filter},
  author = {Fowler, A},
  pages = {14},
  file = {1963 - Fowler - Data Assimilation tutorial on the Kalman filter.pdf;Fowler - Data Assimilation tutorial on the Kalman ﬁlter.pdf},
  language = {en}
}

@article{Foxe2011,
  title = {The {{Role}} of {{Alpha}}-{{Band Brain Oscillations}} as a {{Sensory Suppression Mechanism}} during {{Selective Attention}}},
  author = {Foxe, John J. and Snyder, Adam C.},
  year = {2011},
  volume = {2},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2011.00154},
  abstract = {Evidence has amassed from both animal intracranial recordings and human electrophysiology that neural oscillatory mechanisms play a critical role in a number of cognitive functions such as learning, memory, feature binding and sensory gating. The wide availability of high-density electrical and magnetic recordings (64\textendash 256 channels) over the past two decades has allowed for renewed efforts in the characterization and localization of these rhythms. A variety of cognitive effects that are associated with specific brain oscillations have been reported, which range in spectral, temporal, and spatial characteristics depending on the context. Our laboratory has focused on investigating the role of alpha-band oscillatory activity (8\textendash 14 Hz) as a potential attentional suppression mechanism, and this particular oscillatory attention mechanism will be the focus of the current review. We discuss findings in the context of intersensory selective attention as well as intrasensory spatial and feature-based attention in the visual, auditory, and tactile domains. The weight of evidence suggests that alpha-band oscillations can be actively invoked within cortical regions across multiple sensory systems, particularly when these regions are involved in processing irrelevant or distracting information. That is, a central role for alpha seems to be as an attentional suppression mechanism when objects or features need to be specifically ignored or selected against.},
  file = {2011 - Foxe, Snyder - The role of alpha-band brain oscillations as a sensory suppression mechanism during selective attention.pdf},
  journal = {Frontiers in Psychology},
  language = {en}
}

@article{Francois-Lavet2015,
  title = {How to {{Discount Deep Reinforcement Learning}}: {{Towards New Dynamic Strategies}}},
  shorttitle = {How to {{Discount Deep Reinforcement Learning}}},
  author = {{Fran{\c c}ois-Lavet}, Vincent and Fonteneau, Raphael and Ernst, Damien},
  year = {2015},
  month = dec,
  abstract = {Using deep neural nets as function approximator for reinforcement learning tasks have recently been shown to be very powerful for solving problems approaching real-world complexity such as [1]. Using these results as a benchmark, we discuss the role that the discount factor may play in the quality of the learning process of a deep Q-network (DQN). When the discount factor progressively increases up to its final value, we empirically show that it is possible to significantly reduce the number of learning steps. When used in conjunction with a varying learning rate, we empirically show that it outperforms original DQN on several experiments. We relate this phenomenon with the instabilities of neural networks when they are used in an approximate Dynamic Programming setting. We also describe the possibility to fall within a local optimum during the learning process, thus connecting our discussion with the exploration/exploitation dilemma.},
  archiveprefix = {arXiv},
  eprint = {1512.02011},
  eprinttype = {arxiv},
  file = {2015 - François-Lavet, Fonteneau, Ernst - How to Discount Deep Reinforcement Learning Towards New Dynamic Strategies.pdf;François-Lavet et al. - 2015 - How to Discount Deep Reinforcement Learning Towar.pdf},
  journal = {arXiv:1512.02011 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  language = {en},
  primaryclass = {cs}
}

@article{Frank2006,
  title = {Hold Your Horses: {{A}} Dynamic Computational Role for the Subthalamic Nucleus in Decision Making},
  shorttitle = {Hold Your Horses},
  author = {Frank, Michael J.},
  year = {2006},
  month = oct,
  volume = {19},
  pages = {1120--1136},
  issn = {08936080},
  doi = {10.1016/j.neunet.2006.03.006},
  abstract = {The basal ganglia (BG) coordinate decision making processes by facilitating adaptive frontal motor commands while suppressing others. In previous work, neural network simulations accounted for response selection deficits associated with BG dopamine depletion in Parkinson's disease. Novel predictions from this model have been subsequently confirmed in Parkinson patients and in healthy participants under pharmacological challenge. Nevertheless, one clear limitation of that model is in its omission of the subthalamic nucleus (STN), a key BG structure that participates in both motor and cognitive processes. The present model incorporates the STN and shows that by modulating when a response is executed, the STN reduces premature responding and therefore has substantial effects on which response is ultimately selected, particularly when there are multiple competing responses. Increased cortical response conflict leads to dynamic adjustments in response thresholds via cortico-subthalamicpallidal pathways. The model accurately captures the dynamics of activity in various BG areas during response selection. Simulated dopamine depletion results in emergent oscillatory activity in BG structures, which has been linked with Parkinson's tremor. Finally, the model accounts for the beneficial effects of STN lesions on these oscillations, but suggests that this benefit may come at the expense of impaired decision making.},
  file = {2006 - Frank - Hold your horses A dynamic computational role for the subthalamic nucleus in decision making.pdf},
  journal = {Neural Networks},
  language = {en},
  number = {8}
}

@article{Frank2006a,
  title = {Mechanisms {{Underlying}} the {{Rapid Induction}} and {{Sustained Expression}} of {{Synaptic Homeostasis}}},
  author = {Frank, C. Andrew and Kennedy, Matthew J. and Goold, Carleton P. and Marek, Kurt W. and Davis, Graeme W.},
  year = {2006},
  month = nov,
  volume = {52},
  pages = {663--677},
  issn = {08966273},
  doi = {10.1016/j.neuron.2006.09.029},
  abstract = {Homeostatic signaling systems are thought to interface with the mechanisms of neural plasticity to achieve stable yet flexible neural circuitry. However, the time course, molecular design, and implementation of homeostatic signaling remain poorly defined. Here we demonstrate that a homeostatic increase in presynaptic neurotransmitter release can be induced within minutes following postsynaptic glutamate receptor blockade. The rapid induction of synaptic homeostasis is independent of new protein synthesis and does not require evoked neurotransmission, indicating that a change in the efficacy of spontaneous quantal release events is sufficient to trigger the induction of synaptic homeostasis. Finally, both the rapid induction and the sustained expression of synaptic homeostasis are blocked by mutations that disrupt the pore-forming subunit of the presynaptic CaV2.1 calcium channel encoded by cacophony. These data confirm the presynaptic expression of synaptic homeostasis and implicate presynaptic CaV2.1 in a homeostatic retrograde signaling system.},
  file = {2006 - Frank et al. - Mechanisms Underlying the Rapid Induction and Sustained Expression of Synaptic Homeostasis.pdf},
  journal = {Neuron},
  language = {en},
  number = {4}
}

@article{Frank2007,
  title = {Hold {{Your Horses}}: {{Impulsivity}}, {{Deep Brain Stimulation}}, and {{Medication}} in {{Parkinsonism}}},
  shorttitle = {Hold {{Your Horses}}},
  author = {Frank, M. J. and Samanta, J. and Moustafa, A. A. and Sherman, S. J.},
  year = {2007},
  month = nov,
  volume = {318},
  pages = {1309--1312},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1146157},
  file = {2007 - Frank et al. - Hold Your Horses Impulsivity, Deep Brain Stimulation, and Medication in Parkinsonism.pdf},
  journal = {Science},
  language = {en},
  number = {5854}
}

@article{Frank2012,
  title = {Mechanisms of {{Hierarchical Reinforcement Learning}} in {{Corticostriatal Circuits}} 1: {{Computational Analysis}}},
  shorttitle = {Mechanisms of {{Hierarchical Reinforcement Learning}} in {{Corticostriatal Circuits}} 1},
  author = {Frank, Michael J. and Badre, David},
  year = {2012},
  month = mar,
  volume = {22},
  pages = {509--526},
  issn = {1460-2199, 1047-3211},
  doi = {10.1093/cercor/bhr114},
  abstract = {Growing evidence suggests that the prefrontal cortex (PFC) is organized hierarchically, with more anterior regions having increasingly abstract representations. How does this organization support hierarchical cognitive control and the rapid discovery of abstract action rules? We present computational models at different levels of description. A neural circuit model simulates interacting corticostriatal circuits organized hierarchically. In each circuit the basal ganglia (BG) gate frontal actions, with some striatal units gating the inputs to PFC, and others gating the outputs to influence response selection. Learning at all of these levels is accomplished via dopaminergic reward prediction error signals in each corticostriatal circuit. This functionality allows the system to exhibit conditional if-then hypothesis testing and to learn rapidly in environments with hierarchical structure. We also develop a hybrid Bayesian-RL mixture of experts (MoE) model, which can estimate the most likely hypothesis state of individual participants based on their observed sequence of choices and rewards. This model yields accurate probabilistic estimates about which hypotheses are attended by manipulating attentional states in the generative neural model and recovering them with the MoE model. This two-pronged modeling approach leads to multiple quantitative predictions that are tested with fMRI in the companion paper.},
  file = {2011 - Frank, Badre - Mechanisms of hierarchical reinforcement learning in corticostriatal circuits 1 computational analysis.pdf},
  journal = {Cerebral Cortex},
  language = {en},
  number = {3}
}

@article{Frankle2018,
  title = {The {{Lottery Ticket Hypothesis}}: {{Finding Sparse}}, {{Trainable Neural Networks}}},
  shorttitle = {The {{Lottery Ticket Hypothesis}}},
  author = {Frankle, Jonathan and Carbin, Michael},
  year = {2018},
  month = mar,
  abstract = {Recent work on neural network pruning indicates that, at training time, neural networks need to be significantly larger in size than is necessary to represent the eventual functions that they learn. This paper articulates a new hypothesis to explain this phenomenon. This conjecture, which we term the lottery ticket hypothesis, proposes that successful training depends on lucky random initialization of a smaller subcomponent of the network. Larger networks have more of these ``lottery tickets,'' meaning they are more likely to luck out with a subcomponent initialized in a configuration amenable to successful optimization.},
  archiveprefix = {arXiv},
  eprint = {1803.03635},
  eprinttype = {arxiv},
  file = {Frankle and Carbin - 2018 - The Lottery Ticket Hypothesis Finding Sparse, Tra 2.pdf;Frankle and Carbin - 2018 - The Lottery Ticket Hypothesis Finding Sparse, Tra.pdf},
  journal = {arXiv:1803.03635 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  language = {en},
  primaryclass = {cs}
}

@article{Franks2010,
  title = {Ant Search Strategies after Interrupted Tandem Runs},
  author = {Franks, N. R. and Richardson, T. O. and Keir, S. and Inge, S. J. and Bartumeus, F. and {Sendova-Franks}, A. B.},
  year = {2010},
  month = may,
  volume = {213},
  pages = {1697--1708},
  issn = {0022-0949, 1477-9145},
  doi = {10.1242/jeb.031880},
  abstract = {Tandem runs are a form of recruitment in ants. During a tandem run, a single leader teaches one follower the route to important resources such as sources of food or better nest sites. In the present study, we investigate what tandem leaders and followers do, in the context of nest emigration, if their partner goes missing. Our experiments involved removing either leaders or followers at set points during tandem runs. Former leaders first stand still and wait for their missing follower but then most often proceed alone to the new nest site. By contrast, former followers often first engage in a Brownian search, for almost exactly the time that their former leader should have waited for them, and then former followers switch to a superdiffusive search. In this way, former followers first search their immediate neighbourhood for their lost leader before becoming ever more wide ranging so that in the absence of their former leader they can often find the new nest, re-encounter the old one or meet a new leader. We also show that followers gain useful information even from incomplete tandem runs. These observations point to the important principle that sophisticated communication behaviours may have evolved as anytime algorithms, i.e. procedures that are beneficial even if they do not run to completion.},
  file = {Franks et al. - 2010 - Ant search strategies after interrupted tandem run.pdf},
  journal = {Journal of Experimental Biology},
  language = {en},
  number = {10}
}

@article{Fransen2015,
  title = {Identifying Neuronal Oscillations Using Rhythmicity},
  author = {Fransen, Anne M.M. and {van Ede}, Freek and Maris, Eric},
  year = {2015},
  month = sep,
  volume = {118},
  pages = {256--267},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2015.06.003},
  abstract = {Neuronal oscillations are a characteristic feature of neuronal activity and are typically investigated through measures of power and coherence. However, neither of these measures directly reflects the distinctive feature of oscillations: their rhythmicity. Rhythmicity is the extent to which future phases can be predicted from the present one. Here, we present lagged coherence, a frequency-indexed measure that quantifies the rhythmicity of neuronal activity. We use this method to identify the sensorimotor alpha and beta rhythms in ongoing magnetoencephalographic (MEG) data, and to study their attentional modulation. Using lagged coherence, the sensorimotor rhythms become visible in ongoing activity as local rhythmicity peaks that are separated from the strong posterior activity in the same frequency bands. In contrast, using conventional power analyses, the sensorimotor rhythms cannot be identified in ongoing data, nor can they be separated from the posterior activity. We go on to show that the attentional modulation of these rhythms is also evident in lagged coherence and moreover, that in contrast to power, it can be visualised even without an experimental contrast. These findings suggest that the rhythmicity of neuronal activity is better suited to identify neuronal oscillations than the power in the same frequency band.},
  file = {2015 - Fransen, van Ede, Maris - Identifying neuronal oscillations using rhythmicity.pdf;Fransen et al. - 2015 - Identifying neuronal oscillations using rhythmicit.pdf},
  journal = {NeuroImage},
  language = {en}
}

@article{Freeman1972,
  title = {Linear {{Analysis}} of the {{Dynamics}} of {{Neural Masses}}},
  author = {Freeman, W J},
  year = {1972},
  month = jun,
  volume = {1},
  pages = {225--256},
  issn = {0084-6589, 0084-6589},
  doi = {10.1146/annurev.bb.01.060172.001301},
  file = {1972 - Freeman - Linear analysis of the dynamics of neural masses.pdf;Freeman - 1972 - Linear Analysis of the Dynamics of Neural Masses.pdf},
  journal = {Annual Review of Biophysics and Bioengineering},
  language = {en},
  number = {1}
}

@article{Freeman2015,
  title = {Mapping Nonlinear Receptive Field Structure in Primate Retina at Single Cone Resolution},
  author = {Freeman, Jeremy and Field, Greg D and Li, Peter H and Greschner, Martin and Gunning, Deborah E and Mathieson, Keith and Sher, Alexander and Litke, Alan M and Paninski, Liam and Simoncelli, Eero P and Chichilnisky, Ej},
  year = {2015},
  month = oct,
  volume = {4},
  issn = {2050-084X},
  doi = {10.7554/eLife.05241},
  file = {2015 - Freeman et al. - Mapping nonlinear receptive field structure in primate retina at single cone resolution.pdf;Freeman et al. - 2015 - Mapping nonlinear receptive field structure in pri.pdf},
  journal = {eLife},
  language = {en}
}

@article{Fregnac2017,
  title = {Big Data and the Industrialization of Neuroscience: {{A}} Safe Roadmap for Understanding the Brain?},
  shorttitle = {Big Data and the Industrialization of Neuroscience},
  author = {Fr{\'e}gnac, Yves},
  year = {2017},
  month = oct,
  volume = {358},
  pages = {470--477},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aan8866},
  file = {Frégnac - 2017 - Big data and the industrialization of neuroscience.pdf},
  journal = {Science},
  language = {en},
  number = {6362}
}

@article{Freund1999,
  title = {Adaptive {{Game Playing Using Multiplicative Weights}}},
  author = {Freund, Yoav and Schapire, Robert E.},
  year = {1999},
  month = oct,
  volume = {29},
  pages = {79--103},
  issn = {08998256},
  doi = {10.1006/game.1999.0738},
  abstract = {We present a simple algorithm for playing a repeated game. We show that a player using this algorithm suffers average loss that is guaranteed to come close to the minimum loss achievable by any fixed strategy. Our bounds are non-asymptotic and hold for any opponent. The algorithm, which uses the multiplicative-weight methods of Littlestone and Warmuth, is analyzed using the Kullback-Liebler divergence. This analysis yields a new, simple proof of the minmax theorem, as well as a provable method of approximately solving a game. A variant of our game-playing algorithm is proved to be optimal in a very strong sense.},
  file = {1999 - Freund, Schapire - Adaptive game playing using multiplicative weights.pdf},
  journal = {Games and Economic Behavior},
  language = {en},
  number = {1-2}
}

@article{Fries2001,
  title = {Modulation of {{Oscillatory Neuronal Synchronization}} by {{Selective Visual Attention}}},
  author = {Fries, P.},
  year = {2001},
  month = feb,
  volume = {291},
  pages = {1560--1563},
  issn = {00368075, 10959203},
  doi = {10.1126/science.1055465},
  file = {2001 - Fries - Modulation of Oscillatory Neuronal Synchronization by Selective Visual Attention.pdf},
  journal = {Science},
  language = {en},
  number = {5508}
}

@article{Fries2015,
  title = {Rhythms for {{Cognition}}: {{Communication}} through {{Coherence}}},
  shorttitle = {Rhythms for {{Cognition}}},
  author = {Fries, Pascal},
  year = {2015},
  month = oct,
  volume = {88},
  pages = {220--235},
  issn = {08966273},
  doi = {10.1016/j.neuron.2015.09.034},
  file = {2015 - Fries - Rhythms for Cognition Communication through Coherence.pdf;Fries - 2015 - Rhythms for Cognition Communication through Coher.pdf},
  journal = {Neuron},
  language = {en},
  number = {1}
}

@article{Fries2015a,
  title = {Rhythms for {{Cognition}}: {{Communication}} through {{Coherence}}},
  shorttitle = {Rhythms for {{Cognition}}},
  author = {Fries, Pascal},
  year = {2015},
  month = oct,
  volume = {88},
  pages = {220--235},
  issn = {08966273},
  doi = {10.1016/j.neuron.2015.09.034},
  abstract = {I propose that synchronization affects communication between neuronal groups. Gamma-band (30-90 Hz) synchronization modulates excitation rapidly enough so it escapes the following inhibition and activates postsynaptic neurons effectively. Synchronization also ensures that a presynaptic activation pattern arrives at postsynaptic neurons in a temporally coordinated manner. At a postsynaptic neuron, multiple presynaptic groups converge, e.g. representing different stimuli. If a stimulus is selected by attention, its neuronal representation shows stronger and higher-frequency gamma-band synchronization. Thereby, the attended stimulus representation selectively entrains postsynaptic neurons. The entrainment creates sequences of short excitation and longer inhibition that are coordinated between pre- and postsynaptic groups to transmit the attended representation and shut out competing inputs. The predominantly bottom-up directed gamma-band influences are controlled by predominantly top-down directed alpha-beta band (8-20 Hz) influences. Attention itself samples stimuli at a 7-8 Hz theta rhythm. Thus, several rhythms and their interplay render neuronal communication effective, precise and selective.},
  file = {Fries - 2015 - Rhythms for Cognition Communication through Coher 2.pdf},
  journal = {Neuron},
  language = {en},
  number = {1}
}

@article{Fries2015b,
  title = {Rhythms for {{Cognition}}: {{Communication}} through {{Coherence}}},
  shorttitle = {Rhythms for {{Cognition}}},
  author = {Fries, Pascal},
  year = {2015},
  month = oct,
  volume = {88},
  pages = {220--235},
  issn = {08966273},
  doi = {10.1016/j.neuron.2015.09.034},
  abstract = {I propose that synchronization affects communication between neuronal groups. Gamma-band (30-90 Hz) synchronization modulates excitation rapidly enough so it escapes the following inhibition and activates postsynaptic neurons effectively. Synchronization also ensures that a presynaptic activation pattern arrives at postsynaptic neurons in a temporally coordinated manner. At a postsynaptic neuron, multiple presynaptic groups converge, e.g. representing different stimuli. If a stimulus is selected by attention, its neuronal representation shows stronger and higher-frequency gamma-band synchronization. Thereby, the attended stimulus representation selectively entrains postsynaptic neurons. The entrainment creates sequences of short excitation and longer inhibition that are coordinated between pre- and postsynaptic groups to transmit the attended representation and shut out competing inputs. The predominantly bottom-up directed gamma-band influences are controlled by predominantly top-down directed alpha-beta band (8-20 Hz) influences. Attention itself samples stimuli at a 7-8 Hz theta rhythm. Thus, several rhythms and their interplay render neuronal communication effective, precise and selective.},
  file = {Fries - 2015 - Rhythms for Cognition Communication through Coher 3.pdf},
  journal = {Neuron},
  language = {en},
  number = {1}
}

@article{Friston2014,
  title = {The Anatomy of Choice: Dopamine and Decision-Making},
  shorttitle = {The Anatomy of Choice},
  author = {Friston, K. and Schwartenbeck, P. and FitzGerald, T. and Moutoussis, M. and Behrens, T. and Dolan, R. J.},
  year = {2014},
  month = sep,
  volume = {369},
  pages = {20130481--20130481},
  issn = {0962-8436, 1471-2970},
  doi = {10.1098/rstb.2013.0481},
  file = {Friston et al. - 2014 - The anatomy of choice dopamine and decision-makin.pdf},
  journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  language = {en},
  number = {1655}
}

@article{Fritzke,
  title = {A {{Growing Neural Gas Network Learns Topologies}}},
  author = {Fritzke, Bernd},
  pages = {8},
  abstract = {An incremental network model is introduced which is able to learn the important topological relations in a given set of input vectors by means of a simple Hebb-like learning rule. In contrast to previous approaches like the "neural gas" method of Martinetz and Schulten (1991, 1994), this model has no parameters which change over time and is able to continue learning, adding units and connections, until a performance criterion has been met. Applications of the model include vector quantization, clustering, and interpolation.},
  file = {Fritzke - A Growing Neural Gas Network Learns Topologies.pdf},
  language = {en}
}

@article{Fu2020,
  title = {{{D4RL}}: {{Datasets}} for {{Deep Data}}-{{Driven Reinforcement Learning}}},
  shorttitle = {{{D4RL}}},
  author = {Fu, Justin and Kumar, Aviral and Nachum, Ofir and Tucker, George and Levine, Sergey},
  year = {2020},
  month = apr,
  abstract = {The offline reinforcement learning (RL) problem, also referred to as batch RL, refers to the setting where a policy must be learned from a dataset of previously collected data, without additional online data collection. In supervised learning, large datasets and complex deep neural networks have fueled impressive progress, but in contrast, conventional RL algorithms must collect large amounts of onpolicy data and have had little success leveraging previously collected datasets. As a result, existing RL benchmarks are not well-suited for the offline setting, making progress in this area difficult to measure. To design a benchmark tailored to offline RL, we start by outlining key properties of datasets relevant to applications of offline RL. Based on these properties, we design a set of benchmark tasks and datasets that evaluate offline RL algorithms under these conditions. Examples of such properties include: datasets generated via hand-designed controllers and human demonstrators, multi-objective datasets, where an agent can perform different tasks in the same environment, and datasets consisting of a heterogeneous mix of high-quality and low-quality trajectories. By designing the benchmark tasks and datasets to reflect properties of real-world offline RL problems, our benchmark will focus research effort on methods that drive substantial improvements not just on simulated benchmarks, but ultimately on the kinds of real-world problems where offline RL will have the largest impact.},
  archiveprefix = {arXiv},
  eprint = {2004.07219},
  eprinttype = {arxiv},
  file = {Fu et al. - 2020 - D4RL Datasets for Deep Data-Driven Reinforcement .pdf},
  journal = {arXiv:2004.07219 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryclass = {cs, stat}
}

@article{Fukunaga2014,
  title = {Independent Control of Gamma and Theta Activity by Distinct Interneuron Networks in the Olfactory Bulb},
  author = {Fukunaga, Izumi and Herb, Jan T and Kollo, Mihaly and Boyden, Edward S and Schaefer, Andreas T},
  year = {2014},
  month = sep,
  volume = {17},
  pages = {1208--1216},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.3760},
  file = {Fukunaga et al. - 2014 - Independent control of gamma and theta activity by.pdf},
  journal = {Nature Neuroscience},
  language = {en},
  number = {9}
}

@article{Furlanello,
  title = {Born-{{Again Neural Networks}}},
  author = {Furlanello, Tommaso and Lipton, Zachary C and Tschannen, Michael and Itti, Laurent and Anandkumar, Anima},
  pages = {10},
  abstract = {Knowledge Distillation (KD) consists of transferring ``knowledge'' from one machine learning model (the teacher) to another (the student). Commonly, the teacher is a high-capacity model with formidable performance, while the student is more compact. By transferring knowledge, one hopes to benefit from the student's compactness, without sacrificing too much performance. We study KD from a new perspective: rather than compressing models, we train students parameterized identically to their teachers. Surprisingly, these Born-Again Networks (BANs), outperform their teachers significantly, both on computer vision and language modeling tasks. Our experiments with BANs based on DenseNets demonstrate state-of-the-art performance on the CIFAR-10 (3.5\%) and CIFAR-100 (15.5\%) datasets, by validation error. Additional experiments explore two distillation objectives: (i) Confidence-Weighted by Teacher Max (CWTM) and (ii) Dark Knowledge with Permuted Predictions (DKPP). Both methods elucidate the essential components of KD, demonstrating the effect of the teacher outputs on both predicted and nonpredicted classes.},
  file = {Furlanello et al. - Born-Again Neural Networks.pdf},
  language = {en}
}

@article{Fusi2016,
  title = {Why Neurons Mix: High Dimensionality for Higher Cognition},
  shorttitle = {Why Neurons Mix},
  author = {Fusi, Stefano and Miller, Earl K and Rigotti, Mattia},
  year = {2016},
  month = apr,
  volume = {37},
  pages = {66--74},
  issn = {09594388},
  doi = {10.1016/j.conb.2016.01.010},
  file = {Fusi et al. - 2016 - Why neurons mix high dimensionality for higher co.pdf},
  journal = {Current Opinion in Neurobiology},
  language = {en}
}

@article{Fustinana2021,
  title = {State-Dependent Encoding of Exploratory Behaviour in the Amygdala},
  author = {Fusti{\~n}ana, Maria Sol and Eichlisberger, Tobias and Bouwmeester, Tewis and Bitterman, Yael and L{\"u}thi, Andreas},
  year = {2021},
  month = apr,
  volume = {592},
  pages = {267--271},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/s41586-021-03301-z},
  file = {Fustiñana et al. - 2021 - State-dependent encoding of exploratory behaviour .pdf},
  journal = {Nature},
  language = {en},
  number = {7853}
}

@article{Fyodorov2016,
  title = {Nonlinear Analogue of the {{May}}-{{Wigner}} Instability Transition},
  author = {Fyodorov, Yan V. and Khoruzhenko, Boris A.},
  year = {2016},
  month = jun,
  volume = {113},
  pages = {6827--6832},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1601136113},
  file = {Fyodorov and Khoruzhenko - 2016 - Nonlinear analogue of the May−Wigner instability t.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {25}
}

@article{Gabaix,
  title = {Bounded {{Rationality}} and {{Directed Cognition}}},
  author = {Gabaix, Xavier and Laibson, David},
  pages = {46},
  abstract = {This paper proposes a psychological bounded rationality algorithm that uses partially myopic option value calculations to allocate scarce cognitive resources. The model can be operationalized even when decision problems require an arbitrarily large number of state variables. We evaluate the model using experimental data on a class of complex one-person games with full information. The model explains the experimental data better than the rational actor model with zero cognition costs.},
  file = {2005 - Gabaix, Laibson - Bounded Rationality and Directed Cognition.pdf},
  language = {en}
}

@article{Gabaix2000,
  title = {A {{Boundedly Rational Decision Algorithm}}},
  author = {Gabaix, Xavier and Laibson, David},
  year = {2000},
  volume = {90},
  pages = {433--438},
  file = {2000 - Gabaix, Laibson - A boundedly rational decision algorithm.pdf},
  journal = {The American Economic Review},
  language = {en},
  number = {2,}
}

@article{Gabaix2006,
  title = {Costly {{Information Acquisition}}: {{Experimental Analysis}} of a {{Boundedly Rational Model}}},
  author = {Gabaix, Xavier and Laibson, David and Moloche, Guillermo and Weinberg, Stephen},
  year = {2006},
  volume = {96},
  pages = {26},
  file = {2006 - Gabaix et al. - Costly Information Acquisition Experimental Analysis of a Boundedly Rational Model Costly Information Acquisition.pdf},
  journal = {THE AMERICAN ECONOMIC REVIEW},
  language = {en},
  number = {4}
}

@article{Gabalda-Sagarra2017,
  title = {Recurrence-{{Based Information Processing}} in {{Gene Regulatory Networks}}},
  author = {{Gabalda-Sagarra}, Marcal and Carey, Lucas and {Garcia-Ojalvo}, Jordi},
  year = {2017},
  month = sep,
  doi = {10.1101/010124},
  abstract = {Cellular information processing is generally attributed to the complex networks of genes and proteins that regulate cell behavior. It is still unclear, however, what are the main features of those networks that allow a cell to encode and interpret its ever changing environment. Here we address this question by studying the computational capabilities of the transcriptional regulatory networks of five evolutionary distant organisms. We identify in all cases a cyclic recurrent structure, formed by a small core of genes, that is essential for dynamical encoding and information integration. The recent history of the cell is encoded by the transient dynamics of this recurrent reservoir of nodes, while the rest of the network forms a readout layer devoted to decode and interpret the highdimensional dynamical state of the recurrent core. This separation of roles allows for the integration of temporal information, while facilitating the learning of new environmental conditions and preventing catastrophic interference between those new inputs and the previously stored information. This resembles the reservoir-computing paradigm recently proposed in computational neuroscience and machine learning. Our results reveal that gene regulatory networks act as echo-state networks that perform optimally in standard memory-demanding tasks, and confirms that most of their memory resides in the recurrent reservoir. We also show that the readout layer can learn to decode the information stored in the reservoir via standard evolutionary strategies. Our work thus suggests that recurrent dynamics is a key element for the processing of complex time-dependent information by cells.},
  file = {Gabalda-Sagarra et al. - 2017 - Recurrence-Based Information Processing in Gene Re.pdf},
  journal = {bioRxiv},
  language = {en}
}

@article{Ganguli2008,
  title = {Memory Traces in Dynamical Systems},
  author = {Ganguli, S. and Huh, D. and Sompolinsky, H.},
  year = {2008},
  month = dec,
  volume = {105},
  pages = {18970--18975},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.0804451105},
  file = {2008 - Ganguli, Huh, Sompolinsky - Memory traces in dynamical systems.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {48}
}

@article{Ganguli2010,
  title = {Short-Term Memory in Neuronal Networks through Dynamical Compressed Sensing},
  author = {Ganguli, Surya and Sompolinsky, Haim},
  year = {2010},
  pages = {9},
  abstract = {Recent proposals suggest that large, generic neuronal networks could store memory traces of past input sequences in their instantaneous state. Such a proposal raises important theoretical questions about the duration of these memory traces and their dependence on network size, connectivity and signal statistics. Prior work, in the case of gaussian input sequences and linear neuronal networks, shows that the duration of memory traces in a network cannot exceed the number of neurons (in units of the neuronal time constant), and that no network can out-perform an equivalent feedforward network. However a more ethologically relevant scenario is that of sparse input sequences. In this scenario, we show how linear neural networks can essentially perform compressed sensing (CS) of past inputs, thereby attaining a memory capacity that exceeds the number of neurons. This enhanced capacity is achieved by a class of ``orthogonal'' recurrent networks and not by feedforward networks or generic recurrent networks. We exploit techniques from the statistical physics of disordered systems to analytically compute the decay of memory traces in such networks as a function of network size, signal sparsity and integration time. Alternately, viewed purely from the perspective of CS, this work introduces a new ensemble of measurement matrices derived from dynamical systems, and provides a theoretical analysis of their asymptotic performance.},
  file = {2010 - Ganguli, Sompolinsky - Short-term memory in neuronal networks through dynamical compressed sensing.pdf},
  language = {en}
}

@article{Gao2003,
  title = {Dopamine {{Modulation}} of {{Perisomatic}} and {{Peridendritic Inhibition}} in {{Prefrontal Cortex}}},
  author = {Gao, Wen-Jun and Wang, Yun and {Goldman-Rakic}, Patricia S.},
  year = {2003},
  month = mar,
  volume = {23},
  pages = {1622--1630},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.23-05-01622.2003},
  file = {2003 - Gao, Wang, Goldman-Rakic - Dopamine modulation of perisomatic and peridendritic inhibition in prefrontal cortex.pdf},
  journal = {The Journal of Neuroscience},
  language = {en},
  number = {5}
}

@article{Gao2016,
  title = {Inferring {{Synaptic Excitation}}/{{Inhibition Balance}} from {{Field Potentials}}},
  author = {Gao, Richard D and Peterson, Erik J and Voytek, Bradley},
  year = {2016},
  month = oct,
  doi = {10.1101/081125},
  abstract = {Neural circuits sit in a dynamic balance between excitation (E) and inhibition (I). Fluctuations in this E:I balance have been shown to influence neural computation, working memory, and information processing. While more drastic shifts and aberrant E:I patterns are implicated in numerous neurological and psychiatric disorders, current methods for measuring E:I dynamics require invasive procedures that are difficult to perform in behaving animals, and nearly impossible in humans. This has limited the ability to examine the full impact that E:I shifts have in neural computation and disease. In this study, we develop a computational model to show that E:I ratio can be estimated from the power law exponent (slope) of the electrophysiological power spectrum, and validate this relationship using previously published datasets from two species (rat local field potential and macaque electrocorticography). This simple method--one that can be applied retrospectively to existing data--removes a major hurdle in understanding a currently difficult to measure, yet fundamental, aspect of neural computation.},
  file = {Gao et al. - 2016 - Inferring Synaptic ExcitationInhibition Balance f.pdf},
  journal = {bioRxiv},
  language = {en}
}

@techreport{Gao2016a,
  title = {Inferring {{Synaptic Excitation}}/{{Inhibition Balance}} from {{Field Potentials}}},
  author = {Gao, Richard D. and Peterson, Erik J. and Voytek, Bradley},
  year = {2016},
  month = oct,
  institution = {{Neuroscience}},
  doi = {10.1101/081125},
  abstract = {Neural circuits sit in a dynamic balance between excitation (E) and inhibition (I). Fluctuations in this E:I balance have been shown to influence neural computation, working memory, and information processing. While more drastic shifts and aberrant E:I patterns are implicated in numerous neurological and psychiatric disorders, current methods for measuring E:I dynamics require invasive procedures that are difficult to perform in behaving animals, and nearly impossible in humans. This has limited the ability to examine the full impact that E:I shifts have in neural computation and disease. In this study, we develop a computational model to show that E:I ratio can be estimated from the power law exponent (slope) of the electrophysiological power spectrum, and validate this relationship using previously published datasets from two species (rat local field potential and macaque electrocorticography). This simple method--one that can be applied retrospectively to existing data--removes a major hurdle in understanding a currently difficult to measure, yet fundamental, aspect of neural computation.},
  file = {Gao et al. - 2016 - Inferring Synaptic ExcitationInhibition Balance f 2.pdf},
  language = {en},
  type = {Preprint}
}

@techreport{Gao2017,
  title = {A Theory of Multineuronal Dimensionality, Dynamics and Measurement},
  author = {Gao, Peiran and Trautmann, Eric and Yu, Byron M. and Santhanam, Gopal and Ryu, Stephen and Shenoy, Krishna and Ganguli, Surya},
  year = {2017},
  month = nov,
  institution = {{Neuroscience}},
  doi = {10.1101/214262},
  abstract = {In many experiments, neuroscientists tightly control behavior, record many trials, and obtain trial-averaged firing rates from hundreds of neurons in circuits containing billions of behaviorally relevant neurons. Dimensionality reduction methods reveal a striking simplicity underlying such multi-neuronal data: they can be reduced to a low-dimensional space, and the resulting neural trajectories in this space yield a remarkably insightful dynamical portrait of circuit computation. This simplicity raises profound and timely conceptual questions. What are its origins and its implications for the complexity of neural dynamics? How would the situation change if we recorded more neurons? When, if at all, can we trust dynamical portraits obtained from measuring an infinitesimal fraction of task relevant neurons? We present a theory that answers these questions, and test it using physiological recordings from reaching monkeys. This theory reveals conceptual insights into how task complexity governs both neural dimensionality and accurate recovery of dynamic portraits, thereby providing quantitative guidelines for future large-scale experimental design.},
  file = {Gao et al. - 2017 - A theory of multineuronal dimensionality, dynamics.pdf},
  language = {en},
  type = {Preprint}
}

@article{Garcia-Ojalvo2004,
  title = {Modeling a Synthetic Multicellular Clock: {{Repressilators}} Coupled by Quorum Sensing},
  shorttitle = {Modeling a Synthetic Multicellular Clock},
  author = {{Garcia-Ojalvo}, J. and Elowitz, M. B. and Strogatz, S. H.},
  year = {2004},
  month = jul,
  volume = {101},
  pages = {10955--10960},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.0307095101},
  file = {2004 - Garcia-Ojalvo, Elowitz, Strogatz - Modeling a synthetic multicellular clock Repressilators coupled by quorum sensing.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {30}
}

@article{Geana2016,
  title = {Boredom, {{Information}}-{{Seeking}} and {{Exploration}}},
  author = {Geana, Andra and Daw, Nathaniel},
  year = {2016},
  pages = {6},
  abstract = {Any adaptive organism faces the choice between taking actions with known benefits (exploitation), and sampling new actions to check for other, more valuable opportunities available (exploration). The latter involves informationseeking, a drive so fundamental to learning and long-term reward that it can reasonably be considered, through evolution or development, to have acquired its own value, independent of immediate reward. Similarly, behaviors that fail to yield information may have come to be associated with aversive experiences such as boredom, demotivation, and task disengagement. In accord with these suppositions, we propose that boredom reflects an adaptive signal for managing the exploration-exploitation tradeoff, in the service of optimizing information acquisition and long-term reward. We tested participants in three experiments, manipulating the information content in their immediate task environment, and showed that increased perceptions of boredom arise in environments in which there is little useful information, and that higher boredom correlates with higher exploration. These findings are the first step toward a model formalizing the relationship between exploration, exploitation and boredom.},
  file = {Geana and Daw - Boredom, Information-Seeking and Exploration.pdf},
  journal = {CogSci},
  language = {en}
}

@article{Geirhos,
  title = {Comparing Deep Neural Networks against Humans: Object Recognition When the Signal Gets Weaker},
  author = {Geirhos, Robert and Janssen, David H J and Schutt, Heiko H and Rauber, Jonas and Bethge, Matthias and Wichmann, Felix A},
  pages = {31},
  abstract = {Human visual object recognition is typically rapid and seemingly effortless, as well as largely independent of viewpoint and object orientation. Until very recently, animate visual systems were the only ones capable of this remarkable computational feat. This has changed with the rise of a class of computer vision algorithms called deep neural networks (DNNs) that achieve human-level classification performance on object recognition tasks. Furthermore, a growing number of studies report similarities in the way DNNs and the human visual system process objects, suggesting that current DNNs may be good models of human visual object recognition. Yet there clearly exist important architectural and processing differences between stateof-the-art DNNs and the primate visual system. The potential behavioural consequences of these differences are not well understood. We aim to address this issue by comparing human and DNN generalisation abilities towards image degradations. We find the human visual system to be more robust to image manipulations like contrast reduction, additive noise or novel eidolon-distortions. In addition, we find progressively diverging classification error-patterns between man and DNNs when the signal gets weaker, indicating that there may still be marked differences in the way humans and current DNNs perform visual object recognition. We envision that our findings as well as our carefully measured and freely available behavioural datasets1 provide a new useful benchmark for the computer vision community to improve the robustness of DNNs and a motivation for neuroscientists to search for mechanisms in the brain that could facilitate this robustness.},
  file = {Geirhos et al. - Comparing deep neural networks against humans obj.pdf},
  language = {en}
}

@article{Germain2015,
  title = {{{MADE}}: {{Masked Autoencoder}} for {{Distribution Estimation}}},
  shorttitle = {{{MADE}}},
  author = {Germain, Mathieu and Gregor, Karol and Murray, Iain and Larochelle, Hugo},
  year = {2015},
  month = feb,
  abstract = {There has been a lot of recent interest in designing neural network models to estimate a distribution from a set of examples. We introduce a simple modification for autoencoder neural networks that yields powerful generative models. Our method masks the autoencoder's parameters to respect autoregressive constraints: each input is reconstructed only from previous inputs in a given ordering. Constrained this way, the autoencoder outputs can be interpreted as a set of conditional probabilities, and their product, the full joint probability. We can also train a single network that can decompose the joint probability in multiple different orderings. Our simple framework can be applied to multiple architectures, including deep ones. Vectorized implementations, such as on GPUs, are simple and fast. Experiments demonstrate that this approach is competitive with stateof-the-art tractable distribution estimators. At test time, the method is significantly faster and scales better than other autoregressive estimators.},
  archiveprefix = {arXiv},
  eprint = {1502.03509},
  eprinttype = {arxiv},
  file = {2015 - Germain et al. - MADE Masked Autoencoder for Distribution Estimation.pdf;Germain et al. - 2015 - MADE Masked Autoencoder for Distribution Estimati.pdf},
  journal = {arXiv:1502.03509 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  language = {en},
  primaryclass = {cs, stat}
}

@article{Gershman2018,
  title = {Deconstructing the Human Algorithms for Exploration},
  author = {Gershman, Samuel J.},
  year = {2018},
  month = apr,
  volume = {173},
  pages = {34--42},
  issn = {00100277},
  doi = {10.1016/j.cognition.2017.12.014},
  abstract = {The dilemma between information gathering (exploration) and reward seeking (exploitation) is a fundamental problem for reinforcement learning agents. How humans resolve this dilemma is still an open question, because experiments have provided equivocal evidence about the underlying algorithms used by humans. We show that two families of algorithms can be distinguished in terms of how uncertainty affects exploration. Algorithms based on uncertainty bonuses predict a change in response bias as a function of uncertainty, whereas algorithms based on sampling predict a change in response slope. Two experiments provide evidence for both bias and slope changes, and computational modeling confirms that a hybrid model is the best quantitative account of the data.},
  file = {Gershman - 2018 - Deconstructing the human algorithms for exploratio.pdf},
  journal = {Cognition},
  language = {en}
}

@article{Gershman2018b,
  title = {Deconstructing the Human Algorithms for Exploration},
  author = {Gershman, Samuel J.},
  year = {2018},
  month = apr,
  volume = {173},
  pages = {34--42},
  issn = {00100277},
  doi = {10.1016/j.cognition.2017.12.014},
  abstract = {The dilemma between information gathering (exploration) and reward seeking (exploitation) is a fundamental problem for reinforcement learning agents. How humans resolve this dilemma is still an open question, because experiments have provided equivocal evidence about the underlying algorithms used by humans. We show that two families of algorithms can be distinguished in terms of how uncertainty affects exploration. Algorithms based on uncertainty bonuses predict a change in response bias as a function of uncertainty, whereas algorithms based on sampling predict a change in response slope. Two experiments provide evidence for both bias and slope changes, and computational modeling confirms that a hybrid model is the best quantitative account of the data.},
  file = {Gershman - 2018 - Deconstructing the human algorithms for exploratio 3.pdf},
  journal = {Cognition},
  language = {en}
}

@article{Gershman2021,
  title = {Reconsidering the Evidence for Learning in Single Cells},
  author = {Gershman, Samuel J and Balbi, Petra EM and Gallistel, C Randy and Gunawardena, Jeremy},
  year = {2021},
  month = jan,
  volume = {10},
  pages = {e61907},
  issn = {2050-084X},
  doi = {10.7554/eLife.61907},
  abstract = {The question of whether single cells can learn led to much debate in the early 20th century. The view prevailed that they were capable of non-associative learning but not of associative learning, such as Pavlovian conditioning. Experiments indicating the contrary were considered either non-reproducible or subject to more acceptable interpretations. Recent developments suggest that the time is right to reconsider this consensus. We exhume the experiments of Beatrice Gelber on Pavlovian conditioning in the ciliate Paramecium aurelia, and suggest that criticisms of her findings can now be reinterpreted. Gelber was a remarkable scientist whose absence from the historical record testifies to the prevailing orthodoxy that single cells cannot learn. Her work, and more recent studies, suggest that such learning may be evolutionarily more widespread and fundamental to life than previously thought and we discuss the implications for different aspects of biology.},
  file = {Gershman et al. - 2021 - Reconsidering the evidence for learning in single .pdf},
  journal = {eLife},
  language = {en}
}

@article{Gershman2021a,
  title = {Reconsidering the Evidence for Learning in Single Cells},
  author = {Gershman, Samuel J and Balbi, Petra EM and Gallistel, C Randy and Gunawardena, Jeremy},
  year = {2021},
  month = jan,
  volume = {10},
  pages = {e61907},
  issn = {2050-084X},
  doi = {10.7554/eLife.61907},
  abstract = {The question of whether single cells can learn led to much debate in the early 20th century. The view prevailed that they were capable of non-associative learning but not of associative learning, such as Pavlovian conditioning. Experiments indicating the contrary were considered either non-reproducible or subject to more acceptable interpretations. Recent developments suggest that the time is right to reconsider this consensus. We exhume the experiments of Beatrice Gelber on Pavlovian conditioning in the ciliate Paramecium aurelia, and suggest that criticisms of her findings can now be reinterpreted. Gelber was a remarkable scientist whose absence from the historical record testifies to the prevailing orthodoxy that single cells cannot learn. Her work, and more recent studies, suggest that such learning may be evolutionarily more widespread and fundamental to life than previously thought and we discuss the implications for different aspects of biology.},
  file = {Gershman et al. - 2021 - Reconsidering the evidence for learning in single  2.pdf},
  journal = {eLife},
  language = {en}
}

@book{Gerstner2014,
  title = {Neuronal {{Dynamics}}: {{From Single Neurons}} to {{Networks}} and {{Models}} of {{Cognition}}},
  author = {Gerstner, Wulfram and Kistler, Werner and Naud, Richard and Paninski, Liam},
  year = {2014},
  volume = {1},
  publisher = {{Cambridge University Press}}
}

@article{Getty1991,
  title = {Random {{Prey Detection}} with {{Pause}}-{{Travel Search}}},
  author = {Getty, Thomas and Pulliam, H. R.},
  year = {1991},
  month = dec,
  volume = {138},
  pages = {1459--1477},
  issn = {0003-0147, 1537-5323},
  doi = {10.1086/285296},
  abstract = {We develop a mechanistic approach to the relationship between prey density and predator-prey encounter rate, incorporating perceptual constraints, search costs, and search tactics. We find that encounter rate should be density-dependent and the functional response sigmoid whenever the effects of (1) density on distance to prey and of (2) distance on instantaneous rate of detection do not cancel perfectly. We can predict how search tactics should change with prey density and detectability given the various constraints. If search movements are relatively cheap in both time and energy, then pause-travel search tactics can reduce the density dependence toward linearity, approximating a Type II functional response. If we ignore mechanistic details like search tactics that vary over several orders of magnitude, we can describe very accurately the encounter rates generated by this process with the simple traditional form aDb. We show how the critical exponent b relates to various constraints. The analysis leads to counterintuitive expectations about frequency dependence when prey types are intermixed.},
  file = {Getty and Pulliam - 1991 - Random Prey Detection with Pause-Travel Search.pdf},
  journal = {The American Naturalist},
  language = {en},
  number = {6}
}

@article{Ghanouni2015,
  title = {Transcranial {{MRI}}-{{Guided Focused Ultrasound}}: {{A Review}} of the {{Technologic}} and {{Neurologic Applications}}},
  shorttitle = {Transcranial {{MRI}}-{{Guided Focused Ultrasound}}},
  author = {Ghanouni, Pejman and Pauly, Kim Butts and Elias, W. Jeff and Henderson, Jaimie and Sheehan, Jason and Monteith, Stephen and Wintermark, Max},
  year = {2015},
  month = jul,
  volume = {205},
  pages = {150--159},
  issn = {0361-803X, 1546-3141},
  doi = {10.2214/AJR.14.13632},
  file = {Ghanouni et al. - 2015 - Transcranial MRI-Guided Focused Ultrasound A Revi.pdf},
  journal = {American Journal of Roentgenology},
  language = {en},
  number = {1}
}

@article{Ghuman2010,
  title = {Face {{Adaptation}} without a {{Face}}},
  author = {Ghuman, Avniel Singh and McDaniel, Jonathan R. and Martin, Alex},
  year = {2010},
  month = jan,
  volume = {20},
  pages = {32--36},
  issn = {09609822},
  doi = {10.1016/j.cub.2009.10.077},
  abstract = {Prolonged viewing of a stimulus results in a subsequent perceptual bias [1\textendash 3]. This perceptual adaptation and the resulting aftereffect reveal important characteristics regarding how perceptual systems are tuned [2, 4\textendash 6]. These aftereffects occur not only for simple stimulus features but also for high-level stimulus properties [7\textendash 10]. Here we report a novel cross-category adaptation aftereffect demonstrating that prolonged viewing of a human body without a face shifts the perceptual tuning curve for face gender and face identity. This contradicts a central assumption underlying perceptual adaptation: that adaptation depends on physical similarity between how the adapting and the adapted features are perceived [5]. Additionally, this aftereffect was not due to response bias, because its dependence on adaptation duration resembled traditional perceptual aftereffects. These body-to-face adaptation results demonstrate that bodies alone can alter the tuning properties of neurons that code for the gender and identity of faces. More generally, these results reveal that high-level perceptual adaptation can occur when the property or features being adapted are automatically inferred rather than perceived in the adapting stimulus.},
  file = {2010 - Ghuman, McDaniel, Martin - Face adaptation without a face.pdf},
  journal = {Current Biology},
  language = {en},
  number = {1}
}

@article{Giannicola2010,
  title = {The Effects of Levodopa and Ongoing Deep Brain Stimulation on Subthalamic Beta Oscillations in {{Parkinson}}'s Disease},
  author = {Giannicola, Gaia and Marceglia, Sara and Rossi, Lorenzo and {Mrakic-Sposta}, Simona and Rampini, Paolo and Tamma, Filippo and Cogiamanian, Filippo and Barbieri, Sergio and Priori, Alberto},
  year = {2010},
  month = nov,
  volume = {226},
  pages = {120--127},
  issn = {00144886},
  doi = {10.1016/j.expneurol.2010.08.011},
  abstract = {Local field potentials (LFPs) recorded through electrodes implanted in the subthalamic nucleus (STN) for deep brain stimulation (DBS) in patients with Parkinson's disease (PD) show that oscillations in the beta frequency range (8\textendash 20 Hz) decrease after levodopa intake. Whether and how DBS influences the beta oscillations and whether levodopa- and DBS-induced changes interact remains unclear. We examined the combined effect of levodopa and DBS on subthalamic beta LFP oscillations, recorded in nine patients with PD under four experimental conditions: without levodopa with DBS turned off; without levodopa with DBS turned on; with levodopa with DBS turned on; and with levodopa with DBS turned off. The analysis of STNLFP oscillations showed that whereas levodopa abolished beta STN oscillations in all the patients (p = 0.026), DBS significantly decreased the beta oscillation only in five of the nine patients studied (p = 0.043). Another difference was that whereas levodopa completely suppressed beta oscillations, DBS merely decreased them. When we combined levodopa and DBS, the levodopa-induced beta disruption prevailed and combining levodopa and DBS induced no significant additive effect (p = 0.500). Our observations suggest that levodopa and DBS both modulate LFP beta oscillations.},
  file = {2010 - Giannicola et al. - The effects of levodopa and ongoing deep brain stimulation on subthalamic beta oscillations in Parkinson's di.pdf},
  journal = {Experimental Neurology},
  language = {en},
  number = {1}
}

@article{Gidon2020,
  title = {Dendritic Action Potentials and Computation in Human Layer 2/3 Cortical Neurons},
  author = {Gidon, Albert and Zolnik, Timothy Adam and Fidzinski, Pawel and Bolduan, Felix and Papoutsi, Athanasia and Poirazi, Panayiota and Holtkamp, Martin and Vida, Imre and Larkum, Matthew Evan},
  year = {2020},
  month = jan,
  volume = {367},
  pages = {83--87},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aax6239},
  abstract = {The active electrical properties of dendrites shape neuronal input and output and are fundamental to brain function. However, our knowledge of active dendrites has been almost entirely acquired from studies of rodents. In this work, we investigated the dendrites of layer 2 and 3 (L2/3) pyramidal neurons of the human cerebral cortex ex vivo. In these neurons, we discovered a class of calcium-mediated dendritic action potentials (dCaAPs) whose waveform and effects on neuronal output have not been previously described. In contrast to typical all-or-none action potentials, dCaAPs were graded; their amplitudes were maximal for threshold-level stimuli but dampened for stronger stimuli. These dCaAPs enabled the dendrites of individual human neocortical pyramidal neurons to classify linearly nonseparable inputs\textemdash a computation conventionally thought to require multilayered networks.},
  file = {Gidon et al. - 2020 - Dendritic action potentials and computation in hum.pdf},
  journal = {Science},
  language = {en},
  number = {6473}
}

@article{Gigerenzer2008,
  title = {Why {{Heuristics Work}}},
  author = {Gigerenzer, Gerd},
  year = {2008},
  month = jan,
  volume = {3},
  pages = {20--29},
  issn = {1745-6916, 1745-6924},
  doi = {10.1111/j.1745-6916.2008.00058.x},
  abstract = {The adaptive toolbox is a Darwinian-inspired theory that conceives of the mind as a modular system that is composed of heuristics, their building blocks, and evolved capacities. The study of the adaptive toolbox is descriptive and analyzes the selection and structure of heuristics in social and physical environments. The study of ecological rationality is prescriptive and identifies the structure of environments in which specific heuristics either succeed or fail. Results have been used for designing heuristics and environments to improve professional decision making in the real world.},
  file = {2014 - Gigerenzer - Work Why Heuristics.pdf;Gigerenzer - 2008 - Why Heuristics Work.pdf},
  journal = {Perspectives on Psychological Science},
  language = {en},
  number = {1}
}

@article{Gilbert2011,
  title = {Decoding the {{Content}} of {{Delayed Intentions}}},
  author = {Gilbert, S. J.},
  year = {2011},
  month = feb,
  volume = {31},
  pages = {2888--2894},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.5336-10.2011},
  file = {2011 - Gilbert - Decoding the content of delayed intentions.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {8}
}

@article{Gillary2016,
  title = {The {{Edge}} of {{Stability}}: {{Response Times}} and {{Delta Oscillations}} in {{Balanced Networks}}},
  shorttitle = {The {{Edge}} of {{Stability}}},
  author = {Gillary, Grant and Niebur, Ernst},
  editor = {Battaglia, Francesco P.},
  year = {2016},
  month = sep,
  volume = {12},
  pages = {e1005121},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1005121},
  file = {Gillary and Niebur - 2016 - The Edge of Stability Response Times and Delta Os.pdf},
  journal = {PLOS Computational Biology},
  language = {en},
  number = {9}
}

@article{Gillies1998,
  title = {A Massively Connected Subthalamic Nucleus Leads to the Generation of Widespread Pulses},
  author = {Gillies, A. J. and Willshaw, D. J.},
  year = {1998},
  month = nov,
  volume = {265},
  pages = {2101--2109},
  issn = {1471-2954},
  doi = {10.1098/rspb.1998.0546},
  file = {1998 - Gillies, Willshaw - A massively connected subthalamic nucleus leads to the generation of widespread pulses.pdf},
  journal = {Proceedings of the Royal Society of London. Series B: Biological Sciences},
  language = {en},
  number = {1410}
}

@article{Gillies2004,
  title = {Models of the Subthalamic Nucleus},
  author = {Gillies, A. and Willshaw, D.},
  year = {2004},
  month = nov,
  volume = {26},
  pages = {723--732},
  issn = {13504533},
  doi = {10.1016/j.medengphy.2004.06.003},
  abstract = {A coherent set of models is presented that provide novel and testable predictions about the functional role of the subthalamic nucleus (STN) in the basal ganglia. The STN is emerging as an important target for novel therapeutic strategies for the alleviation of Parkinsonian type symptoms [Lancet 345 (1995) 91; Science 249 (1990) 1436]. Computational and mathematical models based on the properties of the STN and its interactions are reviewed. These models focus on core anatomical and physiological data that span many levels. By assessing models of anatomy, dynamic network models, and a detailed model of a recent pharmacological experiment, we can expose the primary modes of STN function and highlight their underlying properties. We show that the presence of functional interactions between STN projection neurons is critical in defining its behaviour and how it interacts with other basal ganglia nuclei. Pulses or switch-like activity patterns emerge in the models as a consequence of these local interactions. Furthermore, the models demonstrate that this behaviour can break down under abnormal conditions resulting in low frequency bursting oscillations. Such oscillations may play a role in symptoms of Parkinson's disease.},
  file = {2004 - Gillies, Willshaw - Models of the subthalamic nucleus The importance of intranuclear connectivity.pdf},
  journal = {Medical Engineering \& Physics},
  language = {en},
  number = {9}
}

@techreport{Gillon2021,
  title = {Learning from Unexpected Events in the Neocortical Microcircuit},
  author = {Gillon, Colleen J. and Pina, Jason E. and Lecoq, J{\'e}r{\^o}me A. and Ahmed, Ruweida and Billeh, Yazan and Caldejon, Shiella and Groblewski, Peter and Henley, Tim M. and Kato, India and Lee, Eric and Luviano, Jennifer and Mace, Kyla and Nayan, Chelsea and Nguyen, Thuyanh and North, Kat and Perkins, Jed and Seid, Sam and Valley, Matthew and Williford, Ali and Bengio, Yoshua and Lillicrap, Timothy P. and Richards, Blake A. and Zylberberg, Joel},
  year = {2021},
  month = jan,
  institution = {{Neuroscience}},
  doi = {10.1101/2021.01.15.426915},
  abstract = {Scientists have long conjectured that the neocortex learns the structure of the environment in a predictive, hierarchical manner. To do so, expected, predictable features are differentiated from unexpected ones by comparing bottom-up and top-down streams of data. It is theorized that the neocortex then changes the representation of incoming stimuli, guided by differences in the responses to expected and unexpected events. Such differences in cortical responses have been observed; however, it remains unknown whether these unexpected event signals govern subsequent changes in the brain's stimulus representations, and, thus, govern learning. Here, we show that unexpected event signals predict subsequent changes in responses to expected and unexpected stimuli in individual neurons and distal apical dendrites that are tracked over a period of days. These findings were obtained by observing layer 2/3 and layer 5 pyramidal neurons in primary visual cortex of awake, behaving mice using two-photon calcium imaging. We found that many neurons in both layers 2/3 and 5 showed large differences between their responses to expected and unexpected events. These unexpected event signals also determined how the responses evolved over subsequent days, in a manner that was different between the somata and distal apical dendrites. This difference between the somata and distal apical dendrites may be important for hierarchical computation, given that these two compartments tend to receive bottom-up and top-down information, respectively. Together, our results provide novel evidence that the neocortex indeed instantiates a predictive hierarchical model in which unexpected events drive learning.},
  file = {Gillon et al. - 2021 - Learning from unexpected events in the neocortical.pdf},
  language = {en},
  type = {Preprint}
}

@article{Gips2017,
  title = {Discovering Recurring Patterns in Electrophysiological Recordings},
  author = {Gips, Bart and Bahramisharif, Ali and Lowet, Eric and Roberts, Mark J. and {de Weerd}, Peter and Jensen, Ole and {van der Eerden}, Jan},
  year = {2017},
  month = jan,
  volume = {275},
  pages = {66--79},
  issn = {01650270},
  doi = {10.1016/j.jneumeth.2016.11.001},
  abstract = {Background: Fourier-based techniques are used abundantly in the analysis of electrophysiological data. However, these techniques are of limited value when the signal of interest is non-sinusoidal or nonperiodic. New method: We present sliding window matching (SWM): a new data-driven method for discovering recurring temporal patterns in electrophysiological data. SWM is effective in detecting recurring but unknown patterns even when they appear non-periodically. Results: To demonstrate this, we used SWM on oscillations in local field potential (LFP) recordings from the rat hippocampus and monkey V1. The application of SWM yielded two interesting findings. We could show that rat hippocampal theta and monkey V1 gamma oscillations were both skewed (i.e. asymmetric in time), rather than being sinusoidal. Furthermore, gamma oscillations in monkey V1 were skewed differently in the superficial compared to the deeper cortical layers. Second, we used SWM to analyze responses evoked by stimuli or microsaccades even when the onset timing of stimulus or microsaccades was unknown. Comparison with existing methods: We first validated the method on simulated datasets, and we checked that for recordings with a sufficiently low noise level the SWM results were consistent with results from the widely used phase alignment (PA) method. Conclusions: We conclude that the proposed method has wide applicability in the exploration of noisy time series data where the onset times of particular events are unknown by the experimenter such as in resting state and sleep recordings.},
  file = {Gips et al. - 2017 - Discovering recurring patterns in electrophysiolog.pdf},
  journal = {Journal of Neuroscience Methods},
  language = {en}
}

@article{Girard2008,
  title = {Where Neuroscience and Dynamic System Theory Meet Autonomous Robotics: {{A}} Contracting Basal Ganglia Model for Action Selection},
  shorttitle = {Where Neuroscience and Dynamic System Theory Meet Autonomous Robotics},
  author = {Girard, B. and Tabareau, N. and Pham, Q.C. and Berthoz, A. and Slotine, J.-J.},
  year = {2008},
  month = may,
  volume = {21},
  pages = {628--641},
  issn = {08936080},
  doi = {10.1016/j.neunet.2008.03.009},
  abstract = {Action selection, the problem of choosing what to do next, is central to any autonomous agent architecture. We use here a multi-disciplinary approach at the convergence of neuroscience, dynamical system theory and autonomous robotics, in order to propose an efficient action selection mechanism based on a new model of the basal ganglia. We first describe new developments of contraction theory regarding locally projected dynamical systems. We exploit these results to design a stable computational model of the cortico-baso-thalamo-cortical loops. Based on recent anatomical data, we include usually neglected neural projections, which participate in performing accurate selection. Finally, the efficiency of this model as an autonomous robot action selection mechanism is assessed in a standard survival task. The model exhibits valuable dithering avoidance and energy-saving properties, when compared with a simple if-then-else decision rule.},
  file = {2008 - Girard et al. - Where neuroscience and dynamic system theory meet autonomous robotics A contracting basal ganglia model for actio.pdf},
  journal = {Neural Networks},
  language = {en},
  number = {4}
}

@article{Giraud-Carrier,
  title = {Toward a {{Justification}} of {{Meta}}-Learning: {{Is}} the {{No Free Lunch Theorem}} a {{Show}}-Stopper?},
  author = {{Giraud-Carrier}, Christophe and Provost, Foster},
  pages = {9},
  abstract = {We present a preliminary analysis of the fundamental viability of meta-learning, revisiting the No Free Lunch (NFL) theorem. The analysis shows that given some simple and very basic assumptions, the NFL theorem is of little relevance to research in Machine Learning. We augment the basic NFL framework to illustrate that the notion of an Ultimate Learning Algorithm is well defined. We show that, although cross-validation still is not a viable way to construct general-purpose learning algorithms, meta-learning offers a natural alternative. We still have to pay for our lunch, but the cost is reasonable: the necessary fundamental assumptions are ones we all make anyway.},
  file = {Giraud-Carrier and Provost - Toward a Justiﬁcation of Meta-learning Is the No .pdf},
  language = {en}
}

@article{Giraud2012,
  title = {Cortical Oscillations and Speech Processing: Emerging Computational Principles and Operations},
  shorttitle = {Cortical Oscillations and Speech Processing},
  author = {Giraud, Anne-Lise and Poeppel, David},
  year = {2012},
  month = apr,
  volume = {15},
  pages = {511--517},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.3063},
  file = {2012 - Giraud, Poeppel - Cortical oscillations and speech processing emerging computational principles and operations.pdf},
  journal = {Nature Neuroscience},
  language = {en},
  number = {4}
}

@article{Girosi1990,
  title = {Networks and the Best Approximation Property},
  author = {Girosi, F. and Poggio, T.},
  year = {1990},
  month = jul,
  volume = {63},
  pages = {169--176},
  issn = {0340-1200, 1432-0770},
  doi = {10.1007/BF00195855},
  abstract = {Networks can be considered as approximation schemes. Multilayer networks of the perceptron type can approximate arbitrarily well continuous functions (Cybenko 1988, 1989; Funahashi 1989; Stinchcombe and White 1989). We prove that networks derived from regularization theory and including Radial Basis Functions (Poggio and Girosi 1989), have a similar property. From the point of view of approximation theory, however, the property of approximating continuous functions arbitrarily well is not sufficient for characterizing good approximation schemes. More critical is the property of best approximation. The main result of this paper is that multilayer perceptron networks, of the type used in backpropagation, do not have the best approximation property. For regularizadon networks (in particular Radial Basis Function networks) we prove existence and uniqueness of best approximation.},
  file = {Girosi and Poggio - 1990 - Networks and the best approximation property.pdf},
  journal = {Biol. Cybern.},
  language = {en},
  number = {3}
}

@article{Gittins1979,
  title = {Bandit {{Processes}} and {{Dynamic Allocation Indices}}},
  author = {Gittins, J. C.},
  year = {1979},
  volume = {41},
  pages = {148--177},
  abstract = {The paperaimsto givea unifiedaccountofthecentracl onceptsin recentworkon banditprocessesand dynamicallocationindices;to showhow thesereducesome previouslyintractablperoblemsto theproblemof calculatingsuchindices;and to describehow thesecalculationsmay be carriedout. Applicationsto stochastic schedulings,equentialclinicaltrialsand a class of searchproblemsare discussed.},
  file = {Gittins - 1979 - Bandit Processes and Dynamic Allocation Indices.pdf},
  journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
  language = {en},
  number = {2}
}

@article{Gittins1979a,
  title = {A Dynamic Allocation Index for the Discounted Multiarmed Bandit Problem},
  author = {Gittins, J. C. and Jones, D. M.},
  year = {1979},
  volume = {66},
  pages = {561--565},
  issn = {0006-3444, 1464-3510},
  doi = {10.1093/biomet/66.3.561},
  abstract = {Earlier workby the presentauthorshas establishedthe existenceofand a characterization of a priorityindex giving the Bayes rule forthe discounted multiarmedbandit problem. The calculation of this index is described and illustrated,and the results obtained briefly discussed.},
  file = {Gittins and Jones - 1979 - A dynamic allocation index for the discounted mult.pdf},
  journal = {Biometrika},
  language = {en},
  number = {3}
}

@article{Gittins1979b,
  title = {A Dynamic Allocation Index for the Discounted Multiarmed Bandit Problem},
  author = {Gittins, J. C. and Jones, D. M.},
  year = {1979},
  volume = {66},
  pages = {561--565},
  issn = {0006-3444, 1464-3510},
  doi = {10.1093/biomet/66.3.561},
  abstract = {Earlier workby the presentauthorshas establishedthe existenceofand a characterization of a priorityindex giving the Bayes rule forthe discounted multiarmedbandit problem. The calculation of this index is described and illustrated,and the results obtained briefly discussed.},
  file = {Gittins and Jones - 1979 - A dynamic allocation index for the discounted mult 2.pdf},
  journal = {Biometrika},
  language = {en},
  number = {3}
}

@article{Giusti2015,
  title = {Clique Topology Reveals Intrinsic Geometric Structure in Neural Correlations},
  author = {Giusti, Chad and Pastalkova, Eva and Curto, Carina and Itskov, Vladimir},
  year = {2015},
  month = nov,
  volume = {112},
  pages = {13455--13460},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1506407112},
  file = {Giusti et al. - 2015 - Clique topology reveals intrinsic geometric struct.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {44}
}

@article{Gjorgjieva2011,
  title = {A Triplet Spike-Timing-Dependent Plasticity Model Generalizes the {{Bienenstock}}-{{Cooper}}-{{Munro}} Rule to Higher-Order Spatiotemporal Correlations},
  author = {Gjorgjieva, J. and Clopath, C. and Audet, J. and Pfister, J.-P.},
  year = {2011},
  month = nov,
  volume = {108},
  pages = {19383--19388},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1105933108},
  file = {2011 - Gjorgjieva et al. - A triplet spike-timing-dependent plasticity model generalizes the Bienenstock-Cooper-Munro rule to higher-ord.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {48}
}

@article{Glasser2016,
  title = {The {{Human Connectome Project}}'s Neuroimaging Approach},
  author = {Glasser, Matthew F and Smith, Stephen M and Marcus, Daniel S and Andersson, Jesper L R and Auerbach, Edward J and Behrens, Timothy E J and Coalson, Timothy S and Harms, Michael P and Jenkinson, Mark and Moeller, Steen and Robinson, Emma C and Sotiropoulos, Stamatios N and Xu, Junqian and Yacoub, Essa and Ugurbil, Kamil and Van Essen, David C},
  year = {2016},
  month = sep,
  volume = {19},
  pages = {1175--1187},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.4361},
  file = {Glasser et al. - 2016 - The Human Connectome Project's neuroimaging approa.pdf},
  journal = {Nature Neuroscience},
  language = {en},
  number = {9}
}

@article{Gliske2015,
  title = {Narrowband Oscillations from Asynchronous Neural Activity},
  author = {Gliske, Stephen V. and Lim, Eugene and Holman, Katherine A. and Stacey, William C. and Fink, Christian G.},
  year = {2015},
  month = dec,
  abstract = {We investigate the possibility that narrowband oscillations may emerge from completely asynchronous, independent neural firing. We find that a population of asynchronous neurons may produce narrowband oscillations if each neuron fires quasi-periodically, and we deduce bounds on the degree of variability in neural spike-timing which will permit the emergence of such oscillations. These results suggest a novel mechanism of neural rhythmogenesis, and they help to explain recent experimental reports of large-amplitude local field potential oscillations in the absence of neural spike-timing synchrony. Simply put, although synchrony can produce oscillations, oscillations do not always imply the existence of synchrony.},
  archiveprefix = {arXiv},
  eprint = {1512.01622},
  eprinttype = {arxiv},
  file = {2015 - Gliske et al. - Narrowband oscillations from asynchronous neural activity.pdf;Gliske et al. - 2015 - Narrowband oscillations from asynchronous neural a.pdf},
  journal = {arXiv:1512.01622 [q-bio]},
  keywords = {Quantitative Biology - Neurons and Cognition},
  language = {en},
  primaryclass = {q-bio}
}

@article{Globerson2009,
  title = {The Minimum Information Principle and Its Application to Neural Code Analysis},
  author = {Globerson, A. and Stark, E. and Vaadia, E. and Tishby, N.},
  year = {2009},
  month = mar,
  volume = {106},
  pages = {3490--3495},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.0806782106},
  file = {2009 - Globerson et al. - The minimum information principle and its application to neural code analysis.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {9}
}

@article{Gluth2012,
  title = {Deciding {{When}} to {{Decide}}: {{Time}}-{{Variant Sequential Sampling Models Explain}} the {{Emergence}} of {{Value}}-{{Based Decisions}} in the {{Human Brain}}},
  shorttitle = {Deciding {{When}} to {{Decide}}},
  author = {Gluth, S. and Rieskamp, J. and Buchel, C.},
  year = {2012},
  month = aug,
  volume = {32},
  pages = {10686--10698},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.0727-12.2012},
  file = {2012 - Gluth, Rieskamp, Büchel - Deciding when to decide time-variant sequential sampling models explain the emergence of value-based d.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {31}
}

@article{Goense2016,
  title = {{{fMRI}} at {{High Spatial Resolution}}: {{Implications}} for {{BOLD}}-{{Models}}},
  shorttitle = {{{fMRI}} at {{High Spatial Resolution}}},
  author = {Goense, Jozien and Bohraus, Yvette and Logothetis, Nikos K.},
  year = {2016},
  month = jun,
  volume = {10},
  issn = {1662-5188},
  doi = {10.3389/fncom.2016.00066},
  file = {Goense et al. - 2016 - fMRI at High Spatial Resolution Implications for .pdf},
  journal = {Frontiers in Computational Neuroscience},
  language = {en}
}

@article{Goeree1999,
  title = {Stochastic Game Theory: {{For}} Playing Games, Not Just for Doing Theory},
  shorttitle = {Stochastic Game Theory},
  author = {Goeree, J. K. and Holt, C. A.},
  year = {1999},
  month = sep,
  volume = {96},
  pages = {10564--10567},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.96.19.10564},
  file = {1999 - Goeree, Holt - Stochastic game theory For playing games, not just for doing theory.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {19}
}

@article{Goeree2001,
  title = {Ten {{Little Treasures}} of {{Game Theory}} and {{Ten Intuitive Contradictions}}},
  author = {Goeree, Jacob K and Holt, Charles A},
  year = {2001},
  month = dec,
  volume = {91},
  pages = {1402--1422},
  issn = {0002-8282},
  doi = {10.1257/aer.91.5.1402},
  abstract = {This paper reports data for a series of two-person games that are played only once. These games span the standard categories: static and dynamic games with complete and incomplete information. For each game, the treasure is a treatment for which behavior conforms quite nicely to the predictions of the Nash equilibrium or relevant refinement. In each case we change a key payoff parameter in a manner that does not alter the equilibrium predictions, but this theoretically neutral payoff change has a major (often dramatic) effect on observed behavior. These contradictions are typically consistent with simple economic intuition and with some recent theoretical work that incorporates bounded rationality.},
  file = {2001 - Goeree, Holt - Ten little treasure of game theory and ten intuitive contradictions.pdf},
  journal = {American Economic Review},
  language = {en},
  number = {5}
}

@book{Goldman2011,
  title = {Social Epistemology: Essential Readings},
  shorttitle = {Social Epistemology},
  editor = {Goldman, Alvin I. and Whitcomb, Dennis},
  year = {2011},
  publisher = {{Oxford University Press}},
  address = {{Oxford ; New York}},
  annotation = {OCLC: ocn504279282},
  file = {Goldman and Whitcomb - 2011 - Social epistemology essential readings.pdf},
  isbn = {978-0-19-533461-6 978-0-19-533453-1},
  keywords = {Social epistemology},
  language = {en},
  lccn = {BD175 .S622 2011}
}

@article{Gollo2014,
  title = {Mechanisms of {{Zero}}-{{Lag Synchronization}} in {{Cortical Motifs}}},
  author = {Gollo, Leonardo L. and Mirasso, Claudio and Sporns, Olaf and Breakspear, Michael},
  editor = {Gutkin, Boris S.},
  year = {2014},
  month = apr,
  volume = {10},
  pages = {e1003548},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1003548},
  abstract = {Zero-lag synchronization between distant cortical areas has been observed in a diversity of experimental data sets and between many different regions of the brain. Several computational mechanisms have been proposed to account for such isochronous synchronization in the presence of long conduction delays: Of these, the phenomenon of ``dynamical relaying'' \textendash a mechanism that relies on a specific network motif \textendash{} has proven to be the most robust with respect to parameter mismatch and system noise. Surprisingly, despite a contrary belief in the community, the common driving motif is an unreliable means of establishing zero-lag synchrony. Although dynamical relaying has been validated in empirical and computational studies, the deeper dynamical mechanisms and comparison to dynamics on other motifs is lacking. By systematically comparing synchronization on a variety of small motifs, we establish that the presence of a single reciprocally connected pair \textendash{} a ``resonance pair'' \textendash{} plays a crucial role in disambiguating those motifs that foster zero-lag synchrony in the presence of conduction delays (such as dynamical relaying) from those that do not (such as the common driving triad). Remarkably, minor structural changes to the common driving motif that incorporate a reciprocal pair recover robust zerolag synchrony. The findings are observed in computational models of spiking neurons, populations of spiking neurons and neural mass models, and arise whether the oscillatory systems are periodic, chaotic, noise-free or driven by stochastic inputs. The influence of the resonance pair is also robust to parameter mismatch and asymmetrical time delays amongst the elements of the motif. We call this manner of facilitating zero-lag synchrony resonance-induced synchronization, outline the conditions for its occurrence, and propose that it may be a general mechanism to promote zero-lag synchrony in the brain.},
  file = {2014 - Gollo et al. - Mechanisms of Zero-Lag Synchronization in Cortical Motifs.pdf;Gollo et al. - 2014 - Mechanisms of Zero-Lag Synchronization in Cortical.pdf},
  journal = {PLoS Computational Biology},
  language = {en},
  number = {4}
}

@article{Golowasch1999,
  title = {Network {{Stability}} from {{Activity}}-{{Dependent Regulation}} of {{Neuronal Conductances}}},
  author = {Golowasch, Jorge and Casey, Michael and Abbott, L. F. and Marder, Eve},
  year = {1999},
  month = jul,
  volume = {11},
  pages = {1079--1096},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/089976699300016359},
  file = {1999 - Golowasch et al. - Network Stability from Activity-Dependent Regulation of Neuronal Conductances.pdf},
  journal = {Neural Computation},
  language = {en},
  number = {5}
}

@article{Gomez-Marin2019,
  title = {The {{Life}} of {{Behavior}}},
  author = {{Gomez-Marin}, Alex and Ghazanfar, Asif A.},
  year = {2019},
  month = oct,
  volume = {104},
  pages = {25--36},
  issn = {08966273},
  doi = {10.1016/j.neuron.2019.09.017},
  file = {Gomez-Marin and Ghazanfar - 2019 - The Life of Behavior.pdf},
  journal = {Neuron},
  language = {en},
  number = {1}
}

@article{Gomez-Marin2019a,
  title = {The {{Life}} of {{Behavior}}},
  author = {{Gomez-Marin}, Alex and Ghazanfar, Asif A.},
  year = {2019},
  month = oct,
  volume = {104},
  pages = {25--36},
  issn = {08966273},
  doi = {10.1016/j.neuron.2019.09.017},
  file = {Gomez-Marin and Ghazanfar - 2019 - The Life of Behavior 2.pdf},
  journal = {Neuron},
  language = {en},
  number = {1}
}

@article{Goncalves2012,
  title = {Complex Network Classification Using Partially Self-Avoiding Deterministic Walks},
  author = {Gon{\c c}alves, Wesley Nunes and Martinez, Alexandre Souto and Bruno, Odemir Martinez},
  year = {2012},
  month = sep,
  volume = {22},
  pages = {033139},
  issn = {1054-1500, 1089-7682},
  doi = {10.1063/1.4737515},
  file = {Gonçalves et al. - 2012 - Complex network classification using partially sel.pdf},
  journal = {Chaos},
  language = {en},
  number = {3}
}

@book{Goodfellow-et-al-2016,
  title = {Deep Learning},
  author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year = {2016},
  publisher = {{MIT Press}}
}

@article{Goodman2014,
  title = {Brian 2: Neural Simulations on a Variety of Computational Hardware},
  shorttitle = {Brian 2},
  author = {Goodman, Dan FM and Stimberg, Marcel and Yger, Pierre and Brette, Romain},
  year = {2014},
  volume = {15},
  pages = {P199},
  issn = {1471-2202},
  doi = {10.1186/1471-2202-15-S1-P199},
  file = {Goodman et al. - 2014 - Brian 2 neural simulations on a variety of comput.pdf},
  journal = {BMC Neuroscience},
  language = {en},
  number = {Suppl 1}
}

@article{Goos,
  title = {Lecture {{Notes}} in {{Computer Science}}},
  author = {Goos, Gerhard and Hartmanis, Juris and {van Leeuwen}, Jan and Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M and Mattern, Friedemann and Mitchell, John C and Naor, Moni and Nierstrasz, Oscar and Rangan, C Pandu and Steffen, Bernhard},
  pages = {1041},
  file = {Goos et al. - Lecture Notes in Computer Science.pdf},
  language = {en}
}

@book{Gopnik2007,
  title = {Causal {{Learning}}},
  author = {Gopnik, Alison and Schulz, Laura},
  year = {2007},
  month = apr,
  publisher = {{Oxford University Press}},
  doi = {10.1093/acprof:oso/9780195176803.001.0001},
  file = {Gopnik and Schulz - 2007 - Causal Learning.pdf},
  isbn = {978-0-19-517680-3},
  language = {en}
}

@article{Gopnik2012,
  title = {Reconstructing Constructivism: {{Causal}} Models, {{Bayesian}} Learning Mechanisms, and the Theory Theory.},
  shorttitle = {Reconstructing Constructivism},
  author = {Gopnik, Alison and Wellman, Henry M.},
  year = {2012},
  volume = {138},
  pages = {1085--1108},
  issn = {1939-1455, 0033-2909},
  doi = {10.1037/a0028044},
  abstract = {We propose a new version of the ``theory theory'' grounded in the computational framework of probabilistic causal models and Bayesian learning. Probabilistic models allow a constructivist but rigorous and detailed approach to cognitive development. They also explain the learning of both more specific causal hypotheses and more abstract framework theories. We outline the new theoretical ideas, explain the computational framework in an intuitive and non-technical way, and review an extensive but relatively recent body of empirical results that supports these ideas. These include new studies of the mechanisms of learning. Children infer causal structure from statistical information, through their own actions on the world and through observations of the actions of others. Studies demonstrate these learning mechanisms in children from 16 months to 4 years old and include research on causal statistical learning, informal experimentation through play, and imitation and informal pedagogy. They also include studies of the variability and progressive character of intuitive theory change, particularly theory of mind. These studies investigate both the physical and psychological and social domains. We conclude with suggestions for further collaborative projects between developmental and computational cognitive scientists.},
  file = {Gopnik and Wellman - 2012 - Reconstructing constructivism Causal models, Baye.pdf},
  journal = {Psychological Bulletin},
  language = {en},
  number = {6}
}

@article{Gopnik2015,
  title = {When {{Younger Learners Can Be Better}} (or at {{Least More Open}}-{{Minded}}) {{Than Older Ones}}},
  author = {Gopnik, Alison and Griffiths, Thomas L. and Lucas, Christopher G.},
  year = {2015},
  month = apr,
  volume = {24},
  pages = {87--92},
  issn = {0963-7214, 1467-8721},
  doi = {10.1177/0963721414556653},
  abstract = {We describe a surprising developmental pattern we found in studies involving three different kinds of problems and age ranges. Younger learners are better than older ones at learning unusual abstract causal principles from evidence. We explore two factors that might contribute to this counterintuitive result. The first is that as our knowledge grows, we become less open to new ideas. The second is that younger minds and brains are intrinsically more flexible and exploratory, although they are also less efficient as a result.},
  file = {Gopnik et al. - 2015 - When Younger Learners Can Be Better (or at Least M.pdf},
  journal = {Curr Dir Psychol Sci},
  language = {en},
  number = {2}
}

@article{Gopnik2017,
  title = {Changes in Cognitive Flexibility and Hypothesis Search across Human Life History from Childhood to Adolescence to Adulthood},
  author = {Gopnik, Alison and O'Grady, Shaun and Lucas, Christopher G. and Griffiths, Thomas L. and Wente, Adrienne and Bridgers, Sophie and Aboody, Rosie and Fung, Hoki and Dahl, Ronald E.},
  year = {2017},
  month = jul,
  volume = {114},
  pages = {7892--7899},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1700811114},
  abstract = {How was the evolution of our unique biological life history related to distinctive human developments in cognition and culture? We suggest that the extended human childhood and adolescence allows a balance between exploration and exploitation, between wider and narrower hypothesis search, and between innovation and imitation in cultural learning. In particular, different developmental periods may be associated with different learning strategies. This relation between biology and culture was probably coevolutionary and bidirectional: life-history changes allowed changes in learning, which in turn both allowed and rewarded extended life histories. In two studies, we test how easily people learn an unusual physical or social causal relation from a pattern of evidence. We track the development of this ability from early childhood through adolescence and adulthood. In the physical domain, preschoolers, counterintuitively, perform better than school-aged children, who in turn perform better than adolescents and adults. As they grow older learners are less flexible: they are less likely to adopt an initially unfamiliar hypothesis that is consistent with new evidence. Instead, learners prefer a familiar hypothesis that is less consistent with the evidence. In the social domain, both preschoolers and adolescents are actually the most flexible learners, adopting an unusual hypothesis more easily than either 6-y-olds or adults. There may be important developmental transitions in flexibility at the entry into middle childhood and in adolescence, which differ across domains.},
  file = {Gopnik et al. - 2017 - Changes in cognitive flexibility and hypothesis se.pdf},
  journal = {Proc Natl Acad Sci USA},
  language = {en},
  number = {30}
}

@article{Gopnik2020,
  title = {Childhood as a Solution to Explore\textendash Exploit Tensions},
  author = {Gopnik, Alison},
  year = {2020},
  month = jul,
  volume = {375},
  pages = {20190502},
  issn = {0962-8436, 1471-2970},
  doi = {10.1098/rstb.2019.0502},
  abstract = {I argue that the evolution of our life history, with its distinctively long, protected human childhood, allows an early period of broad hypothesis search and exploration, before the demands of goal-directed exploitation set in. This cognitive profile is also found in other animals and is associated with early behaviours such as neophilia and play. I relate this developmental pattern to computational ideas about explore\textendash exploit trade-offs, search and sampling, and to neuroscience findings. I also present several lines of empirical evidence suggesting that young human learners are highly exploratory, both in terms of their search for external information and their search through hypothesis spaces. In fact, they are sometimes more exploratory than older learners and adults.             This article is part of the theme issue `Life history and learning: how childhood, caregiving and old age shape cognition and culture in humans and other animals'.},
  file = {Gopnik - 2020 - Childhood as a solution to explore–exploit tension.pdf},
  journal = {Phil. Trans. R. Soc. B},
  language = {en},
  number = {1803}
}

@article{Gordus2015,
  title = {Feedback from {{Network States Generates Variability}} in a {{Probabilistic Olfactory Circuit}}},
  author = {Gordus, Andrew and Pokala, Navin and Levy, Sagi and Flavell, Steven W. and Bargmann, Cornelia I.},
  year = {2015},
  month = apr,
  volume = {161},
  pages = {215--227},
  issn = {00928674},
  doi = {10.1016/j.cell.2015.02.018},
  abstract = {Variability is a prominent feature of behavior and is an active element of certain behavioral strategies. To understand how neuronal circuits control variability, we examined the propagation of sensory information in a chemotaxis circuit of C. elegans where discrete sensory inputs can drive a probabilistic behavioral response. Olfactory neurons respond to odor stimuli with rapid and reliable changes in activity, but downstream AIB interneurons respond with a probabilistic delay. The interneuron response to odor depends on the collective activity of multiple neurons\textemdash AIB, RIM, and AVA\textemdash when the odor stimulus arrives. Certain activity states of the network correlate with reliable responses to odor stimuli. Artificially generating these activity states by modifying neuronal activity increases the reliability of odor responses in interneurons and the reliability of the behavioral response to odor. The integration of sensory information with network states may represent a general mechanism for generating variability in behavior.},
  file = {Gordus et al. - 2015 - Feedback from Network States Generates Variability.pdf},
  journal = {Cell},
  language = {en},
  number = {2}
}

@article{Gordus2015a,
  title = {Feedback from {{Network States Generates Variability}} in a {{Probabilistic Olfactory Circuit}}},
  author = {Gordus, Andrew and Pokala, Navin and Levy, Sagi and Flavell, Steven W. and Bargmann, Cornelia I.},
  year = {2015},
  month = apr,
  volume = {161},
  pages = {215--227},
  issn = {00928674},
  doi = {10.1016/j.cell.2015.02.018},
  abstract = {Variability is a prominent feature of behavior and is an active element of certain behavioral strategies. To understand how neuronal circuits control variability, we examined the propagation of sensory information in a chemotaxis circuit of C. elegans where discrete sensory inputs can drive a probabilistic behavioral response. Olfactory neurons respond to odor stimuli with rapid and reliable changes in activity, but downstream AIB interneurons respond with a probabilistic delay. The interneuron response to odor depends on the collective activity of multiple neurons\textemdash AIB, RIM, and AVA\textemdash when the odor stimulus arrives. Certain activity states of the network correlate with reliable responses to odor stimuli. Artificially generating these activity states by modifying neuronal activity increases the reliability of odor responses in interneurons and the reliability of the behavioral response to odor. The integration of sensory information with network states may represent a general mechanism for generating variability in behavior.},
  file = {Gordus et al. - 2015 - Feedback from Network States Generates Variability 2.pdf},
  journal = {Cell},
  language = {en},
  number = {2}
}

@article{Goris2014,
  title = {Partitioning Neuronal Variability},
  author = {Goris, Robbe L T and Movshon, J Anthony and Simoncelli, Eero P},
  year = {2014},
  month = jun,
  volume = {17},
  pages = {858--865},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.3711},
  file = {Goris et al. - 2014 - Partitioning neuronal variability.pdf},
  journal = {Nature Neuroscience},
  language = {en},
  number = {6}
}

@techreport{Gornet2017,
  title = {Simulating Extracted Connectomes},
  author = {Gornet, Jonathan and Scheffer, Louis K.},
  year = {2017},
  month = aug,
  institution = {{Neuroscience}},
  doi = {10.1101/177113},
  abstract = {Connectomes derived from volume EM imaging of the brain can generate detailed physical models of every neuron, and simulators such as NEURON or GENESIS are designed to work with such models. In principal, combining these technologies, plus transmitter and channel models, should allow detailed and accurate simulation of real neural circuits. Here we experiment with this combination, using a well-studied system (motion detection in             Drosophila             ). Since simulation requires both the physical geometry (which we have) and the models of the synapses (which are not currently available), we built approximate synapses corresponding to their known and estimated function. Once we did so, we reproduced direction selectivity in T4 cells, one of the main functions of this neural circuit. This verified the basic functionality of both extraction and simulations, and provided a biologically relevant computation we could use in further experiments. We then compared models with different degrees of physical realism, from full detailed models down to models consisting of a single node, to examine the tradeoff of simulation resources required versus accuracy achieved.                                   Our results show that much simpler models may be adequate, at least in the case of medulla neurons in             Drosophila             . Such models can be easily derived from fully detailed models, and result in simulations that are much smaller, much faster, and accurate enough for many purposes. Biologically, we show that a lumped neuron model reproduces the main motion detector operation, confirming the result of Gruntman[1], that dendritic compution is not required for this function.},
  file = {Gornet and Scheffer - 2017 - Simulating extracted connectomes.pdf},
  language = {en},
  type = {Preprint}
}

@article{Gottlieb2012,
  title = {Attention, {{Learning}}, and the {{Value}} of {{Information}}},
  author = {Gottlieb, Jacqueline},
  year = {2012},
  month = oct,
  volume = {76},
  pages = {281--295},
  issn = {08966273},
  doi = {10.1016/j.neuron.2012.09.034},
  file = {Gottlieb - 2012 - Attention, Learning, and the Value of Information.pdf},
  journal = {Neuron},
  language = {en},
  number = {2}
}

@article{Gottlieb2012a,
  title = {Attention, {{Learning}}, and the {{Value}} of {{Information}}},
  author = {Gottlieb, Jacqueline},
  year = {2012},
  month = oct,
  volume = {76},
  pages = {281--295},
  issn = {08966273},
  doi = {10.1016/j.neuron.2012.09.034},
  file = {Gottlieb - 2012 - Attention, Learning, and the Value of Information.pdf},
  journal = {Neuron},
  language = {en},
  number = {2}
}

@article{Gottlieb2018,
  title = {Towards a Neuroscience of Active Sampling and Curiosity},
  author = {Gottlieb, Jacqueline and Oudeyer, Pierre-Yves},
  year = {2018},
  month = dec,
  volume = {19},
  pages = {758--770},
  issn = {1471-003X, 1471-0048},
  doi = {10.1038/s41583-018-0078-0},
  abstract = {In natural behaviour, animals actively interrogate their environments using endogenously generated `question-and-answer' strategies. However, in laboratory settings participants typically engage with externally imposed stimuli and tasks, and the mechanisms of active sampling remain poorly understood. We review a nascent neuroscientific literature that examines active-sampling policies and their relation to attention and curiosity. We distinguish between information sampling, in which organisms reduce uncertainty relevant to a familiar task{$\mkern1mu$}, and information search, in which they investigate in an open-ended fashion to discover new tasks. We review evidence that both sampling and search depend on individual preferences over cognitive states, including attitudes towards uncertainty{$\mkern1mu$}, learning progress and types of information. We propose that, although these preferences are non-instrumental and can on occasion interfere with external goals, they are important heuristics that allow organisms to cope with the high complexity of both sampling and search, and generate curiosity-driven investigations in large, open environments in which rewards are sparse and ex ante unknown.},
  file = {Gottlieb and Oudeyer - 2018 - Towards a neuroscience of active sampling and curi.pdf},
  journal = {Nat Rev Neurosci},
  language = {en},
  number = {12}
}

@techreport{Goyal2018,
  title = {Functionally Distinct High and Low Theta Oscillations in the Human Hippocampus},
  author = {Goyal, Abhinav and Miller, Jonathan and Qasim, Salman E. and Watrous, Andrew J. and Stein, Joel M. and Inman, Cory S. and Gross, Robert E. and Willie, Jon T. and Lega, Bradley and Lin, Jui-Jui and Sharan, Ashwini and Wu, Chengyuan and Sperling, Michael R. and Sheth, Sameer A. and McKhann, Guy M. and Smith, Elliot H. and Schevon, Catherine and Jacobs, Joshua},
  year = {2018},
  month = dec,
  institution = {{Neuroscience}},
  doi = {10.1101/498055},
  abstract = {Abstract           Based on rodent models, researchers have theorized that the hippocampus supports episodic memory and navigation via the theta oscillation, a \textasciitilde 4\textendash 10-Hz rhythm that coordinates brain-wide neural activity. However, recordings from humans have indicated that hippocampal theta oscillations are lower in frequency and less prevalent than in rodents, suggesting interspecies differences in theta's function. To characterize human hippocampal theta, we examined the properties of theta oscillations throughout the anterior\textendash posterior length of the hippocampus as neurosurgical subjects performed a virtual spatial navigation task. During virtual movement, we observed hippocampal oscillations at multiple frequencies from 2 to 14 Hz. The posterior hippocampus prominently displayed oscillations at \textasciitilde 8-Hz and the precise frequency of these oscillations correlated with the speed of movement, implicating these signals in spatial navigation. We also observed slower \textasciitilde 3-Hz oscillations, but these signals were more prevalent in the anterior hippocampus and their frequency did not vary with movement speed. Our results converge with recent findings to suggest an updated view of human hippocampal electrophysiology. Rather than one hippocampal theta oscillation with a single general role, high-and low-theta oscillations, respectively, may reflect spatial and non-spatial cognitive processes.},
  file = {Goyal et al. - 2018 - Functionally distinct high and low theta oscillati.pdf},
  language = {en},
  type = {Preprint}
}

@article{Grabska-Barwinska2014,
  title = {How Well Do Mean Field Theories of Spiking Quadratic-Integrate-and-Fire Networks Work in Realistic Parameter Regimes?},
  author = {{Grabska-Barwi{\'n}ska}, Agnieszka and Latham, Peter E.},
  year = {2014},
  month = jun,
  volume = {36},
  pages = {469--481},
  issn = {0929-5313, 1573-6873},
  doi = {10.1007/s10827-013-0481-5},
  abstract = {We use mean field techniques to compute the distribution of excitatory and inhibitory firing rates in large networks of randomly connected spiking quadratic integrate and fire neurons. These techniques are based on the assumption that activity is asynchronous and Poisson. For most parameter settings these assumptions are strongly violated; nevertheless, so long as the networks are not too synchronous, we find good agreement between mean field prediction and network simulations. Thus, much of the intuition developed for randomly connected networks in the asynchronous regime applies to mildly synchronous networks.},
  file = {2014 - Grabska-Barwiska, Latham - How well do mean field theories of spiking quadratic-integrate-and-fire networks work in realistic par.pdf;Grabska-Barwińska and Latham - 2014 - How well do mean field theories of spiking quadrat.pdf},
  journal = {Journal of Computational Neuroscience},
  language = {en},
  number = {3}
}

@article{Grabska-Barwinska2017,
  title = {A Probabilistic Approach to Demixing Odors},
  author = {{Grabska-Barwi{\'n}ska}, Agnieszka and Barthelm{\'e}, Simon and Beck, Jeff and Mainen, Zachary F and Pouget, Alexandre and Latham, Peter E},
  year = {2017},
  month = jan,
  volume = {20},
  pages = {98--106},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.4444},
  file = {Grabska-Barwińska et al. - 2017 - A probabilistic approach to demixing odors.pdf},
  journal = {Nat Neurosci},
  language = {en},
  number = {1}
}

@book{Grady2010,
  title = {Discrete Calculus: Applied Analysis on Graphs for Computational Science},
  shorttitle = {Discrete Calculus},
  author = {Grady, Leo J. and Polimeni, Jonathan},
  year = {2010},
  publisher = {{Springer}},
  address = {{London ; New York}},
  annotation = {OCLC: ocn651077688},
  file = {Grady and Polimeni - 2010 - Discrete calculus applied analysis on graphs for .pdf},
  isbn = {978-1-84996-289-6 978-1-84996-290-2},
  keywords = {Calculus,Computer science,Digital techniques Mathematics,Graph algorithms,Graphic methods,Image processing,Mathematics},
  language = {en},
  lccn = {QA76.9.M35 G73 2010}
}

@article{Grant2013,
  title = {Simulation of {{Cortico}}-{{Basal Ganglia Oscillations}} and {{Their Suppression}} by {{Closed Loop Deep Brain Stimulation}}},
  author = {Grant, Peadar F. and Lowery, Madeleine M.},
  year = {2013},
  month = jul,
  volume = {21},
  pages = {584--594},
  issn = {1534-4320, 1558-0210},
  doi = {10.1109/TNSRE.2012.2202403},
  abstract = {A new model of deep brain stimulation (DBS) is presented that integrates volume conduction effects with a neural model of pathological beta-band oscillations in the cortico-basal ganglia network. The model is used to test the clinical hypothesis that closed-loop control of the amplitude of DBS may be possible, based on the average rectified value of beta-band oscillations in the local field potential. Simulation of closed-loop high-frequency DBS was shown to yield energy savings, with the magnitude of the energy saved dependent on the strength of coupling between the subthalamic nucleus and the remainder of the cortico-basal ganglia network. When closed-loop DBS was applied to a strongly coupled cortico-basal ganglia network, the stimulation energy delivered over a 480 s period was reduced by up to 42\%. Greater energy reductions were observed for weakly coupled networks, as the stimulation amplitude reduced to zero once the initial desynchronization had occurred. The results provide support for the application of closed-loop high-frequency DBS based on electrophysiological biomarkers.},
  file = {2013 - Grant, Lowery - Simulation of cortico-basal ganglia oscillations and their suppression by closed loop deep brain stimulation.pdf},
  journal = {IEEE Transactions on Neural Systems and Rehabilitation Engineering},
  language = {en},
  number = {4}
}

@article{Graves2014,
  title = {Neural {{Turing Machines}}},
  author = {Graves, Alex and Wayne, Greg and Danihelka, Ivo},
  year = {2014},
  month = oct,
  abstract = {We extend the capabilities of neural networks by coupling them to external memory resources, which they can interact with by attentional processes. The combined system is analogous to a Turing Machine or Von Neumann architecture but is differentiable end-toend, allowing it to be efficiently trained with gradient descent. Preliminary results demonstrate that Neural Turing Machines can infer simple algorithms such as copying, sorting, and associative recall from input and output examples.},
  archiveprefix = {arXiv},
  eprint = {1410.5401},
  eprinttype = {arxiv},
  file = {2014 - Graves, Wayne, Danihelka - Neural Turing Machines.pdf;Graves et al. - 2014 - Neural Turing Machines.pdf},
  journal = {arXiv:1410.5401 [cs]},
  keywords = {Computer Science - Neural and Evolutionary Computing},
  language = {en},
  primaryclass = {cs}
}

@article{Graves2017,
  title = {Automated {{Curriculum Learning}} for {{Neural Networks}}},
  author = {Graves, Alex and Bellemare, Marc G. and Menick, Jacob and Munos, Remi and Kavukcuoglu, Koray},
  year = {2017},
  month = apr,
  abstract = {We introduce a method for automatically selecting the path, or syllabus, that a neural network follows through a curriculum so as to maximise learning efficiency. A measure of the amount that the network learns from each data sample is provided as a reward signal to a nonstationary multiarmed bandit algorithm, which then determines a stochastic syllabus. We consider a range of signals derived from two distinct indicators of learning progress: rate of increase in prediction accuracy, and rate of increase in network complexity. Experimental results for LSTM networks on three curricula demonstrate that our approach can significantly accelerate learning, in some cases halving the time required to attain a satisfactory performance level.},
  archiveprefix = {arXiv},
  eprint = {1704.03003},
  eprinttype = {arxiv},
  file = {Graves et al. - 2017 - Automated Curriculum Learning for Neural Networks.pdf},
  journal = {arXiv:1704.03003 [cs]},
  keywords = {Computer Science - Neural and Evolutionary Computing},
  language = {en},
  primaryclass = {cs}
}

@article{Graves2017a,
  title = {Automated {{Curriculum Learning}} for {{Neural Networks}}},
  author = {Graves, Alex and Bellemare, Marc G. and Menick, Jacob and Munos, Remi and Kavukcuoglu, Koray},
  year = {2017},
  month = apr,
  abstract = {We introduce a method for automatically selecting the path, or syllabus, that a neural network follows through a curriculum so as to maximise learning efficiency. A measure of the amount that the network learns from each data sample is provided as a reward signal to a nonstationary multiarmed bandit algorithm, which then determines a stochastic syllabus. We consider a range of signals derived from two distinct indicators of learning progress: rate of increase in prediction accuracy, and rate of increase in network complexity. Experimental results for LSTM networks on three curricula demonstrate that our approach can significantly accelerate learning, in some cases halving the time required to attain a satisfactory performance level.},
  archiveprefix = {arXiv},
  eprint = {1704.03003},
  eprinttype = {arxiv},
  file = {Graves et al. - 2017 - Automated Curriculum Learning for Neural Networks 2.pdf},
  journal = {arXiv:1704.03003 [cs]},
  keywords = {Computer Science - Neural and Evolutionary Computing},
  language = {en},
  primaryclass = {cs}
}

@article{Gray1995,
  title = {A {{Perceptron Reveals}} the {{Face}} of {{Sex}}},
  author = {Gray, Michael S. and Lawrence, David T. and Golomb, Beatrice A. and Sejnowski, Terrence J.},
  year = {1995},
  month = nov,
  volume = {7},
  pages = {1160--1164},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/neco.1995.7.6.1160},
  file = {1995 - Gray et al. - A perception reveals the face of sex.pdf},
  journal = {Neural Computation},
  language = {en},
  number = {6}
}

@article{Gray2005,
  title = {A Circuit for Navigation in {{Caenorhabditis}} Elegans},
  author = {Gray, J. M. and Hill, J. J. and Bargmann, C. I.},
  year = {2005},
  month = mar,
  volume = {102},
  pages = {3184--3191},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.0409009101},
  file = {Gray et al. - 2005 - A circuit for navigation in Caenorhabditis elegans.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {9}
}

@article{GrEGoire1997,
  title = {Effect of Age on Forward and Backward Digit Spans},
  author = {Gr{\'E}Goire, Jacques and Van Der Linden, Martial},
  year = {1997},
  month = jun,
  volume = {4},
  pages = {140--149},
  issn = {1382-5585, 1744-4128},
  doi = {10.1080/13825589708256642},
  abstract = {A number of studies has suggested that aging is characterized by a decline in the central executive while the automatic processes (in particular operations by the phonological loop) remain intact. According to interpretation, age differences should be minimal in verbal forward digit span while they should be more important in backward verbal digit span. A sample of 1,000 subjects with ages ranging from 16 years to 79 years was used to test this hypothesis. The results show no significant effect of age on the difference between digit span forward and backward. The theoretical implications of these results are discussed.},
  file = {2004 - Gregoire, Van Der Linden - Effect of age on forward and backward span tasks.pdf},
  journal = {Aging, Neuropsychology, and Cognition},
  language = {en},
  number = {2}
}

@article{Grewe2017,
  title = {Synchronous Spikes Are Necessary but Not Sufficient for a Synchrony Code in Populations of Spiking Neurons},
  author = {Grewe, Jan and Kruscha, Alexandra and Lindner, Benjamin and Benda, Jan},
  year = {2017},
  month = mar,
  volume = {114},
  pages = {E1977-E1985},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1615561114},
  file = {Grewe et al. - 2017 - Synchronous spikes are necessary but not sufficien.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {10}
}

@article{Greydanus2019,
  title = {Hamiltonian {{Neural Networks}}},
  author = {Greydanus, Sam and Dzamba, Misko and Yosinski, Jason},
  year = {2019},
  month = jun,
  abstract = {Even though neural networks enjoy widespread use, they still struggle to learn the basic laws of physics. How might we endow them with better inductive biases? In this paper, we draw inspiration from Hamiltonian mechanics to train models that learn and respect exact conservation laws in an unsupervised manner. We evaluate our models on problems where conservation of energy is important, including the two-body problem and pixel observations of a pendulum. Our model trains faster and generalizes better than a regular neural network. An interesting side effect is that our model is perfectly reversible in time.},
  archiveprefix = {arXiv},
  eprint = {1906.01563},
  eprinttype = {arxiv},
  file = {Greydanus et al. - 2019 - Hamiltonian Neural Networks.pdf},
  journal = {arXiv:1906.01563 [cs]},
  keywords = {Computer Science - Neural and Evolutionary Computing},
  language = {en},
  primaryclass = {cs}
}

@article{Greydanus2020,
  title = {Scaling down {{Deep Learning}}},
  author = {Greydanus, Sam},
  year = {2020},
  month = dec,
  abstract = {Though deep learning models have taken on commercial and political relevance, many aspects of their training and operation remain poorly understood. This has sparked interest in "science of deep learning" projects, many of which are run at scale and require enormous amounts of time, money, and electricity. But how much of this research really needs to occur at scale? In this paper, we introduce MNIST-1D: a minimalist, low-memory, and low-compute alternative to classic deep learning benchmarks. The training examples are 20 times smaller than MNIST examples yet they differentiate more clearly between linear, nonlinear, and convolutional models which attain 32, 68, and 94\% accuracy respectively (these models obtain 94, 99+, and 99+\% on MNIST). Then we present example use cases which include measuring the spatial inductive biases of lottery tickets, observing deep double descent, and metalearning an activation function.},
  archiveprefix = {arXiv},
  eprint = {2011.14439},
  eprinttype = {arxiv},
  file = {Greydanus - 2020 - Scaling down Deep Learning.pdf},
  journal = {arXiv:2011.14439 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  language = {en},
  primaryclass = {cs, stat}
}

@article{Griffin2020,
  title = {When {{Is More Uncertainty Better}}? {{A Model}} of {{Uncertainty Regulation}} and {{Effectiveness}}},
  shorttitle = {When {{Is More Uncertainty Better}}?},
  author = {Griffin, Mark A. and Grote, Gudela},
  year = {2020},
  month = oct,
  volume = {45},
  pages = {745--765},
  issn = {0363-7425, 1930-3807},
  doi = {10.5465/amr.2018.0271},
  file = {Griffin and Grote - 2020 - When Is More Uncertainty Better A Model of Uncert.pdf},
  journal = {AMR},
  language = {en},
  number = {4}
}

@article{Griffin2020a,
  title = {When {{Is More Uncertainty Better}}? {{A Model}} of {{Uncertainty Regulation}} and {{Effectiveness}}},
  shorttitle = {When {{Is More Uncertainty Better}}?},
  author = {Griffin, Mark A. and Grote, Gudela},
  year = {2020},
  month = oct,
  volume = {45},
  pages = {745--765},
  issn = {0363-7425, 1930-3807},
  doi = {10.5465/amr.2018.0271},
  file = {Griffin and Grote - 2020 - When Is More Uncertainty Better A Model of Uncert 2.pdf},
  journal = {AMR},
  language = {en},
  number = {4}
}

@article{Grill1995,
  title = {Stimulus Waveforms for Selective Neural Stimulation},
  author = {Grill, W.M. and Mortimer, J.T.},
  year = {July-Aug./1995},
  volume = {14},
  pages = {375--385},
  issn = {07395175},
  doi = {10.1109/51.395310},
  file = {1995 - Grill, Mortimer - Stimulus Waveforms for Selective Neural Stimulation.pdf},
  journal = {IEEE Engineering in Medicine and Biology Magazine},
  language = {en},
  number = {4}
}

@article{Grill1996,
  title = {The Effect of Stimulus Pulse Duration on Selectivity of Neural Stimulation},
  author = {Grill, W.M. and Mortimer, J.T.},
  year = {Feb./1996},
  volume = {43},
  pages = {161--166},
  issn = {00189294},
  doi = {10.1109/10.481985},
  file = {1996 - Grill, Mortimer - The Effect of Stimulus Pulse Duration on Selectivity of Neural Stimulation.pdf},
  journal = {IEEE Transactions on Biomedical Engineering},
  language = {en},
  number = {2}
}

@incollection{Grill2015,
  title = {Model-Based Analysis and Design of Waveforms for Efficient Neural Stimulation},
  booktitle = {Progress in {{Brain Research}}},
  author = {Grill, Warren M.},
  year = {2015},
  volume = {222},
  pages = {147--162},
  publisher = {{Elsevier}},
  doi = {10.1016/bs.pbr.2015.07.031},
  abstract = {The design space for electrical stimulation of the nervous system is extremely large, and because the response to stimulation is highly non-linear, the selection of stimulation parameters to achieve a desired response is a challenging problem. Computational models of the response of neurons to extracellular stimulation allow analysis of the effects of stimulation parameters on neural excitation and provide an approach to select or design optimal parameters of stimulation. Here, I review the use of computational models to understand the effects of stimulation waveform on the energy efficiency of neural excitation and to design novel stimulation waveforms to increase the efficiency of neural stimulation.},
  file = {2015 - Grill - Model-Based Analysis and Design of Waveforms for Efficient Neural Stimulation.pdf;Grill - 2015 - Model-based analysis and design of waveforms for e.pdf},
  isbn = {978-0-444-63546-4},
  language = {en}
}

@article{Grillner2016,
  title = {The {{Basal Ganglia Over}} 500 {{Million Years}}},
  author = {Grillner, Sten and Robertson, Brita},
  year = {2016},
  month = oct,
  volume = {26},
  pages = {R1088-R1100},
  issn = {09609822},
  doi = {10.1016/j.cub.2016.06.041},
  file = {Grillner and Robertson - 2016 - The Basal Ganglia Over 500 Million Years.pdf},
  journal = {Current Biology},
  language = {en},
  number = {20}
}

@article{Grillner2016a,
  title = {The {{Basal Ganglia Over}} 500 {{Million Years}}},
  author = {Grillner, Sten and Robertson, Brita},
  year = {2016},
  month = oct,
  volume = {26},
  pages = {R1088-R1100},
  issn = {09609822},
  doi = {10.1016/j.cub.2016.06.041},
  file = {Grillner and Robertson - 2016 - The Basal Ganglia Over 500 Million Years 2.pdf},
  journal = {Current Biology},
  language = {en},
  number = {20}
}

@article{Grimbert2006,
  title = {Bifurcation {{Analysis}} of {{Jansen}}'s {{Neural Mass Model}}},
  author = {Grimbert, Fran{\c c}ois and Faugeras, Olivier},
  year = {2006},
  month = dec,
  volume = {18},
  pages = {3052--3068},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/neco.2006.18.12.3052},
  file = {2006 - Grimbert, Faugeras - Bifurcation analysis of Jansen's neural mass model.pdf},
  journal = {Neural Computation},
  language = {en},
  number = {12}
}

@article{Grozinger2019,
  title = {Pathways to Cellular Supremacy in Biocomputing},
  author = {Grozinger, Lewis and Amos, Martyn and Gorochowski, Thomas E. and Carbonell, Pablo and Oyarz{\'u}n, Diego A. and Stoof, Ruud and Fellermann, Harold and Zuliani, Paolo and Tas, Huseyin and {Go{\~n}i-Moreno}, Angel},
  year = {2019},
  month = dec,
  volume = {10},
  pages = {5250},
  issn = {2041-1723},
  doi = {10.1038/s41467-019-13232-z},
  file = {Grozinger et al. - 2019 - Pathways to cellular supremacy in biocomputing.pdf},
  journal = {Nat Commun},
  language = {en},
  number = {1}
}

@article{Grunwald2004,
  title = {Shannon {{Information}} and {{Kolmogorov Complexity}}},
  author = {Grunwald, Peter and Vitanyi, Paul},
  year = {2004},
  month = oct,
  abstract = {We compare the elementary theories of Shannon information and Kolmogorov complexity, the extent to which they have a common purpose, and where they are fundamentally different. We discuss and relate the basic notions of both theories: Shannon entropy versus Kolmogorov complexity, the relation of both to universal coding, Shannon mutual information versus Kolmogorov (`algorithmic') mutual information, probabilistic sufficient statistic versus algorithmic sufficient statistic (related to lossy compression in the Shannon theory versus meaningful information in the Kolmogorov theory), and rate distortion theory versus Kolmogorov's structure function. Part of the material has appeared in print before, scattered through various publications, but this is the first comprehensive systematic comparison. The last mentioned relations are new.},
  archiveprefix = {arXiv},
  eprint = {cs/0410002},
  eprinttype = {arxiv},
  file = {Grunwald and Vitanyi - 2004 - Shannon Information and Kolmogorov Complexity.pdf},
  journal = {arXiv:cs/0410002},
  keywords = {Computer Science - Information Theory,E.4; H.1.1},
  language = {en}
}

@inproceedings{Guckelsberger2016,
  title = {Intrinsically Motivated General Companion {{NPCs}} via {{Coupled Empowerment Maximisation}}},
  booktitle = {2016 {{IEEE Conference}} on {{Computational Intelligence}} and {{Games}} ({{CIG}})},
  author = {Guckelsberger, Christian and Salge, Christoph and Colton, Simon},
  year = {2016},
  month = sep,
  pages = {1--8},
  publisher = {{IEEE}},
  address = {{Santorini, Greece}},
  doi = {10.1109/CIG.2016.7860406},
  abstract = {Non-player characters (NPCs) in games are traditionally hard-coded or dependent on pre-specified goals, and consequently struggle to behave sensibly in ever-changing and possibly unpredictable game worlds. To make them fit for new developments in procedural content generation, we introduce the principle of Coupled Empowerment Maximisation as an intrinsic motivation for game NPCs. We focus on the development of a general game companion, designed to support the player in achieving their goals. We evaluate our approach against three intuitive and abstract companion duties. We develop dedicated scenarios for each duty in a dungeon-crawler game testbed, and provide qualitative evidence that the emergent NPC behaviour fulfils these duties. We argue that this generic approach can speed up NPC AI development, improve automatic game evolution and introduce NPCs to full game-generation systems.},
  file = {Guckelsberger et al. - 2016 - Intrinsically motivated general companion NPCs via.pdf},
  isbn = {978-1-5090-1883-3},
  language = {en}
}

@article{Guckelsberger2018,
  title = {New {{And Surprising Ways}} to {{Be Mean}}. {{Adversarial NPCs}} with {{Coupled Empowerment Minimisation}}},
  author = {Guckelsberger, Christian and Salge, Christoph and Togelius, Julian},
  year = {2018},
  month = jun,
  abstract = {Creating Non-Player Characters (NPCs) that can react robustly to unforeseen player behaviour or novel game content is difficult and time-consuming. This hinders the design of believable characters, and the inclusion of NPCs in games that rely heavily on procedural content generation. We have previously addressed this challenge by means of empowerment, a model of intrinsic motivation, and demonstrated how a coupled empowerment maximisation (CEM) policy can yield generic, companion-like behaviour. In this paper, we extend the CEM framework with a minimisation policy to give rise to adversarial behaviour. We conduct a qualitative, exploratory study in a dungeon-crawler game, demonstrating that CEM can exploit the affordances of different content facets in adaptive adversarial behaviour without modifications to the policy. Changes to the level design, underlying mechanics and our character's actions do not threaten our NPC's robustness, but yield new and surprising ways to be mean.},
  archiveprefix = {arXiv},
  eprint = {1806.01387},
  eprinttype = {arxiv},
  file = {Guckelsberger et al. - 2018 - New And Surprising Ways to Be Mean. Adversarial NP.pdf},
  journal = {arXiv:1806.01387 [cs]},
  keywords = {Computer Science - Artificial Intelligence},
  language = {en},
  primaryclass = {cs}
}

@article{Guckelsberger2018a,
  title = {New {{And Surprising Ways}} to {{Be Mean}}. {{Adversarial NPCs}} with {{Coupled Empowerment Minimisation}}},
  author = {Guckelsberger, Christian and Salge, Christoph and Togelius, Julian},
  year = {2018},
  month = jun,
  abstract = {Creating Non-Player Characters (NPCs) that can react robustly to unforeseen player behaviour or novel game content is difficult and time-consuming. This hinders the design of believable characters, and the inclusion of NPCs in games that rely heavily on procedural content generation. We have previously addressed this challenge by means of empowerment, a model of intrinsic motivation, and demonstrated how a coupled empowerment maximisation (CEM) policy can yield generic, companion-like behaviour. In this paper, we extend the CEM framework with a minimisation policy to give rise to adversarial behaviour. We conduct a qualitative, exploratory study in a dungeon-crawler game, demonstrating that CEM can exploit the affordances of different content facets in adaptive adversarial behaviour without modifications to the policy. Changes to the level design, underlying mechanics and our character's actions do not threaten our NPC's robustness, but yield new and surprising ways to be mean.},
  archiveprefix = {arXiv},
  eprint = {1806.01387},
  eprinttype = {arxiv},
  file = {Guckelsberger et al. - 2018 - New And Surprising Ways to Be Mean. Adversarial NP 2.pdf},
  journal = {arXiv:1806.01387 [cs]},
  keywords = {Computer Science - Artificial Intelligence},
  language = {en},
  primaryclass = {cs}
}

@article{Guclu2017,
  title = {Modeling the {{Dynamics}} of {{Human Brain Activity}} with {{Recurrent Neural Networks}}},
  author = {G{\"u}{\c c}l{\"u}, Umut and {van Gerven}, Marcel A. J.},
  year = {2017},
  month = feb,
  volume = {11},
  issn = {1662-5188},
  doi = {10.3389/fncom.2017.00007},
  abstract = {Encoding models are used for predicting brain activity in response to sensory stimuli with the objective of elucidating how sensory information is represented in the brain. Encoding models typically comprise a nonlinear transformation of stimuli to features (feature model) and a linear transformation of features to responses (response model). While there has been extensive work on developing better feature models, the work on developing better response models has been rather limited. Here, we investigate the extent to which recurrent neural network models can use their internal memories for nonlinear processing of arbitrary feature sequences to predict feature-evoked response sequences as measured by functional magnetic resonance imaging. We show that the proposed recurrent neural network models can significantly outperform established response models by accurately estimating long-term dependencies that drive hemodynamic responses. The results open a new window into modeling the dynamics of brain activity in response to sensory stimuli.},
  file = {Güçlü and van Gerven - 2017 - Modeling the Dynamics of Human Brain Activity with.pdf},
  journal = {Frontiers in Computational Neuroscience},
  language = {en}
}

@article{Guergiuev,
  title = {Deep Learning with Segregated Dendrites},
  author = {Guergiuev, Jordan and Lillicrap, Timothy P and Richards, Blake A},
  pages = {29},
  abstract = {Deep learning has led to significant advances in artificial intelligence, in part, by adopting strategies motivated by neurophysiology. However, it is unclear whether deep learning could occur in the real brain. Here, we show that deep learning can be achieved by moving away from point neuron models and towards multi-compartment neurons. Like neocortical pyramidal neurons, neurons in our model receive feedforward sensory information and higher-order feedback in electrotonically segregated compartments. Thanks to this segregation, the network can calculate local synaptic weight updates that allow it to categorize images from the MNIST data-set with good accuracy. We show that our learning algorithm can take advantage of multilayer architectures to identify abstract categories\textemdash the hallmark of deep learning. This work demonstrates that deep learning can be achieved using segregated dendritic compartments for feedforward and feedback information, which may help to explain the dendritic morphology of neocortical pyramidal neurons.},
  file = {Guergiuev et al. - Deep learning with segregated dendrites.pdf},
  language = {en}
}

@article{Guerguiev2017,
  title = {Towards Deep Learning with Segregated Dendrites},
  author = {Guerguiev, Jordan and Lillicrap, Timothy P and Richards, Blake A},
  year = {2017},
  month = dec,
  volume = {6},
  issn = {2050-084X},
  doi = {10.7554/eLife.22901},
  file = {Guerguiev et al. - 2017 - Towards deep learning with segregated dendrites 2.pdf;Guerguiev et al. - 2017 - Towards deep learning with segregated dendrites.pdf},
  journal = {eLife},
  language = {en}
}

@article{Guest2016,
  title = {What the {{Success}} of {{Brain Imaging Implies}} about the {{Neural Code}}},
  author = {Guest, Olivia and Love, Bradley C},
  year = {2016},
  month = sep,
  doi = {10.1101/071076},
  abstract = {The success of fMRI places constraints on the nature of the neural code. The fact that researchers can infer similarities between neural representations, despite limitations in what fMRI measures, implies that certain neural coding schemes are more likely than others. For fMRI to be successful given its low temporal and spatial resolution, the neural code must smooth at the subvoxel and functional level such that similar stimuli engender similar internal representations. Through proof and simulation, we evaluate a number of reasonable coding schemes and demonstrate that only a subset are plausible given both fMRI's successes and its limitations in measuring neural activity. Deep neural network approaches, which have been forwarded as computational accounts of the ventral stream, are consistent with the success of fMRI, though functional smoothness breaks down in the later network layers. These results have implications for the nature of neural code and ventral stream, as well as what can be successfully investigated with fMRI.},
  file = {Guest and Love - 2016 - What the Success of Brain Imaging Implies about th.pdf},
  journal = {bioRxiv},
  language = {en}
}

@article{Guez2013,
  title = {Efficient {{Bayes}}-{{Adaptive Reinforcement Learning}} Using {{Sample}}-{{Based Search}}},
  author = {Guez, Arthur and Silver, David and Dayan, Peter},
  year = {2013},
  month = dec,
  abstract = {Bayesian model-based reinforcement learning is a formally elegant approach to learning optimal behaviour under model uncertainty, trading off exploration and exploitation in an ideal way. Unfortunately, finding the resulting Bayes-optimal policies is notoriously taxing, since the search space becomes enormous. In this paper we introduce a tractable, sample-based method for approximate Bayesoptimal planning which exploits Monte-Carlo tree search. Our approach outperformed prior Bayesian model-based RL algorithms by a significant margin on several well-known benchmark problems \textendash{} because it avoids expensive applications of Bayes rule within the search tree by lazily sampling models from the current beliefs. We illustrate the advantages of our approach by showing it working in an infinite state space domain which is qualitatively out of reach of almost all previous work in Bayesian exploration.},
  archiveprefix = {arXiv},
  eprint = {1205.3109},
  eprinttype = {arxiv},
  file = {Guez et al. - 2013 - Efficient Bayes-Adaptive Reinforcement Learning us.pdf},
  journal = {arXiv:1205.3109 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryclass = {cs, stat}
}

@article{Guilhoto,
  title = {An {{Overview Of Artificial Neural Networks}} for {{Mathematicians}}},
  author = {Guilhoto, Leonardo Ferreira},
  pages = {25},
  abstract = {This expository paper first defines what an Artificial Neural Network is and describes some of the key ideas behind them such as weights, biases, activation functions (mainly sigmoids and the ReLU function), backpropagation, etc. We then focus on interesting properties of the expressive power of feedforward neural networks, presenting several theorems relating to the types of functions that can be approximated by specific types of networks. Finally, in order to help build intuition, a case study of effectiveness in the MNIST database of handwritten digits is carried out, examining how parameters such as learning rate, width, and depth of a network affects its accuracy. This work focuses mainly on theoretical aspects of feedforward neural networks rather than providing a step-by-step guide for programmers.},
  file = {Guilhoto - An Overview Of Artiﬁcial Neural Networks for Mathe.pdf},
  language = {en}
}

@article{Guise2014,
  title = {A {{Bayesian Model}} of {{Polychronicity}}},
  author = {Guise, Mira and Knott, Alistair and Benuskova, Lubica},
  year = {2014},
  month = sep,
  volume = {26},
  pages = {2052--2073},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/NECO_a_00620},
  file = {2014 - Lehky et al. - Dimensionality of object representations in monkey inferotemporal cortex.pdf;Guise et al. - 2014 - A Bayesian Model of Polychronicity.pdf},
  journal = {Neural Computation},
  language = {en},
  number = {9}
}

@article{Guisnet2021,
  title = {A Three-Dimensional Habitat for {{C}}. Elegans Environmental Enrichment},
  author = {Guisnet, Aur{\'e}lie and Maitra, Malosree and Pradhan, Sreeparna and Hendricks, Michael},
  editor = {Curran, Sean P.},
  year = {2021},
  month = jan,
  volume = {16},
  pages = {e0245139},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0245139},
  abstract = {As we learn more about the importance of gene-environment interactions and the effects of environmental enrichment, it becomes evident that minimalistic laboratory conditions can affect gene expression patterns and behaviors of model organisms. In the laboratory, Caenorhabditis elegans is generally cultured on two-dimensional, homogeneous agar plates abundantly covered with axenic bacteria culture as a food source. However, in the wild, this nematode thrives in rotting fruits and plant stems feeding on bacteria and small eukaryotes. This contrast in habitat complexity suggests that studying C. elegans in enriched laboratory conditions can deepen our understanding of its fundamental traits and behaviors. Here, we developed a protocol to create three-dimensional habitable scaffolds for trans-generational culture of C. elegans in the laboratory. Using decellularization and sterilization of fruit tissue, we created an axenic environment that can be navigated throughout and where the microbial environment can be strictly controlled. C. elegans were maintained over generations on this habitat, and showed a clear behavioral bias for the enriched environment. As an initial assessment of behavioral variations, we found that dauer populations in scaffolds exhibit high-frequency, complex nictation behavior including group towering and jumping behavior.},
  file = {Guisnet et al. - 2021 - A three-dimensional habitat for C. elegans environ.pdf},
  journal = {PLoS ONE},
  language = {en},
  number = {1}
}

@article{Gulyas2020,
  title = {The Role of Detours in Individual Human Navigation Patterns of Complex Networks},
  author = {Guly{\'a}s, Andr{\'a}s and B{\'i}r{\'o}, J{\'o}zsef and R{\'e}tv{\'a}ri, G{\'a}bor and Nov{\'a}k, M{\'a}rton and K{\H o}r{\"o}si, Attila and Sl{\'i}z, Mariann and Heszberger, Zal{\'a}n},
  year = {2020},
  month = dec,
  volume = {10},
  pages = {1098},
  issn = {2045-2322},
  doi = {10.1038/s41598-020-57856-4},
  file = {Gulyás et al. - 2020 - The role of detours in individual human navigation.pdf},
  journal = {Sci Rep},
  language = {en},
  number = {1}
}

@article{Gunawardena2014,
  title = {Models in Biology: `Accurate Descriptions of Our Pathetic Thinking'},
  shorttitle = {Models in Biology},
  author = {Gunawardena, Jeremy},
  year = {2014},
  month = dec,
  volume = {12},
  issn = {1741-7007},
  doi = {10.1186/1741-7007-12-29},
  abstract = {In this essay I will sketch some ideas for how to think about models in biology. I will begin by trying to dispel the myth that quantitative modeling is somehow foreign to biology. I will then point out the distinction between forward and reverse modeling and focus thereafter on the former. Instead of going into mathematical technicalities about different varieties of models, I will focus on their logical structure, in terms of assumptions and conclusions. A model is a logical machine for deducing the latter from the former. If the model is correct, then, if you believe its assumptions, you must, as a matter of logic, also believe its conclusions. This leads to consideration of the assumptions underlying models. If these are based on fundamental physical laws, then it may be reasonable to treat the model as `predictive', in the sense that it is not subject to falsification and we can rely on its conclusions. However, at the molecular level, models are more often derived from phenomenology and guesswork. In this case, the model is a test of its assumptions and must be falsifiable. I will discuss three models from this perspective, each of which yields biological insights, and this will lead to some guidelines for prospective model builders.},
  file = {Gunawardena - 2014 - Models in biology ‘accurate descriptions of our p.pdf},
  journal = {BMC Biology},
  language = {en},
  number = {1}
}

@article{Gunnthorsdottir2002,
  title = {Using the {{Machiavellianism}} Instrument to Predict Trustworthiness in a Bargaining Game},
  author = {Gunnthorsdottir, Anna and McCabe, Kevin and Smith, Vernon},
  year = {2002},
  month = feb,
  volume = {23},
  pages = {49--66},
  issn = {01674870},
  doi = {10.1016/S0167-4870(01)00067-8},
  abstract = {Game-theoretic experiments have revealed substantial individual differences where the game allows for off-equilibrium behavior such as trust and reciprocity. We explore the personality psychology and decision making literatures and conclude that these individual differences are likely to be mediated by differential emotional arousal. We argue that Christie and Geis's Machiavellianism scale (Mach-IV) is an instrument that allows the identification of types who vary in cooperativeness. We use that test to predict the behavior of participants in a two-person one-shot constituent game in which subjects face a choice between trust and distrust, and between reciprocation (trustworthiness) and defection. We find that the Mach-IV scale does not predict trusting behavior. It does, however, predict reciprocity. Over one half of those who score low to average on the scale reciprocate trust. High scorers overwhelmingly defect when it is to their advantage to do so. \'O 2002 Elsevier Science B.V. All rights reserved.},
  file = {2002 - Gunnthorsdottir, McCabe, Smith - Using the Machiavellianism instrument to predict trustworthiness in a bargaining game.pdf},
  journal = {Journal of Economic Psychology},
  language = {en},
  number = {1}
}

@article{Guntupalli2016,
  title = {A {{Model}} of {{Representational Spaces}} in {{Human Cortex}}},
  author = {Guntupalli, J. Swaroop and Hanke, Michael and Halchenko, Yaroslav O. and Connolly, Andrew C. and Ramadge, Peter J. and Haxby, James V.},
  year = {2016},
  month = jun,
  volume = {26},
  pages = {2919--2934},
  issn = {1047-3211, 1460-2199},
  doi = {10.1093/cercor/bhw068},
  abstract = {Current models of the functional architecture of human cortex emphasize areas that capture coarse-scale features of cortical topography but provide no account for population responses that encode information in fine-scale patterns of activity. Here, we present a linear model of shared representational spaces in human cortex that captures fine-scale distinctions among population responses with response-tuning basis functions that are common across brains and models cortical patterns of neural responses with individual-specific topographic basis functions. We derive a common model space for the whole cortex using a new algorithm, searchlight hyperalignment, and complex, dynamic stimuli that provide a broad sampling of visual, auditory, and social percepts. The model aligns representations across brains in occipital, temporal, parietal, and prefrontal cortices, as shown by between-subject multivariate pattern classification and intersubject correlation of representational geometry, indicating that structural principles for shared neural representations apply across widely divergent domains of information. The model provides a rigorous account for individual variability of well-known coarse-scale topographies, such as retinotopy and category selectivity, and goes further to account for fine-scale patterns that are multiplexed with coarse-scale topographies and carry finer distinctions.},
  file = {Guntupalli et al. - 2016 - A Model of Representational Spaces in Human Cortex.pdf},
  journal = {Cerebral Cortex},
  language = {en},
  number = {6}
}

@inproceedings{Guo2019,
  title = {Human Learning and Decision-Making in the Bandit Task: {{Three}} Wrongs Make a Right},
  shorttitle = {Human Learning and Decision-Making in the Bandit Task},
  booktitle = {2019 {{Conference}} on {{Cognitive Computational Neuroscience}}},
  author = {Guo, Dalin and Yu, Angela},
  year = {2019},
  publisher = {{Cognitive Computational Neuroscience}},
  address = {{Berlin, Germany}},
  doi = {10.32470/CCN.2019.1356-0},
  abstract = {Humans and animals frequently need to make choices among options with imperfectly known reward outcomes. In neuroscience, this is often studied using the multiarmed bandit task, in which subjects repeatedly choose among bandit arms with fixed but unknown reward rates, thus negotiating a tension between exploitation and exploration. Here, using a modified version of the bandit task in which we query subjects reward expectations of unchosen arms, we investigate how general reward availability in the environment affects human prior beliefs. Based on self-report data and computational modeling of behavioral data, we obtain converging evidence that human subjects systematically under-estimate reward availability. Additional computational analyses reveal that this under-estimation compensates for two other apparent suboptimalities in human behavior, namely a default assumption of environmental non-stationarity, and the use of a simplistic decision policy. This result represents a concrete instance in which multiple sub-optimalities in brain computations synergistically interact to achieve much better-than-expected behavioral outcome. This work raises the intriguing possibility that many apparently isolated limitations in brain computation and representation may actually work together to achieve highly intelligent behavior in a broader context, and also sheds light on computationally efficient algorithms that could be adopted by artificial intelligence systems.},
  file = {Guo and Yu - 2019 - Human learning and decision-making in the bandit t.pdf},
  language = {en}
}

@inproceedings{Guo2019a,
  title = {Human Learning and Decision-Making in the Bandit Task: {{Three}} Wrongs Make a Right},
  shorttitle = {Human Learning and Decision-Making in the Bandit Task},
  booktitle = {2019 {{Conference}} on {{Cognitive Computational Neuroscience}}},
  author = {Guo, Dalin and Yu, Angela},
  year = {2019},
  publisher = {{Cognitive Computational Neuroscience}},
  address = {{Berlin, Germany}},
  doi = {10.32470/CCN.2019.1356-0},
  abstract = {Humans and animals frequently need to make choices among options with imperfectly known reward outcomes. In neuroscience, this is often studied using the multiarmed bandit task, in which subjects repeatedly choose among bandit arms with fixed but unknown reward rates, thus negotiating a tension between exploitation and exploration. Here, using a modified version of the bandit task in which we query subjects reward expectations of unchosen arms, we investigate how general reward availability in the environment affects human prior beliefs. Based on self-report data and computational modeling of behavioral data, we obtain converging evidence that human subjects systematically under-estimate reward availability. Additional computational analyses reveal that this under-estimation compensates for two other apparent suboptimalities in human behavior, namely a default assumption of environmental non-stationarity, and the use of a simplistic decision policy. This result represents a concrete instance in which multiple sub-optimalities in brain computations synergistically interact to achieve much better-than-expected behavioral outcome. This work raises the intriguing possibility that many apparently isolated limitations in brain computation and representation may actually work together to achieve highly intelligent behavior in a broader context, and also sheds light on computationally efficient algorithms that could be adopted by artificial intelligence systems.},
  file = {Guo and Yu - 2019 - Human learning and decision-making in the bandit t 2.pdf},
  language = {en}
}

@article{Gupta2006,
  title = {The {{Interplay}} between {{Exploration}} and {{Exploitation}}},
  author = {Gupta, Anil A. and Smith, Ken and Shalley, Christina},
  year = {2006},
  volume = {49},
  pages = {693--706},
  file = {[No title found].pdf},
  journal = {The Academy of Management Journal},
  language = {en},
  number = {4}
}

@article{Gupta2006a,
  title = {The {{Interplay}} between {{Exploration}} and {{Exploitation}}},
  author = {Gupta, Anil and Smith, Ken and Shalley, Christina},
  year = {2006},
  volume = {49},
  file = {Gupta et al. - 2006 - The Interplay between Exploration and Exploitation.pdf},
  journal = {The Academy of Management Journal},
  language = {en},
  number = {4}
}

@article{Gurney,
  title = {Testing Computational Hypotheses of Brain Systems Function: A Case Study with the Basal Ganglia},
  author = {Gurney, K N and Humphries, M and Wood, R and Prescott, T J and Redgrave, P},
  pages = {29},
  abstract = {We develop a methodology for testing computational hypotheses about neural functionality articulated in models at the systems level of description. In this approach, the first step is to attempt the construction of a model of the underlying brain system which is consistent with the known anatomy and physiology, but which is also able to exhibit functional properties consistent with a putative computational hypothesis. If this is successful, the second step consists of including additional known pathways into the model and testing the new models to see whether they show an improvement in functional performance (using appropriate performance metrics). A positive outcome is taken as evidence in support of the hypothesis. A final step is to construct `control' models by including pathways that are not consistent with biological data. In this case a performance detriment is taken as support for the hypothesis. The methodology is applied to the basal ganglia, and builds on a previously published model of this system (Gurney et al 2001 Biol. Cybern. 84 401\textendash 23) which was based on the hypothesis that the basal ganglia perform action selection. The realistically constrained models show a selection benefit, while control models show a decrement in selection ability. These results, taken together, provide further validation of our selection hypothesis of basal ganglia function.},
  file = {2004 - Gurney et al. - Testing computational hypotheses of brain systems function a case study with the basal ganglia K.pdf},
  language = {en}
}

@article{Gurney2001,
  title = {A Computational Model of Action Selection in the Basal Ganglia. {{I}}. {{A}} New Functional Anatomy},
  author = {Gurney, K. and Prescott, T. J. and Redgrave, P.},
  year = {2001},
  month = may,
  volume = {84},
  pages = {401--410},
  issn = {0340-1200},
  doi = {10.1007/PL00007984},
  abstract = {We present a biologically plausible model of processing intrinsic to the basal ganglia based on the computational premise that action selection is a primary role of these central brain structures. By encoding the propensity for selecting a given action in a scalar value (the salience), it is shown that action selection may be recast in terms of signal selection. The generic properties of signal selection are de\textregistered ned and neural networks for this type of computation examined. A comparison between these networks and basal ganglia anatomy leads to a novel functional decomposition of the basal ganglia architecture into `selection' and `control' pathways. The former pathway performs the selection per se via a feedforward o-centre on-surround network. The control pathway regulates the action of the selection pathway to ensure its eective operation, and synergistically complements its dopaminergic modulation. The model contrasts with the prevailing functional segregation of basal ganglia into `direct' and `indirect' pathways.},
  file = {2001 - Gurney, Prescott, Redgrave - A computational model of action selection in the basal ganglia. I. A new functional anatomy.pdf},
  journal = {Biological Cybernetics},
  language = {en},
  number = {6}
}

@article{Gurney2001a,
  title = {A Computational Model of Action Selection in the Basal Ganglia. {{II}}. {{Analysis}} and Simulation of Behaviour},
  author = {Gurney, K. and Prescott, T. J. and Redgrave, P.},
  year = {2001},
  month = may,
  volume = {84},
  pages = {411--423},
  issn = {0340-1200},
  doi = {10.1007/PL00007985},
  abstract = {In a companion paper a new functional architecture was proposed for the basal ganglia based on the premise that these brain structures play a central role in behavioural action selection. The current paper quantitatively describes the properties of the model using analysis and simulation. The decomposition of the basal ganglia into selection and control pathways is supported in several ways. First, several elegant features are exposed {$\pm$} capacity scaling, enhanced selectivity and synergistic dopamine modulation {$\pm$} which might be expected to exist in a well designed action selection mechanism. The discovery of these features also lends support to the computational premise of selection that underpins our model. Second, good matches between model globus pallidus external segment output and globus pallidus internal segment and substantia nigra reticulata area output, and neurophysiological data, have been found which are indicative of common architectural features in the model and biological basal ganglia. Third, the behaviour of the model as a signal selection mechanism has parallels with some kinds of action selection observed in animals under various levels of dopaminergic modulation.},
  file = {2001 - Gurney, Prescott, Redgrave - A computational model of action selection in the basal ganglia. II. Analysis and simulation of behav.pdf},
  journal = {Biological Cybernetics},
  language = {en},
  number = {6}
}

@article{Gurney2015,
  title = {A {{New Framework}} for {{Cortico}}-{{Striatal Plasticity}}: {{Behavioural Theory Meets In Vitro Data}} at the {{Reinforcement}}-{{Action Interface}}},
  shorttitle = {A {{New Framework}} for {{Cortico}}-{{Striatal Plasticity}}},
  author = {Gurney, Kevin N. and Humphries, Mark D. and Redgrave, Peter},
  editor = {Dayan, Peter},
  year = {2015},
  month = jan,
  volume = {13},
  pages = {e1002034},
  issn = {1545-7885},
  doi = {10.1371/journal.pbio.1002034},
  abstract = {Operant learning requires that reinforcement signals interact with action representations at a suitable neural interface. Much evidence suggests that this occurs when phasic dopamine, acting as a reinforcement prediction error, gates plasticity at cortico-striatal synapses, and thereby changes the future likelihood of selecting the action(s) coded by striatal neurons. But this hypothesis faces serious challenges. First, cortico-striatal plasticity is inexplicably complex, depending on spike timing, dopamine level, and dopamine receptor type. Second, there is a credit assignment problem\textemdash action selection signals occur long before the consequent dopamine reinforcement signal. Third, the two types of striatal output neuron have apparently opposite effects on action selection. Whether these factors rule out the interface hypothesis and how they interact to produce reinforcement learning is unknown. We present a computational framework that addresses these challenges. We first predict the expected activity changes over an operant task for both types of action-coding striatal neuron, and show they co-operate to promote action selection in learning and compete to promote action suppression in extinction. Separately, we derive a complete model of dopamine and spike-timing dependent cortico-striatal plasticity from in vitro data. We then show this model produces the predicted activity changes necessary for learning and extinction in an operant task, a remarkable convergence of a bottom-up data-driven plasticity model with the top-down behavioural requirements of learning theory. Moreover, we show the complex dependencies of cortico-striatal plasticity are not only sufficient but necessary for learning and extinction. Validating the model, we show it can account for behavioural data describing extinction, renewal, and reacquisition, and replicate in vitro experimental data on cortico-striatal plasticity. By bridging the levels between the single synapse and behaviour, our model shows how striatum acts as the action-reinforcement interface.},
  file = {2015 - Gurney, Humphries, Redgrave - A New Framework for Cortico-Striatal Plasticity Behavioural Theory Meets In Vitro Data at the Reinf.pdf;Gurney et al. - 2015 - A New Framework for Cortico-Striatal Plasticity B.pdf},
  journal = {PLoS Biology},
  language = {en},
  number = {1}
}

@article{Gutfreund1995,
  title = {Subthreshold Oscillations and Resonant Frequency in Guinea-Pig Cortical Neurons: Physiology and Modelling.},
  shorttitle = {Subthreshold Oscillations and Resonant Frequency in Guinea-Pig Cortical Neurons},
  author = {Gutfreund, Y and {yarom}, Y and Segev, I},
  year = {1995},
  month = mar,
  volume = {483},
  pages = {621--640},
  issn = {00223751},
  doi = {10.1113/jphysiol.1995.sp020611},
  file = {Gutfreund et al. - 1995 - Subthreshold oscillations and resonant frequency i.pdf},
  journal = {The Journal of Physiology},
  language = {en},
  number = {3}
}

@article{Guthrie1999,
  title = {{{ATP Released}} from {{Astrocytes Mediates Glial Calcium Waves}}},
  author = {Guthrie, Peter B. and Knappenberger, Joshua and Segal, Menahem and Bennett, Michael V. L. and Charles, Andrew C. and Kater, S. B.},
  year = {1999},
  month = jan,
  volume = {19},
  pages = {520--528},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.19-02-00520.1999},
  file = {Guthrie et al. - 1999 - ATP Released from Astrocytes Mediates Glial Calciu.pdf},
  journal = {J. Neurosci.},
  language = {en},
  number = {2}
}

@article{Guthrie1999a,
  title = {{{ATP Released}} from {{Astrocytes Mediates Glial Calcium Waves}}},
  author = {Guthrie, Peter B. and Knappenberger, Joshua and Segal, Menahem and Bennett, Michael V. L. and Charles, Andrew C. and Kater, S. B.},
  year = {1999},
  month = jan,
  volume = {19},
  pages = {520--528},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.19-02-00520.1999},
  file = {Guthrie et al. - 1999 - ATP Released from Astrocytes Mediates Glial Calciu 2.pdf},
  journal = {J. Neurosci.},
  language = {en},
  number = {2}
}

@article{Gutierrez-Ibanez2018,
  title = {Parrots Have Evolved a Primate-like Telencephalic-Midbrain-Cerebellar Circuit},
  author = {{Guti{\'e}rrez-Ib{\'a}{\~n}ez}, Cristi{\'a}n and Iwaniuk, Andrew N. and Wylie, Douglas R.},
  year = {2018},
  month = dec,
  volume = {8},
  pages = {9960},
  issn = {2045-2322},
  doi = {10.1038/s41598-018-28301-4},
  file = {Gutiérrez-Ibáñez et al. - 2018 - Parrots have evolved a primate-like telencephalic-.pdf},
  journal = {Sci Rep},
  language = {en},
  number = {1}
}

@article{Gutierrez2013,
  title = {Multiple {{Mechanisms Switch}} an {{Electrically Coupled}}, {{Synaptically Inhibited Neuron}} between {{Competing Rhythmic Oscillators}}},
  author = {Gutierrez, Gabrielle J. and O'Leary, Timothy and Marder, Eve},
  year = {2013},
  month = mar,
  volume = {77},
  pages = {845--858},
  issn = {08966273},
  doi = {10.1016/j.neuron.2013.01.016},
  abstract = {Rhythmic oscillations are common features of nervous systems. One of the fundamental questions posed by these rhythms is how individual neurons or groups of neurons are recruited into different network oscillations. We modeled competing fast and slow oscillators connected to a hub neuron with electrical and inhibitory synapses. We explore the patterns of coordination shown in the network as a function of the electrical coupling and inhibitory synapse strengths with the help of a novel visualization method that we call the ``parameterscape.'' The hub neuron can be switched between the fast and slow oscillators by multiple network mechanisms, indicating that a given change in network state can be achieved by degenerate cellular mechanisms. These results have importance for interpreting experiments employing optogenetic, genetic, and pharmacological manipulations to understand circuit dynamics.},
  file = {2013 - Gutierrez, O'Leary, Marder - Multiple mechanisms switch an electrically coupled, synaptically inhibited neuron between competing.pdf},
  journal = {Neuron},
  language = {en},
  number = {5}
}

@article{Gutierrez2015,
  title = {A Neural Coding Scheme Reproducing Foraging Trajectories},
  author = {Guti{\'e}rrez, Esther D. and Cabrera, Juan Luis},
  year = {2015},
  month = dec,
  volume = {5},
  pages = {18009},
  issn = {2045-2322},
  doi = {10.1038/srep18009},
  file = {Gutiérrez and Cabrera - 2015 - A neural coding scheme reproducing foraging trajec.pdf},
  journal = {Sci Rep},
  language = {en},
  number = {1}
}

@article{Gutig2016,
  title = {Spiking Neurons Can Discover Predictive Features by Aggregate-Label Learning},
  author = {Gutig, R.},
  year = {2016},
  month = mar,
  volume = {351},
  pages = {aab4113-aab4113},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aab4113},
  file = {Gutig - 2016 - Spiking neurons can discover predictive features b.pdf},
  journal = {Science},
  language = {en},
  number = {6277}
}

@article{Guyon,
  title = {An {{Introduction}} to {{Variable}} and {{Feature Selection}}},
  author = {Guyon, Isabelle and Elisseeff, Andre},
  pages = {26},
  abstract = {Variable and feature selection have become the focus of much research in areas of application for which datasets with tens or hundreds of thousands of variables are available. These areas include text processing of internet documents, gene expression array analysis, and combinatorial chemistry. The objective of variable selection is three-fold: improving the prediction performance of the predictors, providing faster and more cost-effective predictors, and providing a better understanding of the underlying process that generated the data. The contributions of this special issue cover a wide range of aspects of such problems: providing a better definition of the objective function, feature construction, feature ranking, multivariate feature selection, efficient search methods, and feature validity assessment methods.},
  file = {2003 - Guyon, Elisseeff - An introduction to variable and feature selection.pdf},
  language = {en}
}

@article{Ha2018,
  title = {World {{Models}}},
  author = {Ha, David and Schmidhuber, J{\"u}rgen},
  year = {2018},
  month = mar,
  doi = {10.5281/zenodo.1207631},
  abstract = {We explore building generative neural network models of popular reinforcement learning environments. Our world model can be trained quickly in an unsupervised manner to learn a compressed spatial and temporal representation of the environment. By using features extracted from the world model as inputs to an agent, we can train a very compact and simple policy that can solve the required task. We can even train our agent entirely inside of its own hallucinated dream generated by its world model, and transfer this policy back into the actual environment.},
  archiveprefix = {arXiv},
  eprint = {1803.10122},
  eprinttype = {arxiv},
  file = {Ha and Schmidhuber - 2018 - World Models.pdf},
  journal = {arXiv:1803.10122 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryclass = {cs, stat}
}

@article{Ha2018a,
  title = {Recurrent {{World Models Facilitate Policy Evolution}}},
  author = {Ha, David and Schmidhuber, J{\"u}rgen},
  year = {2018},
  pages = {13},
  file = {Ha and Schmidhuber - Recurrent World Models Facilitate Policy Evolution.pdf},
  journal = {NeurIPS},
  language = {en}
}

@article{Haarnoja,
  title = {Reinforcement {{Learning}} with {{Deep Energy}}-{{Based Policies}}},
  author = {Haarnoja, Tuomas and Tang, Haoran and Abbeel, Pieter and Levine, Sergey},
  pages = {16},
  abstract = {We propose a method for learning expressive energy-based policies for continuous states and actions, which has been feasible only in tabular domains before. We apply our method to learning maximum entropy policies, resulting into a new algorithm, called soft Q-learning, that expresses the optimal policy via a Boltzmann distribution. We use the recently proposed amortized Stein variational gradient descent to learn a stochastic sampling network that approximates samples from this distribution. The benefits of the proposed algorithm include improved exploration and compositionality that allows transferring skills between tasks, which we confirm in simulated experiments with swimming and walking robots. We also draw a connection to actorcritic methods, which can be viewed performing approximate inference on the corresponding energy-based model.},
  file = {Haarnoja et al. - Reinforcement Learning with Deep Energy-Based Poli.pdf},
  language = {en}
}

@article{Haarnoja2018,
  title = {Soft {{Actor}}-{{Critic Algorithms}} and {{Applications}}},
  author = {Haarnoja, Tuomas and Zhou, Aurick and Hartikainen, Kristian and Tucker, George and Ha, Sehoon and Tan, Jie and Kumar, Vikash and Zhu, Henry and Gupta, Abhishek and Abbeel, Pieter and Levine, Sergey},
  year = {2018},
  month = dec,
  abstract = {Model-free deep reinforcement learning (RL) algorithms have been successfully applied to a range of challenging sequential decision making and control tasks. However, these methods typically suffer from two major challenges: high sample complexity and brittleness to hyperparameters. Both of these challenges limit the applicability of such methods to real-world domains. In this paper, we describe Soft Actor-Critic (SAC), our recently introduced off-policy actor-critic algorithm based on the maximum entropy RL framework. In this framework, the actor aims to simultaneously maximize expected return and entropy. That is, to succeed at the task while acting as randomly as possible. We extend SAC to incorporate a number of modifications that accelerate training and improve stability with respect to the hyperparameters, including a constrained formulation that automatically tunes the temperature hyperparameter. We systematically evaluate SAC on a range of benchmark tasks, as well as real-world challenging tasks such as locomotion for a quadrupedal robot and robotic manipulation with a dexterous hand. With these improvements, SAC achieves state-of-the-art performance, outperforming prior on-policy and off-policy methods in sample-efficiency and asymptotic performance. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving similar performance across different random seeds. These results suggest that SAC is a promising candidate for learning in real-world robotics tasks.},
  archiveprefix = {arXiv},
  eprint = {1812.05905},
  eprinttype = {arxiv},
  file = {2015 - Haarnoja et al. - Soft Actor-Critic Algorithms and Applications.pdf;Haarnoja et al. - 2018 - Soft Actor-Critic Algorithms and Applications.pdf},
  journal = {arXiv:1812.05905 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},
  language = {en},
  primaryclass = {cs, stat}
}

@article{Haarnojaa,
  title = {Soft {{Actor}}-{{Critic}}:  {{Off}}-{{Policy Maximum Entropy Deep Reinforcement Learning}} with a {{Stochastic Actor}}},
  author = {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  pages = {14},
  abstract = {Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an offpolicy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.},
  file = {Haarnoja et al. - Soft Actor-Critic  Off-Policy Maximum Entropy Dee.pdf},
  language = {en}
}

@techreport{Haber2020,
  title = {Learning the Architectural Features That Predict Functional Similarity of Neural Networks},
  author = {Haber, Adam and Schneidman, Elad},
  year = {2020},
  month = apr,
  institution = {{Neuroscience}},
  doi = {10.1101/2020.04.27.057752},
  abstract = {The mapping of the wiring diagrams of neural circuits promises to allow us to link structure and function of neural networks. Current approaches to analyzing connectomes rely mainly on graph-theoretical tools, but these may downplay the complex nonlinear dynamics of single neurons and networks, and the way networks respond to their inputs. Here, we measure the functional similarity of simulated networks of neurons, by quantifying the similitude of their spiking patterns in response to the same stimuli. We find that common graph theory metrics convey little information about the similarity of networks' responses. Instead, we learn a functional metric between networks based on their synaptic differences, and show that it accurately predicts the similarity of novel networks, for a wide range of stimuli. We then show that a sparse set of architectural features - the sum of synaptic inputs that each neuron receives and the sum of each neuron's synaptic outputs - predicts the functional similarity of networks of up to 100 cells, with high accuracy. We thus suggest new architectural design principles that shape the function of neural networks, which conform with experimental evidence of homeostatic mechanisms.},
  file = {Haber and Schneidman - 2020 - Learning the architectural features that predict f.pdf},
  language = {en},
  type = {Preprint}
}

@article{Haegens2011,
  title = {-{{Oscillations}} in the Monkey Sensorimotor Network Influence Discrimination Performance by Rhythmical Inhibition of Neuronal Spiking},
  author = {Haegens, S. and Nacher, V. and Luna, R. and Romo, R. and Jensen, O.},
  year = {2011},
  month = nov,
  volume = {108},
  pages = {19377--19382},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1117190108},
  file = {2011 - Haegens et al. - α-Oscillations in the monkey sensorimotor network influence discrimination performance by rhythmical inhibition.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {48}
}

@article{Haeri2005,
  title = {Modeling the {{Parkinson}}'s Tremor and Its Treatments},
  author = {Haeri, Mohammad and Sarbaz, Yashar and Gharibzadeh, Shahriar},
  year = {2005},
  month = oct,
  volume = {236},
  pages = {311--322},
  issn = {00225193},
  doi = {10.1016/j.jtbi.2005.03.014},
  abstract = {In this paper, we discuss modeling issues of the Parkinson's tremor. Through the work we have employed physiological structure as well as functioning of the parts in brain that are involved in the disease. To obtain more practical similarity, random behaviors of the connection paths are also considered. Medication or treatment of the disease both by drug prescription and electrical signal stimulation are modeled based on the same model introduced for the disease itself. Two new medication strategies are proposed based on the model to reduce the side effects caused by the present drug prescription.},
  file = {2005 - Haeri, Sarbaz, Gharibzadeh - Modeling the Parkinson's tremor and its treatments.pdf},
  journal = {Journal of Theoretical Biology},
  language = {en},
  number = {3}
}

@article{Hagen2016,
  title = {Hybrid {{Scheme}} for {{Modeling Local Field Potentials}} from {{Point}}-{{Neuron Networks}}},
  author = {Hagen, Espen and Dahmen, David and Stavrinou, Maria L. and Lind{\'e}n, Henrik and Tetzlaff, Tom and {van Albada}, Sacha J. and Gr{\"u}n, Sonja and Diesmann, Markus and Einevoll, Gaute T.},
  year = {2016},
  month = dec,
  volume = {26},
  pages = {4461--4496},
  issn = {1047-3211, 1460-2199},
  doi = {10.1093/cercor/bhw237},
  abstract = {With rapidly advancing multi-electrode recording technology, the local field potential (LFP) has again become a popular measure of neuronal activity in both research and clinical applications. Proper understanding of the LFP requires detailed mathematical modeling incorporating the anatomical and electrophysiological features of neurons near the recording electrode, as well as synaptic inputs from the entire network. Here we propose a hybrid modeling scheme combining efficient point-neuron network models with biophysical principles underlying LFP generation by real neurons. The LFP predictions rely on populations of network-equivalent multicompartment neuron models with layer-specific synaptic connectivity, can be used with an arbitrary number of point-neuron network populations, and allows for a full separation of simulated network dynamics and LFPs. We apply the scheme to a full-scale cortical network model for a {$\sim$}1 mm2 patch of primary visual cortex, predict laminar LFPs for different network states, assess the relative LFP contribution from different laminar populations, and investigate effects of input correlations and neuron density on the LFP. The generic nature of the hybrid scheme and its public implementation in hybridLFPy form the basis for LFP predictions from other and larger pointneuron network models, as well as extensions of the current application with additional biological detail.},
  file = {Hagen et al. - 2016 - Hybrid Scheme for Modeling Local Field Potentials .pdf},
  journal = {Cerebral Cortex},
  language = {en},
  number = {12}
}

@article{Hahn2010,
  title = {Modeling Shifts in the Rate and Pattern of Subthalamopallidal Network Activity during Deep Brain Stimulation},
  author = {Hahn, Philip J. and McIntyre, Cameron C.},
  year = {2010},
  month = jun,
  volume = {28},
  pages = {425--441},
  issn = {0929-5313, 1573-6873},
  doi = {10.1007/s10827-010-0225-8},
  abstract = {Deep brain stimulation (DBS) of the subthlamic nucleus (STN) represents an effective treatment for medically refractory Parkinson's disease; however, understanding of its effects on basal ganglia network activity remains limited. We constructed a computational model of the subthalamopallidal network, trained it to fit in vivo recordings from parkinsonian monkeys, and evaluated its response to STN DBS. The network model was created with synaptically connected single compartment biophysical models of STN and pallidal neurons, and stochastically defined inputs driven by cortical beta rhythms. A least mean square error training algorithm was developed to parameterize network connections and minimize error when compared to experimental spike and burst rates in the parkinsonian condition. The output of the trained network was then compared to experimental data not used in the training process. We found that reducing the influence of the cortical beta input on the model generated activity that agreed well with recordings from normal monkeys. Further, during STN DBS in the parkinsonian condition the simulations reproduced the reduction in GPi bursting found in existing experimental data. The model also provided the opportunity to greatly expand analysis of GPi bursting activity, generating three major predictions. First, its reduction was proportional to the volume of STN activated by DBS. Second, GPi bursting decreased in a stimulation frequency dependent manner, saturating at values consistent with clinically therapeutic DBS. And third, ablating STN neurons, reported to generate similar therapeutic outcomes as STN DBS, also reduced GPi bursting. Our theoretical analysis of stimulation induced network activity suggests that regularization of GPi firing is dependent on the volume of STN tissue activated and a threshold level of burst reduction may be necessary for therapeutic effect.},
  file = {2010 - Hahn, McIntyre - Modeling shifts in the rate and pattern of subthalamopallidal network activity during deep brain stimulation.pdf},
  journal = {Journal of Computational Neuroscience},
  language = {en},
  number = {3}
}

@article{Halgren2018,
  title = {The {{Generation}} and {{Propagation}} of the {{Human Alpha Rhythm}}},
  author = {Halgren, Milan and Ulbert, Istvan and Bastuji, Helene and Fabo, Daniel and Eross, Lorand and Rey, Marc and Devinsky, Orrin and Doyle, Werner K and {Mak-McCully}, Rachel and Halgren, Eric and Wittner, Lucia and Chauvel, Patrick and Heit, Gary and Eskandar, Emad and Mandell, Arnold and Cash, Sydney S},
  year = {2018},
  month = jun,
  doi = {10.1101/202564},
  abstract = {The alpha rhythm is the longest studied brain oscillation and has been theorized to play a key role in cognition. Still, its physiology is poorly understood. In this study, we used micro and macro electrodes in surgical epilepsy patients to measure the intracortical and thalamic generators of the alpha rhythm during quiet wakefulness. We first found that alpha in posterior cortex propagates from higher-order anterosuperior areas towards the occipital pole, consistent with alpha effecting top-down processing. This cortical alpha leads pulvinar alpha, complicating prevailing theories of a thalamic pacemaker. Finally, alpha is dominated by currents and firing in supragranular cortical layers. Together, these results suggest that the alpha rhythm likely reflects short-range supragranular feedback which propagates from higher to lower-order cortex and cortex to thalamus. These physiological insights suggest how alpha could mediate feedback throughout the thalamocortical system.},
  file = {Halgren et al. - 2018 - The Generation and Propagation of the Human Alpha .pdf},
  journal = {bioRxiv},
  language = {en}
}

@techreport{Haller2018,
  title = {Parameterizing Neural Power Spectra},
  author = {Haller, Matar and Donoghue, Thomas and Peterson, Erik and Varma, Paroma and Sebastian, Priyadarshini and Gao, Richard and Noto, Torben and Knight, Robert T. and Shestyuk, Avgusta and Voytek, Bradley},
  year = {2018},
  month = apr,
  institution = {{Neuroscience}},
  doi = {10.1101/299859},
  abstract = {Electrophysiological signals across species and recording scales exhibit both periodic and aperiodic features. Periodic oscillations have been widely studied and linked to numerous physiological, cognitive, behavioral, and disease states, while the aperiodic ``background'' 1/f component of neural power spectra has received far less attention. Most analyses of oscillations are conducted on a priori, canonically-defined frequency bands without consideration of the underlying aperiodic structure, or verification that a periodic signal even exists in addition to the aperiodic signal. This is problematic, as recent evidence shows that the aperiodic signal is dynamic, changing with age, task demands, and cognitive state. It has also been linked to the relative excitation/inhibition of the underlying neuronal population. This means that standard analytic approaches easily conflate changes in the periodic and aperiodic signals with one another because the aperiodic parameters\textemdash along with oscillation center frequency, power, and bandwidth\textemdash are all dynamic in physiologically meaningful, but likely different, ways. In order to overcome the limitations of traditional narrowband analyses and to reduce the potentially deleterious effects of conflating these features, we introduce a novel algorithm for automatic parameterization of neural power spectral densities (PSDs) as a combination of the aperiodic signal and putative periodic oscillations. Notably, this algorithm requires no a priori specification of band limits and accounts for potentially-overlapping oscillations while minimizing the degree to which they are confounded with one another. This algorithm is amenable to large-scale data exploration and analysis, providing researchers with a tool to quickly and accurately parameterize neural power spectra.},
  file = {Haller et al. - 2018 - Parameterizing neural power spectra.pdf},
  language = {en},
  type = {Preprint}
}

@techreport{Haller2018a,
  title = {Parameterizing Neural Power Spectra},
  author = {Haller, Matar and Donoghue, Thomas and Peterson, Erik and Varma, Paroma and Sebastian, Priyadarshini and Gao, Richard and Noto, Torben and Knight, Robert T. and Shestyuk, Avgusta and Voytek, Bradley},
  year = {2018},
  month = apr,
  institution = {{Neuroscience}},
  doi = {10.1101/299859},
  abstract = {Electrophysiological signals across species and recording scales exhibit both periodic and aperiodic features. Periodic oscillations have been widely studied and linked to numerous physiological, cognitive, behavioral, and disease states, while the aperiodic ``background'' 1/f component of neural power spectra has received far less attention. Most analyses of oscillations are conducted on a priori, canonically-defined frequency bands without consideration of the underlying aperiodic structure, or verification that a periodic signal even exists in addition to the aperiodic signal. This is problematic, as recent evidence shows that the aperiodic signal is dynamic, changing with age, task demands, and cognitive state. It has also been linked to the relative excitation/inhibition of the underlying neuronal population. This means that standard analytic approaches easily conflate changes in the periodic and aperiodic signals with one another because the aperiodic parameters\textemdash along with oscillation center frequency, power, and bandwidth\textemdash are all dynamic in physiologically meaningful, but likely different, ways. In order to overcome the limitations of traditional narrowband analyses and to reduce the potentially deleterious effects of conflating these features, we introduce a novel algorithm for automatic parameterization of neural power spectral densities (PSDs) as a combination of the aperiodic signal and putative periodic oscillations. Notably, this algorithm requires no a priori specification of band limits and accounts for potentially-overlapping oscillations while minimizing the degree to which they are confounded with one another. This algorithm is amenable to large-scale data exploration and analysis, providing researchers with a tool to quickly and accurately parameterize neural power spectra.},
  file = {Haller et al. - 2018 - Parameterizing neural power spectra 2.pdf},
  language = {en},
  type = {Preprint}
}

@techreport{Haller2018b,
  title = {Parameterizing Neural Power Spectra},
  author = {Haller, Matar and Donoghue, Thomas and Peterson, Erik and Varma, Paroma and Sebastian, Priyadarshini and Gao, Richard and Noto, Torben and Knight, Robert T. and Shestyuk, Avgusta and Voytek, Bradley},
  year = {2018},
  month = apr,
  institution = {{Neuroscience}},
  doi = {10.1101/299859},
  abstract = {Electrophysiological signals across species and recording scales exhibit both periodic and aperiodic features. Periodic oscillations have been widely studied and linked to numerous physiological, cognitive, behavioral, and disease states, while the aperiodic ``background'' 1/f component of neural power spectra has received far less attention. Most analyses of oscillations are conducted on a priori, canonically-defined frequency bands without consideration of the underlying aperiodic structure, or verification that a periodic signal even exists in addition to the aperiodic signal. This is problematic, as recent evidence shows that the aperiodic signal is dynamic, changing with age, task demands, and cognitive state. It has also been linked to the relative excitation/inhibition of the underlying neuronal population. This means that standard analytic approaches easily conflate changes in the periodic and aperiodic signals with one another because the aperiodic parameters\textemdash along with oscillation center frequency, power, and bandwidth\textemdash are all dynamic in physiologically meaningful, but likely different, ways. In order to overcome the limitations of traditional narrowband analyses and to reduce the potentially deleterious effects of conflating these features, we introduce a novel algorithm for automatic parameterization of neural power spectral densities (PSDs) as a combination of the aperiodic signal and putative periodic oscillations. Notably, this algorithm requires no a priori specification of band limits and accounts for potentially-overlapping oscillations while minimizing the degree to which they are confounded with one another. This algorithm is amenable to large-scale data exploration and analysis, providing researchers with a tool to quickly and accurately parameterize neural power spectra.},
  file = {Haller et al. - 2018 - Parameterizing neural power spectra 3.pdf},
  language = {en},
  type = {Preprint}
}

@article{Hammond2007,
  title = {Pathological Synchronization in {{Parkinson}}'s Disease: Networks, Models and Treatments},
  shorttitle = {Pathological Synchronization in {{Parkinson}}'s Disease},
  author = {Hammond, Constance and Bergman, Hagai and Brown, Peter},
  year = {2007},
  month = jul,
  volume = {30},
  pages = {357--364},
  issn = {01662236},
  doi = {10.1016/j.tins.2007.05.004},
  file = {2007 - Hammond, Bergman, Brown - Pathological synchronization in Parkinson's disease networks, models and treatments.pdf},
  journal = {Trends in Neurosciences},
  language = {en},
  number = {7}
}

@article{Hampel2015,
  title = {A Neural Command Circuit for Grooming Movement Control},
  author = {Hampel, Stefanie and Franconville, Romain and Simpson, Julie H and Seeds, Andrew M},
  year = {2015},
  month = sep,
  volume = {4},
  pages = {e08758},
  issn = {2050-084X},
  doi = {10.7554/eLife.08758},
  abstract = {Animals perform many stereotyped movements, but how nervous systems are organized for controlling specific movements remains unclear. Here we use anatomical, optogenetic, behavioral, and physiological techniques to identify a circuit in Drosophila melanogaster that can elicit stereotyped leg movements that groom the antennae. Mechanosensory chordotonal neurons detect displacements of the antennae and excite three different classes of functionally connected interneurons, which include two classes of brain interneurons and different parallel descending neurons. This multilayered circuit is organized such that neurons within each layer are sufficient to specifically elicit antennal grooming. However, we find differences in the durations of antennal grooming elicited by neurons in the different layers, suggesting that the circuit is organized to both command antennal grooming and control its duration. As similar features underlie stimulus-induced movements in other animals, we infer the possibility of a common circuit organization for movement control that can be dissected in Drosophila.},
  file = {Hampel et al. - 2015 - A neural command circuit for grooming movement con.pdf},
  journal = {eLife},
  language = {en}
}

@article{Hansel1992,
  title = {Synchronization and Computation in a Chaotic Neural Network},
  author = {Hansel, D. and Sompolinsky, H.},
  year = {1992},
  month = feb,
  volume = {68},
  pages = {718--721},
  issn = {0031-9007},
  doi = {10.1103/PhysRevLett.68.718},
  file = {1992 - Hansel, Sompolinsky - Synchronization and computation in a chaotic neural network.pdf},
  journal = {Physical Review Letters},
  language = {en},
  number = {5}
}

@article{Hansel2002,
  title = {How {{Noise Contributes}} to {{Contrast Invariance}} of {{Orientation Tuning}} in {{Cat Visual Cortex}}},
  author = {Hansel, D. and {van Vreeswijk}, C.},
  year = {2002},
  month = jun,
  volume = {22},
  pages = {5118--5128},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.22-12-05118.2002},
  file = {2002 - Hansel, van Vreeswijk - How noise contributes to contrast invariance of orientation tuning in cat visual cortex.pdf},
  journal = {The Journal of Neuroscience},
  language = {en},
  number = {12}
}

@article{Hanson2008,
  title = {Brain {{Reading Using Full Brain Support Vector Machines}} for {{Object Recognition}}: {{There Is No}} ``{{Face}}'' {{Identification Area}}},
  shorttitle = {Brain {{Reading Using Full Brain Support Vector Machines}} for {{Object Recognition}}},
  author = {Hanson, Stephen Jos{\'e} and Halchenko, Yaroslav O.},
  year = {2008},
  month = feb,
  volume = {20},
  pages = {486--503},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/neco.2007.09-06-340},
  file = {2008 - Hanson, Halchenko - Brain reading using full brain support vector machines for object recognition there is no face identification.pdf},
  journal = {Neural Computation},
  language = {en},
  number = {2}
}

@article{Hapca2009,
  title = {Anomalous Diffusion of Heterogeneous Populations Characterized by Normal Diffusion at the Individual Level},
  author = {Hapca, Simona and Crawford, John W and Young, Iain M},
  year = {2009},
  month = jan,
  volume = {6},
  pages = {111--122},
  issn = {1742-5689, 1742-5662},
  doi = {10.1098/rsif.2008.0261},
  abstract = {The characterization of the dispersal of populations of non-identical individuals is relevant to most ecological and epidemiological processes. In practice, the movement is quantified by observing relatively few individuals, and averaging to estimate the rate of dispersal of the population as a whole. Here, we show that this can lead to serious errors in the predicted movement of the population if the individuals disperse at different rates. We develop a stochastic model for the diffusion of heterogeneous populations, inspired by the movement of the parasitic nematode               Phasmarhabditis hermaphrodita               . Direct observations of this nematode in homogeneous and heterogeneous environments reveal a large variation in individual behaviour within the population as reflected initially in the speed of the movement. Further statistical analysis shows that the movement is characterized by temporal correlations and in a heterogeneously structured environment the correlations that occur are of shorter range compared with those in a homogeneous environment. Therefore, by using the first-order correlated random walk techniques, we derive an effective diffusion coefficient for each individual, and show that there is a significant variation in this parameter among the population that follows a gamma distribution. Based on these findings, we build a new dispersal model in which we maintain the classical assumption that individual movement can be described by normal diffusion, but due to the variability in individual dispersal rates, the diffusion coefficient is not constant at the population level and follows a continuous distribution. The conclusions and methodology presented are relevant to any heterogeneous population of individuals with widely different diffusion rates.},
  file = {Hapca et al. - 2009 - Anomalous diffusion of heterogeneous populations c.pdf},
  journal = {J. R. Soc. Interface.},
  language = {en},
  number = {30}
}

@article{Harlow2020,
  title = {Characterizing the {{University}} of {{California}}'s Tenure-Track Teaching Position from the Faculty and Administrator Perspectives},
  author = {Harlow, Ashley and Lo, Stanley M. and Saichaie, Kem and Sato, Brian K.},
  editor = {Bianchi, Cesario},
  year = {2020},
  month = jan,
  volume = {15},
  pages = {e0227633},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0227633},
  file = {Harlow et al. - 2020 - Characterizing the University of California’s tenu.pdf},
  journal = {PLoS ONE},
  language = {en},
  number = {1}
}

@article{Harnack2015,
  title = {Stability of {{Neuronal Networks}} with {{Homeostatic Regulation}}},
  author = {Harnack, Daniel and Pelko, Miha and Chaillet, Antoine and Chitour, Yacine and {van Rossum}, Mark C.W.},
  editor = {Gutkin, Boris S.},
  year = {2015},
  month = jul,
  volume = {11},
  pages = {e1004357},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1004357},
  file = {Harnack et al. - 2015 - Stability of Neuronal Networks with Homeostatic Re 2.PDF},
  journal = {PLOS Computational Biology},
  language = {en},
  number = {7}
}

@article{Harper2009,
  title = {The {{Replicator Equation}} as an {{Inference Dynamic}}},
  author = {Harper, Marc},
  year = {2009},
  month = nov,
  abstract = {The replicator equation is interpreted as a continuous inference equation and a formal similarity between the discrete replicator equation and Bayesian inference is described. Further connections between inference and the replicator equation are given including a discussion of information divergences, evolutionary stability, and exponential families as solutions for the replicator dynamic, using Fisher information and information geometry.},
  archiveprefix = {arXiv},
  eprint = {0911.1763},
  eprinttype = {arxiv},
  file = {Harper - 2009 - The Replicator Equation as an Inference Dynamic.pdf},
  journal = {arXiv:0911.1763 [cs, math]},
  keywords = {37N25; Secondary: 62F15,Computer Science - Information Theory,Mathematics - Dynamical Systems},
  language = {en},
  primaryclass = {cs, math}
}

@article{Harris-White1998,
  title = {Spiral {{Intercellular Calcium Waves}} in {{Hippocampal Slice Cultures}}},
  author = {{Harris-White}, Marni E. and Zanotti, Stephen A. and Frautschy, Sally A. and Charles, Andrew C.},
  year = {1998},
  month = feb,
  volume = {79},
  pages = {1045--1052},
  issn = {0022-3077, 1522-1598},
  doi = {10.1152/jn.1998.79.2.1045},
  abstract = {Harris-White, Marni E., Stephen A. Zanotti, Sally A. Frautschy, and Andrew C. Charles. Spiral intercellular calcium waves in hippocampal slice cultures. J. Neurophysiol. 79: 1045\textendash 1052, 1998. Complex patterns of intercellular calcium signaling occur in the CA1 and CA2 regions of hippocampal slice organotypic cultures from neonatal mice. Spontaneous localized intercellular Ca               2+               waves involving 5\textendash 15 cells propagate concentrically from multiple foci in the stratum oriens and s. radiatum. In these same regions, extensive Ca               2+               waves involving hundreds of cells propagate as curvilinear and spiral wavefronts across broad areas of CA1 and CA2. Ca               2+               waves travel at rates of 5\textendash 10 {$\mu$}m/s, are abolished by thapsigargin, and do not require extracellular Ca               2+               . Staining for astrocytes and neurons indicates that these intercellular waves occur primarily in astrocytes. The frequency and amplitude of Ca               2+               waves increase in response to bath application of N-methyl-d-aspartate (NMDA) and decrease in response to removal of extracellular Ca               2+               or application of tetrodotoxin. This novel pattern of intercellular Ca               2+               signaling is characteristic of the behavior of an excitable medium. Networks of glial cells in the hippocampus may behave as an excitable medium whose spatial and temporal signaling properties are modulated by neuronal activity.},
  file = {Harris-White et al. - 1998 - Spiral Intercellular Calcium Waves in Hippocampal .pdf},
  journal = {Journal of Neurophysiology},
  language = {en},
  number = {2}
}

@incollection{Hart2001,
  title = {A {{Reinforcement Procedure Leading}} to {{Correlated Equilibrium}}},
  booktitle = {Economics {{Essays}}},
  author = {Hart, Sergiu and {Mas-Colell}, Andreu},
  editor = {Debreu, G{\'e}rard and Neuefeind, Wilhelm and Trockel, Walter},
  year = {2001},
  pages = {181--200},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-662-04623-4_12},
  abstract = {We consider repeated games where at any period each player knows only his set of actions and the stream of payoffs that he has received in the past. He knows neither his own payoff function, nor the characteristics of the other players (how many there are, their strategies and payoffs). In this context, we present an adaptive procedure for play \textemdash{} called ``modified-regret-matching'' \textemdash{} which is interpretable as a stimulus-response or reinforcement procedure, and which has the property that any limit point of the empirical distribution of play is a correlated equilibrium of the stage game.},
  file = {2001 - Hart, Mas-colell - Economics Essays.pdf},
  isbn = {978-3-642-07539-1 978-3-662-04623-4},
  language = {en}
}

@article{Hart2005,
  title = {Adaptive {{Heuristics}}},
  author = {Hart, Sergiu},
  year = {2005},
  month = sep,
  volume = {73},
  pages = {1401--1430},
  issn = {0012-9682, 1468-0262},
  doi = {10.1111/j.1468-0262.2005.00625.x},
  file = {Hart - 2005 - Adaptive Heuristics.pdf},
  journal = {Econometrica},
  language = {en},
  number = {5}
}

@article{Hassabis2009,
  title = {Decoding {{Neuronal Ensembles}} in the {{Human Hippocampus}}},
  author = {Hassabis, Demis and Chu, Carlton and Rees, Geraint and Weiskopf, Nikolaus and Molyneux, Peter D. and Maguire, Eleanor A.},
  year = {2009},
  month = apr,
  volume = {19},
  pages = {546--554},
  issn = {09609822},
  doi = {10.1016/j.cub.2009.02.033},
  abstract = {Background: The hippocampus underpins our ability to navigate, to form and recollect memories, and to imagine future experiences. How activity across millions of hippocampal neurons supports these functions is a fundamental question in neuroscience, wherein the size, sparseness, and organization of the hippocampal neural code are debated. Results: Here, by using multivariate pattern classification and high spatial resolution functional MRI, we decoded activity across the population of neurons in the human medial temporal lobe while participants navigated in a virtual reality environment. Remarkably, we could accurately predict the position of an individual within this environment solely from the pattern of activity in his hippocampus even when visual input and task were held constant. Moreover, we observed a dissociation between responses in the hippocampus and parahippocampal gyrus, suggesting that they play differing roles in navigation. Conclusions: These results show that highly abstracted representations of space are expressed in the human hippocampus. Furthermore, our findings have implications for understanding the hippocampal population code and suggest that, contrary to current consensus, neuronal ensembles representing place memories must be large and have an anisotropic structure.},
  file = {2009 - Hassabis et al. - Decoding neuronal ensembles in the human hippocampus.pdf},
  journal = {Current Biology},
  language = {en},
  number = {7}
}

@article{Hassabis2017,
  title = {Neuroscience-{{Inspired Artificial Intelligence}}},
  author = {Hassabis, Demis and Kumaran, Dharshan and Summerfield, Christopher and Botvinick, Matthew},
  year = {2017},
  month = jul,
  volume = {95},
  pages = {245--258},
  issn = {08966273},
  doi = {10.1016/j.neuron.2017.06.011},
  file = {Hassabis et al. - 2017 - Neuroscience-Inspired Artificial Intelligence.pdf},
  journal = {Neuron},
  language = {en},
  number = {2}
}

@article{Hassinger1995,
  title = {Evidence for Glutamate-Mediated Activation of Hippocampal Neurons by Glial Calcium Waves},
  author = {Hassinger, Tim D. and Atkinson, Paul B. and Strecker, George J. and Whalen, L. Ray and Dudek, F. Edward and Kossel, Albrecht H. and Kater, S. B.},
  year = {1995},
  month = oct,
  volume = {28},
  pages = {159--170},
  issn = {0022-3034, 1097-4695},
  doi = {10.1002/neu.480280204},
  abstract = {Communication from astrocytes to neurons has recently been reported by two laboratories, but different mechanisms were thought to underlie glial calcium wave activation of associated neurons. Neuronal calcium elevation by glia observed in the present report is similar to that reported previously, where an increase in neuronal calcium was demonstrated in response to glial stimulation. In the present study hippocampal neurons plated on a confluent glial monolayer displayed a transient increase in intracellular calcium following a short delay after the passage of a wave of increased calcium in underlying glia. Activated cells displayed action potentials in response to glial waves and showed antineurofilament immunoreactivity. Finally, the N-methyl+-aspartate glutamate receptor antagonist \textasciitilde r,-2-amino-5-phosphonovalericacid and the non-NMDA glutamate receptor antagonist 6,7dinitroquinoxaline-2,3-dione significantly reduced the responsiveness of neurons to glial calcium waves. O u r results indicate that hippocampal neurons growing on hippocampal or cortical astrocytes respond to glial calcium waves with elevations in calcium and increased electrical activity. Furthermore, we show that in most cases this communication appears to be mediated by ionotropic glutamate receptor channels. 01995 John Wiley \& Sons, Inc.},
  file = {Hassinger et al. - 1995 - Evidence for glutamate-mediated activation of hipp.pdf},
  journal = {J. Neurobiol.},
  language = {en},
  number = {2}
}

@article{Hassinger1996,
  title = {An Extracellular Signaling Component in Propagation of Astrocytic Calcium Waves},
  author = {Hassinger, T. D. and Guthrie, P. B. and Atkinson, P. B. and Bennett, M. V. L. and Kater, S. B.},
  year = {1996},
  month = nov,
  volume = {93},
  pages = {13268--13273},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.93.23.13268},
  abstract = {Focally evoked calcium waves in astrocyte cultures have been thought to propagate by gap-junctionmediated intercellular passage of chemical signal(s). In contrast to this mechanism we observed isolated astrocytes, which had no physical contact with other astrocytes in the culture, participating in a calcium wave. This observation requires an extracellular route of astrocyte signaling. To directly test for extracellular signaling we made cell-free lanes 10\textendash 300 ␮m wide in conf luent cultures by deleting astrocytes with a glass pipette. After 4\textendash 8 hr of recovery, regions of conf luent astrocytes separated by lanes devoid of cells were easily located. Electrical stimulation was used to initiate calcium waves. Waves crossed narrow ({$<$}120 ␮m) cell-free lanes in 15 of 36 cases, but failed to cross lanes wider than 120 ␮m in eight of eight cases. The probability of crossing narrow lanes was not correlated with the distance from the stimulation site, suggesting that cells along the path of the calcium wave release the extracellular messenger(s). Calculated velocity across the acellular lanes was not significantly different from velocity through regions of conf luent astrocytes. Focal superfusion altered both the extent and the direction of calcium waves in conf luent regions. These data indicate that extracellular signals may play a role in astrocyte\textendash astrocyte communication in situ.},
  file = {Hassinger et al. - 1996 - An extracellular signaling component in propagatio 2.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {23}
}

@article{Hasson2004,
  title = {Intersubject {{Synchronization}} of {{Cortical Activity During Natural Vision}}},
  author = {Hasson, U.},
  year = {2004},
  month = mar,
  volume = {303},
  pages = {1634--1640},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1089506},
  file = {2004 - Hasson et al. - Intersubject synchronization of cortical activity during natural vision.pdf},
  journal = {Science},
  language = {en},
  number = {5664}
}

@article{Hattori2019,
  title = {Area-{{Specificity}} and {{Plasticity}} of {{History}}-{{Dependent Value Coding During Learning}}},
  author = {Hattori, Ryoma and Danskin, Bethanny and Babic, Zeljana and Mlynaryk, Nicole and Komiyama, Takaki},
  year = {2019},
  month = jun,
  volume = {177},
  pages = {1858-1872.e15},
  issn = {00928674},
  doi = {10.1016/j.cell.2019.04.027},
  abstract = {Decision making is often driven by the subjective value of available options, a value which is formed through experience. To support this fundamental behavior, the brain must encode and maintain the subjective value. To investigate the area specificity and plasticity of value coding, we trained mice in a value-based decision task and imaged neural activity in 6 cortical areas with cellular resolution. History- and value-related signals were widespread across areas, but their strength and temporal patterns differed. In expert mice, the retrosplenial cortex (RSC) uniquely encoded history- and value-related signals with persistent population activity patterns across trials. This unique encoding of RSC emerged during task learning with a strong increase in more distant history signals. Acute inactivation of RSC selectively impaired the reward-history-based behavioral strategy. Our results indicate that RSC flexibly changes its history coding and persistently encodes valuerelated signals to support adaptive behaviors.},
  file = {Hattori et al. - 2019 - Area-Specificity and Plasticity of History-Depende.pdf},
  journal = {Cell},
  language = {en},
  number = {7}
}

@article{Hausler2003,
  title = {Perspectives of the High-Dimensional Dynamics of Neural Microcircuits from the Point of View of Low-Dimensional Readouts},
  author = {H{\"a}usler, Stefan and Markram, Henry and Maass, Wolfgang},
  year = {2003},
  month = mar,
  volume = {8},
  pages = {39--50},
  issn = {10762787},
  doi = {10.1002/cplx.10089},
  file = {2003 - Hausler, Markram, Maass - Perspectives of the high-dimensional dynamics of neural microcircuits from the point of view of low-dim.pdf},
  journal = {Complexity},
  language = {en},
  number = {4}
}

@article{Haxby2001,
  title = {Distributed and {{Overlapping Representations}} of {{Faces}} and {{Objects}} in {{Ventral Temporal Cortex}}},
  author = {Haxby, J. V.},
  year = {2001},
  month = sep,
  volume = {293},
  pages = {2425--2430},
  issn = {00368075, 10959203},
  doi = {10.1126/science.1063736},
  file = {2001 - Haxby et al. - Distributed and overlapping representations of faces and objects in ventral temporal cortex.pdf},
  journal = {Science},
  language = {en},
  number = {5539}
}

@article{Haxby2011,
  title = {A {{Common}}, {{High}}-{{Dimensional Model}} of the {{Representational Space}} in {{Human Ventral Temporal Cortex}}},
  author = {Haxby, James V. and Guntupalli, J. Swaroop and Connolly, Andrew C. and Halchenko, Yaroslav O. and Conroy, Bryan R. and Gobbini, M. Ida and Hanke, Michael and Ramadge, Peter J.},
  year = {2011},
  month = oct,
  volume = {72},
  pages = {404--416},
  issn = {08966273},
  doi = {10.1016/j.neuron.2011.08.026},
  abstract = {We present a high-dimensional model of the representational space in human ventral temporal (VT) cortex in which dimensions are response-tuning functions that are common across individuals and patterns of response are modeled as weighted sums of basis patterns associated with these response tunings. We map response-pattern vectors, measured with fMRI, from individual subjects' voxel spaces into this common model space using a new method, ``hyperalignment.'' Hyperalignment parameters based on responses during one experiment\textemdash movie viewing\textemdash identified 35 common response-tuning functions that captured fine-grained distinctions among a wide range of stimuli in the movie and in two category perception experiments. Between-subject classification (BSC, multivariate pattern classification based on other subjects' data) of response-pattern vectors in common model space greatly exceeded BSC of anatomically aligned responses and matched withinsubject classification. Results indicate that population codes for complex visual stimuli in VT cortex are based on response-tuning functions that are common across individuals.},
  file = {2011 - Haxby et al. - A common, high-dimensional model of the representational space in human ventral temporal cortex.pdf},
  journal = {Neuron},
  language = {en},
  number = {2}
}

@article{Haynes2005,
  title = {Predicting the Orientation of Invisible Stimuli from Activity in Human Primary Visual Cortex},
  author = {Haynes, John-Dylan and Rees, Geraint},
  year = {2005},
  month = may,
  volume = {8},
  pages = {686--691},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn1445},
  file = {2005 - Haynes, Rees - Predicting the orientation of invisible stimuli from activity in human primary visual cortex.pdf},
  journal = {Nature Neuroscience},
  language = {en},
  number = {5}
}

@article{Haynes2006,
  title = {Decoding Mental States from Brain Activity in Humans},
  author = {Haynes, John-Dylan and Rees, Geraint},
  year = {2006},
  month = jul,
  volume = {7},
  pages = {523--534},
  issn = {1471-003X, 1471-0048},
  doi = {10.1038/nrn1931},
  abstract = {Recent advances in human neuroimaging have shown that it is possible to accurately decode a person's conscious experience based only on non-invasive measurements of their brain activity. Such `brain reading' has mostly been studied in the domain of visual perception, where it helps reveal the way in which individual experiences are encoded in the human brain. The same approach can also be extended to other types of mental state, such as covert attitudes and lie detection. Such applications raise important ethical issues concerning the privacy of personal thought.},
  file = {2006 - Haynes, Rees - Decoding mental states from brain activity in humans.pdf},
  journal = {Nature Reviews Neuroscience},
  language = {en},
  number = {7}
}

@article{Hays2012,
  title = {High Activity and {{L\'evy}} Searches: Jellyfish Can Search the Water Column like Fish},
  shorttitle = {High Activity and {{L\'evy}} Searches},
  author = {Hays, Graeme C. and Bastian, Thomas and Doyle, Thomas K. and Fossette, Sabrina and Gleiss, Adrian C. and Gravenor, Michael B. and Hobson, Victoria J. and Humphries, Nicolas E. and Lilley, Martin K. S. and Pade, Nicolas G. and Sims, David W.},
  year = {2012},
  month = feb,
  volume = {279},
  pages = {465--473},
  issn = {0962-8452, 1471-2954},
  doi = {10.1098/rspb.2011.0978},
  abstract = {Over-fishing may lead to a decrease in fish abundance and a proliferation of jellyfish. Active movements and prey search might be thought to provide a competitive advantage for fish, but here we use data-loggers to show that the frequently occurring coastal jellyfish (               Rhizostoma octopus               ) does not simply passively drift to encounter prey. Jellyfish (327 days of data from 25 jellyfish with depth collected every 1 min) showed very dynamic vertical movements, with their integrated vertical movement averaging 619.2 m d               -1               , more than 60 times the water depth where they were tagged. The majority of movement patterns were best approximated by exponential models describing normal random walks. However, jellyfish also showed switching behaviour from exponential patterns to patterns best fitted by a truncated L\'evy distribution with exponents (mean               {$\mu$}               = 1.96, range 1.2\textendash 2.9) close to the theoretical optimum for searching for sparse prey (               {$\mu$}               opt               {$\approx$} 2.0). Complex movements in these `simple' animals may help jellyfish to compete effectively with fish for plankton prey, which may enhance their ability to increase in dominance in perturbed ocean systems.},
  file = {Hays et al. - 2012 - High activity and Lévy searches jellyfish can sea.pdf},
  journal = {Proc. R. Soc. B.},
  language = {en},
  number = {1728}
}

@techreport{He2019,
  title = {Co-{{Increasing Neuronal Noise}} and {{Beta Power}} in the {{Developing Brain}}},
  author = {He, Wei and Donoghue, Thomas and Sowman, Paul F and Seymour, Robert A and Brock, Jon and Crain, Stephen and Voytek, Bradley and Hillebrand, Arjan},
  year = {2019},
  month = nov,
  institution = {{Neuroscience}},
  doi = {10.1101/839258},
  abstract = {Accumulating evidence across species indicates that brain oscillations are superimposed upon an aperiodic 1/f - like power spectrum. Maturational changes in neuronal oscillations have not been assessed in tandem with this underlying aperiodic spectrum. The current study uncovers co-maturation of the aperiodic component alongside the periodic components (oscillations) in spontaneous magnetoencephalography (MEG) data. Beamformer-reconstructed MEG time-series allowed a direct comparison of power in the source domain between 24 children (8.0 {$\pm$} 2.5 years, 17 males) and 24 adults (40.6 {$\pm$} 17.4 years, 16 males). Our results suggest that the redistribution of oscillatory power from lower to higher frequencies that is observed in childhood does not hold once the age-related changes in the aperiodic signal are controlled for. When estimating both the periodic and aperiodic components, we found that power increases with age in the beta band only, and that the 1/f signal is flattened in adults compared to children. These results suggest a pattern of co-maturing beta oscillatory power with the aperiodic 1/f signal in typical childhood development.},
  file = {He et al. - 2019 - Co-Increasing Neuronal Noise and Beta Power in the.pdf},
  language = {en},
  type = {Preprint}
}

@article{Hebbink,
  title = {Activity Types in a Neural Mass Model},
  author = {Hebbink, Jurgen},
  pages = {50},
  file = {../../Zotero/storage/HJHQ5CAE/1995 - Gray et al. - A perception reveals the face of sex.pdf;2014 - Hebbink - Activity types in a neural mass model.pdf;Hebbink - Activity types in a neural mass model.pdf},
  language = {en}
}

@article{Heeger1992,
  title = {Half-Squaring in Responses of Cat Striate Cells},
  author = {Heeger, David J.},
  year = {1992},
  month = nov,
  volume = {9},
  pages = {427--443},
  issn = {0952-5238, 1469-8714},
  doi = {10.1017/S095252380001124X},
  abstract = {Simple cells in striate cortex have been depicted as rectified linear operators, and complex cells have been depicted as energy mechanisms (constructed from the squared sums of linear operator outputs). This paper discusses two essential hypotheses of the linear/energy model: (1) that a cell's selectivity is due to an underlying (spatiotemporal and binocular) linear stage; and (2) that a cell's firing rate depends on the squared output of the underlying linear stage. This paper reviews physiological measurements of cat striate cell responses, and concludes that both of these hypotheses are supported by the data.},
  file = {1992 - Heeger - Half-squaring in responses of cat striate cells.pdf},
  journal = {Visual Neuroscience},
  language = {en},
  number = {05}
}

@article{Heeger2017,
  title = {Theory of Cortical Function},
  author = {Heeger, David J.},
  year = {2017},
  month = feb,
  volume = {114},
  pages = {1773--1782},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1619788114},
  file = {Heeger - 2017 - Theory of cortical function.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {8}
}

@article{Heekeren2004,
  title = {A General Mechanism for Perceptual Decision-Making in the Human Brain},
  author = {Heekeren, H. R. and Marrett, S. and Bandettini, P. A. and Ungerleider, L. G.},
  year = {2004},
  month = oct,
  volume = {431},
  pages = {859--862},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature02966},
  file = {2004 - Heekeren et al. - A general mechanism for perceptual decision-making in the human brain.pdf},
  journal = {Nature},
  language = {en},
  number = {7010}
}

@article{Hein2016,
  title = {Natural Search Algorithms as a Bridge between Organisms, Evolution, and Ecology},
  author = {Hein, Andrew M. and Carrara, Francesco and Brumley, Douglas R. and Stocker, Roman and Levin, Simon A.},
  year = {2016},
  month = aug,
  volume = {113},
  pages = {9413--9420},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1606195113},
  abstract = {The ability to navigate is a hallmark of living systems, from single cells to higher animals. Searching for targets, such as food or mates in particular, is one of the fundamental navigational tasks many organisms must execute to survive and reproduce. Here, we argue that a recent surge of studies of the proximate mechanisms that underlie search behavior offers a new opportunity to integrate the biophysics and neuroscience of sensory systems with ecological and evolutionary processes, closing a feedback loop that promises exciting new avenues of scientific exploration at the frontier of systems biology.},
  file = {Hein et al. - 2016 - Natural search algorithms as a bridge between orga.pdf},
  journal = {Proc Natl Acad Sci USA},
  language = {en},
  number = {34}
}

@article{Helfrich2014,
  title = {Selective {{Modulation}} of {{Interhemispheric Functional Connectivity}} by {{HD}}-{{tACS Shapes Perception}}},
  author = {Helfrich, Randolph F. and Knepper, Hannah and Nolte, Guido and Str{\"u}ber, Daniel and Rach, Stefan and Herrmann, Christoph S. and Schneider, Till R. and Engel, Andreas K.},
  editor = {Jensen, Ole},
  year = {2014},
  month = dec,
  volume = {12},
  pages = {e1002031},
  issn = {1545-7885},
  doi = {10.1371/journal.pbio.1002031},
  abstract = {Oscillatory neuronal synchronization between cortical areas has been suggested to constitute a flexible mechanism to coordinate information flow in the human cerebral cortex. However, it remains unclear whether synchronized neuronal activity merely represents an epiphenomenon or whether it is causally involved in the selective gating of information. Here, we combined bilateral high-density transcranial alternating current stimulation (HD-tACS) at 40 Hz with simultaneous electroencephalographic (EEG) recordings to study immediate electrophysiological effects during the selective entrainment of oscillatory gamma-band signatures. We found that interhemispheric functional connectivity was modulated in a predictable, phase-specific way: In-phase stimulation enhanced synchronization, anti-phase stimulation impaired functional coupling. Perceptual correlates of these connectivity changes were found in an ambiguous motion task, which strongly support the functional relevance of long-range neuronal coupling. Additionally, our results revealed a decrease in oscillatory alpha power in response to the entrainment of gamma band signatures. This finding provides causal evidence for the antagonistic role of alpha and gamma oscillations in the parieto-occipital cortex and confirms that the observed gamma band modulations were physiological in nature. Our results demonstrate that synchronized cortical network activity across several spatiotemporal scales is essential for conscious perception and cognition.},
  file = {Helfrich et al. - 2014 - Selective Modulation of Interhemispheric Functiona.pdf},
  journal = {PLoS Biology},
  language = {en},
  number = {12}
}

@article{Helfrich2018,
  title = {Neural {{Mechanisms}} of {{Sustained Attention Are Rhythmic}}},
  author = {Helfrich, Randolph F. and Fiebelkorn, Ian C. and Szczepanski, Sara M. and Lin, Jack J. and Parvizi, Josef and Knight, Robert T. and Kastner, Sabine},
  year = {2018},
  month = aug,
  volume = {99},
  pages = {854-865.e5},
  issn = {08966273},
  doi = {10.1016/j.neuron.2018.07.032},
  file = {Helfrich et al. - 2018 - Neural Mechanisms of Sustained Attention Are Rhyth.pdf},
  journal = {Neuron},
  language = {en},
  number = {4}
}

@article{Helmstaedter2007,
  title = {Reconstruction of an Average Cortical Column in Silico},
  author = {Helmstaedter, M. and {de Kock}, C.P.J. and Feldmeyer, D. and Bruno, R.M. and Sakmann, B.},
  year = {2007},
  month = oct,
  volume = {55},
  pages = {193--203},
  issn = {01650173},
  doi = {10.1016/j.brainresrev.2007.07.011},
  abstract = {The characterization of individual neurons by Golgi and Cajal has been the basis of neuroanatomy for a century. A new challenge is to anatomically describe, at cellular resolution, complete local circuits that can drive behavior. In this essay, we review the possibilities to obtain a model cortical column by using in vitro and in vivo pair recordings, followed by anatomical reconstructions of the projecting and target cells. These pairs establish connection modules that eventually may be useful to synthesize an average cortical column in silico. Together with data on sensory evoked neuronal activity measured in vivo, this will allow to model the anatomical and functional cellular basis of behavior based on more realistic assumptions than previously attempted.},
  file = {2007 - Helmstaedter et al. - Reconstruction of an average cortical column in silico.pdf},
  journal = {Brain Research Reviews},
  language = {en},
  number = {2}
}

@article{Henderson,
  title = {Deep {{Reinforcement Learning}} That {{Matters}}},
  author = {Henderson, Peter and Islam, Riashat and Bachman, Philip and Pineau, Joelle and Precup, Doina and Meger, David},
  pages = {26},
  abstract = {In recent years, significant progress has been made in solving challenging problems across various domains using deep reinforcement learning (RL). Reproducing existing work and accurately judging the improvements offered by novel methods is vital to sustaining this progress. Unfortunately, reproducing results for state-of-the-art deep RL methods is seldom straightforward. In particular, non-determinism in standard benchmark environments, combined with variance intrinsic to the methods, can make reported results tough to interpret. Without significance metrics and tighter standardization of experimental reporting, it is difficult to determine whether improvements over the prior state-of-the-art are meaningful. In this paper, we investigate challenges posed by reproducibility, proper experimental techniques, and reporting procedures. We illustrate the variability in reported metrics and results when comparing against common baselines and suggest guidelines to make future results in deep RL more reproducible. We aim to spur discussion about how to ensure continued progress in the field by minimizing wasted effort stemming from results that are non-reproducible and easily misinterpreted.},
  file = {Henderson et al. - Deep Reinforcement Learning that Matters.pdf},
  language = {en}
}

@article{Hendricks2015,
  title = {Neuroecology: {{Tuning Foraging Strategies}} to {{Environmental Variability}}},
  shorttitle = {Neuroecology},
  author = {Hendricks, Michael},
  year = {2015},
  month = jun,
  volume = {25},
  pages = {R498-R500},
  issn = {09609822},
  doi = {10.1016/j.cub.2015.04.042},
  file = {Hendricks - 2015 - Neuroecology Tuning Foraging Strategies to Enviro.pdf},
  journal = {Current Biology},
  language = {en},
  number = {12}
}

@article{Hennequin,
  title = {Characterizing Variability in Nonlinear, Recurrent Neuronal Networks},
  author = {Hennequin, Guillaume and Lengyel, Mate},
  pages = {17},
  abstract = {In this note, we develop semi-analytical techniques to obtain the full correlational structure of a stochastic network of nonlinear neurons described by rate variables. Under the assumption that pairs of membrane potentials are jointly Gaussian \textendash{} which they tend to be in large networks \textendash{} we obtain deterministic equations for the temporal evolution of the mean firing rates and the noise covariance matrix that can be solved straightforwardly given the network connectivity. We also obtain spike count statistics such as Fano factors and pairwise correlations, assuming doubly-stochastic action potential firing. Importantly, our theory does not require fluctuations to be small, and works for several biologically motivated, convex single-neuron nonlinearities.},
  file = {Hennequin and Lengyel - Characterizing variability in nonlinear, recurrent.pdf},
  language = {en}
}

@article{Hennequina,
  title = {Fast {{Sampling}}-{{Based Inference}} in {{Balanced Neuronal Networks}}},
  author = {Hennequin, Guillaume and Aitchison, Laurence and Lengyel, Mate},
  pages = {9},
  abstract = {Multiple lines of evidence support the notion that the brain performs probabilistic inference in multiple cognitive domains, including perception and decision making. There is also evidence that probabilistic inference may be implemented in the brain through the (quasi-)stochastic activity of neural circuits, producing samples from the appropriate posterior distributions, effectively implementing a Markov chain Monte Carlo algorithm. However, time becomes a fundamental bottleneck in such sampling-based probabilistic representations: the quality of inferences depends on how fast the neural circuit generates new, uncorrelated samples from its stationary distribution (the posterior). We explore this bottleneck in a simple, linear-Gaussian latent variable model, in which posterior sampling can be achieved by stochastic neural networks with linear dynamics. The well-known Langevin sampling (LS) recipe, so far the only sampling algorithm for continuous variables of which a neural implementation has been suggested, naturally fits into this dynamical framework. However, we first show analytically and through simulations that the symmetry of the synaptic weight matrix implied by LS yields critically slow mixing when the posterior is high-dimensional. Next, using methods from control theory, we construct and inspect networks that are optimally fast, and hence orders of magnitude faster than LS, while being far more biologically plausible. In these networks, strong \textendash{} but transient \textendash{} selective amplification of external noise generates the spatially correlated activity fluctuations prescribed by the posterior. Intriguingly, although a detailed balance of excitation and inhibition is dynamically maintained, detailed balance of Markov chain steps in the resulting sampler is violated, consistent with recent findings on how statistical irreversibility can overcome the speed limitation of random walks in other domains.},
  file = {2014 - Hennequin, Aitchison, Lengyel - Fast Sampling-Based Inference in Balanced Neuronal Networks.pdf;Hennequin et al. - Fast Sampling-Based Inference in Balanced Neuronal.pdf},
  language = {en}
}

@article{HerbertRobbins1952,
  title = {Some Aspects of the Sequential Design of Experiments},
  author = {{Herbert Robbins}},
  year = {1952},
  month = sep,
  volume = {58},
  pages = {527--535},
  journal = {Bulletin of the American Mathematical Society},
  number = {5}
}

@article{Hermes2015,
  title = {Stimulus {{Dependence}} of {{Gamma Oscillations}} in {{Human Visual Cortex}}},
  author = {Hermes, D. and Miller, K.J. and Wandell, B.A. and Winawer, J.},
  year = {2015},
  month = sep,
  volume = {25},
  pages = {2951--2959},
  issn = {1047-3211, 1460-2199},
  doi = {10.1093/cercor/bhu091},
  abstract = {A striking feature of some field potential recordings in visual cortex is a rhythmic oscillation within the gamma band (30\textendash 80 Hz). These oscillations have been proposed to underlie computations in perception, attention, and information transmission. Recent studies of cortical field potentials, including human electrocorticography (ECoG), have emphasized another signal within the gamma band, a nonoscillatory, broadband signal, spanning 80\textendash 200 Hz. It remains unclear under what conditions gamma oscillations are elicited in visual cortex, whether they are necessary and ubiquitous in visual encoding, and what relationship they have to nonoscillatory, broadband field potentials. We demonstrate that ECoG responses in human visual cortex (V1/V2/V3) can include robust narrowband gamma oscillations, and that these oscillations are reliably elicited by some spatial contrast patterns (luminance gratings) but not by others (noise patterns and many natural images). The gamma oscillations can be conspicuous and robust, but because they are absent for many stimuli, which observers can see and recognize, the oscillations are not necessary for seeing. In contrast, all visual stimuli induced broadband spectral changes in ECoG responses. Asynchronous neural signals in visual cortex, reflected in the broadband ECoG response, can support transmission of information for perception and recognition in the absence of pronounced gamma oscillations.},
  file = {Hermes et al. - 2015 - Stimulus Dependence of Gamma Oscillations in Human.pdf},
  journal = {Cerebral Cortex},
  language = {en},
  number = {9}
}

@article{Hermes2015a,
  title = {Gamma Oscillations in Visual Cortex: The Stimulus Matters},
  shorttitle = {Gamma Oscillations in Visual Cortex},
  author = {Hermes, Dora and Miller, Kai J. and Wandell, Brian A. and Winawer, Jonathan},
  year = {2015},
  month = feb,
  volume = {19},
  pages = {57--58},
  issn = {13646613},
  doi = {10.1016/j.tics.2014.12.009},
  file = {Hermes et al. - 2015 - Gamma oscillations in visual cortex the stimulus .pdf},
  journal = {Trends in Cognitive Sciences},
  language = {en},
  number = {2}
}

@article{Hermundstad2011,
  title = {Learning, {{Memory}}, and the {{Role}} of {{Neural Network Architecture}}},
  author = {Hermundstad, Ann M. and Brown, Kevin S. and Bassett, Danielle S. and Carlson, Jean M.},
  editor = {Sporns, Olaf},
  year = {2011},
  month = jun,
  volume = {7},
  pages = {e1002063},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1002063},
  abstract = {The performance of information processing systems, from artificial neural networks to natural neuronal ensembles, depends heavily on the underlying system architecture. In this study, we compare the performance of parallel and layered network architectures during sequential tasks that require both acquisition and retention of information, thereby identifying tradeoffs between learning and memory processes. During the task of supervised, sequential function approximation, networks produce and adapt representations of external information. Performance is evaluated by statistically analyzing the error in these representations while varying the initial network state, the structure of the external information, and the time given to learn the information. We link performance to complexity in network architecture by characterizing local error landscape curvature. We find that variations in error landscape structure give rise to tradeoffs in performance; these include the ability of the network to maximize accuracy versus minimize inaccuracy and produce specific versus generalizable representations of information. Parallel networks generate smooth error landscapes with deep, narrow minima, enabling them to find highly specific representations given sufficient time. While accurate, however, these representations are difficult to generalize. In contrast, layered networks generate rough error landscapes with a variety of local minima, allowing them to quickly find coarse representations. Although less accurate, these representations are easily adaptable. The presence of measurable performance tradeoffs in both layered and parallel networks has implications for understanding the behavior of a wide variety of natural and artificial learning systems.},
  file = {Hermundstad et al. - 2011 - Learning, Memory, and the Role of Neural Network A.PDF},
  journal = {PLoS Comput Biol},
  language = {en},
  number = {6}
}

@article{Herrera2020,
  title = {A {{Minimal Biophysical Model}} of {{Neocortical Pyramidal Cells}}: {{Implications}} for {{Frontal Cortex Microcircuitry}} and {{Field Potential Generation}}},
  shorttitle = {A {{Minimal Biophysical Model}} of {{Neocortical Pyramidal Cells}}},
  author = {Herrera, Beatriz and Sajad, Amirsaman and Woodman, Geoffrey F. and Schall, Jeffrey D. and Riera, Jorge J.},
  year = {2020},
  month = oct,
  volume = {40},
  pages = {8513--8529},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.0221-20.2020},
  file = {Herrera et al. - 2020 - A Minimal Biophysical Model of Neocortical Pyramid.pdf},
  journal = {J. Neurosci.},
  language = {en},
  number = {44}
}

@article{HertA$g2014,
  title = {Analytical Approximations of the Firing Rate of an Adaptive Exponential Integrate-and-Fire Neuron in the Presence of Synaptic Noise},
  author = {Hert{\~A}{\textcurrency}g, Loreen and Durstewitz, Daniel and Brunel, Nicolas},
  year = {2014},
  month = sep,
  volume = {8},
  issn = {1662-5188},
  doi = {10.3389/fncom.2014.00116},
  file = {HertÃ¤g et al. - 2014 - Analytical approximations of the firing rate of an.pdf},
  journal = {Frontiers in Computational Neuroscience},
  language = {en}
}

@article{Hertag2012,
  title = {An {{Approximation}} to the {{Adaptive Exponential Integrate}}-and-{{Fire Neuron Model Allows Fast}} and {{Predictive Fitting}} to {{Physiological Data}}},
  author = {Hert{\"a}g, Loreen and Hass, Joachim and Golovko, Tatiana and Durstewitz, Daniel},
  year = {2012},
  volume = {6},
  issn = {1662-5188},
  doi = {10.3389/fncom.2012.00062},
  file = {2012 - Hertäg et al. - An Approximation to the Adaptive Exponential Integrate-and-Fire Neuron Model Allows Fast and Predictive Fitting.pdf},
  journal = {Frontiers in Computational Neuroscience},
  language = {en}
}

@article{Hessel,
  title = {Rainbow: {{Combining Improvements}} in {{Deep Reinforcement Learning}}},
  author = {Hessel, Matteo and Modayil, Joseph and {van Hasselt}, Hado and Schaul, Tom and Ostrovski, Georg},
  pages = {14},
  abstract = {The deep reinforcement learning community has made several independent improvements to the DQN algorithm. However, it is unclear which of these extensions are complementary and can be fruitfully combined. This paper examines six extensions to the DQN algorithm and empirically studies their combination. Our experiments show that the combination provides state-of-the-art performance on the Atari 2600 benchmark, both in terms of data efficiency and final performance. We also provide results from a detailed ablation study that shows the contribution of each component to overall performance.},
  file = {Hessel et al. - Rainbow Combining Improvements in Deep Reinforcem.pdf},
  language = {en}
}

@techreport{Heusser2018,
  title = {Geometric Models Reveal Behavioral and Neural Signatures of Transforming Naturalistic Experiences into Episodic Memories},
  author = {Heusser, Andrew C. and Fitzpatrick, Paxton C. and Manning, Jeremy R.},
  year = {2018},
  month = sep,
  institution = {{Neuroscience}},
  doi = {10.1101/409987},
  abstract = {Abstract                        The mental contexts in which we interpret experiences are often person-specific, even when the experiences themselves are shared. We developed a geometric framework for mathematically characterizing the subjective conceptual content of dynamic naturalistic experiences. We model experiences and memories as             trajectories             through word embedding spaces whose coordinates reflect the universe of thoughts under consideration. Memory encoding can then be modeled as geometrically preserving or distorting the             shape             of the original experience. We applied our approach to data collected as participants watched and verbally recounted a television episode while undergoing functional neuroimaging. Participants' recountings all preserved coarse spatial properties (essential narrative elements), but not fine spatial scale (low-level) details, of the episode's trajectory. We also identified networks of brain structures sensitive to these trajectory shapes. Our work provides insights into how we preserve and distort our ongoing experiences when we encode them into episodic memories.},
  file = {Heusser et al. - 2018 - Geometric models reveal behavioral and neural sign.pdf},
  language = {en},
  type = {Preprint}
}

@article{Hewitt1994,
  title = {A Computer Model of Amplitude-modulation Sensitivity of Single Units in the Inferior Colliculus},
  author = {Hewitt, Michael J. and Meddis, Ray},
  year = {1994},
  month = apr,
  volume = {95},
  pages = {2145--2159},
  issn = {0001-4966},
  doi = {10.1121/1.408676},
  file = {1994 - Hewitt, Meddis - A computer model of amplitude‐modulation sensitivity of single units in the inferior colliculus.pdf},
  journal = {The Journal of the Acoustical Society of America},
  language = {en},
  number = {4}
}

@article{Higgins2014,
  title = {Memory {{Maintenance}} in {{Synapses}} with {{Calcium}}-{{Based Plasticity}} in the {{Presence}} of {{Background Activity}}},
  author = {Higgins, David and Graupner, Michael and Brunel, Nicolas},
  editor = {Latham, Peter E.},
  year = {2014},
  month = oct,
  volume = {10},
  pages = {e1003834},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1003834},
  abstract = {Most models of learning and memory assume that memories are maintained in neuronal circuits by persistent synaptic modifications induced by specific patterns of pre- and postsynaptic activity. For this scenario to be viable, synaptic modifications must survive the ubiquitous ongoing activity present in neural circuits in vivo. In this paper, we investigate the time scales of memory maintenance in a calcium-based synaptic plasticity model that has been shown recently to be able to fit different experimental data-sets from hippocampal and neocortical preparations. We find that in the presence of background activity on the order of 1 Hz parameters that fit pyramidal layer 5 neocortical data lead to a very fast decay of synaptic efficacy, with time scales of minutes. We then identify two ways in which this memory time scale can be extended: (i) the extracellular calcium concentration in the experiments used to fit the model are larger than estimated concentrations in vivo. Lowering extracellular calcium concentration to in vivo levels leads to an increase in memory time scales of several orders of magnitude; (ii) adding a bistability mechanism so that each synapse has two stable states at sufficiently low background activity leads to a further boost in memory time scale, since memory decay is no longer described by an exponential decay from an initial state, but by an escape from a potential well. We argue that both features are expected to be present in synapses in vivo. These results are obtained first in a single synapse connecting two independent Poisson neurons, and then in simulations of a large network of excitatory and inhibitory integrate-and-fire neurons. Our results emphasise the need for studying plasticity at physiological extracellular calcium concentration, and highlight the role of synaptic bi- or multistability in the stability of learned synaptic structures.},
  file = {2014 - Higgins, Graupner, Brunel - Memory maintenance in synapses with calcium-based plasticity in the presence of background activity.pdf;Higgins et al. - 2014 - Memory Maintenance in Synapses with Calcium-Based .pdf},
  journal = {PLoS Computational Biology},
  language = {en},
  number = {10}
}

@article{Hill1985,
  title = {Towards a Model of Boredom},
  author = {Hill, A. B. and Perkins, R. E.},
  year = {1985},
  month = may,
  volume = {76},
  pages = {235--240},
  issn = {00071269},
  doi = {10.1111/j.2044-8295.1985.tb01947.x},
  file = {Hill and Perkins - 1985 - Towards a model of boredom.pdf},
  journal = {British Journal of Psychology},
  language = {en},
  number = {2}
}

@article{Hills,
  title = {Optimal {{Foraging}} in {{Semantic Memory}}},
  author = {Hills, Thomas T},
  pages = {7},
  abstract = {When searching for items in memory, people explore internal representations in much the same way that animals forage in space. Results from a number of fields support this notion at a deeper level of evolutionary homology, with evidence that goal-directed cognition is an evolutionary descendent of animal foraging behavior (Hills, 2006). Is it possible then that humans forage in memory using similar search policies to the way that animals forage in space? To investigate this, we examine how people retrieve items from memory in the category fluency task: Participants were asked to retrieve as many types of animals from memory as they could in 3 minutes. Clusters or patches of these items, along with their semantic similarity and frequency, were found with an automatic Wikipedia corpus analysis using the BEAGLE semantic memory model (Jones \& Mewhort, 2007), and via hand-coded category membership from Troyer et al. (1997). Participants did not seem to use static patch boundaries, such as `pets', to search memory, but instead used fluid patch boundaries that were updated with each new item retrieved. We found that participants leave patches in memory when the marginal (i.e., current) rate of finding items is near the average rate for the entire task, as predicted by optimal foraging theory. Furthermore, participants appear to search within patches using item similarity, but decide where to ``land'' when moving between patches using item frequency.},
  file = {Hills - Optimal Foraging in Semantic Memory.pdf},
  language = {en}
}

@article{Hills2013,
  title = {Adaptive {{L\'evy Processes}} and {{Area}}-{{Restricted Search}} in {{Human Foraging}}},
  author = {Hills, Thomas T. and Kalff, Christopher and Wiener, Jan M.},
  editor = {Moreno, Yamir},
  year = {2013},
  month = apr,
  volume = {8},
  pages = {e60488},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0060488},
  abstract = {A considerable amount of research has claimed that animals' foraging behaviors display movement lengths with power-law distributed tails, characteristic of Le\textasciiacute vy flights and Le\textasciiacute vy walks. Though these claims have recently come into question, the proposal that many animals forage using Le\textasciiacute vy processes nonetheless remains. A Le\textasciiacute vy process does not consider when or where resources are encountered, and samples movement lengths independently of past experience. However, Le\textasciiacute vy processes too have come into question based on the observation that in patchy resource environments resource-sensitive foraging strategies, like area-restricted search, perform better than Le\textasciiacute vy flights yet can still generate heavy-tailed distributions of movement lengths. To investigate these questions further, we tracked humans as they searched for hidden resources in an open-field virtual environment, with either patchy or dispersed resource distributions. Supporting previous research, for both conditions logarithmic binning methods were consistent with Le\textasciiacute vy flights and rank-frequency methods\textendash comparing alternative distributions using maximum likelihood methods\textendash showed the strongest support for bounded power-law distributions (truncated Le\textasciiacute vy flights). However, goodness-of-fit tests found that even bounded power-law distributions only accurately characterized movement behavior for 4 (out of 32) participants. Moreover, paths in the patchy environment (but not the dispersed environment) showed a transition to intensive search following resource encounters, characteristic of area-restricted search. Transferring paths between environments revealed that paths generated in the patchy environment were adapted to that environment. Our results suggest that though power-law distributions do not accurately reflect human search, Le\textasciiacute vy processes may still describe movement in dispersed environments, but not in patchy environments\textendash where search was area-restricted. Furthermore, our results indicate that search strategies cannot be inferred without knowing how organisms respond to resources\textendash as both patched and dispersed conditions led to similar Le\textasciiacute vy-like movement distributions.},
  file = {Hills et al. - 2013 - Adaptive Lévy Processes and Area-Restricted Search.pdf},
  journal = {PLoS ONE},
  language = {en},
  number = {4}
}

@article{Hinde1956,
  title = {Ethological {{Models}} and the {{Concept}} of '{{Drive}}'},
  author = {Hinde, R. A.},
  year = {1956},
  volume = {6},
  pages = {321--331},
  file = {Hinde - 1956 - Ethological Models and the Concept of 'Drive'.pdf},
  journal = {The British Journal for the Philosophy of Science},
  language = {en},
  number = {24}
}

@article{Hirase2004,
  title = {Calcium {{Dynamics}} of {{Cortical Astrocytic Networks In Vivo}}},
  author = {Hirase, Hajime and Qian, Lifen and Barth{\'o}, Peter and Buzs{\'a}ki, Gy{\"o}rgy},
  editor = {{Winfred Denk}},
  year = {2004},
  month = apr,
  volume = {2},
  pages = {e96},
  issn = {1545-7885},
  doi = {10.1371/journal.pbio.0020096},
  file = {Hirase et al. - 2004 - Calcium Dynamics of Cortical Astrocytic Networks I.PDF},
  journal = {PLoS Biol},
  language = {en},
  number = {4}
}

@article{Histed2017,
  title = {Feedforward Inhibition Allows Input Summation to Vary in Recurrent Cortical Networks},
  author = {Histed, Mark H},
  year = {2017},
  month = jun,
  doi = {10.1101/109736},
  abstract = {Brain computations depend on how neurons transform inputs to spike outputs. Here, to understand input-output transformations in cortical networks, we recorded spiking responses from visual cortex (V1) of awake mice of either sex while pairing sensory stimuli with optogenetic perturbation of excitatory and parvalbumin-positive inhibitory neurons.  We found V1 neurons' average responses were primarily additive (linear). We used a recurrent cortical network model to determine if these data, as well as past observations of nonlinearity, could be described by a common circuit architecture.  The model showed cortical input-output transformations can be changed from linear to sublinear with moderate (\textasciitilde 20\%) strengthening of connections between inhibitory neurons, but this change depends on the presence of feedforward inhibition.  Thus, feedforward inhibition, a common feature of cortical circuitry, enables networks to flexibly change their spiking responses via changes in recurrent connectivity.},
  file = {Histed - 2017 - Feedforward inhibition allows input summation to v.pdf},
  journal = {bioRxiv},
  language = {en}
}

@article{Ho2007,
  title = {Self-Tuning Experience Weighted Attraction Learning in Games},
  author = {Ho, Teck H. and Camerer, Colin F. and Chong, Juin-Kuan},
  year = {2007},
  month = mar,
  volume = {133},
  pages = {177--198},
  issn = {00220531},
  doi = {10.1016/j.jet.2005.12.008},
  file = {2007 - Ho, Camerer, Chong - Self-tuning Experience-Weighted Attraction Learning in Games.pdf},
  journal = {Journal of Economic Theory},
  language = {en},
  number = {1}
}

@article{Hocker2019,
  title = {Myopic Control of Neural Dynamics},
  author = {Hocker, David and Park, Il Memming},
  year = {2019},
  volume = {15},
  pages = {24},
  abstract = {Manipulating the dynamics of neural systems through targeted stimulation is a frontier of research and clinical neuroscience; however, the control schemes considered for neural systems are mismatched for the unique needs of manipulating neural dynamics. An appropriate control method should respect the variability in neural systems, incorporating moment to moment ``input'' to the neural dynamics and behaving based on the current neural state, irrespective of the past trajectory. We propose such a controller under a nonlinear statespace feedback framework that steers one dynamical system to function as through it were another dynamical system entirely. This ``myopic'' controller is formulated through a novel variant of a model reference control cost that manipulates dynamics in a short-sighted manner that only sets a target trajectory of a single time step into the future (hence its myopic nature), which omits the need to pre-calculate a rigid and computationally costly neural feedback control solution. To demonstrate the breadth of this control's utility, two examples with distinctly different applications in neuroscience are studied. First, we show the myopic control's utility to probe the causal link between dynamics and behavior for cognitive processes by transforming a winner-take-all decision-making system to operate as a robust neural integrator of evidence. Second, an unhealthy motor-like system containing an unwanted betaoscillation spiral attractor is controlled to function as a healthy motor system, a relevant clinical example for neurological disorders.},
  file = {Hocker and Park - Myopic control of neural dynamics.pdf},
  journal = {PLOS Computational Biology},
  language = {en},
  number = {3}
}

@article{Hodgkin1952,
  title = {A Quantitative Description of Membrane Current and Its Application to Conduction and Excitation in Nerve},
  author = {Hodgkin, A. L. and Huxley, A. F.},
  year = {1952},
  volume = {117},
  pages = {500--544},
  file = {jphysiol01442-0106.pdf},
  journal = {J Physiol},
  number = {4}
}

@article{Hofbauer,
  title = {{{EVOLUTIONARY GAME DYNAMICS}}},
  author = {Hofbauer, Josef and Sigmund, Karl},
  pages = {41},
  abstract = {Evolutionary game dynamics is the application of population dynamical methods to game theory. It has been introduced by evolutionary biologists, anticipated in part by classical game theorists. In this survey, we present an overview of the many brands of deterministic dynamical systems motivated by evolutionary game theory, including ordinary differential equations (and, in particular, the replicator equation), differential inclusions (the best response dynamics), difference equations (as, for instance, fictitious play) and reaction-diffusion systems. A recurrent theme (the so-called `folk theorem of evolutionary game theory') is the close connection of the dynamical approach with the Nash equilibrium, but we show that a static, equilibriumbased viewpoint is, on principle, unable to always account for the long-term behaviour of players adjusting their behaviour to maximise their payoff.},
  file = {2003 - Hofbauer, Sigmund - Evolutionary Game Dynamics.pdf},
  language = {en}
}

@article{Hofer2002,
  title = {Control and {{Plasticity}} of {{Intercellular Calcium Waves}} in {{Astrocytes}}: {{A Modeling Approach}}},
  shorttitle = {Control and {{Plasticity}} of {{Intercellular Calcium Waves}} in {{Astrocytes}}},
  author = {H{\"o}fer, Thomas and Venance, Laurent and Giaume, Christian},
  year = {2002},
  month = jun,
  volume = {22},
  pages = {4850--4859},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.22-12-04850.2002},
  file = {Höfer et al. - 2002 - Control and Plasticity of Intercellular Calcium Wa.pdf},
  journal = {J. Neurosci.},
  language = {en},
  number = {12}
}

@article{Holgado2010,
  title = {Conditions for the {{Generation}} of {{Beta Oscillations}} in the {{Subthalamic Nucleus}}-{{Globus Pallidus Network}}},
  author = {Holgado, A. J. N. and Terry, J. R. and Bogacz, R.},
  year = {2010},
  month = sep,
  volume = {30},
  pages = {12340--12352},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.0817-10.2010},
  file = {2010 - Holgado, Terry, Bogacz - Conditions for the Generation of Beta Oscillations in the Subthalamic Nucleus-Globus Pallidus Network.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {37}
}

@techreport{Holler-Rickauer2019a,
  title = {Structure and Function of a Neocortical Synapse},
  author = {{Holler-Rickauer}, Simone and K{\"o}stinger, German and Martin, Kevan A.C. and Schuhknecht, Gregor F.P. and Stratford, Ken J.},
  year = {2019},
  month = dec,
  institution = {{Neuroscience}},
  doi = {10.1101/2019.12.13.875971},
  abstract = {Thirty-four years since the small nervous system of the nematode             C. elegans             was manually reconstructed in the electron microscope (EM)             1             , `high-throughput' EM techniques now enable the dense reconstruction of neural circuits within increasingly large brain volumes at synaptic resolution             2\textendash 6             . As with             C. elegans             , however, a key limitation for inferring brain function from neuronal wiring diagrams is that it remains unknown how the structure of a synapse seen in EM relates to its physiological transmission strength. Here, we related structure and function of the same synapses to bridge this gap: we combined paired whole-cell recordings of synaptically connected pyramidal neurons in slices of mouse somatosensory cortex with correlated light microscopy and high-resolution EM of all putative synaptic contacts between the neurons. We discovered a linear relationship between synapse size (postsynaptic density area) and synapse strength (excitatory postsynaptic potential amplitude), which provides an experimental foundation for assigning the actual physiological weights to synaptic connections seen in the EM. Furthermore, quantal analysis revealed that the number of vesicle release sites exceeded the number of anatomical synapses formed by a connection by a factor of at least 2.6, which challenges the current understanding of synaptic release in neocortex and suggests that neocortical synapses operate with multivesicular release, like hippocampal synapses             7\textendash 11             . Thus, neocortical synapses are more complex computational devices and may modulate their strength more flexibly than previously thought, with the corollary that the canonical neocortical microcircuitry possesses significantly higher computational power than estimated by current models.},
  file = {Holler-Rickauer et al. - 2019 - Structure and function of a neocortical synapse 2.pdf},
  language = {en},
  type = {Preprint}
}

@article{Holmgren2001,
  title = {Coincident {{Spiking Activity Induces Long}}-{{Term Changes}} in {{Inhibition}} of {{Neocortical Pyramidal Cells}}},
  author = {Holmgren, Carl D. and Zilberter, Yuri},
  year = {2001},
  month = oct,
  volume = {21},
  pages = {8270--8277},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.21-20-08270.2001},
  file = {Holmgren and Zilberter - 2001 - Coincident Spiking Activity Induces Long-Term Chan.pdf},
  journal = {J. Neurosci.},
  language = {en},
  number = {20}
}

@article{Holt1996,
  title = {Comparison of Discharge Variability in Vitro and in Vivo in Cat Visual Cortex Neurons},
  author = {Holt, G. R. and Softky, W. R. and Koch, C. and Douglas, R. J.},
  year = {1996},
  month = may,
  volume = {75},
  pages = {1806--1814},
  issn = {0022-3077, 1522-1598},
  doi = {10.1152/jn.1996.75.5.1806},
  file = {1996 - Holt et al. - Comparison of discharge variability in vitro and in vivo in cat visual cortex neurons.pdf},
  journal = {Journal of Neurophysiology},
  language = {en},
  number = {5}
}

@article{Holt1997,
  title = {Shunting {{Inhibition Does Not Have}} a {{Divisive Effect}} on {{Firing Rates}}},
  author = {Holt, Gary R. and Koch, Christof},
  year = {1997},
  month = jul,
  volume = {9},
  pages = {1001--1013},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/neco.1997.9.5.1001},
  file = {1997 - Holt, Koch - Shunting inhibition does not have a divisive effect on firing rates.pdf},
  journal = {Neural Computation},
  language = {en},
  number = {5}
}

@article{Holt2014,
  title = {Origins and Suppression of Oscillations in a Computational Model of {{Parkinson}}'s Disease},
  author = {Holt, Abbey B. and Netoff, Theoden I.},
  year = {2014},
  month = dec,
  volume = {37},
  pages = {505--521},
  issn = {0929-5313, 1573-6873},
  doi = {10.1007/s10827-014-0523-7},
  abstract = {Efficacy of deep brain stimulation (DBS) for motor signs of Parkinson's disease (PD) depends in part on post-operative programming of stimulus parameters. There is a need for a systematic approach to tuning parameters based on patient physiology. We used a physiologically realistic computational model of the basal ganglia network to investigate the emergence of a 34 Hz oscillation in the PD state and its optimal suppression with DBS. Discrete time transfer functions were fit to post-stimulus time histograms (PSTHs) collected in open-loop, by simulating the pharmacological block of synaptic connections, to describe the behavior of the basal ganglia nuclei. These functions were then connected to create a mean-field model of the closed-loop system, which was analyzed to determine the origin of the emergent 34 Hz pathological oscillation. This analysis determined that the oscillation could emerge from the coupling between the globus pallidus external (GPe) and subthalamic nucleus (STN). When coupled, the two resonate with each other in the PD state but not in the healthy state. By characterizing how this oscillation is affected by subthreshold DBS pulses, we hypothesize that it is possible to predict stimulus frequencies capable of suppressing this oscillation. To characterize the response to the stimulus, we developed a new method for estimating phase response curves (PRCs) from population data. Using the population PRC we were able to predict frequencies that enhance and suppress the 34 Hz pathological oscillation. This provides a systematic approach to tuning DBS frequencies and could enable closed-loop tuning of stimulation parameters.},
  file = {2014 - Holt, Netoff - Origins and suppression of oscillations in a computational model of Parkinson’s disease.pdf;Holt and Netoff - 2014 - Origins and suppression of oscillations in a compu.pdf},
  journal = {Journal of Computational Neuroscience},
  language = {en},
  number = {3}
}

@article{Holt2016,
  title = {Phasic {{Burst Stimulation}}: {{A Closed}}-{{Loop Approach}} to {{Tuning Deep Brain Stimulation Parameters}} for {{Parkinson}}'s {{Disease}}},
  shorttitle = {Phasic {{Burst Stimulation}}},
  author = {Holt, Abbey B. and Wilson, Dan and Shinn, Max and Moehlis, Jeff and Netoff, Theoden I.},
  editor = {Sporns, Olaf},
  year = {2016},
  month = jul,
  volume = {12},
  pages = {e1005011},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1005011},
  file = {Holt et al. - 2016 - Phasic Burst Stimulation A Closed-Loop Approach t.pdf},
  journal = {PLOS Computational Biology},
  language = {en},
  number = {7}
}

@article{Honegger2019,
  title = {Idiosyncratic Neural Coding and Neuromodulation of Olfactory Individuality in {{{\emph{Drosophila}}}}},
  author = {Honegger, Kyle S. and Smith, Matthew A.-Y. and Churgin, Matthew A. and Turner, Glenn C. and {de Bivort}, Benjamin L.},
  year = {2019},
  month = aug,
  pages = {201901623},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1901623116},
  abstract = {Innate behavioral biases and preferences can vary significantly among individuals of the same genotype. Though individuality is a fundamental property of behavior, it is not currently understood how individual differences in brain structure and physiology produce idiosyncratic behaviors. Here we present evidence for idiosyncrasy in olfactory behavior and neural responses in               Drosophila               . We show that individual female               Drosophila               from a highly inbred laboratory strain exhibit idiosyncratic odor preferences that persist for days. We used in vivo calcium imaging of neural responses to compare projection neuron (second-order neurons that convey odor information from the sensory periphery to the central brain) responses to the same odors across animals. We found that, while odor responses appear grossly stereotyped, upon closer inspection, many individual differences are apparent across antennal lobe (AL) glomeruli (compact microcircuits corresponding to different odor channels). Moreover, we show that neuromodulation, environmental stress in the form of altered nutrition, and activity of certain AL local interneurons affect the magnitude of interfly behavioral variability. Taken together, this work demonstrates that individual               Drosophila               exhibit idiosyncratic olfactory preferences and idiosyncratic neural responses to odors, and that behavioral idiosyncrasies are subject to neuromodulation and regulation by neurons in the AL.},
  file = {Honegger et al. - 2019 - Idiosyncratic neural coding and neuromodulation of.pdf},
  journal = {Proc Natl Acad Sci USA},
  language = {en}
}

@article{Honey2017,
  title = {Switching between Internal and External Modes: {{A}} Multiscale Learning Principle},
  shorttitle = {Switching between Internal and External Modes},
  author = {Honey, Christopher J. and Newman, Ehren L. and Schapiro, Anna C.},
  year = {2017},
  month = dec,
  volume = {1},
  pages = {339--356},
  issn = {2472-1751},
  doi = {10.1162/NETN_a_00024},
  file = {Honey et al. - 2017 - Switching between internal and external modes A m.pdf},
  journal = {Network Neuroscience},
  language = {en},
  number = {4}
}

@article{Hong,
  title = {Diversity-{{Driven Exploration Strategy}} for {{Deep Reinforcement Learning}}},
  author = {Hong, Zhang-Wei and Shann, Tzu-Yun and Su, Shih-Yang and Chang, Yi-Hsiang and Fu, Tsu-Jui and Lee, Chun-Yi},
  pages = {12},
  abstract = {Efficient exploration remains a challenging research problem in reinforcement learning, especially when an environment contains large state spaces, deceptive or sparse rewards. To tackle this problem, we present a diversity-driven approach for exploration, which can be easily combined with both off- and on-policy reinforcement learning algorithms. We show that by simply adding a distance measure regularization to the loss function, the proposed methodology significantly enhances an agent's exploratory behavior, and thus prevents the policy from being trapped in local optima. We further propose an adaptive scaling strategy to enhance the performance. We demonstrate the effectiveness of our method in huge 2D gridworlds and a variety of benchmark environments, including Atari 2600 and MuJoCo. Experimental results validate that our method outperforms baseline approaches in most tasks in terms of mean scores and exploration efficiency.},
  file = {Hong et al. - Diversity-Driven Exploration Strategy for Deep Rei.pdf},
  language = {en}
}

@article{Hong2011,
  title = {Conformists and Contrarians in a {{Kuramoto}} Model with Identical Natural Frequencies},
  author = {Hong, Hyunsuk and Strogatz, Steven H.},
  year = {2011},
  month = oct,
  volume = {84},
  issn = {1539-3755, 1550-2376},
  doi = {10.1103/PhysRevE.84.046202},
  file = {2011 - Hong, Strogatz - Conformists and contrarians in a Kuramoto model with identical natural frequencies.pdf},
  journal = {Physical Review E},
  language = {en},
  number = {4}
}

@article{Hong2012,
  title = {Mean-Field Behavior in Coupled Oscillators with Attractive and Repulsive Interactions},
  author = {Hong, Hyunsuk and Strogatz, Steven H.},
  year = {2012},
  month = may,
  volume = {85},
  issn = {1539-3755, 1550-2376},
  doi = {10.1103/PhysRevE.85.056210},
  file = {2012 - Hong, Strogatz - Mean-field behavior in coupled oscillators with attractive and repulsive interactions.pdf},
  journal = {Physical Review E},
  language = {en},
  number = {5}
}

@article{Hong2016,
  title = {Phase Coherence Induced by Correlated Disorder},
  author = {Hong, Hyunsuk and O'Keeffe, Kevin P. and Strogatz, Steven H.},
  year = {2016},
  month = feb,
  volume = {93},
  issn = {2470-0045, 2470-0053},
  doi = {10.1103/PhysRevE.93.022219},
  file = {Hong et al. - 2016 - Phase coherence induced by correlated disorder.pdf},
  journal = {Physical Review E},
  language = {en},
  number = {2}
}

@article{Hong2018,
  title = {Diversity-{{Driven Exploration Strategy}} for {{Deep Reinforcement Learning}}},
  author = {Hong, Zhang-Wei and Shann, Tzu-Yun and Su, Shih-Yang and Chang, Yi-Hsiang and Lee, Chun-Yi},
  year = {2018},
  month = oct,
  abstract = {Efficient exploration remains a challenging research problem in reinforcement learning, especially when an environment contains large state spaces, deceptive or sparse rewards. To tackle this problem, we present a diversity-driven approach for exploration, which can be easily combined with both off- and on-policy reinforcement learning algorithms. We show that by simply adding a distance measure regularization to the loss function, the proposed methodology significantly enhances an agent's exploratory behavior, and thus prevents the policy from being trapped in local optima. We further propose an adaptive scaling strategy to enhance the performance. We demonstrate the effectiveness of our method in huge 2D gridworlds and a variety of benchmark environments, including Atari 2600 and MuJoCo. Experimental results validate that our method outperforms baseline approaches in most tasks in terms of mean scores and exploration efficiency.},
  archiveprefix = {arXiv},
  eprint = {1802.04564},
  eprinttype = {arxiv},
  file = {Hong et al. - 2018 - Diversity-Driven Exploration Strategy for Deep Rei.pdf},
  journal = {arXiv:1802.04564 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Statistics - Machine Learning},
  language = {en},
  primaryclass = {cs, stat}
}

@article{Hoogland2009a,
  title = {Radially Expanding Transglial Calcium Waves in the Intact Cerebellum},
  author = {Hoogland, T. M. and Kuhn, B. and Gobel, W. and Huang, W. and Nakai, J. and Helmchen, F. and Flint, J. and Wang, S. S.-H.},
  year = {2009},
  month = mar,
  volume = {106},
  pages = {3496--3501},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.0809269106},
  file = {Hoogland et al. - 2009 - Radially expanding transglial calcium waves in the 2.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {9}
}

@article{Hopcroft,
  title = {Foundations of {{Data Science}}},
  author = {Hopcroft, John and Kannan, Ravindran},
  pages = {414},
  file = {Hopcroft and Kannan - Foundations of Data Science.pdf},
  language = {en}
}

@article{Hopfield2010,
  title = {Neurodynamics of Mental Exploration},
  author = {Hopfield, J. J.},
  year = {2010},
  month = jan,
  volume = {107},
  pages = {1648--1653},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.0913991107},
  file = {Hopfield - 2010 - Neurodynamics of mental exploration.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {4}
}

@article{Hopfield2010a,
  title = {Neurodynamics of Mental Exploration},
  author = {Hopfield, J. J.},
  year = {2010},
  month = jan,
  volume = {107},
  pages = {1648--1653},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.0913991107},
  file = {Hopfield - 2010 - Neurodynamics of mental exploration 2.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {4}
}

@article{Hopkins2005,
  title = {Attainability of Boundary Points under Reinforcement Learning},
  author = {Hopkins, Ed and Posch, Martin},
  year = {2005},
  month = oct,
  volume = {53},
  pages = {110--125},
  issn = {08998256},
  doi = {10.1016/j.geb.2004.08.002},
  abstract = {This paper investigates the properties of the most common form of reinforcement learning (the ``basic model'' of Erev and Roth, American Economic Review, 88, 848-881, 1998). Stochastic approximation theory has been used to analyse the local stability of fixed points under this learning process. However, as we show, when such points are on the boundary of the state space, for example, pure strategy equilibria, standard results from the theory of stochastic approximation do not apply. We offer what we believe to be the correct treatment of boundary points, and provide a new and more general result: this model of learning converges with zero probability to fixed points which are unstable under the Maynard Smith or adjusted version of the evolutionary replicator dynamics. For two player games these are the fixed points that are linearly unstable under the standard replicator dynamics.},
  file = {2005 - Hopkins, Posch - Attainability of boundary points under reinforcement learning.pdf},
  journal = {Games and Economic Behavior},
  language = {en},
  number = {1}
}

@article{Hornik1989,
  title = {Multilayer Feedforward Networks Are Universal Approximators},
  author = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
  year = {1989},
  month = jan,
  volume = {2},
  pages = {359--366},
  issn = {08936080},
  doi = {10.1016/0893-6080(89)90020-8},
  abstract = {This paper rigorously establishes thut standard rnultiluyer feedforward networks with as f\&v us one hidden layer using arbitrary squashing functions ure capable of upproximating uny Bore1 measurable function from one finite dimensional space to another to any desired degree of uccuracy, provided sujficirntly muny hidden units are available. In this sense, multilayer feedforward networks are u class of universul rlpproximators.},
  file = {Hornik et al. - 1989 - Multilayer feedforward networks are universal appr.pdf},
  journal = {Neural Networks},
  language = {en},
  number = {5}
}

@article{Hornik1989a,
  title = {Multilayer Feedforward Networks Are Universal Approximators},
  author = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
  year = {1989},
  month = jan,
  volume = {2},
  pages = {359--366},
  issn = {08936080},
  doi = {10.1016/0893-6080(89)90020-8},
  abstract = {This paper rigorously establishes thut standard rnultiluyer feedforward networks with as f\&v us one hidden layer using arbitrary squashing functions ure capable of upproximating uny Bore1 measurable function from one finite dimensional space to another to any desired degree of uccuracy, provided sujficirntly muny hidden units are available. In this sense, multilayer feedforward networks are u class of universul rlpproximators.},
  file = {Hornik et al. - 1989 - Multilayer feedforward networks are universal appr 2.pdf},
  journal = {Neural Networks},
  language = {en},
  number = {5}
}

@article{Horvath2015,
  title = {Evidence That Transcranial Direct Current Stimulation ({{tDCS}}) Generates Little-to-No Reliable Neurophysiologic Effect beyond {{MEP}} Amplitude Modulation in Healthy Human Subjects: {{A}} Systematic Review},
  shorttitle = {Evidence That Transcranial Direct Current Stimulation ({{tDCS}}) Generates Little-to-No Reliable Neurophysiologic Effect beyond {{MEP}} Amplitude Modulation in Healthy Human Subjects},
  author = {Horvath, Jared Cooney and Forte, Jason D. and Carter, Olivia},
  year = {2015},
  month = jan,
  volume = {66},
  pages = {213--236},
  issn = {00283932},
  doi = {10.1016/j.neuropsychologia.2014.11.021},
  abstract = {Background: Transcranial direct current stimulation (tDCS) is a form of neuromodulation that is increasingly being utilized to examine and modify a number of cognitive and behavioral measures. The theoretical mechanisms by which tDCS generates these changes are predicated upon a rather large neurophysiological literature. However, a robust systematic review of this neurophysiological data has not yet been undertaken. 28 29 Keywords: 30 Transcranial direct current stimulation (tDCS) 31 Systematic review 32 Neurophysiology 33 Transcranial magnetic stimulation (TMS) 34 Event related potential (ERP) 35 Electroencephalography (EEG) Functional magnetic resonance imaging 36 (fMRI) 37 38 Methods: tDCS data in healthy adults (18\textendash 50) from every neurophysiological outcome measure reported by at least two different research groups in the literature was collected. When possible, data was pooled and quantitatively analyzed to assess significance. When pooling was not possible, data was qualitatively compared to assess reliability. Results: Of the 30 neurophysiological outcome measures reported by at least two different research groups, tDCS was found to have a reliable effect on only one: MEP amplitude. Interestingly, the magnitude of this effect has been significantly decreasing over the last 14 years. Conclusion: Our systematic review does not support the idea that tDCS has a reliable neurophysiological effect beyond MEP amplitude modulation \textendash{} though important limitations of this review (and conclusion) are discussed. This work raises questions concerning the mechanistic foundations and general efficacy of this device \textendash{} the implications of which extend to the steadily increasing tDCS psychological literature.},
  file = {Horvath et al. - 2015 - Evidence that transcranial direct current stimulat.pdf},
  journal = {Neuropsychologia},
  language = {en}
}

@article{Horvath2015a,
  title = {Quantitative {{Review Finds No Evidence}} of {{Cognitive Effects}} in {{Healthy Populations From Single}}-Session {{Transcranial Direct Current Stimulation}} ({{tDCS}})},
  author = {Horvath, Jared Cooney and Forte, Jason D. and Carter, Olivia},
  year = {2015},
  month = may,
  volume = {8},
  pages = {535--550},
  issn = {1935861X},
  doi = {10.1016/j.brs.2015.01.400},
  abstract = {Background: Over the last 15-years, transcranial direct current stimulation (tDCS), a relatively novel form of neuromodulation, has seen a surge of popularity in both clinical and academic settings. Despite numerous claims suggesting that a single session of tDCS can modulate cognition in healthy adult populations (especially working memory and language production), the paradigms utilized and results reported in the literature are extremely variable. To address this, we conduct the largest quantitative review of the cognitive data to date. Methods: Single-session tDCS data in healthy adults (18e50) from every cognitive outcome measure reported by at least two different research groups in the literature was collected. Outcome measures were divided into 4 broad categories: executive function, language, memory, and miscellaneous. To account for the paradigmatic variability in the literature, we undertook a three-tier analysis system; each with less-stringent inclusion criteria than the prior. Standard mean difference values with 95\% CIs were generated for included studies and pooled for each analysis. Results: Of the 59 analyses conducted, tDCS was found to not have a significant effect on any e regardless of inclusion laxity. This includes no effect on any working memory outcome or language production task. Conclusion: Our quantitative review does not support the idea that tDCS generates a reliable effect on cognition in healthy adults. Reasons for and limitations of this finding are discussed. This work raises important questions regarding the efficacy of tDCS, state-dependency effects, and future directions for this tool in cognitive research.},
  file = {Horvath et al. - 2015 - Quantitative Review Finds No Evidence of Cognitive.pdf},
  journal = {Brain Stimulation},
  language = {en},
  number = {3}
}

@article{Houtekamer1998,
  title = {Data {{Assimilation Using}} an {{Ensemble Kalman Filter Technique}}},
  author = {Houtekamer, P. L. and Mitchell, Herschel L.},
  year = {1998},
  month = mar,
  volume = {126},
  pages = {796--811},
  issn = {0027-0644, 1520-0493},
  doi = {10.1175/1520-0493(1998)126<0796:DAUAEK>2.0.CO;2},
  abstract = {The possibility of performing data assimilation using the flow-dependent statistics calculated from an ensemble of short-range forecasts (a technique referred to as ensemble Kalman filtering) is examined in an idealized environment. Using a three-level, quasigeostrophic, T21 model and simulated observations, experiments are performed in a perfect-model context. By using forward interpolation operators from the model state to the observations, the ensemble Kalman filter is able to utilize nonconventional observations.},
  file = {1998 - Houtekamer et al. - Data Assimilation Using an Ensemble Kalman Filter Technique.pdf;1998 - Service - Data Assimilation Using an Ensemble Kalman Filter Technique.pdf},
  journal = {Monthly Weather Review},
  language = {en},
  number = {3}
}

@article{Howard2019,
  title = {Numerical Cognition in Honeybees Enables Addition and Subtraction},
  author = {Howard, Scarlett R. and {Avargu{\`e}s-Weber}, Aurore and Garcia, Jair E. and Greentree, Andrew D. and Dyer, Adrian G.},
  year = {2019},
  month = feb,
  volume = {5},
  pages = {eaav0961},
  issn = {2375-2548},
  doi = {10.1126/sciadv.aav0961},
  file = {Howard et al. - 2019 - Numerical cognition in honeybees enables addition .pdf},
  journal = {Science Advances},
  language = {en},
  number = {2}
}

@article{Hsieh2010,
  title = {``{{Brain}}-Reading'' of Perceived Colors Reveals a Feature Mixing Mechanism Underlying Perceptual Filling-in in Cortical Area {{V1}}},
  author = {Hsieh, Po-Jang and Tse, Peter U.},
  year = {2010},
  month = sep,
  volume = {31},
  pages = {1395--1407},
  issn = {10659471},
  doi = {10.1002/hbm.20946},
  abstract = {Visual filling-in occurs when a retinally stabilized object undergoes perceptual fading. As the term ``filling-in'' implies, it is commonly believed that information about the apparently vanished object is lost and replaced solely by information arising from the surrounding background. Here we report multivoxel pattern analysis fMRI data that challenge this long-held belief. When subjects view blue disks on a red background while fixating, the stimulus and background appear to turn a uniform purple upon perceptual fading, suggesting that a feature mixing mechanism may underlie color fillingin. We find that ensemble fMRI signals in retinotopic visual areas reliably predict (i) which of three colors a subject reports seeing; (ii) whether a subject is in a perceptually filled-in state or not; and (iii) furthermore, while subjects are in the perceptual state of filling-in, the BOLD signal activation pattern in the sub-areas of V1 corresponding to the location of the blue disks behaves as if subjects are in fact viewing a perceptually mixed color (purple), rather than the color of the disks (blue) or the color of the background (red). These results imply that the mechanism of filling-in in stimuli in which figure and background surfaces are equated is a process of ``feature mixing'', not ``feature replacement''. These data indicate that feature mixing may involve cortical areas as early as V1. Hum Brain Mapp 31:1395\textendash 1407, 2010. VC 2010 Wiley-Liss, Inc.},
  file = {2010 - Hsieh, Tse - Brain-reading of perceived colors reveals a feature mixing mechanism underlying perceptual filling-in in cortical ar.pdf},
  journal = {Human Brain Mapping},
  language = {en},
  number = {9}
}

@article{Hsu2004,
  title = {Quantifying Variability in Neural Responses and Its Application for the Validation of Model Predictions},
  author = {Hsu, Anne and Borst, Alexander and Theunissen, Fr{\'e}d{\'e}ric},
  year = {2004},
  month = may,
  volume = {15},
  pages = {91--109},
  issn = {0954-898X, 1361-6536},
  doi = {10.1088/0954-898X/15/2/002},
  abstract = {A rate code assumes that a neuron's response is completely characterized by its time-varying mean firing rate. This assumption has successfully described neural responses in many systems. The noise in rate coding neurons can be quantified by the coherence function or the correlation coefficient between the neuron's deterministic time-varying mean rate and noise corrupted single spike trains. Because of the finite data size, the mean rate cannot be known exactly and must be approximated. We introduce novel unbiased estimators for the measures of coherence and correlation which are based on the extrapolation of the signal to noise ratio in the neural response to infinite data size. We then describe the application of these estimates to the validation of the class of stimulus\textendash response models that assume that the mean firing rate captures all the information embedded in the neural response. We explain how these quantifiers can be used to separate response prediction errors that are due to inaccurate model assumptions from errors due to noise inherent in neuronal spike trains.},
  file = {2004 - Hsu, Borst, Theunissen - Quantifying variability in neural responses and its application for the validation of model predictions.pdf},
  journal = {Network: Computation in Neural Systems},
  language = {en},
  number = {2}
}

@article{Hu2016,
  title = {Painful {{Issues}} in {{Pain Prediction}}},
  author = {Hu, Li and Iannetti, Gian Domenico},
  year = {2016},
  month = apr,
  volume = {39},
  pages = {212--220},
  issn = {01662236},
  doi = {10.1016/j.tins.2016.01.004},
  file = {Hu and Iannetti - 2016 - Painful Issues in Pain Prediction.pdf},
  journal = {Trends in Neurosciences},
  language = {en},
  number = {4}
}

@article{Huang1998,
  title = {The Empirical Mode Decomposition and the {{Hilbert}} Spectrum for Nonlinear and Non-Stationary Time Series Analysis},
  author = {Huang, Norden E. and Shen, Zheng and Long, Steven R. and Wu, Manli C. and Shih, Hsing H. and Zheng, Quanan and Yen, Nai-Chyuan and Tung, Chi Chao and Liu, Henry H.},
  year = {1998},
  month = mar,
  volume = {454},
  pages = {903--995},
  issn = {1471-2946},
  doi = {10.1098/rspa.1998.0193},
  file = {1998 - Branch et al. - (1998) N. E. Huang, et.al. (1998) The Empirical Mode Decomposition and the Hilbert Spectrum for Nonlinear and Non.pdf},
  journal = {Proceedings of the Royal Society of London. Series A: Mathematical, Physical and Engineering Sciences},
  language = {en},
  number = {1971}
}

@article{Huang2006,
  title = {Universal {{Approximation Using Incremental Constructive Feedforward Networks With Random Hidden Nodes}}},
  author = {Huang, G.-B. and Chen, L. and Siew, C.-K.},
  year = {2006},
  month = jul,
  volume = {17},
  pages = {879--892},
  issn = {1045-9227},
  doi = {10.1109/TNN.2006.875977},
  abstract = {According to conventional neural network theories, single-hidden-layer feedforward networks (SLFNs) with additive or radial basis function (RBF) hidden nodes are universal approximators when all the parameters of the networks are allowed adjustable. However, as observed in most neural network implementations, tuning all the parameters of the networks may cause learning complicated and inefficient, and it may be difficult to train networks with nondifferential activation functions such as threshold networks. Unlike conventional neural network theories, this paper proves in an incremental constructive method that in order to let SLFNs work as universal approximators, one may simply randomly choose hidden nodes and then only need to adjust the output weights linking the hidden layer and the output layer. In such SLFNs implementations, the activation functions for additive nodes can be any bounded nonconstant piecewise continuous functions : and the activation functions for RBF nodes can be any integrable piecewise continuous functions : and ( ) = 0.},
  file = {Huang et al. - 2006 - Universal Approximation Using Incremental Construc.pdf},
  journal = {IEEE Trans. Neural Netw.},
  language = {en},
  number = {4}
}

@inproceedings{Huang2017,
  title = {Densely {{Connected Convolutional Networks}}},
  booktitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Huang, Gao and Liu, Zhuang and van der Maaten, Laurens and Weinberger, Kilian Q.},
  year = {2017},
  month = jul,
  pages = {2261--2269},
  publisher = {{IEEE}},
  address = {{Honolulu, HI}},
  doi = {10.1109/CVPR.2017.243},
  file = {Huang et al. - 2017 - Densely Connected Convolutional Networks.pdf},
  isbn = {978-1-5386-0457-1},
  language = {en}
}

@article{Huber2014,
  title = {Investigation of the Neurovascular Coupling in Positive and Negative {{BOLD}} Responses in Human Brain at {{7T}}},
  author = {Huber, Laurentius and Goense, Jozien and Kennerley, Aneurin J. and Ivanov, Dimo and Krieger, Steffen N. and Lepsien, J{\"o}ran and Trampel, Robert and Turner, Robert and M{\"o}ller, Harald E.},
  year = {2014},
  month = aug,
  volume = {97},
  pages = {349--362},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2014.04.022},
  file = {2014 - Huber et al. - Investigation of the neurovascular coupling in positive and negative BOLD responses in human brain at 7T.pdf;Huber et al. - 2014 - Investigation of the neurovascular coupling in pos.pdf},
  journal = {NeuroImage},
  language = {en}
}

@article{Hughes1997,
  title = {Intrinsic Exploration in Animals: Motives and Measurement},
  shorttitle = {Intrinsic Exploration in Animals},
  author = {Hughes, Robert N},
  year = {1997},
  month = dec,
  volume = {41},
  pages = {213--226},
  issn = {03766357},
  doi = {10.1016/S0376-6357(97)00055-7},
  abstract = {Intrinsic exploration involves exploratory acts that are not instrumental in achieving any particular goal other than performance of the acts themselves. Of the theories proposed to account for the motivation of intrinsic exploration in animals, concepts of exploratory drive, optimal arousal and fear have featured prominently. But since no single approach has adequate explanatory or predictive power, it is probably sufficient to go no further than accept that organisms may have some type of `need' for sensory change which can be satisfied mainly by intrinsic exploration. Attempts to measure the phenomenon in the laboratory can be divided into forced tests in which locomotion and other motor responses are recorded in animals placed into a totally novel environments, and free tests involving measurements of active choices of differing degrees of novelty. Because of the difficulty of distinguishing between extrinsic and intrinsic exploration with activity indices, tests of free exploration are always preferable. These include novelty-related location preferences (including spontaneous alternation and responses to brightness change), object exploration and learning for exploratory rewards all of which can be viewed as reasonably valid measures of intrinsic exploration to a greater or lesser extent. \textcopyright{} 1997 Elsevier Science B.V.},
  file = {Hughes - 1997 - Intrinsic exploration in animals motives and meas.pdf},
  journal = {Behavioural Processes},
  language = {en},
  number = {3}
}

@article{Hughes2019,
  title = {Wave {{Physics}} as an {{Analog Recurrent Neural Network}}},
  author = {Hughes, Tyler W. and Williamson, Ian A. D. and Minkov, Momchil and Fan, Shanhui},
  year = {2019},
  month = apr,
  abstract = {Analog machine learning hardware platforms promise to be faster and more energy-efficient than their digital counterparts. Wave physics, as found in acoustics and optics, is a natural candidate for building analog processors for time-varying signals. Here we identify a mapping between the dynamics of wave physics, and the computation in recurrent neural networks. This mapping indicates that physical wave systems can be trained to learn complex features in temporal data, using standard training techniques for neural networks. As a demonstration, we show that an inversely-designed inhomogeneous medium can perform vowel classification on raw audio data by simple propagation of waves through such a medium, achieving performance that is comparable to a standard digital implementation of a recurrent neural network. These findings pave the way for a new class of analog machine learning platforms, capable of fast and efficient processing of information in its native domain.},
  archiveprefix = {arXiv},
  eprint = {1904.12831},
  eprinttype = {arxiv},
  file = {Hughes et al. - 2019 - Wave Physics as an Analog Recurrent Neural Network.pdf},
  journal = {arXiv:1904.12831 [physics]},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Physics - Computational Physics,Physics - Optics},
  language = {en},
  primaryclass = {physics}
}

@article{Huh2017,
  title = {Gradient {{Descent}} for {{Spiking Neural Networks}}},
  author = {Huh, Dongsung and Sejnowski, Terrence J.},
  year = {2017},
  month = jun,
  abstract = {Much of studies on neural computation are based on network models of static neurons that produce analog output, despite the fact that information processing in the brain is predominantly carried out by dynamic neurons that produce discrete pulses called spikes. Research in spike-based computation has been impeded by the lack of efficient supervised learning algorithm for spiking networks. Here, we present a gradient descent method for optimizing spiking network models by introducing a differentiable formulation of spiking networks and deriving the exact gradient calculation. For demonstration, we trained recurrent spiking networks on two dynamic tasks: one that requires optimizing fast ({$\approx$} millisecond) spike-based interactions for efficient encoding of information, and a delayed-memory XOR task over extended duration ({$\approx$} second). The results show that our method indeed optimizes the spiking network dynamics on the time scale of individual spikes as well as the behavioral time scales. In conclusion, our result offers a general purpose supervised learning algorithm for spiking neural networks, thus advancing further investigations on spike-based computation.},
  archiveprefix = {arXiv},
  eprint = {1706.04698},
  eprinttype = {arxiv},
  file = {Huh and Sejnowski - 2017 - Gradient Descent for Spiking Neural Networks.pdf},
  journal = {arXiv:1706.04698 [cs, q-bio, stat]},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Quantitative Biology - Neurons and Cognition,Statistics - Machine Learning},
  language = {en},
  primaryclass = {cs, q-bio, stat}
}

@inproceedings{Huizinga2014,
  title = {Evolving Neural Networks That Are Both Modular and Regular: {{HyperNEAT}} plus the Connection Cost Technique},
  shorttitle = {Evolving Neural Networks That Are Both Modular and Regular},
  booktitle = {Proceedings of the 2014 Conference on {{Genetic}} and Evolutionary Computation - {{GECCO}} '14},
  author = {Huizinga, Joost and Clune, Jeff and Mouret, Jean-Baptiste},
  year = {2014},
  pages = {697--704},
  publisher = {{ACM Press}},
  address = {{Vancouver, BC, Canada}},
  doi = {10.1145/2576768.2598232},
  abstract = {One of humanity's grand scientific challenges is to create artificially intelligent robots that rival natural animals in intelligence and agility. A key enabler of such animal complexity is the fact that animal brains are structurally organized in that they exhibit modularity and regularity, amongst other attributes. Modularity is the localization of function within an encapsulated unit. Regularity refers to the compressibility of the information describing a structure, and typically involves symmetries and repetition. These properties improve evolvability, but they rarely emerge in evolutionary algorithms without specific techniques to encourage them. It has been shown that (1) modularity can be evolved in neural networks by adding a cost for neural connections and, separately, (2) that the HyperNEAT algorithm produces neural networks with complex, functional regularities. In this paper we show that adding the connection cost technique to HyperNEAT produces neural networks that are significantly more modular, regular, and higher performing than HyperNEAT without a connection cost, even when compared to a variant of HyperNEAT that was specifically designed to encourage modularity. Our results represent a stepping stone towards the goal of producing artificial neural networks that share key organizational properties with the brains of natural animals.},
  file = {Huizinga et al. - 2014 - Evolving neural networks that are both modular and.pdf},
  isbn = {978-1-4503-2662-9},
  language = {en}
}

@article{Humphries2006,
  title = {A {{Physiologically Plausible Model}} of {{Action Selection}} and {{Oscillatory Activity}} in the {{Basal Ganglia}}},
  author = {Humphries, M. D. and Stewart, R. D. and Gurney, K. N.},
  year = {2006},
  month = dec,
  volume = {26},
  pages = {12921--12942},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.3486-06.2006},
  file = {2006 - Humphries, Stewart, Gurney - A Physiologically Plausible Model of Action Selection and Oscillatory Activity in the Basal Ganglia.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {50}
}

@article{Humphries2009,
  title = {Dopamine-Modulated Dynamic Cell Assemblies Generated by the {{GABAergic}} Striatal Microcircuit},
  author = {Humphries, Mark D. and Wood, Ric and Gurney, Kevin},
  year = {2009},
  month = oct,
  volume = {22},
  pages = {1174--1188},
  issn = {08936080},
  doi = {10.1016/j.neunet.2009.07.018},
  abstract = {The striatum, the principal input structure of the basal ganglia, is crucial to both motor control and learning. It receives convergent input from all over the neocortex, hippocampal formation, amygdala and thalamus, and is the primary recipient of dopamine in the brain. Within the striatum is a GABAergic microcircuit that acts upon these inputs, formed by the dominant medium-spiny projection neurons (MSNs) and fast-spiking interneurons (FSIs). There has been little progress in understanding the computations it performs, hampered by the non-laminar structure that prevents identification of a repeating canonical microcircuit. We here begin the identification of potential dynamically-defined computational elements within the striatum. We construct a new three-dimensional model of the striatal microcircuit's connectivity, and instantiate this with our dopamine-modulated neuron models of the MSNs and FSIs. A new model of gap junctions between the FSIs is introduced and tuned to experimental data. We introduce a novel multiple spike-train analysis method, and apply this to the outputs of the model to find groups of synchronised neurons at multiple time-scales. We find that, with realistic in vivo background input, small assemblies of synchronised MSNs spontaneously appear, consistent with experimental observations, and that the number of assemblies and the time-scale of synchronisation is strongly dependent on the simulated concentration of dopamine. We also show that feed-forward inhibition from the FSIs counter-intuitively increases the firing rate of the MSNs. Such small cell assemblies forming spontaneously only in the absence of dopamine may contribute to motor control problems seen in humans and animals following a loss of dopamine cells.},
  file = {2009 - Humphries, Wood, Gurney - Dopamine-modulated dynamic cell assemblies generated by the GABAergic striatal microcircuit.pdf},
  journal = {Neural Networks},
  language = {en},
  number = {8}
}

@article{Humphries2010,
  title = {Reconstructing the {{Three}}-{{Dimensional GABAergic Microcircuit}} of the {{Striatum}}},
  author = {Humphries, Mark D. and Wood, Ric and Gurney, Kevin},
  editor = {Friston, Karl J.},
  year = {2010},
  month = nov,
  volume = {6},
  pages = {e1001011},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1001011},
  abstract = {A system's wiring constrains its dynamics, yet modelling of neural structures often overlooks the specific networks formed by their neurons. We developed an approach for constructing anatomically realistic networks and reconstructed the GABAergic microcircuit formed by the medium spiny neurons (MSNs) and fast-spiking interneurons (FSIs) of the adult rat striatum. We grew dendrite and axon models for these neurons and extracted probabilities for the presence of these neurites as a function of distance from the soma. From these, we found the probabilities of intersection between the neurites of two neurons given their inter-somatic distance, and used these to construct three-dimensional striatal networks. The MSN dendrite models predicted that half of all dendritic spines are within 100mm of the soma. The constructed networks predict distributions of gap junctions between FSI dendrites, synaptic contacts between MSNs, and synaptic inputs from FSIs to MSNs that are consistent with current estimates. The models predict that to achieve this, FSIs should be at most 1\% of the striatal population. They also show that the striatum is sparsely connected: FSI-MSN and MSN-MSN contacts respectively form 7\% and 1.7\% of all possible connections. The models predict two striking network properties: the dominant GABAergic input to a MSN arises from neurons with somas at the edge of its dendritic field; and FSIs are interconnected on two different spatial scales: locally by gap junctions and distally by synapses. We show that both properties influence striatal dynamics: the most potent inhibition of a MSN arises from a region of striatum at the edge of its dendritic field; and the combination of local gap junction and distal synaptic networks between FSIs sets a robust input-output regime for the MSN population. Our models thus intimately link striatal micro-anatomy to its dynamics, providing a biologically grounded platform for further study.},
  file = {2010 - Humphries, Wood, Gurney - Reconstructing the Three-Dimensional GABAergic Microcircuit of the Striatum.pdf},
  journal = {PLoS Computational Biology},
  language = {en},
  number = {11}
}

@article{Humphries2010a,
  title = {Environmental Context Explains {{L\'evy}} and {{Brownian}} Movement Patterns of Marine Predators},
  author = {Humphries, Nicolas E. and Queiroz, Nuno and Dyer, Jennifer R. M. and Pade, Nicolas G. and Musyl, Michael K. and Schaefer, Kurt M. and Fuller, Daniel W. and Brunnschweiler, Juerg M. and Doyle, Thomas K. and Houghton, Jonathan D. R. and Hays, Graeme C. and Jones, Catherine S. and Noble, Leslie R. and Wearmouth, Victoria J. and Southall, Emily J. and Sims, David W.},
  year = {2010},
  month = jun,
  volume = {465},
  pages = {1066--1069},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature09116},
  file = {Humphries et al. - 2010 - Environmental context explains Lévy and Brownian m.pdf},
  journal = {Nature},
  language = {en},
  number = {7301}
}

@article{Humphries2012,
  title = {Network Effects of Subthalamic Deep Brain Stimulation Drive a Unique Mixture of Responses in Basal Ganglia Output: {{STN DBS}} Causes Mix of Output Responses},
  shorttitle = {Network Effects of Subthalamic Deep Brain Stimulation Drive a Unique Mixture of Responses in Basal Ganglia Output},
  author = {Humphries, Mark D. and Gurney, Kevin},
  year = {2012},
  month = jul,
  volume = {36},
  pages = {2240--2251},
  issn = {0953816X},
  doi = {10.1111/j.1460-9568.2012.08085.x},
  abstract = {Deep brain stimulation (DBS) is a remarkably successful treatment for the motor symptoms of Parkinson's disease. High-frequency stimulation of the subthalamic nucleus (STN) within the basal ganglia is a main clinical target, but the physiological mechanisms of therapeutic STN DBS at the cellular and network level are unclear. We set out to begin to address the hypothesis that a mixture of responses in the basal ganglia output nuclei, combining regularized firing and inhibition, is a key contributor to the effectiveness of STN DBS. We used our computational model of the complete basal ganglia circuit to show how such a mixture of responses in basal ganglia output naturally arises from the network effects of STN DBS. We replicated the diversification of responses recorded in a primate STN DBS study to show that the model's predicted mixture of responses is consistent with therapeutic STN DBS. We then showed how this `mixture of response' perspective suggests new ideas for DBS mechanisms: first, that the therapeutic frequency of STN DBS is above 100 Hz because the diversification of responses exhibits a step change above this frequency; and second, that optogenetic models of direct STN stimulation during DBS have proven therapeutically ineffective because they do not replicate the mixture of basal ganglia output responses evoked by electrical DBS.},
  file = {2012 - Humphries, Gurney - Network effects of subthalamic deep brain stimulation drive a unique mixture of responses in basal ganglia ou.pdf},
  journal = {European Journal of Neuroscience},
  language = {en},
  number = {2}
}

@article{Humphries2012a,
  title = {Foraging Success of Biological {{Levy}} Flights Recorded in Situ},
  author = {Humphries, N. E. and Weimerskirch, H. and Queiroz, N. and Southall, E. J. and Sims, D. W.},
  year = {2012},
  month = may,
  volume = {109},
  pages = {7169--7174},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1121201109},
  file = {Humphries et al. - 2012 - Foraging success of biological Levy flights record.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {19}
}

@article{Hung2019,
  title = {Optimizing Agent Behavior over Long Time Scales by Transporting Value},
  author = {Hung, Chia-Chun and Lillicrap, Timothy and Abramson, Josh and Wu, Yan and Mirza, Mehdi and Carnevale, Federico and Ahuja, Arun and Wayne, Greg},
  year = {2019},
  month = dec,
  volume = {10},
  pages = {5223},
  issn = {2041-1723},
  doi = {10.1038/s41467-019-13073-w},
  file = {Hung et al. - 2019 - Optimizing agent behavior over long time scales by.pdf},
  journal = {Nat Commun},
  language = {en},
  number = {1}
}

@article{Hunt2014,
  title = {Hierarchical Competitions Subserving Multi-Attribute Choice},
  author = {Hunt, Laurence T and Dolan, Raymond J and Behrens, Timothy E J},
  year = {2014},
  month = nov,
  volume = {17},
  pages = {1613--1622},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.3836},
  file = {Hunt et al. - 2014 - Hierarchical competitions subserving multi-attribu.pdf},
  journal = {Nature Neuroscience},
  language = {en},
  number = {11}
}

@article{Hurzook,
  title = {Visual Motion Processing and Perceptual Decision Making},
  author = {Hurzook, Aziz and Trujillo, Oliver and Eliasmith, Chris},
  pages = {1},
  abstract = {We present a biologically plausible spiking network model of visual motion processing and perceptual decision making, independent of the number of choice alternatives. As an application we simulate the two-alternative forced choice (2AFC) task.},
  file = {2010 - Hurzook, Trujillo, Eliasmith - Visual motion processing and perceptual decision making.pdf;2010 - Hurzook, Trujillo, Eliasmith - Visual motion processing and perceptual decision making(2).pdf},
  language = {en}
}

@article{Hutchinson2018,
  title = {Entrotaxis as a Strategy for Autonomous Search and Source Reconstruction in Turbulent Conditions},
  author = {Hutchinson, Michael and Oh, Hyondong and Chen, Wen-Hua},
  year = {2018},
  month = jul,
  volume = {42},
  pages = {179--189},
  issn = {15662535},
  doi = {10.1016/j.inffus.2017.10.009},
  abstract = {This paper proposes a strategy for performing an efficient autonomous search to find an emitting source of sporadic cues of noisy information. We focus on the search for a source of unknown strength, releasing particles into the atmosphere where turbulence can cause irregular gradients and intermittent patches of sensory cues. Bayesian inference, implemented via the sequential Monte Carlo method, is used to update posterior probability distributions of the source location and strength in response to sensor measurements. Posterior sampling is then used to approximate a reward function, leading to the manoeuvre to where the entropy of the predictive distribution is the greatest. As it is developed based on the maximum entropy sampling principle, the proposed framework is termed as Entrotaxis. We compare the performance and search behaviour of Entrotaxis with the popular Infotaxis algorithm, for searching in sparse and turbulent conditions where typical gradient-based approaches become inefficient or fail. The algorithms are assessed via Monte Carlo simulations with simulated data and an experimental dataset. Whilst outperforming the Infotaxis algorithm in most of our simulated scenarios, by achieving a faster mean search time, the proposed strategy is also more computationally efficient during the decision making process.},
  file = {Hutchinson et al. - 2018 - Entrotaxis as a strategy for autonomous search and.pdf},
  journal = {Information Fusion},
  language = {en}
}

@incollection{Hutchison2010,
  title = {Indirectly {{Encoding Neural Plasticity}} as a {{Pattern}} of {{Local Rules}}},
  booktitle = {From {{Animals}} to {{Animats}} 11},
  author = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Risi, Sebastian and Stanley, Kenneth O.},
  editor = {Doncieux, St{\'e}phane and Girard, Beno{\^i}t and Guillot, Agn{\`e}s and Hallam, John and Meyer, Jean-Arcady and Mouret, Jean-Baptiste},
  year = {2010},
  volume = {6226},
  pages = {533--543},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-15193-4_50},
  abstract = {Biological brains can adapt and learn from past experience. In neuroevolution, i.e. evolving artificial neural networks (ANNs), one way that agents controlled by ANNs can evolve the ability to adapt is by encoding local learning rules. However, a significant problem with most such approaches is that local learning rules for every connection in the network must be discovered separately. This paper aims to show that learning rules can be effectively indirectly encoded by extending the Hypercube-based NeuroEvolution of Augmenting Topologies (HyperNEAT) method. Adaptive HyperNEAT is introduced to allow not only patterns of weights across the connectivity of an ANN to be generated by a function of its geometry, but also patterns of arbitrary learning rules. Several such adaptive models with different levels of generality are explored and compared. The long-term promise of the new approach is to evolve large-scale adaptive ANNs, which is a major goal for neuroevolution.},
  file = {Hutchison et al. - 2010 - Indirectly Encoding Neural Plasticity as a Pattern.pdf},
  isbn = {978-3-642-15192-7 978-3-642-15193-4},
  language = {en}
}

@article{Huth2012,
  title = {A {{Continuous Semantic Space Describes}} the {{Representation}} of {{Thousands}} of {{Object}} and {{Action Categories}} across the {{Human Brain}}},
  author = {Huth, Alexander G. and Nishimoto, Shinji and Vu, An T. and Gallant, Jack L.},
  year = {2012},
  month = dec,
  volume = {76},
  pages = {1210--1224},
  issn = {08966273},
  doi = {10.1016/j.neuron.2012.10.014},
  abstract = {Humans can see and name thousands of distinct object and action categories, so it is unlikely that each category is represented in a distinct brain area. A more efficient scheme would be to represent categories as locations in a continuous semantic space mapped smoothly across the cortical surface. To search for such a space, we used functional magnetic resonance imaging (fMRI) to measure human brain activity evoked by natural movies. We then used voxel-wise models to examine the cortical representation of 1705 object and action categories. The first few dimensions of the underlying semantic space were recovered from the fit models by principal components analysis. Projection of the recovered semantic space onto cortical flat maps shows that semantic selectivity is organized into smooth gradients that cover much of visual and nonvisual cortex. Furthermore, both the recovered semantic space and the cortical organization of the space are shared across different individuals.},
  file = {2012 - Huth et al. - A continuous semantic space describes the representation of thousands of object and action categories across the hu.pdf},
  journal = {Neuron},
  language = {en},
  number = {6}
}

@article{Huth2016,
  title = {Natural Speech Reveals the Semantic Maps That Tile Human Cerebral Cortex},
  author = {Huth, Alexander G. and {de Heer}, Wendy A. and Griffiths, Thomas L. and Theunissen, Fr{\'e}d{\'e}ric E. and Gallant, Jack L.},
  year = {2016},
  month = apr,
  volume = {532},
  pages = {453--458},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature17637},
  file = {Huth et al. - 2016 - Natural speech reveals the semantic maps that tile.pdf},
  journal = {Nature},
  language = {en},
  number = {7600}
}

@article{Hutt2010,
  title = {Activity Spread and Breathers Induced by Finite Transmission Speeds in Two-Dimensional Neural Fields},
  author = {Hutt, Axel and Rougier, Nicolas},
  year = {2010},
  month = nov,
  volume = {82},
  issn = {1539-3755, 1550-2376},
  doi = {10.1103/PhysRevE.82.055701},
  file = {2010 - Hutt, Rougier - Activity spread and breathers induced by finite transmission speeds in two-dimensional neural fields.pdf},
  journal = {Physical Review E},
  language = {en},
  number = {5}
}

@article{Huttegger2013,
  title = {Methodology in {{Biological Game Theory}}},
  author = {Huttegger, S. M. and Zollman, K. J. S.},
  year = {2013},
  month = sep,
  volume = {64},
  pages = {637--658},
  issn = {0007-0882, 1464-3537},
  doi = {10.1093/bjps/axs035},
  abstract = {Game theory has a prominent role in evolutionary biology, in particular in the ecological study of various phenomena ranging from conflict behaviour to altruism to signalling and beyond. The two central methodological tools in biological game theory are the concepts of Nash equilibrium and evolutionarily stable strategy. While both were inspired by a dynamic conception of evolution, these concepts are essentially static\textemdash they only show that a population is uninvadable, but not that a population is likely to evolve. In this article, we argue that a static methodology can lead to misleading views about dynamic evolutionary processes. We advocate, instead, a more pluralistic methodology, which includes both static and dynamic game theoretic tools. Such an approach provides a more complete picture of the evolution of strategic behaviour.},
  file = {Huttegger and Zollman - 2013 - Methodology in Biological Game Theory.pdf},
  journal = {The British Journal for the Philosophy of Science},
  language = {en},
  number = {3}
}

@article{Huys2009,
  title = {Smoothing of, and {{Parameter Estimation}} from, {{Noisy Biophysical Recordings}}},
  author = {Huys, Quentin J. M. and Paninski, Liam},
  editor = {Friston, Karl J.},
  year = {2009},
  month = may,
  volume = {5},
  pages = {e1000379},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1000379},
  abstract = {Biophysically detailed models of single cells are difficult to fit to real data. Recent advances in imaging techniques allow simultaneous access to various intracellular variables, and these data can be used to significantly facilitate the modelling task. These data, however, are noisy, and current approaches to building biophysically detailed models are not designed to deal with this. We extend previous techniques to take the noisy nature of the measurements into account. Sequential Monte Carlo (``particle filtering'') methods, in combination with a detailed biophysical description of a cell, are used for principled, model-based smoothing of noisy recording data. We also provide an alternative formulation of smoothing where the neural nonlinearities are estimated in a non-parametric manner. Biophysically important parameters of detailed models (such as channel densities, intercompartmental conductances, input resistances, and observation noise) are inferred automatically from noisy data via expectation-maximisation. Overall, we find that model-based smoothing is a powerful, robust technique for smoothing of noisy biophysical data and for inference of biophysical parameters in the face of recording noise.},
  file = {2009 - Huys, Paninski - Smoothing of, and parameter estimation from, noisy biophysical recordings.pdf},
  journal = {PLoS Computational Biology},
  language = {en},
  number = {5}
}

@article{Hyafil2015,
  title = {Speech Encoding by Coupled Cortical Theta and Gamma Oscillations},
  author = {Hyafil, Alexandre and Fontolan, Lorenzo and Kabdebon, Claire and Gutkin, Boris and Giraud, Anne-Lise},
  year = {2015},
  month = may,
  volume = {4},
  issn = {2050-084X},
  doi = {10.7554/eLife.06213},
  file = {Hyafil et al. - 2015 - Speech encoding by coupled cortical theta and gamm.pdf},
  journal = {eLife},
  language = {en}
}

@article{Ibata2008,
  title = {Rapid {{Synaptic Scaling Induced}} by {{Changes}} in {{Postsynaptic Firing}}},
  author = {Ibata, Keiji and Sun, Qian and Turrigiano, Gina G.},
  year = {2008},
  month = mar,
  volume = {57},
  pages = {819--826},
  issn = {08966273},
  doi = {10.1016/j.neuron.2008.02.031},
  abstract = {Homeostatic synaptic scaling adjusts a neuron's excitatory synaptic strengths up or down to compensate for perturbations in activity. Little is known about the molecular pathway(s) involved, nor is it clear which aspect of ``activity''\textemdash local synaptic signaling, postsynaptic firing, or large-scale changes in network activity\textemdash is required to induce synaptic scaling. Here, we selectively block either postsynaptic firing in individual neurons or a fraction of presynaptic inputs, while optically monitoring changes in synaptic strength. We find that synaptic scaling is rapidly induced by block of postsynaptic firing, but not by local synaptic blockade, and is mediated through a drop in somatic calcium influx, reduced activation of CaMKIV, and an increase in transcription. Cortical neurons thus homeostatically adjust synaptic strengths in response to changes in their own firing rate, a mechanism with the computational advantage of efficiently normalizing synaptic strengths without interfering with synapse-specific mechanisms of information storage.},
  file = {2008 - Ibata, Sun, Turrigiano - Rapid Synaptic Scaling Induced by Changes in Postsynaptic Firing.pdf},
  journal = {Neuron},
  language = {en},
  number = {6}
}

@article{Iemi2017,
  title = {Spontaneous {{Neural Oscillations Bias Perception}} by {{Modulating Baseline Excitability}}},
  author = {Iemi, Luca and Chaumon, Maximilien and Crouzet, S{\'e}bastien M. and Busch, Niko A.},
  year = {2017},
  month = jan,
  volume = {37},
  pages = {807--819},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.1432-16.2017},
  file = {Iemi et al. - 2017 - Spontaneous Neural Oscillations Bias Perception by.pdf},
  journal = {The Journal of Neuroscience},
  language = {en},
  number = {4}
}

@article{Inagaki2017,
  title = {Discrete Attractor Dynamics Underlying Selective Persistent Activity in Frontal Cortex},
  author = {Inagaki, Hidehiko K. and Fontolan, Lorenzo and Romani, Sandro and Svoboda, Karel},
  year = {2017},
  month = oct,
  doi = {10.1101/203448},
  abstract = {Short-term memories link events separated in time, such as past sensation and future actions. Short-term memories are correlated with selective persistent activity, which can be maintained over seconds. In a delayed response task that requires short-term memory, neurons in mouse anterior lateral motor cortex (ALM) show persistent activity that instructs future actions. To elucidate the mechanisms underlying this persistent activity we combined intracellular and extracellular electrophysiology with optogenetic perturbations and network modeling. During the delay epoch, both membrane potential and population activity of ALM neurons funneled towards discrete endpoints related to specific movement directions. These endpoints were robust to transient shifts in ALM activity caused by optogenetic perturbations. Perturbations occasionally switched the population dynamics to the other endpoint, followed by incorrect actions. Our results are consistent with discrete attractor dynamics underlying short-term memory related to motor planning.},
  file = {Inagaki et al. - 2017 - Discrete attractor dynamics underlying selective p.pdf},
  journal = {bioRxiv},
  language = {en}
}

@techreport{Ingiosi2019,
  title = {A Role for Astroglial Calcium in Mammalian Sleep},
  author = {Ingiosi, Ashley M. and Hayworth, Christopher R. and Harvey, Daniel O. and Singletary, Kristan G. and Rempe, Michael J. and Wisor, Jonathan P. and Frank, Marcos G.},
  year = {2019},
  month = aug,
  institution = {{Neuroscience}},
  doi = {10.1101/728931},
  abstract = {Mammalian sleep is characterized by dramatic changes in neuronal activity, and waking neuronal activity is thought to increase sleep need. Changes in other brain cells (glia) across the natural sleep-wake cycle and their role in sleep regulation are comparatively unexplored. We show that sleep is also accompanied by large changes in astroglial activity as measured by intracellular calcium concentrations in unanesthetized mice. These changes in calcium vary across different vigilance states and are most pronounced in distal astroglial processes. We find that reducing intracellular calcium in astrocytes impaired the homeostatic response to sleep deprivation. Thus, astroglial calcium changes dynamically across vigilance states and is a component of the sleep homeostat.},
  file = {Ingiosi et al. - 2019 - A role for astroglial calcium in mammalian sleep.pdf},
  language = {en},
  type = {Preprint}
}

@article{Inglis2001,
  title = {An Information Primacy Model of Exploratory and Foraging Behaviour},
  author = {Inglis, Ian R. and Langton, Steve and Forkman, Bj{\"o}rn and Lazarus, John},
  year = {2001},
  month = sep,
  volume = {62},
  pages = {543--557},
  issn = {00033472},
  doi = {10.1006/anbe.2001.1780},
  file = {Inglis et al. - 2001 - An information primacy model of exploratory and fo.pdf},
  journal = {Animal Behaviour},
  language = {en},
  number = {3}
}

@article{Ingram2020,
  title = {Elevated Energy Requirement of Cone Photoreceptors},
  author = {Ingram, Norianne T. and Fain, Gordon L. and Sampath, Alapakkam P.},
  year = {2020},
  month = aug,
  volume = {117},
  pages = {19599--19603},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.2001776117},
  abstract = {We have used recent measurements of mammalian cone light responses and voltage-gated currents to calculate cone ATP utilization and compare it to that of rods. The largest expenditure of ATP results from ion transport, particularly from removal of Na               +               entering outer segment light-dependent channels and inner segment hyperpolarization-activated cyclic nucleotide-gated channels, and from ATP-dependent pumping of Ca               2+               entering voltage-gated channels at the synaptic terminal. Single cones expend nearly twice as much energy as single rods in darkness, largely because they make more synapses with second-order retinal cells and thus must extrude more Ca               2+               . In daylight, cone ATP utilization per cell remains high because cones never remain saturated and must continue to export Na               +               and synaptic Ca               2+               even in bright illumination. In mouse and human retina, rods greatly outnumber cones and consume more energy overall even in background light. In primates, however, the high density of cones in the fovea produces a pronounced peak of ATP utilization, which becomes particularly prominent in daylight and may make this part of the retina especially sensitive to changes in energy availability.},
  file = {Ingram et al. - 2020 - Elevated energy requirement of cone photoreceptors.pdf},
  journal = {Proc Natl Acad Sci USA},
  language = {en},
  number = {32}
}

@article{Insanally2019,
  title = {Spike-Timing-Dependent Ensemble Encoding by Non-Classically Responsive Cortical Neurons},
  author = {Insanally, Michele N and Carcea, Ioana and Field, Rachel E and Rodgers, Chris C and DePasquale, Brian and Rajan, Kanaka and DeWeese, Michael R and Albanna, Badr F and Froemke, Robert C},
  year = {2019},
  month = jan,
  volume = {8},
  issn = {2050-084X},
  doi = {10.7554/eLife.42409},
  file = {Insanally et al. - 2019 - Spike-timing-dependent ensemble encoding by non-cl.pdf;Insanally et al. - 2019 - Spike-timing-dependent ensemble encoding by non-cl.pdf},
  journal = {eLife},
  language = {en}
}

@article{Insanally2019a,
  title = {Spike-Timing-Dependent Ensemble Encoding by Non-Classically Responsive Cortical Neurons},
  author = {Insanally, Michele N and Carcea, Ioana and Field, Rachel E and Rodgers, Chris C and DePasquale, Brian and Rajan, Kanaka and DeWeese, Michael R and Albanna, Badr F and Froemke, Robert C},
  year = {2019},
  month = jan,
  volume = {8},
  pages = {e42409},
  issn = {2050-084X},
  doi = {10.7554/eLife.42409},
  abstract = {Neurons recorded in behaving animals often do not discernibly respond to sensory input and are not overtly task-modulated. These non-classically responsive neurons are difficult to interpret and are typically neglected from analysis, confounding attempts to connect neural activity to perception and behavior. Here, we describe a trial-by-trial, spike-timing-based algorithm to reveal the coding capacities of these neurons in auditory and frontal cortex of behaving rats. Classically responsive and non-classically responsive cells contained significant information about sensory stimuli and behavioral decisions. Stimulus category was more accurately represented in frontal cortex than auditory cortex, via ensembles of non-classically responsive cells coordinating the behavioral meaning of spike timings on correct but not error trials. This unbiased approach allows the contribution of all recorded neurons \textendash{} particularly those without obvious task-related, trial-averaged firing rate modulation \textendash{} to be assessed for behavioral relevance on single trials.           ,              Neurons encode information in the form of electrical signals called spikes. Certain neurons increase the rate at which they produce spikes under specific circumstances, e.g., whenever an animal hears a particular sound. These neurons are said to be 'classically responsive'. But not all neurons behave in this way. Others produce spikes at a variable rate that does not obviously relate to the animal's behavior. These neurons are said to be 'non-classically responsive'. They are often omitted from analyses, despite typically outnumbering their classically responsive counterparts. So, what are these neurons doing?             To find out, Insanally et al. trained rats to respond to sounds. The animals learned to poke their nose into a window whenever they heard a specific tone, and to avoid responding whenever they heard any other tone. As the rats performed the task, Insanally et al. recorded from neurons in two areas of the brain, the frontal cortex and the auditory cortex. A computer then analyzed the activity of individual neurons during each trial.             As expected, the firing rate of non-classically responsive cells did not relate to the animals' behavior. But the timing of this firing did. The interval between spikes contained information about which tone the animals had heard and/or how they had responded. The cells worked together in groups to encode this information. Over the course of each trial, every neuron in the group varied the interval between its spikes. Eventually, the group reached a consensus, with all neurons using the same interval to represent information relevant to the task. Groups of neurons in the frontal cortex encoded more information about the category of the tone than those in the auditory cortex.             By including all neurons \textendash{} both classically and non-classically responsive \textendash{} this model offers a more comprehensive view of how neural activity relates to behavior. This may in turn help us understand the variable and complex neural activity seen in people with sensory and cognitive disorders.},
  file = {Insanally et al. - 2019 - Spike-timing-dependent ensemble encoding by non-cl 2.pdf},
  journal = {eLife},
  language = {en}
}

@article{Inzlicht2018,
  title = {The {{Effort Paradox}}: {{Effort Is Both Costly}} and {{Valued}}},
  shorttitle = {The {{Effort Paradox}}},
  author = {Inzlicht, Michael and Shenhav, Amitai and Olivola, Christopher Y.},
  year = {2018},
  month = apr,
  volume = {22},
  pages = {337--349},
  issn = {13646613},
  doi = {10.1016/j.tics.2018.01.007},
  file = {Inzlicht et al. - 2018 - The Effort Paradox Effort Is Both Costly and Valu.pdf},
  journal = {Trends in Cognitive Sciences},
  language = {en},
  number = {4}
}

@article{Ioffe2015,
  title = {Batch {{Normalization}}: {{Accelerating Deep Network Training}} by {{Reducing Internal Covariate Shift}}},
  shorttitle = {Batch {{Normalization}}},
  author = {Ioffe, Sergey and Szegedy, Christian},
  year = {2015},
  month = feb,
  abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batchnormalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9\% top-5 validation error (and 4.8\% test error), exceeding the accuracy of human raters.},
  archiveprefix = {arXiv},
  eprint = {1502.03167},
  eprinttype = {arxiv},
  file = {2015 - Ioffe, Szedegy - Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift.pdf;Ioffe and Szegedy - 2015 - Batch Normalization Accelerating Deep Network Tra.pdf},
  journal = {arXiv:1502.03167 [cs]},
  keywords = {Computer Science - Machine Learning},
  language = {en},
  primaryclass = {cs}
}

@article{Ishida,
  title = {Do {{We Need Zero Training Loss A}} Er {{Achieving Zero Training Error}}?},
  author = {Ishida, Takashi and Yamane, Ikko and Sakai, Tomoya and Niu, Gang and Sugiyama, Masashi},
  pages = {27},
  abstract = {Overparameterized deep networks have the capacity to memorize training data with zero training error. Even a er memorization, the training loss continues to approach zero, making the model overcon dent and the test performance degraded. Since existing regularizers do not directly aim to avoid zero training loss, they o en fail to maintain a moderate level of training loss, ending up with a too small or too large loss. We propose a direct solution called ooding that intentionally prevents further reduction of the training loss when it reaches a reasonably small value, which we call the ooding level. Our approach makes the loss oat around the ooding level by doing mini-batched gradient descent as usual but gradient ascent if the training loss is below the ooding level. is can be implemented with one line of code, and is compatible with any stochastic optimizer and other regularizers. With ooding, the model will continue to ``random walk'' with the same non-zero training loss, and we expect it to dri into an area with a at loss landscape that leads to be er generalization. We experimentally show that ooding improves performance and as a byproduct, induces a double descent curve of the test loss.},
  file = {Ishida et al. - Do We Need Zero Training Loss A er Achieving Zero .pdf},
  language = {en}
}

@article{Ishida2020,
  title = {Do {{We Need Zero Training Loss After Achieving Zero Training Error}}?},
  author = {Ishida, Takashi and Yamane, Ikko and Sakai, Tomoya and Niu, Gang and Sugiyama, Masashi},
  year = {2020},
  month = feb,
  abstract = {Overparameterized deep networks have the capacity to memorize training data with zero training error. Even a er memorization, the training loss continues to approach zero, making the model overcon dent and the test performance degraded. Since existing regularizers do not directly aim to avoid zero training loss, they o en fail to maintain a moderate level of training loss, ending up with a too small or too large loss. We propose a direct solution called ooding that intentionally prevents further reduction of the training loss when it reaches a reasonably small value, which we call the ooding level. Our approach makes the loss oat around the ooding level by doing mini-batched gradient descent as usual but gradient ascent if the training loss is below the ooding level. is can be implemented with one line of code, and is compatible with any stochastic optimizer and other regularizers. With ooding, the model will continue to ``random walk'' with the same non-zero training loss, and we expect it to dri into an area with a at loss landscape that leads to be er generalization. We experimentally show that ooding improves performance and as a byproduct, induces a double descent curve of the test loss.},
  archiveprefix = {arXiv},
  eprint = {2002.08709},
  eprinttype = {arxiv},
  file = {Ishida et al. - 2020 - Do We Need Zero Training Loss After Achieving Zero.pdf},
  journal = {arXiv:2002.08709 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryclass = {cs, stat}
}

@article{Ishii2002,
  title = {Control of Exploitation\textendash Exploration Meta-Parameter in Reinforcement Learning},
  author = {Ishii, Shin and Yoshida, Wako and Yoshimoto, Junichiro},
  year = {2002},
  month = jun,
  volume = {15},
  pages = {665--687},
  issn = {08936080},
  doi = {10.1016/S0893-6080(02)00056-4},
  abstract = {In reinforcement learning, the duality between exploitation and exploration has long been an important issue. This paper presents a new method that controls the balance between exploitation and exploration. Our learning scheme is based on model-based reinforcement learning, in which the Bayes inference with forgetting effect estimates the state-transition probability of the environment. The balance parameter, which corresponds to the randomness in action selection, is controlled based on variation of action results and perception of environmental change. When applied to maze tasks, our method successfully obtains good controls by adapting to environmental changes. Recently, Usher et al. [60] has suggested that noradrenergic neurons in the locus coeruleus may control the exploitation-exploration balance in a real brain and that the balance may correspond to the level of animal's selective attention. According to this scenario, we also discuss a possible implementation in the brain.},
  file = {Ishii et al. - 2002 - Control of exploitation–exploration meta-parameter.pdf},
  journal = {Neural Networks},
  language = {en},
  number = {4-6}
}

@article{Ishii2002a,
  title = {Control of Exploitation\textendash Exploration Meta-Parameter in Reinforcement Learning},
  author = {Ishii, Shin and Yoshida, Wako and Yoshimoto, Junichiro},
  year = {2002},
  month = jun,
  volume = {15},
  pages = {665--687},
  issn = {08936080},
  doi = {10.1016/S0893-6080(02)00056-4},
  abstract = {In reinforcement learning, the duality between exploitation and exploration has long been an important issue. This paper presents a new method that controls the balance between exploitation and exploration. Our learning scheme is based on model-based reinforcement learning, in which the Bayes inference with forgetting effect estimates the state-transition probability of the environment. The balance parameter, which corresponds to the randomness in action selection, is controlled based on variation of action results and perception of environmental change. When applied to maze tasks, our method successfully obtains good controls by adapting to environmental changes. Recently, Usher et al. [60] has suggested that noradrenergic neurons in the locus coeruleus may control the exploitation-exploration balance in a real brain and that the balance may correspond to the level of animal's selective attention. According to this scenario, we also discuss a possible implementation in the brain.},
  file = {Ishii et al. - 2002 - Control of exploitation–exploration meta-parameter 2.pdf},
  journal = {Neural Networks},
  language = {en},
  number = {4-6}
}

@article{Ispolatov2015,
  title = {Chaos in High-Dimensional Dynamical Systems},
  author = {Ispolatov, Iaroslav and Doebeli, Michael and Allende, Sebastian and Madhok, Vaibhav},
  year = {2015},
  month = dec,
  volume = {5},
  issn = {2045-2322},
  doi = {10.1038/srep12506},
  abstract = {For general dissipative dynamical systems we study what fraction of solutions exhibit chaotic behavior depending on the dimensionality \$d\$ of the phase space. We find that a system of \$d\$ globally coupled ODE's with quadratic and cubic non-linearities with random coefficients and initial conditions, the probability of a trajectory to be chaotic increases universally from \$\textbackslash sim 10\^\{-5\} - 10\^\{-4\}\$ for \$d=3\$ to essentially one for \$d\textbackslash sim 50\$. In the limit of large \$d\$, the invariant measure of the dynamical systems exhibits universal scaling that depends on the degree of non-linearity but does not depend on the choice of coefficients, and the largest Lyapunov exponent converges to a universal scaling limit. Using statistical arguments, we provide analytical explanations for the observed scaling and for the probability of chaos.},
  archiveprefix = {arXiv},
  eprint = {1410.6403},
  eprinttype = {arxiv},
  file = {2014 - Ispolatov et al. - Chaos in high-dimensional dynamical systems.pdf;2015 - Ispolatov et al. - Chaos in high-dimensional dissipative dynamical systems.pdf;Ispolatov et al. - 2015 - Chaos in high-dimensional dissipative dynamical sy.pdf;Ispolatov et al. - 2015 - Chaos in high-dimensional dynamical systems.pdf},
  journal = {Scientific Reports},
  keywords = {Condensed Matter - Disordered Systems and Neural Networks,Nonlinear Sciences - Chaotic Dynamics},
  language = {en},
  number = {1}
}

@article{Itti2009,
  title = {Bayesian Surprise Attracts Human Attention},
  author = {Itti, Laurent and Baldi, Pierre},
  year = {2009},
  month = jun,
  volume = {49},
  pages = {1295--1306},
  issn = {00426989},
  doi = {10.1016/j.visres.2008.09.007},
  abstract = {We propose a formal Bayesian definition of surprise to capture subjective aspects of sensory information. Surprise measures how data affects an observer, in terms of differences between posterior and prior beliefs about the world. Only data observations which substantially affect the observer's beliefs yield surprise, irrespectively of how rare or informative in Shannon's sense these observations are. We test the framework by quantifying the extent to which humans may orient attention and gaze towards surprising events or items while watching television. To this end, we implement a simple computational model where a low-level, sensory form of surprise is computed by simple simulated early visual neurons. Bayesian surprise is a strong attractor of human attention, with 72\% of all gaze shifts directed towards locations more surprising than the average, a figure rising to 84\% when focusing the analysis onto regions simultaneously selected by all observers. The proposed theory of surprise is applicable across different spatiotemporal scales, modalities, and levels of abstraction.},
  file = {2009 - Itti, Baldi - Bayesian surprise attracts human attention.pdf},
  journal = {Vision Research},
  language = {en},
  number = {10}
}

@article{Ivanov,
  title = {Axonal {{Conduction Velocity Impacts Neuronal Network Oscillations}}},
  author = {Ivanov, Vladimir A and Polykretis, Ioannis E and Michmizos, Konstantinos P},
  pages = {4},
  abstract = {Increasing experimental evidence suggests that axonal action potential conduction velocity is a highly adaptive parameter in the adult central nervous system. Yet, the effects of this newfound plasticity on global brain dynamics is poorly understood. In this work, we analyzed oscillations in biologically plausible neuronal networks with different conduction velocity distributions. Changes of \dbend\dbend\dbend\dbend\dbend\dbend{} - \dbend\dbend\dbend\dbend\dbend\dbend{} (ms) in network mean signal transmission time resulted in substantial network oscillation frequency changes ranging in \dbend\dbend\dbend\dbend\dbend\dbend{} - \dbend\dbend\dbend\dbend\dbend\dbend\dbend\dbend\dbend\dbend\dbend\dbend\dbend\dbend\dbend\dbend\dbend\dbend{} (Hz). Our results suggest that changes in axonal conduction velocity may significantly affect both the frequency and synchrony of brain rhythms, which have well established connections to learning, memory, and other cognitive processes.},
  file = {../../Downloads/lee2011.pdf;Ivanov et al. - Axonal Conduction Velocity Impacts Neuronal Networ.pdf},
  language = {en}
}

@article{Iyer2013,
  title = {The {{Influence}} of {{Synaptic Weight Distribution}} on {{Neuronal Population Dynamics}}},
  author = {Iyer, Ramakrishnan and Menon, Vilas and Buice, Michael and Koch, Christof and Mihalas, Stefan},
  editor = {Sporns, Olaf},
  year = {2013},
  month = oct,
  volume = {9},
  pages = {e1003248},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1003248},
  abstract = {The manner in which different distributions of synaptic weights onto cortical neurons shape their spiking activity remains open. To characterize a homogeneous neuronal population, we use the master equation for generalized leaky integrateand-fire neurons with shot-noise synapses. We develop fast semi-analytic numerical methods to solve this equation for either current or conductance synapses, with and without synaptic depression. We show that its solutions match simulations of equivalent neuronal networks better than those of the Fokker-Planck equation and we compute bounds on the network response to non-instantaneous synapses. We apply these methods to study different synaptic weight distributions in feed-forward networks. We characterize the synaptic amplitude distributions using a set of measures, called tail weight numbers, designed to quantify the preponderance of very strong synapses. Even if synaptic amplitude distributions are equated for both the total current and average synaptic weight, distributions with sparse but strong synapses produce higher responses for small inputs, leading to a larger operating range. Furthermore, despite their small number, such synapses enable the network to respond faster and with more stability in the face of external fluctuations.},
  file = {2013 - Iyer et al. - The Influence of Synaptic Weight Distribution on Neuronal Population Dynamics.pdf},
  journal = {PLoS Computational Biology},
  language = {en},
  number = {10}
}

@article{Izhikevich2003,
  title = {Relating {{STDP}} to {{BCM}}},
  author = {Izhikevich, Eugene M. and Desai, Niraj S.},
  year = {2003},
  month = jul,
  volume = {15},
  pages = {1511--1523},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/089976603321891783},
  file = {2003 - Izhikevich, Desai - Relating STDP to BCM.pdf},
  journal = {Neural Computation},
  language = {en},
  number = {7}
}

@article{Izhikevich2003a,
  title = {Simple Model of Spiking Neurons},
  author = {Izhikevich, E.M.},
  year = {2003},
  month = nov,
  volume = {14},
  pages = {1569--1572},
  issn = {1045-9227},
  doi = {10.1109/TNN.2003.820440},
  abstract = {A model is presented that reproduces spiking and bursting behavior of known types of cortical neurons. The model combines the biologically plausibility of Hodgkin\textendash Huxley-type dynamics and the computational efficiency of integrate-and-fire neurons. Using this model, one can simulate tens of thousands of spiking cortical neurons in real time (1 ms resolution) using a desktop PC.},
  file = {2003 - Izhikevich - Simple model of spiking neurons.pdf},
  journal = {IEEE Transactions on Neural Networks},
  language = {en},
  number = {6}
}

@article{Izhikevich2004,
  title = {Which {{Model}} to {{Use}} for {{Cortical Spiking Neurons}}?},
  author = {Izhikevich, E.M.},
  year = {2004},
  month = sep,
  volume = {15},
  pages = {1063--1070},
  issn = {1045-9227},
  doi = {10.1109/TNN.2004.832719},
  abstract = {We discuss the biological plausibility and computational efficiency of some of the most useful models of spiking and bursting neurons. We compare their applicability to large-scale simulations of cortical neural networks.},
  file = {2004 - Izhikevich - Which model to use for cortical spiking neurons.pdf},
  journal = {IEEE Transactions on Neural Networks},
  language = {en},
  number = {5}
}

@book{Izhikevich2006,
  title = {Dynamical {{Systems}} in {{Neuroscience}}: {{The Geometry}} of {{Excitability}} and {{Bursting}}},
  shorttitle = {Dynamical {{Systems}} in {{Neuroscience}}},
  author = {Izhikevich, Eugene M.},
  year = {2006},
  publisher = {{The MIT Press}},
  doi = {10.7551/mitpress/2526.001.0001},
  file = {2004 - Izhikevich - Dynamical Systems in Neuroscience.pdf},
  isbn = {978-0-262-27607-8},
  language = {en}
}

@article{Izhikevich2006a,
  title = {Polychronization: {{Computation}} with {{Spikes}}},
  shorttitle = {Polychronization},
  author = {Izhikevich, Eugene M.},
  year = {2006},
  month = feb,
  volume = {18},
  pages = {245--282},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/089976606775093882},
  file = {2006 - Izhikevich - Polychronization computation with spikes.pdf},
  journal = {Neural Computation},
  language = {en},
  number = {2}
}

@techreport{Izhikevich2018,
  title = {Measuring the Average Power of Neural Oscillations},
  author = {Izhikevich, Liz and Gao, Richard and Peterson, Erik and Voytek, Bradley},
  year = {2018},
  month = oct,
  institution = {{Neuroscience}},
  doi = {10.1101/441626},
  abstract = {Background: Neural oscillations are often quantified as average power relative to a cognitive, perceptual, and/or behavioral task. This is commonly done using Fourier-based techniques, such as Welch's method for estimating the power spectral density, and/or by estimating narrowband oscillatory power across trials, conditions, and/or groups. The core assumption underlying these approaches is that the mean is an appropriate measure of central tendency. Despite the importance of this assumption, it has not been rigorously tested. New method: We introduce extensions of common approaches that are better suited for the physiological reality of how neural oscillations often manifest: as nonstationary, high-power bursts, rather than sustained rhythms. Log-transforming, or taking the median power, significantly reduces erroneously inflated power estimates. Results: Analyzing 101 participants' worth of human electrophysiology, totaling 3,560 channels and over 40 hours data, we show that, in all cases examined, spectral power is not Gaussian distributed. This is true even when oscillations are prominent and sustained, such as visual cortical alpha. Power across time, at every frequency, is characterized by a substantial long tail, which implies that estimates of average power are skewed toward large, infrequent high-power oscillatory bursts. Comparison with existing methods: In a simulated event-related experiment we show how introducing just a few high-power oscillatory bursts, as seen in real data, can, perhaps erroneously, cause significant differences between conditions using traditional methods. These erroneous effects are substantially reduced with our new methods. Conclusions: These results call into question the validity of common statistical practices in neural oscillation research.},
  file = {Izhikevich et al. - 2018 - Measuring the average power of neural oscillations.pdf},
  language = {en},
  type = {Preprint}
}

@article{Jackson2014,
  title = {Reversal of Theta Rhythm Flow through Intact Hippocampal Circuits},
  author = {Jackson, Jesse and Amilhon, B{\'e}n{\'e}dicte and Goutagny, Romain and Bott, Jean-Bastien and Manseau, Fr{\'e}d{\'e}ric and Kortleven, Christian and Bressler, Steven L and Williams, Sylvain},
  year = {2014},
  month = oct,
  volume = {17},
  pages = {1362--1370},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.3803},
  file = {Jackson et al. - 2014 - Reversal of theta rhythm flow through intact hippo.pdf},
  journal = {Nature Neuroscience},
  language = {en},
  number = {10}
}

@article{Jacobs2014,
  title = {Hippocampal Theta Oscillations Are Slower in Humans than in Rodents: Implications for Models of Spatial Navigation and Memory},
  shorttitle = {Hippocampal Theta Oscillations Are Slower in Humans than in Rodents},
  author = {Jacobs, Joshua},
  year = {2014},
  month = feb,
  volume = {369},
  pages = {20130304},
  issn = {0962-8436, 1471-2970},
  doi = {10.1098/rstb.2013.0304},
  file = {Jacobs - 2014 - Hippocampal theta oscillations are slower in human.pdf},
  journal = {Phil. Trans. R. Soc. B},
  language = {en},
  number = {1635}
}

@article{Jaderberg,
  title = {Decoupled {{Neural Interfaces}} Using {{Synthetic Gradients}}},
  author = {Jaderberg, Max and Czarnecki, Wojciech Marian and Osindero, Simon and Vinyals, Oriol and Graves, Alex and Kavukcuoglu, Koray},
  pages = {16},
  abstract = {Training directed neural networks typically requires forward-propagating data through a computation graph, followed by backpropagating error signal, to produce weight updates. All layers, or more generally, modules, of the network are therefore locked, in the sense that they must wait for the remainder of the network to execute forwards and propagate error backwards before they can be updated. In this work we break this constraint by decoupling modules by introducing a model of the future computation of the network graph. These models predict what the result of the modelled subgraph will produce using only local information. In particular we focus on modelling error gradients: by using the modelled synthetic gradient in place of true backpropagated error gradients we decouple subgraphs, and can update them independently and asynchronously i.e. we realise decoupled neural interfaces. We show results for feed-forward models, where every layer is trained asynchronously, recurrent neural networks (RNNs) where predicting one's future gradient extends the time over which the RNN can effectively model, and also a hierarchical RNN system with ticking at different timescales. Finally, we demonstrate that in addition to predicting gradients, the same framework can be used to predict inputs, resulting in models which are decoupled in both the forward and backwards pass \textendash{} amounting to independent networks which co-learn such that they can be composed into a single functioning corporation.},
  file = {Jaderberg et al. - Decoupled Neural Interfaces using Synthetic Gradie.pdf},
  language = {en}
}

@article{Jaderberg2017,
  title = {Population {{Based Training}} of {{Neural Networks}}},
  author = {Jaderberg, Max and Dalibard, Valentin and Osindero, Simon and Czarnecki, Wojciech M. and Donahue, Jeff and Razavi, Ali and Vinyals, Oriol and Green, Tim and Dunning, Iain and Simonyan, Karen and Fernando, Chrisantha and Kavukcuoglu, Koray},
  year = {2017},
  month = nov,
  abstract = {Neural networks dominate the modern machine learning landscape, but their training and success still suffer from sensitivity to empirical choices of hyperparameters such as model architecture, loss function, and optimisation algorithm. In this work we present Population Based Training (PBT), a simple asynchronous optimisation algorithm which effectively utilises a fixed computational budget to jointly optimise a population of models and their hyperparameters to maximise performance. Importantly, PBT discovers a schedule of hyperparameter settings rather than following the generally sub-optimal strategy of trying to find a single fixed set to use for the whole course of training. With just a small modification to a typical distributed hyperparameter training framework, our method allows robust and reliable training of models. We demonstrate the effectiveness of PBT on deep reinforcement learning problems, showing faster wall-clock convergence and higher final performance of agents by optimising over a suite of hyperparameters. In addition, we show the same method can be applied to supervised learning for machine translation, where PBT is used to maximise the BLEU score directly, and also to training of Generative Adversarial Networks to maximise the Inception score of generated images. In all cases PBT results in the automatic discovery of hyperparameter schedules and model selection which results in stable training and better final performance.},
  archiveprefix = {arXiv},
  eprint = {1711.09846},
  eprinttype = {arxiv},
  file = {Jaderberg et al. - 2017 - Population Based Training of Neural Networks.pdf},
  journal = {arXiv:1711.09846 [cs]},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  language = {en},
  primaryclass = {cs}
}

@article{Jadi2012,
  title = {Location-{{Dependent Effects}} of {{Inhibition}} on {{Local Spiking}} in {{Pyramidal Neuron Dendrites}}},
  author = {Jadi, Monika and Polsky, Alon and Schiller, Jackie and Mel, Bartlett W.},
  editor = {Gutkin, Boris S.},
  year = {2012},
  month = jun,
  volume = {8},
  pages = {e1002550},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1002550},
  abstract = {Cortical computations are critically dependent on interactions between pyramidal neurons (PNs) and a menagerie of inhibitory interneuron types. A key feature distinguishing interneuron types is the spatial distribution of their synaptic contacts onto PNs, but the location-dependent effects of inhibition are mostly unknown, especially under conditions involving active dendritic responses. We studied the effect of somatic vs. dendritic inhibition on local spike generation in basal dendrites of layer 5 PNs both in neocortical slices and in simple and detailed compartmental models, with equivalent results: somatic inhibition divisively suppressed the amplitude of dendritic spikes recorded at the soma while minimally affecting dendritic spike thresholds. In contrast, distal dendritic inhibition raised dendritic spike thresholds while minimally affecting their amplitudes. On-the-path dendritic inhibition modulated both the gain and threshold of dendritic spikes depending on its distance from the spike initiation zone. Our findings suggest that cortical circuits could assign different mixtures of gain vs. threshold inhibition to different neural pathways, and thus tailor their local computations, by managing their relative activation of soma- vs. dendrite-targeting interneurons.},
  file = {2012 - Jadi et al. - Location-dependent effects of inhibition on local spiking in pyramidal neuron dendrites.pdf},
  journal = {PLoS Computational Biology},
  language = {en},
  number = {6}
}

@article{Jadi2014,
  title = {Cortical Oscillations Arise from Contextual Interactions That Regulate Sparse Coding},
  author = {Jadi, M. P. and Sejnowski, T. J.},
  year = {2014},
  month = may,
  volume = {111},
  pages = {6780--6785},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1405300111},
  file = {2015 - Jadi, Sejnowski - Correction for Jadi and Sejnowski, Cortical oscillations arise from contextual interactions that regulate spars.pdf;Jadi and Sejnowski - 2014 - Cortical oscillations arise from contextual intera 2.pdf;Jadi and Sejnowski - 2014 - Cortical oscillations arise from contextual intera 3.pdf;Jadi and Sejnowski - 2014 - Cortical oscillations arise from contextual intera.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {18}
}

@article{Jadi2014a,
  title = {Regulating {{Cortical Oscillations}} in an {{Inhibition}}-{{Stabilized Network}}},
  author = {Jadi, Monika P. and Sejnowski, Terrence J.},
  year = {2014},
  month = may,
  volume = {102},
  pages = {830--842},
  issn = {0018-9219, 1558-2256},
  doi = {10.1109/JPROC.2014.2313113},
  abstract = {Understanding the anatomical and functional brain. Empirically testing such predictions is still challenging, architecture of the brain is essential for designing neurally and implementing the proposed coding and communication inspired intelligent systems. Theoretical and empirical studies strategies in neuromorphic systems could assist in our suggest a role for narrowband oscillations in shaping the understanding of the biological system.},
  file = {2014 - Jadi, Sejnowski - Regulating Cortical Oscillation in an Inhibition-Stabilized Network.pdf;Jadi and Sejnowski - 2014 - Regulating Cortical Oscillations in an Inhibition-.pdf},
  journal = {Proceedings of the IEEE},
  language = {en},
  number = {5}
}

@article{Jaegle2019,
  title = {Visual Novelty, Curiosity, and Intrinsic Reward in Machine Learning and the Brain},
  author = {Jaegle, Andrew and Mehrpour, Vahid and Rust, Nicole},
  year = {2019},
  volume = {1901.02478},
  pages = {13},
  abstract = {A strong preference for novelty emerges in infancy and is prevalent across the animal kingdom. When incorporated into reinforcement-based machine learning algorithms, visual novelty can act as an intrinsic reward signal that vastly increases the efficiency of exploration and expedites learning, particularly in situations where external rewards are difficult to obtain. Here we review parallels between recent developments in novelty-driven machine learning algorithms and our understanding of how visual novelty is computed and signaled in the primate brain. We propose that in the visual system, novelty representations are not configured with the principal goal of detecting novel objects, but rather with the broader goal of flexibly generalizing novelty information across different states in the service of driving novelty-based learning.},
  file = {Jaegle et al. - Visual novelty, curiosity, and intrinsic reward in.pdf},
  journal = {Arxiv},
  language = {en}
}

@article{Jakel2009,
  title = {Does {{Cognitive Science Need Kernels}}?},
  author = {J{\"a}kel, Frank and Sch{\"o}lkopf, Bernhard and Wichmann, Felix A.},
  year = {2009},
  month = sep,
  volume = {13},
  pages = {381--388},
  issn = {13646613},
  doi = {10.1016/j.tics.2009.06.002},
  file = {2009 - Jäkel, Schölkopf, Wichmann - Does cognitive science need kernels.pdf},
  journal = {Trends in Cognitive Sciences},
  language = {en},
  number = {9}
}

@article{James2011,
  title = {Assessing {{L\'evy}} Walks as Models of Animal Foraging},
  author = {James, Alex and Plank, Michael J. and Edwards, Andrew M.},
  year = {2011},
  month = sep,
  volume = {8},
  pages = {1233--1247},
  issn = {1742-5689, 1742-5662},
  doi = {10.1098/rsif.2011.0200},
  abstract = {The hypothesis that the optimal search strategy is a L\'evy walk (LW) or L\'evy flight, originally suggested in 1995, has generated an explosion of interest and controversy. Long-standing empirical evidence supporting the LW hypothesis has been overturned, while new models and data are constantly being published. Statistical methods have been criticized and new methods put forward. In parallel with the empirical studies, theoretical search models have been developed. Some theories have been disproved while others remain. Here, we gather together the current state of the art on the role of LWs in optimal foraging theory. We examine the body of theory underpinning the subject. Then we present new results showing that deviations from the idealized one-dimensional search model greatly reduce or remove the advantage of LWs. The search strategy of an LW with exponent               {$\mu$}               = 2 is therefore not as robust as is widely thought. We also review the available techniques, and their potential pitfalls, for analysing field data. It is becoming increasingly recognized that there is a wide range of mechanisms that can lead to the apparent observation of power-law patterns. The consequence of this is that the detection of such patterns in field data implies neither that the foragers in question are performing an LW, nor that they have evolved to do so. We conclude that LWs are neither a universal optimal search strategy, nor are they as widespread in nature as was once thought.},
  file = {James et al. - 2011 - Assessing Lévy walks as models of animal foraging.pdf},
  journal = {J. R. Soc. Interface.},
  language = {en},
  number = {62}
}

@article{Jamieson,
  title = {Lil' {{UCB}} : {{An Optimal Exploration Algorithm}} for {{Multi}}-{{Armed Bandits}}},
  author = {Jamieson, Kevin and Malloy, Matthew and Nowak, Robert and Bubeck, Sebastien},
  pages = {17},
  abstract = {The paper proposes a novel upper confidence bound (UCB) procedure for identifying the arm with the largest mean in a multi-armed bandit game in the fixed confidence setting using a small number of total samples. The procedure cannot be improved in the sense that the number of samples required to identify the best arm is within a constant factor of a lower bound based on the law of the iterated logarithm (LIL). Inspired by the LIL, we construct our confidence bounds to explicitly account for the infinite time horizon of the algorithm. In addition, by using a novel stopping time for the algorithm we avoid a union bound over the arms that has been observed in other UCBtype algorithms. We prove that the algorithm is optimal up to constants and also show through simulations that it provides superior performance with respect to the state-of-the-art.},
  file = {Jamieson et al. - lil’ UCB  An Optimal Exploration Algorithm for Mu.pdf},
  language = {en}
}

@article{Jang2017,
  title = {Task-Specific Feature Extraction and Classification of {{fMRI}} Volumes Using a Deep Neural Network Initialized with a Deep Belief Network: {{Evaluation}} Using Sensorimotor Tasks},
  shorttitle = {Task-Specific Feature Extraction and Classification of {{fMRI}} Volumes Using a Deep Neural Network Initialized with a Deep Belief Network},
  author = {Jang, Hojin and Plis, Sergey M. and Calhoun, Vince D. and Lee, Jong-Hwan},
  year = {2017},
  month = jan,
  volume = {145},
  pages = {314--328},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2016.04.003},
  abstract = {Feedforward deep neural networks (DNN), artificial neural networks with multiple hidden layers, have recently demonstrated a record-breaking performance in multiple areas of applications in computer vision and speech processing. Following the success, DNNs have been applied to neuroimaging modalities including functional/structural magnetic resonance imaging (MRI) and positron-emission tomography data. However, no study has explicitly applied DNNs to 3D wholebrain fMRI volumes and thereby extracted hidden volumetric representations of fMRI that are discriminative for a task performed as the fMRI volume was acquired. Our study applied fully connected feedforward DNN to fMRI volumes collected in four sensorimotor tasks (i.e., left-hand clenching, right-hand clenching, auditory attention, and visual stimulus) undertaken by 12 healthy participants. Using a leave-one-subject-out cross-validation scheme, a restricted Boltzmann machinebased deep belief network was pretrained and used to initialize weights of the DNN. The pretrained DNN was fine-tuned while systematically controlling weight-sparsity levels across hidden layers. Optimal weight-sparsity levels were determined from a minimum validation error rate of fMRI volume classification. Minimum error rates (mean  standard deviation; \%) of 6.9 ( 3.8) were obtained from the three-layer DNN with the sparsest condition of weights across the three hidden layers. These error rates were even lower than the error rates from the single-layer network (9.4 {$\pm$} 4.6) and the two-layer network (7.4 {$\pm$} 4.1). The estimated DNN weights showed spatial patterns that are remarkably task-specific, particularly in the higher layers. The output values of the third hidden layer represented distinct patterns/codes of the 3D whole-brain fMRI volume and encoded the information of the tasks as evaluated from representational similarity analysis. Our reported findings show the ability of the DNN to classify a single fMRI volume based on the extraction of hidden representations of fMRI volumes associated with tasks across multiple hidden layers. Our study may be beneficial to the automatic classification/diagnosis of neuropsychiatric and neurological diseases and prediction of disease severity and recovery in (pre-) clinical settings using fMRI volumes without requiring an estimation of activation patterns or ad hoc statistical evaluation.},
  file = {Jang et al. - 2017 - Task-specific feature extraction and classificatio.pdf},
  journal = {NeuroImage},
  language = {en}
}

@article{Jansen2012,
  title = {Comment on "{{Levy Walks Evolve Through Interaction Between Movement}} and {{Environmental Complexity}}"},
  author = {Jansen, V. A. A. and Mashanova, A. and Petrovskii, S.},
  year = {2012},
  month = feb,
  volume = {335},
  pages = {918--918},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1215747},
  file = {Jansen et al. - 2012 - Comment on Levy Walks Evolve Through Interaction .pdf},
  journal = {Science},
  language = {en},
  number = {6071}
}

@article{Janson2007,
  title = {What Wild Primates Know about Resources: Opening up the Black Box},
  shorttitle = {What Wild Primates Know about Resources},
  author = {Janson, Charles H. and Byrne, Richard},
  year = {2007},
  month = jun,
  volume = {10},
  pages = {357--367},
  issn = {1435-9448, 1435-9456},
  doi = {10.1007/s10071-007-0080-9},
  file = {Janson and Byrne - 2007 - What wild primates know about resources opening u.pdf},
  journal = {Anim Cogn},
  language = {en},
  number = {3}
}

@article{Japyassu2017,
  title = {Extended Spider Cognition},
  author = {Japyass{\'u}, Hilton F. and Laland, Kevin N.},
  year = {2017},
  month = may,
  volume = {20},
  pages = {375--395},
  issn = {1435-9448, 1435-9456},
  doi = {10.1007/s10071-017-1069-7},
  abstract = {There is a tension between the conception of cognition as a central nervous system (CNS) process and a view of cognition as extending towards the body or the contiguous environment. The centralised conception requires large or complex nervous systems to cope with complex environments. Conversely, the extended conception involves the outsourcing of information processing to the body or environment, thus making fewer demands on the processing power of the CNS. The evolution of extended cognition should be particularly favoured among small, generalist predators such as spiders, and here, we review the literature to evaluate the fit of empirical data with these contrasting models of cognition. Spiders do not seem to be cognitively limited, displaying a large diversity of learning processes, from habituation to contextual learning, including a sense of numerosity. To tease apart the central from the extended cognition, we apply the mutual manipulability criterion, testing the existence of reciprocal causal links between the putative elements of the system. We conclude that the web threads and configurations are integral parts of the cognitive systems. The extension of cognition to the web helps to explain some puzzling features of spider behaviour and seems to promote evolvability within the group, enhancing innovation through cognitive connectivity to variable habitat features.},
  file = {Japyassú and Laland - 2017 - Extended spider cognition.pdf},
  journal = {Anim Cogn},
  language = {en},
  number = {3}
}

@article{Jastorff2009,
  title = {Visual {{Learning Shapes}} the {{Processing}} of {{Complex Movement Stimuli}} in the {{Human Brain}}},
  author = {Jastorff, J. and Kourtzi, Z. and Giese, M. A.},
  year = {2009},
  month = nov,
  volume = {29},
  pages = {14026--14038},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.3070-09.2009},
  file = {2009 - Jastorff, Kourtzi, Giese - Visual learning shapes the processing of complex movement stimuli in the human brain.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {44}
}

@article{Jefferys,
  title = {Sharpening {{Ockham}}'s {{Razor}} on a {{Bayesian Strop}} ({{Key}} Terms: {{Bayes}}' Theorem {{Ockham}}'s Razor)},
  author = {Jefferys, William H and Berger, James O},
  pages = {14},
  file = {1991 - Je, Berger - Sharpening Ockham ' s Razor on a Bayesian Strop ( Key terms Bayes ' theorem Ockham ' s razor ).pdf},
  language = {en}
}

@article{Jefferys1992,
  title = {Ockham's {{Razor}} and {{Bayesian Analysis}}},
  author = {Jefferys, William H. and {work(s):}, James O. Berger Reviewed},
  year = {1992},
  volume = {80},
  pages = {64--72},
  file = {1992 - Jefferys, Berger - Ockham's Razor and Bayesian Analysis.pdf},
  journal = {American Scientist},
  language = {en},
  number = {1}
}

@article{Jehiel,
  title = {Analogy Based Expectation Equilibrium},
  author = {Jehiel, P},
  pages = {38},
  abstract = {It is assumed that players bundle nodes in which other players must move into analogy classes, and players only have expectations about the average behavior in every class. A solution concept is proposed for multi-stage games with perfect information: at every node players choose best-responses to their analogy-based expectations, and expectations are correct on average over those various nodes pooled together into the same analogy classes. The approach is applied to a variety of games. It is shown that a player may bene\TH t from having a coarse analogy partitioning. And for simple analogy partitioning, (1) initial cooperation followed by an end opportunistic behavior may emerge in the \TH nitely repeated prisoner's dilemma (or in the centipede game), (2) an agreement need not be reached immediately in bargaining games with complete information.},
  file = {2005 - Jehiel - Analogy-based expectation equilibrium.pdf},
  language = {en}
}

@article{Jensen1998,
  title = {An {{Oscillatory Short}}-{{Term Memory Buffer Model Can Account}} for {{Data}} on the {{Sternberg Task}}},
  author = {Jensen, Ole and Lisman, John E.},
  year = {1998},
  month = dec,
  volume = {18},
  pages = {10688--10699},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.18-24-10688.1998},
  file = {1998 - Jensen, Lisman - An oscillatory short-term memory buffer model can account for data on the Sternberg task.pdf},
  journal = {The Journal of Neuroscience},
  language = {en},
  number = {24}
}

@article{Jensen2002,
  title = {Oscillations in the {{Alpha Band}} (9-12 {{Hz}}) {{Increase}} with {{Memory Load}} during {{Retention}} in a {{Short}}-Term {{Memory Task}}},
  author = {Jensen, O.},
  year = {2002},
  month = aug,
  volume = {12},
  pages = {877--882},
  issn = {14602199},
  doi = {10.1093/cercor/12.8.877},
  file = {2002 - Jensen et al. - Oscillations in the alpha band (9-12 Hz) increase with memory load during retention in a short-term memory task.pdf},
  journal = {Cerebral Cortex},
  language = {en},
  number = {8}
}

@article{Jensen2016,
  title = {Discriminating {{Valid}} from {{Spurious Indices}} of {{Phase}}-{{Amplitude Coupling}}},
  author = {Jensen, Ole and Spaak, Eelke and Park, Hyojin},
  year = {2016},
  volume = {3},
  pages = {ENEURO.0334-16.2016},
  issn = {2373-2822},
  doi = {10.1523/ENEURO.0334-16.2016},
  abstract = {Recently there has been a strong interest in cross-frequency coupling, the interaction between neuronal oscillations in different frequency bands. In particular, measures quantifying the coupling between the phase of slow oscillations and the amplitude of fast oscillations have been applied to a wide range of data recorded from animals and humans. Some of the measures applied to detect phase-amplitude coupling have been criticized for being sensitive to nonsinusoidal properties of the oscillations and thus spuriously indicate the presence of coupling. While such instances of spurious identification of coupling have been observed, in this commentary we give concrete examples illustrating cases when the identification of cross-frequency coupling can be trusted. These examples are based on control analyses and empirical observations rather than signal-processing tools. Finally, we provide concrete advice on how to determine when measures of phase-amplitude coupling can be considered trustworthy.},
  file = {Jensen et al. - 2016 - Discriminating Valid from Spurious Indices of Phas.pdf},
  journal = {eneuro},
  language = {en},
  number = {6}
}

@techreport{Ji2020,
  title = {A Neural Circuit for Flexible Control of Persistent Behavioral States},
  author = {Ji, Ni and Madan, Gurrein K. and Fabre, Guadalupe I. and Dayan, Alyssa and Baker, Casey M. and Nwabudike, Ijeoma and Flavell, Steven W.},
  year = {2020},
  month = feb,
  institution = {{Neuroscience}},
  doi = {10.1101/2020.02.04.934547},
  abstract = {To adapt to their environments, animals must generate behaviors that are closely aligned to a rapidly changing sensory world. However, behavioral states such as foraging or courtship typically persist over long time scales to ensure proper execution. It remains unclear how neural circuits generate persistent behavioral states while maintaining the flexibility to select among alternative states when the sensory context changes. Here, we elucidate the functional architecture of a neural circuit controlling the choice between roaming and dwelling states, which underlie exploration and exploitation during foraging in C. elegans. By imaging ensemble-level neural activity in freely-moving animals, we identify stable, circuit-wide activity patterns corresponding to each behavioral state. Combining circuit-wide imaging with genetic analysis, we find that mutual inhibition between two antagonistic neuromodulatory systems underlies the persistence and mutual exclusivity of the opposing network states. Through machine learning analysis and circuit perturbations, we identify a sensory processing neuron that can transmit information about food odors to both the roaming and dwelling circuits and bias the animal towards different states in different sensory contexts, giving rise to context-appropriate state transitions. Our findings reveal a potentially general circuit architecture that enables flexible, sensory-driven control of persistent behavioral states.},
  file = {Ji et al. - 2020 - A neural circuit for flexible control of persisten.pdf},
  language = {en},
  type = {Preprint}
}

@article{Jiang2019,
  title = {Fantastic {{Generalization Measures}} and {{Where}} to {{Find Them}}},
  author = {Jiang, Yiding and Neyshabur, Behnam and Mobahi, Hossein and Krishnan, Dilip and Bengio, Samy},
  year = {2019},
  month = dec,
  abstract = {Generalization of deep networks has been of great interest in recent years, resulting in a number of theoretically and empirically motivated complexity measures. However, most papers proposing such measures study only a small set of models, leaving open the question of whether the conclusion drawn from those experiments would remain valid in other settings. We present the first large scale study of generalization in deep networks. We investigate more then 40 complexity measures taken from both theoretical bounds and empirical studies. We train over 10,000 convolutional networks by systematically varying commonly used hyperparameters. Hoping to uncover potentially causal relationships between each measure and generalization, we analyze carefully controlled experiments and show surprising failures of some measures as well as promising measures for further research.},
  archiveprefix = {arXiv},
  eprint = {1912.02178},
  eprinttype = {arxiv},
  file = {Jiang et al. - 2019 - Fantastic Generalization Measures and Where to Fin.pdf},
  journal = {arXiv:1912.02178 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryclass = {cs, stat}
}

@article{Jimura2012,
  title = {Analyses of Regional-Average Activation and Multivoxel Pattern Information Tell Complementary Stories},
  author = {Jimura, Koji and Poldrack, Russell A.},
  year = {2012},
  month = mar,
  volume = {50},
  pages = {544--552},
  issn = {00283932},
  doi = {10.1016/j.neuropsychologia.2011.11.007},
  abstract = {Multivariate pattern analysis (MVPA) has recently received increasing attention in functional neuroimaging due to its ability to decode mental states from fMRI signals. However, questions remain regarding both the empirical and conceptual relationships between results from MVPA and standard univariate analyses. In the current study, whole-brain univariate and searchlight MVPAs of parametric manipulations of monetary gain and loss in a decision making task (Tom et al., 2007) were compared to identify the differences in the results across these methods and the implications for understanding the underlying mental processes. The MVPA and univariate results did identify some overlapping regions in whole brain analyses. However, an analysis of consistency revealed that in many regions the effect size estimates obtained from MVPA and univariate analysis were uncorrelated. Moreover, comparison of sensitivity showed a general trend towards greater sensitivity to task manipulations by MVPA compared to univariate analysis. These results demonstrate that MVPA methods may provide a different view of the functional organization of mental processing compared to univariate analysis, wherein MVPA is more sensitive to distributed coding of information whereas univariate analysis is more sensitive to global engagement in ongoing tasks. The results also highlight the need for better ways to integrate these methods.},
  file = {2012 - Jimura, Poldrack - Analyses of regional-average activation and multivoxel pattern information tell complementary stories.pdf},
  journal = {Neuropsychologia},
  language = {en},
  number = {4}
}

@article{Jin2010,
  title = {Start/Stop Signals Emerge in Nigrostriatal Circuits during Sequence Learning},
  author = {Jin, Xin and Costa, Rui M.},
  year = {2010},
  month = jul,
  volume = {466},
  pages = {457--462},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature09263},
  file = {2010 - Jin, Costa - Startstop signals emerge in nigrostriatal circuits during sequence learning.pdf},
  journal = {Nature},
  language = {en},
  number = {7305}
}

@article{Jin2014,
  title = {Basal Ganglia Subcircuits Distinctively Encode the Parsing and Concatenation of Action Sequences},
  author = {Jin, Xin and Tecuapetla, Fatuel and Costa, Rui M},
  year = {2014},
  month = mar,
  volume = {17},
  pages = {423--430},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.3632},
  file = {2014 - Jin, Tecuapetla, Costa - Basal ganglia subcircuits distinctively encode the parsing and concatenation of action sequences.pdf;Jin et al. - 2014 - Basal ganglia subcircuits distinctively encode the.pdf},
  journal = {Nature Neuroscience},
  language = {en},
  number = {3}
}

@techreport{Jin2018,
  title = {Classical-Contextual Interactions in {{V1}} May Rely on Dendritic Computations},
  author = {Jin, Lei and Behabadi, Bardia F. and Jadi, Monica P. and Ramachandra, Chaithanya A. and Mel, Bartlett W.},
  year = {2018},
  month = oct,
  institution = {{Neuroscience}},
  doi = {10.1101/436956},
  abstract = {A signature feature of the neocortex is the dense network of horizontal connections (HCs) through which pyramidal neurons (PNs) exchange "contextual" information. In primary visual cortex (V1), HCs are thought to facilitate boundary detection, a crucial operation for object recognition, but how HCs modulate PN responses to boundary cues within their classical receptive fields (CRF) remains unknown. We began by ``asking'' natural images, through a structured data collection and ground truth labeling process, what function a V1 cell should use to compute boundary probability from aligned edge cues within and outside its CRF. The ``answer'' was an asymmetric 2-D sigmoidal function, whose nonlinear form provides the first normative account for the ``multiplicative'' center-flanker interactions previously reported in V1 neurons (Kapadia et al. 1995, 2000; Polat et al. 1998). Using a detailed compartmental model, we then show that this boundary-detecting classical-contextual interaction function can be computed with near perfect accuracy by NMDAR-dependent spatial synaptic interactions within PN dendrites \textendash{} the site where classical and contextual inputs first converge in the cortex. In additional simulations, we show that local interneuron circuitry activated by HCs can powerfully leverage the nonlinear spatial computing capabilities of PN dendrites, providing the cortex with a highly flexible substrate for integration of classical and contextual information.},
  file = {Jin et al. - 2018 - Classical-contextual interactions in V1 may rely o.pdf},
  language = {en},
  type = {Preprint}
}

@article{Jirsa2011,
  title = {Neural {{Population Modes Capture Biologically Realistic Large Scale Network Dynamics}}},
  author = {Jirsa, Viktor K. and Stefanescu, Roxana A.},
  year = {2011},
  month = feb,
  volume = {73},
  pages = {325--343},
  issn = {0092-8240, 1522-9602},
  doi = {10.1007/s11538-010-9573-9},
  abstract = {Large scale brain networks are understood nowadays to underlie the emergence of cognitive functions, though the detailed mechanisms are hitherto unknown. The challenges in the study of large scale brain networks are amongst others their high dimensionality requiring significant computational efforts, the complex connectivity across brain areas and the associated transmission delays, as well as the stochastic nature of neuronal processes. To decrease the computational effort, neurons are clustered into neural masses, which then are approximated by reduced descriptions of population dynamics. Here, we implement a neural population mode approach (Assisi et al. in Phys. Rev. Lett. 94(1):018106, 2005; Stefanescu and Jirsa in PLoS Comput. Biol. 4(11):e1000219, 2008), which parsimoniously captures various types of population behavior. We numerically demonstrate that the reduced population mode system favorably captures the high-dimensional dynamics of neuron networks with an architecture involving homogeneous local connectivity and a large-scale, fiber-like connection with time delay.},
  file = {2011 - Jirsa, Stefanescu - Neural Population Modes Capture Biologically Realistic Large Scale Network Dynamics.pdf},
  journal = {Bulletin of Mathematical Biology},
  language = {en},
  number = {2}
}

@article{Joampx000E3,
  title = {Internal State Dynamics Shape Brainwide Activity and Foraging Behaviour},
  author = {Joamp\#x000E3, o C Marques},
  pages = {27},
  file = {Joamp#x000E3 - Internal state dynamics shape brainwide activity a.pdf},
  language = {en}
}

@article{Johnson2018,
  title = {Deep, {{Skinny Neural Networks}} Are Not {{Universal Approximators}}},
  author = {Johnson, Jesse},
  year = {2018},
  month = sep,
  abstract = {In order to choose a neural network architecture that will be effective for a particular modeling problem, one must understand the limitations imposed by each of the potential options. These limitations are typically described in terms of information theoretic bounds, or by comparing the relative complexity needed to approximate example functions between different architectures. In this paper, we examine the topological constraints that the architecture of a neural network imposes on the level sets of all the functions that it is able to approximate. This approach is novel for both the nature of the limitations and the fact that they are independent of network depth for a broad family of activation functions.},
  archiveprefix = {arXiv},
  eprint = {1810.00393},
  eprinttype = {arxiv},
  file = {Johnson - 2018 - Deep, Skinny Neural Networks are not Universal App.pdf},
  journal = {arXiv:1810.00393 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryclass = {cs, stat}
}

@article{Jolivet2004,
  title = {Generalized {{Integrate}}-and-{{Fire Models}} of {{Neuronal Activity Approximate Spike Trains}} of a {{Detailed Model}} to a {{High Degree}} of {{Accuracy}}},
  author = {Jolivet, Renaud and Lewis, Timothy J. and Gerstner, Wulfram},
  year = {2004},
  month = aug,
  volume = {92},
  pages = {959--976},
  issn = {0022-3077, 1522-1598},
  doi = {10.1152/jn.00190.2004},
  file = {2004 - Jolivet, Lewis, Gerstner - Generalized integrate-and-fire models of neuronal activity approximate spike trains of a detailed mode.pdf},
  journal = {Journal of Neurophysiology},
  language = {en},
  number = {2}
}

@article{Jonas2016,
  title = {Could a Neuroscientist Understand a Microprocessor?},
  author = {Jonas, Eric and Kording, Konrad},
  year = {2016},
  month = nov,
  doi = {10.1101/055624},
  abstract = {There is a popular belief in neuroscience that we are primarily data limited, that producing large, multimodal, and complex datasets will, enabled by data analysis algorithms, lead to fundamental insights into the way the brain processes information. Microprocessors are among those artificial information processing systems that are both complex and that we understand at all levels, from the overall logical flow, via logical gates, to the dynamics of transistors. Here we take a simulated classical microprocessor as a model organism, and use our ability to perform arbitrary experiments on it to see if popular data analysis methods from neuroscience can elucidate the way it processes information. We show that the approaches reveal interesting structure in the data but do not meaningfully describe the hierarchy of information processing in the processor. This suggests that current approaches in neuroscience may fall short of producing meaningful models of the brain.},
  file = {Jonas and Kording - 2016 - Could a neuroscientist understand a microprocessor.pdf},
  journal = {bioRxiv},
  language = {en}
}

@article{Jones1990,
  title = {Constructive Approximations for Neural Networks by Sigmoidal Functions},
  author = {Jones, L.K.},
  year = {Oct./1990},
  volume = {78},
  pages = {1586--1589},
  issn = {00189219},
  doi = {10.1109/5.58342},
  file = {Jones - 1990 - Constructive approximations for neural networks by.pdf},
  journal = {Proc. IEEE},
  language = {en},
  number = {10}
}

@article{Jones2014,
  title = {Analyzability, Ad Hoc Restrictions, and Excessive Flexibility of Evidence-Accumulation Models: {{Reply}} to Two Critical Commentaries.},
  shorttitle = {Analyzability, Ad Hoc Restrictions, and Excessive Flexibility of Evidence-Accumulation Models},
  author = {Jones, Matt and Dzhafarov, Ehtibar N.},
  year = {2014},
  volume = {121},
  pages = {689--695},
  issn = {1939-1471, 0033-295X},
  doi = {10.1037/a0037701},
  file = {2014 - Jones, Dzhafarov - Analyzability, ad hoc restrictions, and excessive flexibility of evidence-accumulation models Reply to two cri.pdf;Jones and Dzhafarov - 2014 - Analyzability, ad hoc restrictions, and excessive .pdf},
  journal = {Psychological Review},
  language = {en},
  number = {4}
}

@article{Jones2014a,
  title = {Unfalsifiability and Mutual Translatability of Major Modeling Schemes for Choice Reaction Time.},
  author = {Jones, Matt and Dzhafarov, Ehtibar N.},
  year = {2014},
  volume = {121},
  pages = {1--32},
  issn = {1939-1471, 0033-295X},
  doi = {10.1037/a0034190},
  abstract = {Much current research on speeded choice utilizes models in which the response is triggered by a stochastic process crossing a deterministic threshold. This article focuses on two such model classes, one based on continuous-time diffusion and the other on linear ballistic accumulation (LBA). Both models assume random variability in growth rates and in other model components across trials. We show that if the form of this variability is unconstrained, the models can exactly match any possible pattern of response probabilities and response time distributions. Thus, the explanatory or predictive content of these models is determined not by their structural assumptions, but rather by distributional assumptions (e.g., Gaussian distributions) that are traditionally regarded as implementation details. Selective influence assumptions (i.e., which experimental manipulations affect which model parameters) are shown to have no restrictive effect, except for the theoretically questionable assumption that speed-accuracy instructions do not affect growth rates. The second contribution of this article concerns translation of falsifiable models between universal modeling languages. Specifically, we translate the predictions of the diffusion and LBA models (with their parametric and selective influence assumptions intact) into the Grice modeling framework, in which accumulation processes are deterministic and thresholds are random variables. The Grice framework is also known to reproduce any possible pattern of response probabilities and times, and hence it can be used as a common language for comparing models. It is found that only a few simple properties of empirical data are necessary predictions of the diffusion and LBA models.},
  file = {2014 - Jones, Dzhafarov - Unfalsifiability and mutual translatability of major modelingschemes for choice reaction time.pdf;Jones and Dzhafarov - 2014 - Unfalsifiability and mutual translatability of maj.pdf},
  journal = {Psychological Review},
  language = {en},
  number = {1}
}

@article{Jones2016,
  title = {When Brain Rhythms Aren't `Rhythmic': Implication for Their Mechanisms and Meaning},
  shorttitle = {When Brain Rhythms Aren't `Rhythmic'},
  author = {Jones, Stephanie R},
  year = {2016},
  volume = {40},
  pages = {72--80},
  issn = {09594388},
  doi = {10.1016/j.conb.2016.06.010},
  abstract = {Rhythms are a prominent signature of brain activity. Their expression is correlated with numerous examples of healthy information processing and their fluctuations are a marker of disease states. Yet, their causal or epiphenomenal role in brain function is still highly debated. We review recent studies showing brain rhythms are not always ``rhythmic'', by which we mean representative of repeated cycles of activity. Rather, high power and continuous rhythms in averaged signals can represent brief transient events on single trials whose density accumulates in the average. We also review evidence showing time-domain signals with vastly different waveforms can exhibit identical spectral-domain frequency and power. Further, non-oscillatory waveform feature can create spurious high spectral power. Knowledge of these possibilities is essential when interpreting rhythms and is easily missed without considering pre-processed data. Lastly, we discuss how these finding suggest new directions to pursue in our quest to discover the mechanism and meaning of brain rhythms.},
  file = {Jones - 2016 - When brain rhythms aren't ‘rhythmic’ implication  2.pdf;Jones - 2016 - When brain rhythms aren't ‘rhythmic’ implication .pdf},
  journal = {Current Opinion in Neurobiology},
  language = {en}
}

@article{Jones2016a,
  title = {When Brain Rhythms Aren't `Rhythmic': Implication for Their Mechanisms and Meaning},
  shorttitle = {When Brain Rhythms Aren't `Rhythmic'},
  author = {Jones, Stephanie R},
  year = {2016},
  month = oct,
  volume = {40},
  pages = {72--80},
  issn = {09594388},
  doi = {10.1016/j.conb.2016.06.010},
  file = {Jones - 2016 - When brain rhythms aren't ‘rhythmic’ implication  3.pdf},
  journal = {Current Opinion in Neurobiology},
  language = {en}
}

@article{Jones2020,
  title = {Can {{Single Neurons Solve MNIST}}? {{The Computational Power}} of {{Biological Dendritic Trees}}},
  shorttitle = {Can {{Single Neurons Solve MNIST}}?},
  author = {Jones, Ilenna Simone and Kording, Konrad Paul},
  year = {2020},
  month = sep,
  abstract = {Physiological experiments have highlighted how the dendrites of biological neurons can nonlinearly process distributed synaptic inputs. This is in stark contrast to units in artificial neural networks that are generally linear apart from an output nonlinearity. If dendritic trees can be nonlinear, biological neurons may have far more computational power than their artificial counterparts. Here we use a simple model where the dendrite is implemented as a sequence of thresholded linear units. We find that such dendrites can readily solve machine learning problems, such as MNIST or CIFAR-10, and that they benefit from having the same input onto several branches of the dendritic tree. This dendrite model is a special case of sparse network. This work suggests that popular neuron models may severely underestimate the computational power enabled by the biological fact of nonlinear dendrites and multiple synapses per pair of neurons. The next generation of artificial neural networks may significantly benefit from these biologically inspired dendritic architectures.},
  archiveprefix = {arXiv},
  eprint = {2009.01269},
  eprinttype = {arxiv},
  file = {Jones and Kording - 2020 - Can Single Neurons Solve MNIST The Computational .pdf},
  journal = {arXiv:2009.01269 [q-bio]},
  keywords = {Quantitative Biology - Neurons and Cognition},
  language = {en},
  primaryclass = {q-bio}
}

@article{Jones2021,
  title = {Do Biological Constraints Impair Dendritic Computation?},
  author = {Jones, Ilenna Simone and Kording, Konrad Paul},
  year = {2021},
  month = mar,
  abstract = {Computations on the dendritic trees of neurons have important constraints. Voltage dependent conductances in dendrites are not similar to arbitrary direct-current generation, they are the basis for dendritic nonlinearities and they do not allow converting positive currents into negative currents. While it has been speculated that the dendritic tree of a neuron can be seen as a multi-layer neural network and it has been shown that such an architecture could be computationally strong, we do not know if that computational strength is preserved under these biological constraints. Here we simulate models of dendritic computation with and without these constraints. We find that dendritic model performance on interesting machine learning tasks is not hurt by these constraints but may benefit from them. Our results suggest that single real dendritic trees may be able to learn a surprisingly broad range of tasks.},
  archiveprefix = {arXiv},
  eprint = {2103.03274},
  eprinttype = {arxiv},
  file = {Jones and Kording - 2021 - Do biological constraints impair dendritic computa.pdf},
  journal = {arXiv:2103.03274 [q-bio]},
  keywords = {Quantitative Biology - Neurons and Cognition},
  language = {en},
  primaryclass = {q-bio}
}

@article{Jones2021a,
  title = {Might a {{Single Neuron Solve Interesting Machine Learning Problems Through Successive Computations}} on {{Its Dendritic Tree}}?},
  author = {Jones, Ilenna Simone and Kording, Konrad Paul},
  year = {2021},
  month = may,
  volume = {33},
  pages = {1554--1571},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/neco_a_01390},
  abstract = {Physiological experiments have highlighted how the dendrites of biological neurons can nonlinearly process distributed synaptic inputs. However, it is unclear how aspects of a dendritic tree, such as its branched morphology or its repetition of presynaptic inputs, determine neural computation beyond this apparent nonlinearity. Here we use a simple model where the dendrite is implemented as a sequence of thresholded linear units. We manipulate the architecture of this model to investigate the impacts of binary branching constraints and repetition of synaptic inputs on neural computation. We find that models with such manipulations can perform well on machine learning tasks, such as Fashion MNIST or Extended MNIST. We find that model performance on these tasks is limited by binary tree branching and dendritic asymmetry and is improved by the repetition of synaptic inputs to different dendritic branches. These computational experiments further neuroscience theory on how different dendritic properties might determine neural computation of clearly defined tasks.},
  file = {Jones and Kording - 2021 - Might a Single Neuron Solve Interesting Machine Le.pdf},
  journal = {Neural Computation},
  language = {en},
  number = {6}
}

@article{Jorgensen1987,
  title = {Exponential {{Dispersion Models}}},
  author = {Jorgensen, Bent},
  year = {1987},
  volume = {49},
  pages = {127--162},
  abstract = {We studygeneralpropertiesof the class of exponentialdispersionmodels,whichis the multivariategeneralizationof the errordistributionof Nelder and Wedderburn's(1972) generalizedlinear models. Since any given momentgeneratingfunctiongeneratesan exponentialdispersionmodel,thereexistsa multitudeof exponentialdispersionmodels, and some new examplesare introducedG. eneral resultson convolutionand asymptotic normalityofexponentialdispersionmodelsare presentedA. symptotitcheoryis discussed, includinga new small-dispersionasymptoticframeworkw, hichextendsthe domain of applicationoflarge-sampletheoryP. roceduresforconstructinngewexponentiadl ispersion models forcorrelateddata are introduced,includingmodels forlongitudinaldata and variancecomponentsT. he resultsof the paper unifyand generalizestandardresultsfor distributionsuch as the Poisson, the binomial,the negativebinomial,the normal,the gamma,and theinverseGaussian distributions.},
  file = {Jorgensen - 1987 - Exponential Dispersion Models.pdf},
  journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
  language = {en},
  number = {2}
}

@article{Jorgenson2015,
  title = {The {{BRAIN Initiative}}: Developing Technology to Catalyse Neuroscience Discovery},
  shorttitle = {The {{BRAIN Initiative}}},
  author = {Jorgenson, L. A. and Newsome, W. T. and Anderson, D. J. and Bargmann, C. I. and Brown, E. N. and Deisseroth, K. and Donoghue, J. P. and Hudson, K. L. and Ling, G. S. F. and MacLeish, P. R. and Marder, E. and Normann, R. A. and Sanes, J. R. and Schnitzer, M. J. and Sejnowski, T. J. and Tank, D. W. and Tsien, R. Y. and Ugurbil, K. and Wingfield, J. C.},
  year = {2015},
  month = mar,
  volume = {370},
  pages = {20140164--20140164},
  issn = {0962-8436, 1471-2970},
  doi = {10.1098/rstb.2014.0164},
  file = {Jorgenson et al. - 2015 - The BRAIN Initiative developing technology to cat.pdf},
  journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  language = {en},
  number = {1668}
}

@article{Joseph2017,
  title = {All for {{One But Not One}} for {{All}}: {{Excitatory Synaptic Scaling}} and {{Intrinsic Excitability Are Coregulated}} by {{CaMKIV}}, {{Whereas Inhibitory Synaptic Scaling Is Under Independent Control}}},
  shorttitle = {All for {{One But Not One}} for {{All}}},
  author = {Joseph, Annelise and Turrigiano, Gina G.},
  year = {2017},
  month = jul,
  volume = {37},
  pages = {6778--6785},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.0618-17.2017},
  file = {Joseph and Turrigiano - 2017 - All for One But Not One for All Excitatory Synapt.pdf},
  journal = {The Journal of Neuroscience},
  language = {en},
  number = {28}
}

@article{Jozefowicz,
  title = {An {{Empirical Exploration}} of {{Recurrent Network Architectures}}},
  author = {Jozefowicz, Rafal and Zaremba, Wojciech and Sutskever, Ilya},
  pages = {9},
  abstract = {The Recurrent Neural Network (RNN) is an extremely powerful sequence model that is often difficult to train. The Long Short-Term Memory (LSTM) is a specific RNN architecture whose design makes it much easier to train. While wildly successful in practice, the LSTM's architecture appears to be ad-hoc so it is not clear if it is optimal, and the significance of its individual components is unclear.},
  file = {2015 - Dosovitskiy, Springenberg, Brox - Learning to generate chairs with convolutional neural networks.pdf;Jozefowicz et al. - An Empirical Exploration of Recurrent Network Arch.pdf},
  language = {en}
}

@techreport{Juechems2019,
  title = {Where Does Value Come From?},
  author = {Juechems, Keno and Summerfield, Christopher},
  year = {2019},
  month = apr,
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/rxf7e},
  abstract = {The computational framework of reinforcement learning (RL) has allowed us to both understand biological brains and build successful artificial agents. However, in this article we highlight open challenges for RL as a model of animal behaviour in natural environments. We ask how the external reward function is designed for biological systems, and how we can account for the context sensitivity of valuation. We argue that rather than optimizing receipt of external reward signals, animals track current and desired internal states and seek to minimise the distance to goal across multiple value dimensions. Our framework can readily account for canonical phenomena observed in the fields of psychology, behavioural ecology, and economics, and recent findings from brain imaging studies of value-guided decision-making.},
  file = {Juechems and Summerfield - 2019 - Where does value come from.pdf},
  language = {en},
  type = {Preprint}
}

@inproceedings{Juefei-Xu2018,
  title = {Perturbative {{Neural Networks}}},
  booktitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {{Juefei-Xu}, Felix and Boddeti, Vishnu Naresh and Savvides, Marios},
  year = {2018},
  month = jun,
  pages = {3310--3318},
  publisher = {{IEEE}},
  address = {{Salt Lake City, UT}},
  doi = {10.1109/CVPR.2018.00349},
  abstract = {Convolutional neural networks are witnessing wide adoption in computer vision systems with numerous applications across a range of visual recognition tasks. Much of this progress is fueled through advances in convolutional neural network architectures and learning algorithms even as the basic premise of a convolutional layer has remained unchanged. In this paper, we seek to revisit the convolutional layer that has been the workhorse of state-of-the-art visual recognition models. We introduce a very simple, yet effective, module called a perturbation layer as an alternative to a convolutional layer. The perturbation layer does away with convolution in the traditional sense and instead computes its response as a weighted linear combination of non-linearly activated additive noise perturbed inputs. We demonstrate both analytically and empirically that this perturbation layer can be an effective replacement for a standard convolutional layer. Empirically, deep neural networks with perturbation layers, called Perturbative Neural Networks (PNNs), in lieu of convolutional layers perform comparably with standard CNNs on a range of visual datasets (MNIST, CIFAR-10, PASCAL VOC, and ImageNet) with fewer parameters.},
  file = {Juefei-Xu et al. - 2018 - Perturbative Neural Networks.pdf},
  isbn = {978-1-5386-6420-9},
  language = {en}
}

@article{Kadmon2015,
  title = {Transition to Chaos in Random Neuronal Networks},
  author = {Kadmon, Jonathan and Sompolinsky, Haim},
  year = {2015},
  month = nov,
  volume = {5},
  issn = {2160-3308},
  doi = {10.1103/PhysRevX.5.041030},
  abstract = {Firing patterns in the central nervous system often exhibit strong temporal irregularity and heterogeneity in their time averaged response properties. Previous studies suggested that these properties are outcome of an intrinsic chaotic dynamics. Indeed, simplified rate-based large neuronal networks with random synaptic connections are known to exhibit sharp transition from fixed point to chaotic dynamics when the synaptic gain is increased. However, the existence of a similar transition in neuronal circuit models with more realistic architectures and firing dynamics has not been established. In this work we investigate rate based dynamics of neuronal circuits composed of several subpopulations and random connectivity. Nonzero connections are either positive-for excitatory neurons, or negative for inhibitory ones, while single neuron output is strictly positive; in line with known constraints in many biological systems. Using Dynamic Mean Field Theory, we find the phase diagram depicting the regimes of stable fixed point, unstable dynamic and chaotic rate fluctuations. We characterize the properties of systems near the chaotic transition and show that dilute excitatory-inhibitory architectures exhibit the same onset to chaos as a network with Gaussian connectivity. Interestingly, the critical properties near transition depend on the shape of the single- neuron input-output transfer function near firing threshold. Finally, we investigate network models with spiking dynamics. When synaptic time constants are slow relative to the mean inverse firing rates, the network undergoes a sharp transition from fast spiking fluctuations and static firing rates to a state with slow chaotic rate fluctuations. When the synaptic time constants are finite, the transition becomes smooth and obeys scaling properties, similar to crossover phenomena in statistical mechanics},
  archiveprefix = {arXiv},
  eprint = {1508.06486},
  eprinttype = {arxiv},
  file = {2015 - Kadmon, Sompolinsky - Transition to chaos in random neuronal networks.pdf;Kadmon and Sompolinsky - 2015 - Transition to chaos in random neuronal networks.pdf},
  journal = {Physical Review X},
  keywords = {Condensed Matter - Disordered Systems and Neural Networks,Nonlinear Sciences - Chaotic Dynamics,Quantitative Biology - Neurons and Cognition},
  language = {en},
  number = {4}
}

@article{Kaelbling1998,
  title = {Planning and Acting in Partially Observable Stochastic Domains},
  author = {Kaelbling, Leslie Pack and Littman, Michael L. and Cassandra, Anthony R.},
  year = {1998},
  month = may,
  volume = {101},
  pages = {99--134},
  issn = {00043702},
  doi = {10.1016/S0004-3702(98)00023-X},
  abstract = {In this paper, we bring techniques from operations research to bear on the problem of choosing optimal actions in partially observable stochastic domains. We begin by introducing the theory of Markov decision processes (MDPS) and partially observable MDPS (POMDPS). We then outline a novel algorithm for solving POMDPS off line and show how, in some cases, a finite-memory controller can be extracted from the solution to a POMDP. We conclude with a discussion of how our approach relates to previous work, the complexity of finding exact solutions to POMDPS, and of some possibilities for finding approximate solutions. 0 1998 Elsevier Science B.V. All rights reserved.},
  file = {Kaelbling et al. - 1998 - Planning and acting in partially observable stocha.pdf},
  journal = {Artificial Intelligence},
  language = {en},
  number = {1-2}
}

@article{Kahnt2011,
  title = {Decoding Different Roles for {{vmPFC}} and {{dlPFC}} in Multi-Attribute Decision Making},
  author = {Kahnt, Thorsten and Heinzle, Jakob and Park, Soyoung Q. and Haynes, John-Dylan},
  year = {2011},
  month = may,
  volume = {56},
  pages = {709--715},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2010.05.058},
  abstract = {In everyday life, successful decision making requires precise representations of expected values. However, for most behavioral options more than one attribute can be relevant in order to predict the expected reward. Thus, to make good or even optimal choices the reward predictions of multiple attributes need to be integrated into a combined expected value. Importantly, the individual attributes of such multi-attribute objects can agree or disagree in their reward prediction. Here we address where the brain encodes the combined reward prediction (averaged across attributes) and where it encodes the variability of the value predictions of the individual attributes. We acquired fMRI data while subjects performed a task in which they had to integrate reward predictions from multiple attributes into a combined value. Using time-resolved pattern recognition techniques (support vector regression) we find that (1) the combined value is encoded in distributed fMRI patterns in the ventromedial prefrontal cortex (vmPFC) and that (2) the variability of value predictions of the individual attributes is encoded in the dorsolateral prefrontal cortex (dlPFC). The combined value could be used to guide choices, whereas the variability of the value predictions of individual attributes indicates an ambiguity that results in an increased difficulty of the value-integration. These results demonstrate that the different features defining multi-attribute objects are encoded in non-overlapping brain regions and therefore suggest different roles for vmPFC and dlPFC in multi-attribute decision making. \textcopyright{} 2010 Elsevier Inc. All rights reserved.},
  file = {2011 - Kahnt et al. - Decoding different roles for vmPFC and dlPFC in multi-attribute decision making.pdf},
  journal = {NeuroImage},
  language = {en},
  number = {2}
}

@article{Kaiser2017,
  title = {One {{Model To Learn Them All}}},
  author = {Kaiser, Lukasz and Gomez, Aidan N. and Shazeer, Noam and Vaswani, Ashish and Parmar, Niki and Jones, Llion and Uszkoreit, Jakob},
  year = {2017},
  month = jun,
  abstract = {Deep learning yields great results across many fields, from speech recognition, image classification, to translation. But for each problem, getting a deep model to work well involves research into the architecture and a long period of tuning. We present a single model that yields good results on a number of problems spanning multiple domains. In particular, this single model is trained concurrently on ImageNet, multiple translation tasks, image captioning (COCO dataset), a speech recognition corpus, and an English parsing task. Our model architecture incorporates building blocks from multiple domains. It contains convolutional layers, an attention mechanism, and sparsely-gated layers. Each of these computational blocks is crucial for a subset of the tasks we train on. Interestingly, even if a block is not crucial for a task, we observe that adding it never hurts performance and in most cases improves it on all tasks. We also show that tasks with less data benefit largely from joint training with other tasks, while performance on large tasks degrades only slightly if at all.},
  archiveprefix = {arXiv},
  eprint = {1706.05137},
  eprinttype = {arxiv},
  file = {Kaiser et al. - 2017 - One Model To Learn Them All.pdf},
  journal = {arXiv:1706.05137 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryclass = {cs, stat}
}

@article{Kakade2002,
  title = {Dopamine: Generalization and Bonuses},
  shorttitle = {Dopamine},
  author = {Kakade, Sham and Dayan, Peter},
  year = {2002},
  month = jun,
  volume = {15},
  pages = {549--559},
  issn = {08936080},
  doi = {10.1016/S0893-6080(02)00048-5},
  abstract = {In the temporal difference model of primate dopamine neurons, their phasic activity reports a prediction error for future reward. This model is supported by a wealth of experimental data. However, in certain circumstances, the activity of the dopamine cells seems anomalous under the model, as they respond in particular ways to stimuli that are not obviously related to predictions of reward. In this paper, we address two important sets of anomalies, those having to do with generalization and novelty. Generalization responses are treated as the natural consequence of partial information; novelty responses are treated by the suggestion that dopamine cells multiplex information about reward bonuses, including exploration bonuses and shaping bonuses. We interpret this additional role for dopamine in terms of the mechanistic attentional and psychomotor effects of dopamine, having the computational role of guiding exploration. q 2002 Published by Elsevier Science Ltd.},
  file = {2002 - Kakade, Dayan - Dopamine Generalization and bonuses.pdf},
  journal = {Neural Networks},
  language = {en},
  number = {4-6}
}

@article{Kalyanakrishnan,
  title = {{{PAC Subset Selection}} in {{Stochastic Multi}}-Armed {{Bandits}}},
  author = {Kalyanakrishnan, Shivaram and Tewari, Ambuj and Auer, Peter and Stone, Peter},
  pages = {8},
  abstract = {We consider the problem of selecting, from among the arms of a stochastic n-armed bandit, a subset of size m of those arms with the highest expected rewards, based on efficiently sampling the arms. This ``subset selection'' problem finds application in a variety of areas. In the authors' previous work (Kalyanakrishnan \& Stone, 2010), this problem is framed under a PAC setting (denoted ``Explore-m''), and corresponding sampling algorithms are analyzed. Whereas the formal analysis therein is restricted to the worst case sample complexity of algorithms, in this paper, we design and analyze an algorithm (``LUCB'') with improved expected sample complexity. Interestingly LUCB bears a close resemblance to the wellknown UCB algorithm for regret minimization. The expected sample complexity bound we show for LUCB is novel even for singlearm selection (Explore-1). We also give a lower bound on the worst case sample complexity of PAC algorithms for Explore-m.},
  file = {Kalyanakrishnan et al. - PAC Subset Selection in Stochastic Multi-armed Ban.pdf},
  language = {en}
}

@article{Kamimura2010,
  title = {Group Chase and Escape},
  author = {Kamimura, Atsushi and Ohira, Toru},
  year = {2010},
  month = may,
  volume = {12},
  pages = {053013},
  issn = {1367-2630},
  doi = {10.1088/1367-2630/12/5/053013},
  abstract = {We describe here a new concept of one group chasing another, called `group chase and escape', by presenting a simple model. We will show that even a simple model can demonstrate rather rich and complex behavior. In particular, there are cases where an optimal number of chasers exists for a given number of escapees (or targets) to minimize the cost of catching all targets. We have also found an indication of self-organized spatial structures formed by both groups.},
  file = {Kamimura and Ohira - 2010 - Group chase and escape.pdf},
  journal = {New J. Phys.},
  language = {en},
  number = {5}
}

@article{Kamitani2005,
  title = {Decoding the Visual and Subjective Contents of the Human Brain},
  author = {Kamitani, Yukiyasu and Tong, Frank},
  year = {2005},
  month = may,
  volume = {8},
  pages = {679--685},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn1444},
  file = {2005 - Kamitani, Tong - Decoding the visual and subjective contents of the human brain.pdf},
  journal = {Nature Neuroscience},
  language = {en},
  number = {5}
}

@article{Kamps2003,
  title = {A {{Simple}} and {{Stable Numerical Solution}} for the {{Population Density Equation}}},
  author = {de Kamps, M.},
  year = {2003},
  month = sep,
  volume = {15},
  pages = {2129--2146},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/089976603322297322},
  file = {2003 - de Kamps - A simple and stable numerical solution for the population density equation.pdf},
  journal = {Neural Computation},
  language = {en},
  number = {9}
}

@article{Kanaya2020,
  title = {A Sleep-like State in {{{\emph{Hydra}}}} Unravels Conserved Sleep Mechanisms during the Evolutionary Development of the Central Nervous System},
  author = {Kanaya, Hiroyuki J. and Park, Sungeon and Kim, Ji-hyung and Kusumi, Junko and Krenenou, Sofian and Sawatari, Etsuko and Sato, Aya and Lee, Jongbin and Bang, Hyunwoo and Kobayakawa, Yoshitaka and Lim, Chunghun and Itoh, Taichi Q.},
  year = {2020},
  month = oct,
  volume = {6},
  pages = {eabb9415},
  issn = {2375-2548},
  doi = {10.1126/sciadv.abb9415},
  abstract = {Sleep behaviors are observed even in nematodes and arthropods, yet little is known about how sleep-regulatory mechanisms have emerged during evolution. Here, we report a sleep-like state in the cnidarian               Hydra vulgaris               with a primitive nervous organization.               Hydra               sleep was shaped by homeostasis and necessary for cell proliferation, but it lacked free-running circadian rhythms. Instead, we detected 4-hour rhythms that might be generated by ultradian oscillators underlying               Hydra               sleep. Microarray analysis in sleep-deprived               Hydra               revealed sleep-dependent expression of 212 genes, including cGMP-dependent protein kinase 1 (PRKG1) and ornithine aminotransferase. Sleep-promoting effects of melatonin, GABA, and PRKG1 were conserved in               Hydra               . However, arousing dopamine unexpectedly induced               Hydra               sleep. Opposing effects of ornithine metabolism on sleep were also evident between               Hydra               and               Drosophila               , suggesting the evolutionary switch of their sleep-regulatory functions. Thus, sleep-relevant physiology and sleep-regulatory components may have already been acquired at molecular levels in a brain-less metazoan phylum and reprogrammed accordingly.},
  file = {Kanaya et al. - 2020 - A sleep-like state in Hydra unravels conser.pdf},
  journal = {Sci. Adv.},
  language = {en},
  number = {41}
}

@article{Kann2014,
  title = {Highly {{Energized Inhibitory Interneurons}} Are a {{Central Element}} for {{Information Processing}} in {{Cortical Networks}}},
  author = {Kann, Oliver and Papageorgiou, Ismini E and Draguhn, Andreas},
  year = {2014},
  month = aug,
  volume = {34},
  pages = {1270--1282},
  issn = {0271-678X, 1559-7016},
  doi = {10.1038/jcbfm.2014.104},
  file = {Kann et al. - 2014 - Highly Energized Inhibitory Interneurons are a Cen.pdf},
  journal = {Journal of Cerebral Blood Flow \& Metabolism},
  language = {en},
  number = {8}
}

@article{Kao,
  title = {Considerations in Using Recurrent Neural Networks to Probe Neural Dynamics},
  author = {Kao, Jonathan C},
  pages = {39},
  file = {Kao - Considerations in using recurrent neural networks .pdf},
  language = {en}
}

@article{Kao2009,
  title = {Multi-{{Objective Optimal Experimental Designs}} for {{ER}}-{{fMRI Using MATLAB}}},
  author = {Kao, Ming-Hung},
  year = {2009},
  volume = {30},
  issn = {1548-7660},
  doi = {10.18637/jss.v030.i11},
  abstract = {Designs for event-related functional magnetic resonance imaging (ER-fMRI) that help to efficiently achieve the statistical goals while taking into account the psychological constraints and customized requirements are in great demand. This is not only because of the popularity of ER-fMRI but also because of the high cost of ER-fMRI experiments; being able to collect highly informative data is crucial. In this paper, we develop a MATLAB program which can accommodate many user-specified experimental conditions to efficiently find ER-fMRI optimal designs.},
  file = {2009 - Kao - Multi-Objective Optimal Experimental Designs for.pdf},
  journal = {Journal of Statistical Software},
  language = {en},
  number = {11}
}

@incollection{Kaplan2007,
  title = {The Progress Drive Hypothesis: An Interpretation of Early Imitation},
  shorttitle = {The Progress Drive Hypothesis},
  booktitle = {Imitation and {{Social Learning}} in {{Robots}}, {{Humans}} and {{Animals}}},
  author = {Kaplan, Fr{\'e}d{\'e}ric and Oudeyer, Pierre-Yves},
  editor = {Nehaniv, Chrystopher L. and Dautenhahn, Kerstin},
  year = {2007},
  pages = {361--378},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge}},
  doi = {10.1017/CBO9780511489808.024},
  abstract = {Children seem to have a natural tendency to imitate and their interest for particular kinds of imitative behaviour varies greatly with the infant's age. We argue that different forms of children's early imitation may be the result of an intrinsic motivation system driving the infant into situations of maximal learning progress. We present a computational model showing how an agent can learn to focus on ``progress niches'', situations neither completely predictable nor too difficult to predict given its anticipation capabilities. The existence of such a drive could explain why certain types of imitative behaviour are produced by children at a certain age, and how discriminations between self, others and objects gradually appear.},
  file = {Kaplan and Oudeyer - 2007 - The progress drive hypothesis an interpretation o.pdf},
  isbn = {978-0-511-48980-8},
  language = {en}
}

@article{Kaplan2011,
  title = {The {{Explanatory Force}} of {{Dynamical}} and {{Mathematical Models}} in {{Neuroscience}}: {{A Mechanistic Perspective}}*},
  shorttitle = {The {{Explanatory Force}} of {{Dynamical}} and {{Mathematical Models}} in {{Neuroscience}}},
  author = {Kaplan, David Michael and Craver, Carl F.},
  year = {2011},
  month = oct,
  volume = {78},
  pages = {601--627},
  issn = {0031-8248, 1539-767X},
  doi = {10.1086/661755},
  file = {Kaplan and Craver - 2011 - The Explanatory Force of Dynamical and Mathematica.pdf},
  journal = {Philosophy of Science},
  language = {en},
  number = {4}
}

@article{Kaplan2015,
  title = {Multivariate Cross-Classification: Applying Machine Learning Techniques to Characterize Abstraction in Neural Representations},
  shorttitle = {Multivariate Cross-Classification},
  author = {Kaplan, Jonas T. and Man, Kingson and Greening, Steven G.},
  year = {2015},
  month = mar,
  volume = {9},
  issn = {1662-5161},
  doi = {10.3389/fnhum.2015.00151},
  abstract = {Here we highlight an emerging trend in the use of machine learning classifiers to test for abstraction across patterns of neural activity. When a classifier algorithm is trained on data from one cognitive context, and tested on data from another, conclusions can be drawn about the role of a given brain region in representing information that abstracts across those cognitive contexts. We call this kind of analysis Multivariate CrossClassification (MVCC), and review several domains where it has recently made an impact. MVCC has been important in establishing correspondences among neural patterns across cognitive domains, including motor-perception matching and cross-sensory matching. It has been used to test for similarity between neural patterns evoked by perception and those generated from memory. Other work has used MVCC to investigate the similarity of representations for semantic categories across different kinds of stimulus presentation, and in the presence of different cognitive demands. We use these examples to demonstrate the power of MVCC as a tool for investigating neural abstraction and discuss some important methodological issues related to its application.},
  file = {2015 - Kaplan, Man, Greening - Multivariate cross-classification applying machine learning techniques to characterize abstraction in neu.pdf;Kaplan et al. - 2015 - Multivariate cross-classification applying machin.pdf},
  journal = {Frontiers in Human Neuroscience},
  language = {en}
}

@article{Karnin,
  title = {Almost {{Optimal Exploration}} in {{Multi}}-{{Armed Bandits}}},
  author = {Karnin, Zohar and Koren, Tomer and Somekh, Oren},
  pages = {9},
  abstract = {We study the problem of exploration in stochastic Multi-Armed Bandits. Even in the simplest setting of identifying the best arm, there remains a logarithmic multiplicative gap between the known lower and upper bounds for the number of arm pulls required for the task. This extra logarithmic factor is quite meaningful in nowadays large-scale applications. We present two novel, parameterfree algorithms for identifying the best arm, in two different settings: given a target confidence and given a target budget of arm pulls, for which we prove upper bounds whose gap from the lower bound is only doublylogarithmic in the problem parameters. We corroborate our theoretical results with experiments demonstrating that our algorithm outperforms the state-of-the-art and scales better as the size of the problem increases.},
  file = {Karnin et al. - Almost Optimal Exploration in Multi-Armed Bandits.pdf},
  language = {en}
}

@article{Karpas2017,
  title = {Information Socialtaxis and Efficient Collective Behavior Emerging in Groups of Information-Seeking Agents},
  author = {Karpas, Ehud D. and Shklarsh, Adi and Schneidman, Elad},
  year = {2017},
  month = may,
  volume = {114},
  pages = {5589--5594},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1618055114},
  abstract = {Individual behavior, in biology, economics, and computer science, is often described in terms of balancing exploration and exploitation. Foraging has been a canonical setting for studying reward seeking and information gathering, from bacteria to humans, mostly focusing on individual behavior. Inspired by the gradient-climbing nature of chemotaxis, the infotaxis algorithm showed that locally maximizing the expected information gain leads to efficient and ethological individual foraging. In nature, as well as in theoretical settings, conspecifics can be a valuable source of information about the environment. Whereas the nature and role of interactions between animals have been studied extensively, the design principles of information processing in such groups are mostly unknown. We present an algorithm for group foraging, which we term ``socialtaxis,'' that unifies infotaxis and social interactions, where each individual in the group simultaneously maximizes its own sensory information and a social information term. Surprisingly, we show that when individuals aim to increase their information diversity, efficient collective behavior emerges in groups of opportunistic agents, which is comparable to the optimal group behavior. Importantly, we show the high efficiency of biologically plausible socialtaxis settings, where agents share little or no information and rely on simple computations to infer information from the behavior of their conspecifics. Moreover, socialtaxis does not require parameter tuning and is highly robust to sensory and behavioral noise. We use socialtaxis to predict distinct optimal couplings in groups of selfish vs. altruistic agents, reflecting how it can be naturally extended to study social dynamics and collective computation in general settings.},
  file = {Karpas et al. - 2017 - Information socialtaxis and efficient collective b.pdf},
  journal = {Proc Natl Acad Sci USA},
  language = {en},
  number = {22}
}

@article{Kass2011,
  title = {Assessment of Synchrony in Multiple Neural Spike Trains Using Loglinear Point Process Models},
  author = {Kass, Robert E. and Kelly, Ryan C. and Loh, Wei-Liem},
  year = {2011},
  month = jun,
  volume = {5},
  pages = {1262--1292},
  issn = {1932-6157},
  doi = {10.1214/10-AOAS429},
  file = {2011 - Kass, Kelly, Loh - Assessment of synchrony in multiple neural spike trains using loglinear point process models.pdf},
  journal = {The Annals of Applied Statistics},
  language = {en},
  number = {2B}
}

@article{Katz,
  title = {Embodying Probabilistic Inference in Biochemical Circuits},
  author = {Katz, Yarden and Springer, Michael and Fontana, Walter},
  pages = {23},
  abstract = {Probabilistic inference provides a language for describing how organisms may learn from and adapt to their environment. The computations needed to implement probabilistic inference often require specific representations, akin to having the suitable data structures for implementing certain algorithms in computer programming. Yet it is unclear how such representations can be instantiated in the stochastic, parallel-running biochemical machinery found in cells (such as single-celled organisms). Here, we show how representations for supporting inference in Markov models can be embodied in cellular circuits, by combining a concentration-dependent scheme for encoding probabilities with a mechanism for directional counting. We show how the logic of protein production and degradation constrains the computation we set out to implement. We argue that this process by which an abstract computation is shaped by its biochemical realization strikes a compromise between ``rationalistic'' information-processing perspectives and alternative approaches that emphasize embodiment.},
  file = {Katz et al. - Embodying probabilistic inference in biochemical c.pdf},
  language = {en}
}

@article{Katz1995,
  title = {Ideas of {{Calculus}} in {{Islam}} and {{India}}},
  author = {Katz, Victor J.},
  year = {1995},
  month = jun,
  volume = {68},
  pages = {163},
  issn = {0025570X},
  doi = {10.2307/2691411},
  file = {1995 - Katz - Ideas of Calculus in Islam and India.pdf},
  journal = {Mathematics Magazine},
  language = {en},
  number = {3}
}

@article{Kaufman2013,
  title = {The Roles of Monkey {{M1}} Neuron Classes in Movement Preparation and Execution},
  author = {Kaufman, Matthew T. and Churchland, Mark M. and Shenoy, Krishna V.},
  year = {2013},
  month = aug,
  volume = {110},
  pages = {817--825},
  issn = {0022-3077, 1522-1598},
  doi = {10.1152/jn.00892.2011},
  file = {2013 - Kaufman, Churchland, Shenoy - The roles of monkey M1 neuron classes in movement preparation and execution.pdf},
  journal = {Journal of Neurophysiology},
  language = {en},
  number = {4}
}

@article{Kaufman2014,
  title = {Cortical Activity in the Null Space: Permitting Preparation without Movement},
  shorttitle = {Cortical Activity in the Null Space},
  author = {Kaufman, Matthew T and Churchland, Mark M and Ryu, Stephen I and Shenoy, Krishna V},
  year = {2014},
  month = mar,
  volume = {17},
  pages = {440--448},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.3643},
  file = {2014 - Kaufman et al. - Cortical activity in the null space permitting preparation without movement.pdf;Kaufman et al. - 2014 - Cortical activity in the null space permitting pr.pdf},
  journal = {Nature Neuroscience},
  language = {en},
  number = {3}
}

@article{Kay2008,
  title = {Identifying Natural Images from Human Brain Activity},
  author = {Kay, Kendrick N. and Naselaris, Thomas and Prenger, Ryan J. and Gallant, Jack L.},
  year = {2008},
  month = mar,
  volume = {452},
  pages = {352--355},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature06713},
  file = {2008 - Kay et al. - Identifying natural images from human brain activity.pdf},
  journal = {Nature},
  language = {en},
  number = {7185}
}

@article{Kayser2010,
  title = {Neural {{Representations}} of {{Relevant}} and {{Irrelevant Features}} in {{Perceptual Decision Making}}},
  author = {Kayser, A. S. and Erickson, D. T. and Buchsbaum, B. R. and D'Esposito, M.},
  year = {2010},
  month = nov,
  volume = {30},
  pages = {15778--15789},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.3163-10.2010},
  file = {2010 - Kayser et al. - Neural representations of relevant and irrelevant features in perceptual decision making.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {47}
}

@article{Kearns,
  title = {Near-{{Optimal Reinforcement Learning}} in {{Polynomial Time}}},
  author = {Kearns, Michael},
  pages = {24},
  abstract = {We present new algorithms for reinforcement learning and prove that they have polynomial bounds on the resources required to achieve near-optimal return in general Markov decision processes. After observing that the number of actions required to approach the optimal return is lower bounded by the mixing time T of the optimal policy (in the undiscounted case) or by the horizon time T (in the discounted case), we then give algorithms requiring a number of actions and total computation time that are only polynomial in T and the number of states and actions, for both the undiscounted and discounted cases. An interesting aspect of our algorithms is their explicit handling of the Exploration-Exploitation trade-off.},
  file = {Kearns - Near-Optimal Reinforcement Learning in Polynomial  2.pdf},
  language = {en}
}

@article{Kearns2002,
  title = {Near-{{Optimal Reinforcement Learning}} in {{Polynomial Time}}},
  author = {Kearns, Michael and Singh, Satinder},
  year = {2002},
  pages = {24},
  abstract = {We present new algorithms for reinforcement learning and prove that they have polynomial bounds on the resources required to achieve near-optimal return in general Markov decision processes. After observing that the number of actions required to approach the optimal return is lower bounded by the mixing time T of the optimal policy (in the undiscounted case) or by the horizon time T (in the discounted case), we then give algorithms requiring a number of actions and total computation time that are only polynomial in T and the number of states and actions, for both the undiscounted and discounted cases. An interesting aspect of our algorithms is their explicit handling of the Exploration-Exploitation trade-off.},
  file = {Kearns - Near-Optimal Reinforcement Learning in Polynomial .pdf},
  journal = {Machine Learning},
  language = {en}
}

@book{Keller1994,
  title = {Curiosity and {{Exploration}}},
  editor = {Keller, Heidi and Schneider, Klaus and Henderson, Bruce},
  year = {1994},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-77132-3},
  file = {Keller et al. - 1994 - Curiosity and Exploration.pdf},
  isbn = {978-3-540-54867-6 978-3-642-77132-3},
  language = {en}
}

@article{Kelly1956,
  title = {A {{New Interpretation}} of {{Information Rate}}},
  author = {Kelly, J L},
  year = {1956},
  pages = {10},
  file = {Kelly - 1956 - A New Interpretation of Information Rate.pdf},
  journal = {the bell system technical journal},
  language = {en}
}

@article{Kelly2012,
  title = {A {{Framework}} for {{Evaluating Pairwise}} and {{Multiway Synchrony Among Stimulus}}-{{Driven Neurons}}},
  author = {Kelly, Ryan C. and Kass, Robert E.},
  year = {2012},
  month = aug,
  volume = {24},
  pages = {2007--2032},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/NECO_a_00307},
  file = {2012 - Kelly, Kass - A Framework for Evaluating Pairwise and Multiway Synchrony Among Stimulus-Driven Neurons.pdf},
  journal = {Neural Computation},
  language = {en},
  number = {8}
}

@article{Kelly2014,
  title = {The {{Role}} of {{Thalamic Population Synchrony}} in the {{Emergence}} of {{Cortical Feature Selectivity}}},
  author = {Kelly, Sean T. and Kremkow, Jens and Jin, Jianzhong and Wang, Yushi and Wang, Qi and Alonso, Jose-Manuel and Stanley, Garrett B.},
  editor = {Graham, Lyle J.},
  year = {2014},
  month = jan,
  volume = {10},
  pages = {e1003418},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1003418},
  abstract = {In a wide range of studies, the emergence of orientation selectivity in primary visual cortex has been attributed to a complex interaction between feed-forward thalamic input and inhibitory mechanisms at the level of cortex. Although it is well known that layer 4 cortical neurons are highly sensitive to the timing of thalamic inputs, the role of the stimulus-driven timing of thalamic inputs in cortical orientation selectivity is not well understood. Here we show that the synchronization of thalamic firing contributes directly to the orientation tuned responses of primary visual cortex in a way that optimizes the stimulus information per cortical spike. From the recorded responses of geniculate X-cells in the anesthetized cat, we synthesized thalamic sub-populations that would likely serve as the synaptic input to a common layer 4 cortical neuron based on anatomical constraints. We used this synchronized input as the driving input to an integrate-and-fire model of cortical responses and demonstrated that the tuning properties match closely to those measured in primary visual cortex. By modulating the overall level of synchronization at the preferred orientation, we show that efficiency of information transmission in the cortex is maximized for levels of synchronization which match those reported in thalamic recordings in response to naturalistic stimuli, a property which is relatively invariant to the orientation tuning width. These findings indicate evidence for a more prominent role of the feed-forward thalamic input in cortical feature selectivity based on thalamic synchronization.},
  file = {Kelly et al. - 2014 - The Role of Thalamic Population Synchrony in the E.pdf},
  journal = {PLoS Computational Biology},
  language = {en},
  number = {1}
}

@incollection{Kelso2009,
  title = {Coordination {{Dynamics}}},
  booktitle = {Encyclopedia of {{Complexity}} and {{Systems Science}}},
  author = {Kelso, James A. S.},
  editor = {Meyers, Robert A.},
  year = {2009},
  pages = {1537--1565},
  publisher = {{Springer New York}},
  address = {{New York, NY}},
  doi = {10.1007/978-0-387-30440-3_101},
  file = {Kelso - 2009 - Coordination Dynamics.pdf},
  isbn = {978-0-387-75888-6 978-0-387-30440-3},
  language = {en}
}

@article{Kempster2007,
  title = {Patterns of Levodopa Response in {{Parkinson}}'s Disease: A Clinico-Pathological Study},
  shorttitle = {Patterns of Levodopa Response in {{Parkinson}}'s Disease},
  author = {Kempster, P. A. and Williams, D. R. and Selikhova, M. and Holton, J. and Revesz, T. and Lees, A. J.},
  year = {2007},
  month = aug,
  volume = {130},
  pages = {2123--2128},
  issn = {0006-8950, 1460-2156},
  doi = {10.1093/brain/awm142},
  file = {2007 - Kempster et al. - Patterns of levodopa response in Parkinson's disease A clinico-pathological study.pdf},
  journal = {Brain},
  language = {en},
  number = {8}
}

@article{Kendal2015,
  title = {Self-Organized Criticality Attributed to a Central Limit-like Convergence Effect},
  author = {Kendal, Wayne S.},
  year = {2015},
  month = mar,
  volume = {421},
  pages = {141--150},
  issn = {03784371},
  doi = {10.1016/j.physa.2014.11.035},
  abstract = {Self-organized criticality is a hypothesis used to explain the origin of 1/f noise and other scaling behaviors. Despite being proposed nearly 30 years ago, no consensus exists as to its exact definition or mathematical mechanism(s). Recently, a model for 1/f noise was proposed based on a family of statistical distributions known as the Tweedie exponential dispersion models. These distributions are characterized by an inherent scale invariance that manifests as a variance to mean power law, called fluctuation scaling; they also serve as foci of convergence in a limit theorem on independent and identically distributed distributions. Fluctuation scaling can be modeled by self-similar stochastic processes that relate the variance to mean power law to 1/f noise through their correlation structure. A hypothesis is proposed whereby the effects of self-organized criticality are mathematically modeled by the Tweedie distributions and their convergence behavior as applied to selfsimilar stochastic processes. Sandpile model fluctuations are shown to manifest 1/f noise, fluctuation scaling, and to conform to the Tweedie compound Poisson distribution. The Tweedie models and their convergence theorem allow for a mechanistic explanation of 1/f noise and fluctuation scaling in phenomena conventionally attributed to self-organized criticality, thus providing a paradigm shift in our understanding of these phenomena.},
  file = {Kendal - 2015 - Self-organized criticality attributed to a central.pdf},
  journal = {Physica A: Statistical Mechanics and its Applications},
  language = {en}
}

@article{Keramati2014,
  title = {Homeostatic Reinforcement Learning for Integrating Reward Collection and Physiological Stability},
  author = {Keramati, Mehdi and Gutkin, Boris},
  year = {2014},
  month = dec,
  volume = {3},
  pages = {e04811},
  issn = {2050-084X},
  doi = {10.7554/eLife.04811},
  abstract = {Efficient regulation of internal homeostasis and defending it against perturbations requires adaptive behavioral strategies. However, the computational principles mediating the interaction between homeostatic and associative learning processes remain undefined. Here we use a definition of primary rewards, as outcomes fulfilling physiological needs, to build a normative theory showing how learning motivated behaviors may be modulated by internal states. Within this framework, we mathematically prove that seeking rewards is equivalent to the fundamental objective of physiological stability, defining the notion of physiological rationality of behavior. We further suggest a formal basis for temporal discounting of rewards by showing that discounting motivates animals to follow the shortest path in the space of physiological variables toward the desired setpoint. We also explain how animals learn to act predictively to preclude prospective homeostatic challenges, and several other behavioral patterns. Finally, we suggest a computational role for interaction between hypothalamus and the brain reward system.           ,              Our survival depends on our ability to maintain internal states, such as body temperature and blood sugar levels, within narrowly defined ranges, despite being subject to constantly changing external forces. This process, which is known as homeostasis, requires humans and other animals to carry out specific behaviors\textemdash such as seeking out warmth or food\textemdash to compensate for changes in their environment. Animals must also learn to prevent the potential impact of changes that can be anticipated.             A network that includes different regions of the brain allows animals to perform the behaviors that are needed to maintain homeostasis. However, this network is distinct from the network that supports the learning of new behaviors in general. These two systems must, therefore, interact so that animals can learn novel strategies to support their physiological stability, but it is not clear how animals do this.             Keramati and Gutkin have now devised a mathematical model that explains the nature of this interaction, and that can account for many behaviors seen among animals, even those that might otherwise appear irrational. There are two assumptions at the heart of the model. First, it is assumed that animals are capable of guessing the impact of the outcome of their behaviors on their internal state. Second, it is assumed that animals find a behavior rewarding if they believe that the predicted impact of its outcome will reduce the difference between a particular internal state and its ideal value. For example, a form of behavior for a human might be going to the kitchen, and an outcome might be eating chocolate.             Based on these two assumptions, the model shows that animals stabilize their internal state around its ideal value by simply learning to perform behaviors that lead to rewarding outcomes (such as going into the kitchen and eating chocolate). Their theory also explains the physiological importance of a type of behavior known as `delay discounting'. Animals displaying this form of behavior regard a positive outcome as less rewarding the longer they have to wait for it. The model proves mathematically that delay discounting is a logical way to optimize homeostasis.             In addition to making a number of predictions that could be tested in experiments, Keramati and Gutkin argue that their model can account for the failure of homeostasis to limit food consumption whenever foods loaded with salt, sugar or fat are freely available.},
  file = {Keramati and Gutkin - 2014 - Homeostatic reinforcement learning for integrating.pdf},
  journal = {eLife},
  language = {en}
}

@article{Khakh2019,
  title = {The {{Emerging Nature}} of {{Astrocyte Diversity}}},
  author = {Khakh, Baljit S. and Deneen, Benjamin},
  year = {2019},
  month = jul,
  volume = {42},
  pages = {187--207},
  issn = {0147-006X, 1545-4126},
  doi = {10.1146/annurev-neuro-070918-050443},
  abstract = {Astrocytes are morphologically complex, ubiquitous cells that are viewed as a homogeneous population tiling the entire central nervous system (CNS). However, this view has been challenged in the last few years with the availability of RNA sequencing, immunohistochemistry, electron microscopy, morphological reconstruction, and imaging data. These studies suggest that astrocytes represent a diverse population of cells and that they display brain area\textendash{} and disease\textendash specific properties and functions. In this review, we summarize these observations, emphasize areas where clear conclusions can be made, and discuss potential unifying themes. We also identify knowledge gaps that need to be addressed in order to exploit astrocyte diversity as a biological phenomenon of physiological relevance in the CNS. We thus provide a summary and a perspective on astrocyte diversity in the vertebrate CNS.},
  file = {Khakh and Deneen - 2019 - The Emerging Nature of Astrocyte Diversity.pdf},
  journal = {Annu. Rev. Neurosci.},
  language = {en},
  number = {1}
}

@article{Kharkwal2016,
  title = {Parkinsonism {{Driven}} by {{Antipsychotics Originates}} from {{Dopaminergic Control}} of {{Striatal Cholinergic Interneurons}}},
  author = {Kharkwal, Geetika and {Brami-Cherrier}, Karen and {Lizardi-Ortiz}, Jos{\'e} E. and Nelson, Alexandra B. and Ramos, Maria and Del Barrio, Daniel and Sulzer, David and Kreitzer, Anatol C. and Borrelli, Emiliana},
  year = {2016},
  month = jul,
  volume = {91},
  pages = {67--78},
  issn = {08966273},
  doi = {10.1016/j.neuron.2016.06.014},
  abstract = {Typical antipsychotics can cause disabling side effects. Specifically, antagonism of D2R signaling by the typical antipsychotic haloperidol induces parkinsonism in humans and catalepsy in rodents. Striatal dopamine D2 receptors (D2R) are major regulators of motor activity through their signaling on striatal projection neurons and interneurons. We show that D2R signaling on cholinergic interneurons contributes to an in vitro pause in firing of these otherwise tonically active neurons and to the striatal dopamine/acetylcholine balance. The selective ablation of D2R from cholinergic neurons allows discrimination between the motor-reducing and cataleptic effects of antipsychotics. The cataleptic effect of antipsychotics is triggered by blockade of D2R on cholinergic interneurons and the consequent increase of acetylcholine signaling on striatal projection neurons. These studies illuminate the critical role of D2R-mediated signaling in regulating the activity of striatal cholinergic interneurons and the mechanisms of typical antipsychotic side effects.},
  file = {Kharkwal et al. - 2016 - Parkinsonism Driven by Antipsychotics Originates f.pdf},
  journal = {Neuron},
  language = {en},
  number = {1}
}

@article{Kheradpisheh2018,
  title = {{{STDP}}-Based Spiking Deep Convolutional Neural Networks for Object Recognition},
  author = {Kheradpisheh, Saeed Reza and Ganjtabesh, Mohammad and Thorpe, Simon J. and Masquelier, Timoth{\'e}e},
  year = {2018},
  month = mar,
  volume = {99},
  pages = {56--67},
  issn = {08936080},
  doi = {10.1016/j.neunet.2017.12.005},
  file = {Kheradpisheh et al. - 2018 - STDP-based spiking deep convolutional neural netwo.pdf},
  journal = {Neural Networks},
  language = {en}
}

@article{Khodagholy2013,
  title = {In Vivo Recordings of Brain Activity Using Organic Transistors},
  author = {Khodagholy, Dion and Doublet, Thomas and Quilichini, Pascale and Gurfinkel, Moshe and Leleux, Pierre and Ghestem, Antoine and Ismailova, Esma and Herv{\'e}, Thierry and Sanaur, S{\'e}bastien and Bernard, Christophe and Malliaras, George G.},
  year = {2013},
  month = dec,
  volume = {4},
  issn = {2041-1723},
  doi = {10.1038/ncomms2573},
  file = {2013 - Khodagholy et al. - In vivo recordings of brain activity using organic transistors.pdf},
  journal = {Nature Communications},
  language = {en},
  number = {1}
}

@article{Khodagholy2015,
  title = {{{NeuroGrid}}: Recording Action Potentials from the Surface of the Brain},
  shorttitle = {{{NeuroGrid}}},
  author = {Khodagholy, Dion and Gelinas, Jennifer N and Thesen, Thomas and Doyle, Werner and Devinsky, Orrin and Malliaras, George G and Buzs{\'a}ki, Gy{\"o}rgy},
  year = {2015},
  month = feb,
  volume = {18},
  pages = {310--315},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.3905},
  file = {2015 - Khodagholy et al. - NeuroGrid recording action potentials from the surface of the brain.pdf;Khodagholy et al. - 2015 - NeuroGrid recording action potentials from the su.pdf},
  journal = {Nature Neuroscience},
  language = {en},
  number = {2}
}

@article{Kidd2015,
  title = {The {{Psychology}} and {{Neuroscience}} of {{Curiosity}}},
  author = {Kidd, Celeste and Hayden, Benjamin Y.},
  year = {2015},
  month = nov,
  volume = {88},
  pages = {449--460},
  issn = {08966273},
  doi = {10.1016/j.neuron.2015.09.010},
  file = {Kidd and Hayden - 2015 - The Psychology and Neuroscience of Curiosity.pdf},
  journal = {Neuron},
  language = {en},
  number = {3}
}

@article{Kiefer,
  title = {{{SEQUENTIAL MINIMAX SEARCH FOR A MAXIMUM}}},
  author = {Kiefer, J},
  pages = {5},
  file = {Kiefer - SEQUENTIAL MINIMAX SEARCH FOR A MAXIMUM.pdf},
  language = {en}
}

@article{Kim1994,
  title = {Glutamate-Induced Calcium Signaling in Astrocytes},
  author = {Kim, Warren T. and Rioult, Marc G. and {Cornell-Bell}, Ann H.},
  year = {1994},
  month = jun,
  volume = {11},
  pages = {173--184},
  issn = {0894-1491, 1098-1136},
  doi = {10.1002/glia.440110211},
  abstract = {Astrocytes respond to the excitatory neurotransmitter glutamate with dynamic spatio-temporal changes in intracellular calcium [Ca"],. Although they share a common wave-likeappearance, the different lCaz+ji changes-an initial spike, sustained elevation, oscillatory intracellular waves, and regenerative intercellular waves-are actually separate and distinct phenomena. These separate components of the astrocytic Ca2+response appear to be generated by two different signal transduction pathways. The metabotropic response evokes an initial spatial Ca"' spike that can propagate rapidly from cell to cell and appears to involve IP,. The metabotropic response can also produce oscillatory intracellular waves of various amplitudes and frequencies that propagate within cells and are sustained only in the presence of external Ca2+.The ionotropic response, however, evokes a sustained elevation in [Ca2+Jiassociated with receptormediated Na+ and Ca2+influx, depolarization, and voltage-dependent Ca2+influx. In addition, the ionotropicresponse can lead to regenerative intercellular waves that propagate smoothly and nondecrementally from cell to cell, possibly involving Na+/Ca2+ exchange. All these astrocytic [Ca2+Iichanges tend to appear wave-like, traveling from region t o region as a transient rise in [Ca" Ii. Nevertheless, as our understanding of the cellular events that underlie these [Ca2+Iichanges grows, it becomes increasingly clear that glutamate-induced Ca2+signaling is a composite of separate and distinct phenomena, which may be distinguished not based on appearance alone, but rather on their underlying mechanisms. o 1994 Wiley-Liss, Inc.},
  file = {Kim et al. - 1994 - Glutamate-induced calcium signaling in astrocytes.pdf},
  journal = {Glia},
  language = {en},
  number = {2}
}

@article{Kim2014,
  title = {Layer 6 {{Corticothalamic Neurons Activate}} a {{Cortical Output Layer}}, {{Layer}} 5a},
  author = {Kim, J. and Matney, C. J. and Blankenship, A. and Hestrin, S. and Brown, S. P.},
  year = {2014},
  month = jul,
  volume = {34},
  pages = {9656--9664},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.1325-14.2014},
  file = {2014 - Kim et al. - Layer 6 Corticothalamic Neurons Activate a Cortical Output Layer, Layer 5a.pdf;Kim et al. - 2014 - Layer 6 Corticothalamic Neurons Activate a Cortica.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {29}
}

@article{Kim2016,
  title = {Low-Dielectric-Constant Polyimide Aerogel Composite Films with Low Water Uptake},
  author = {Kim, Jinyoung and Kwon, Jinuk and Kim, Myeongsoo and Do, Jeonguk and Lee, Daero and Han, Haksoo},
  year = {2016},
  month = jul,
  volume = {48},
  pages = {829--834},
  issn = {0032-3896, 1349-0540},
  doi = {10.1038/pj.2016.37},
  file = {Kim et al. - 2016 - Low-dielectric-constant polyimide aerogel composit.pdf},
  journal = {Polymer Journal},
  language = {en},
  number = {7}
}

@article{Kim2020,
  title = {Active {{World Model Learning}} with {{Progress Curiosity}}},
  author = {Kim, Kuno and Sano, Megumi and De Freitas, Julian and Haber, Nick and Yamins, Daniel},
  year = {2020},
  month = jul,
  abstract = {World models are self-supervised predictive models of how the world evolves. Humans learn world models by curiously exploring their environment, in the process acquiring compact abstractions of high bandwidth sensory inputs, the ability to plan across long temporal horizons, and an understanding of the behavioral patterns of other agents. In this work, we study how to design such a curiosity-driven Active World Model Learning (AWML) system. To do so, we construct a curious agent building world models while visually exploring a 3D physical environment rich with distillations of representative real-world agents. We propose an AWML system driven by \$\textbackslash gamma\$-Progress: a scalable and effective learning progress-based curiosity signal. We show that \$\textbackslash gamma\$-Progress naturally gives rise to an exploration policy that directs attention to complex but learnable dynamics in a balanced manner, thus overcoming the "white noise problem". As a result, our \$\textbackslash gamma\$-Progress-driven controller achieves significantly higher AWML performance than baseline controllers equipped with state-of-the-art exploration strategies such as Random Network Distillation and Model Disagreement.},
  archiveprefix = {arXiv},
  eprint = {2007.07853},
  eprinttype = {arxiv},
  file = {Kim et al. - 2020 - Active World Model Learning with Progress Curiosit.pdf},
  journal = {arXiv:2007.07853 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryclass = {cs, stat}
}

@article{King2013,
  title = {Inhibitory {{Interneurons Decorrelate Excitatory Cells}} to {{Drive Sparse Code Formation}} in a {{Spiking Model}} of {{V1}}},
  author = {King, P. D. and Zylberberg, J. and DeWeese, M. R.},
  year = {2013},
  month = mar,
  volume = {33},
  pages = {5475--5485},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.4188-12.2013},
  file = {2013 - King, Zylberberg, DeWeese - Inhibitory interneurons decorrelate excitatory cells to drive sparse code formation in a spiking mode.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {13}
}

@article{Kingma2013,
  title = {Auto-{{Encoding Variational Bayes}}},
  author = {Kingma, Diederik P. and Welling, Max},
  year = {2013},
  month = dec,
  abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
  archiveprefix = {arXiv},
  eprint = {1312.6114},
  eprinttype = {arxiv},
  file = {2014 - Kingman, Welling - Auto-Encoding Variational Bayes.pdf;Kingma and Welling - 2013 - Auto-Encoding Variational Bayes.pdf},
  journal = {arXiv:1312.6114 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryclass = {cs, stat}
}

@article{Kinkhabwala2011,
  title = {A Structural and Functional Ground Plan for Neurons in the Hindbrain of Zebrafish},
  author = {Kinkhabwala, A. and Riley, M. and Koyama, M. and Monen, J. and Satou, C. and Kimura, Y. and Higashijima, S.-i. and Fetcho, J.},
  year = {2011},
  month = jan,
  volume = {108},
  pages = {1164--1169},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1012185108},
  file = {Kinkhabwala et al. - 2011 - A structural and functional ground plan for neuron.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {3}
}

@article{Kira2015,
  title = {A {{Neural Implementation}} of {{Wald}}'s {{Sequential Probability Ratio Test}}},
  author = {Kira, Shinichiro and Yang, Tianming and Shadlen, Michael N.},
  year = {2015},
  month = feb,
  volume = {85},
  pages = {861--873},
  issn = {08966273},
  doi = {10.1016/j.neuron.2015.01.007},
  abstract = {Difficult decisions often require evaluation of samples of evidence acquired sequentially. A sensible strategy is to accumulate evidence, weighted by its reliability, until sufficient support is attained. An optimal statistical approach would accumulate evidence in units of logarithms of likelihood ratios (logLR) to a desired level. Studies of perceptual decisions suggest that the brain approximates an analogous procedure, but a direct test of accumulation, in units of logLR, to a threshold in units of cumulative logLR is lacking. We trained rhesus monkeys to make decisions based on a sequence of evanescent, visual cues assigned different logLR, hence different reliability. Firing rates of neurons in the lateral intraparietal area (LIP) reflected the accumulation of logLR and reached a stereotyped level before the monkeys committed to a decision. The monkeys' choices and reaction times, including their variability, were explained by LIP activity in the context of accumulation of logLR to a threshold.},
  file = {Kira et al. - 2015 - A Neural Implementation of Wald’s Sequential Proba.pdf},
  journal = {Neuron},
  language = {en},
  number = {4}
}

@article{Kirkpatrick2017,
  title = {Overcoming Catastrophic Forgetting in Neural Networks},
  author = {Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A. and Milan, Kieran and Quan, John and Ramalho, Tiago and {Grabska-Barwinska}, Agnieszka and Hassabis, Demis and Clopath, Claudia and Kumaran, Dharshan and Hadsell, Raia},
  year = {2017},
  month = mar,
  volume = {114},
  pages = {3521--3526},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1611835114},
  file = {Kirkpatrick et al. - 2017 - Overcoming catastrophic forgetting in neural netwo.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {13}
}

@article{Kister,
  title = {Online {{Supplement}}: {{About}} the {{Model}}},
  author = {Kister, Robert},
  pages = {20},
  file = {2003 - Poirazi, Brannon, Mel - Pyramidal Neuron as Two-Layered Neural Network(2).pdf},
  language = {en}
}

@article{Kita1983,
  title = {The Morphology of Intracellularly Labeled Rat Subthalamic Neurons: {{A}} Light Microscopic Analysis},
  shorttitle = {The Morphology of Intracellularly Labeled Rat Subthalamic Neurons},
  author = {Kita, H. and Chang, H. T. and Kitai, S. T.},
  year = {1983},
  month = apr,
  volume = {215},
  pages = {245--257},
  issn = {0021-9967, 1096-9861},
  doi = {10.1002/cne.902150302},
  abstract = {Light microscopic analysis of rat subthalamic (STH) neurons which were intracellularly labeled with horseradish peroxidase, following the acquisition of electrophysiologicaldata, revealed the following: (1)The somata of STH neurons were polygonal or oval with occasionally a few somatic spines. Uusally three or four primary dendrites arose from the soma. Dendritic trunks tapered slightly and divided into long, thin, sparsely spined branches. Dendrites of some STH neurons extended into the cerebral peduncle. (2) Reconstruction of the dendritic field was made in three different planes. In either sagittal or frontal planes, the dendritic field was usually oval and the long axis was parallel to the main axis of STH. In the horizontal plane, the dendritic field of all neurons was polygonal. (3)The axons of all the neurons analyzed originated from the soma and were traced beyond the borders of STH, thus indicating that they were projection neurons. All the parent axons bifurcated at least once. After bifurcation, one axon branch coursed dorsolaterally within the cerebral peduncle and terminated in the globus pallidus. The other branch coursed caudally or mediocaudally and arborized in the substantia nigra. Frequently, the axon branches projecting toward the globus pallidus emitted fine axon collaterals within the entopeduncular nucleus. (4)About one-half of the analyzed STH neurons had intranuclear axon collaterals. The neurons with intranuclear collaterals had a higher dendritic tipslstems ratio than neurons without intranuclear collatera l \textasciitilde T. his observation indicated that STH neurons could be divided into two groups according to their axonal morphology. (5)The axonal terminal arborization observed in all the target sites (i.e., globus pallidus, entopeduncular nucleus, STH, and substantia nigra) were formed with varicose collateral branches which also gave rise to short filaments with beaded endings. Some of these projection neurons could therefore communicate with the target neurons in the globus pallidus, substantia nigra, entopeduncular nucleus, as well as STH through their collateral system.},
  file = {1983 - Kita, Chang, Kitai - The morphology of intracellularly labelled rat subthalamic nucleus a light microscopic analysis.pdf;Kita et al. - 1983 - The morphology of intracellularly labeled rat subt.pdf},
  journal = {The Journal of Comparative Neurology},
  language = {en},
  number = {3}
}

@article{Kivinen2004,
  title = {Online {{Learning}} with {{Kernels}}},
  author = {Kivinen, J. and Smola, A.J. and Williamson, R.C.},
  year = {2004},
  month = aug,
  volume = {52},
  pages = {2165--2176},
  issn = {1053-587X},
  doi = {10.1109/TSP.2004.830991},
  abstract = {Kernel-based algorithms such as support vector machines have achieved considerable success in various problems in batch setting, where all of the training data is available in advance. Support vector machines combine the so-called kernel trick with the large margin idea. There has been little use of these methods in an online setting suitable for real-time applications. In this paper, we consider online learning in a reproducing kernel Hilbert space. By considering classical stochastic gradient descent within a feature space and the use of some straightforward tricks, we develop simple and computationally efficient algorithms for a wide range of problems such as classification, regression, and novelty detection.},
  file = {2004 - Kivinen, Smola, Williamson - Online Learning with Kernels.pdf},
  journal = {IEEE Transactions on Signal Processing},
  language = {en},
  number = {8}
}

@article{Klimesch2012,
  title = {Alpha-Band Oscillations, Attention, and Controlled Access to Stored Information},
  author = {Klimesch, Wolfgang},
  year = {2012},
  month = dec,
  volume = {16},
  pages = {606--617},
  issn = {13646613},
  doi = {10.1016/j.tics.2012.10.007},
  file = {2012 - Klimesch - Alpha-band oscillations, attention, and controlled access to stored information.pdf},
  journal = {Trends in Cognitive Sciences},
  language = {en},
  number = {12}
}

@article{Klyubin2008,
  title = {Keep {{Your Options Open}}: {{An Information}}-{{Based Driving Principle}} for {{Sensorimotor Systems}}},
  shorttitle = {Keep {{Your Options Open}}},
  author = {Klyubin, Alexander S. and Polani, Daniel and Nehaniv, Chrystopher L.},
  editor = {Sporns, Olaf},
  year = {2008},
  month = dec,
  volume = {3},
  pages = {e4018},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0004018},
  abstract = {The central resource processed by the sensorimotor system of an organism is information. We propose an informationbased quantity that allows one to characterize the efficiency of the perception-action loop of an abstract organism model. It measures the potential of the organism to imprint information on the environment via its actuators in a way that can be recaptured by its sensors, essentially quantifying the options available and visible to the organism. Various scenarios suggest that such a quantity could identify the preferred direction of evolution or adaptation of the sensorimotor loop of organisms.},
  file = {Klyubin et al. - 2008 - Keep Your Options Open An Information-Based Drivi.pdf},
  journal = {PLoS ONE},
  language = {en},
  number = {12}
}

@article{Knight1972,
  title = {Dynamics of {{Encoding}} in a {{Population}} of {{Neurons}}},
  author = {Knight, B. W.},
  year = {1972},
  month = jun,
  volume = {59},
  pages = {734--766},
  issn = {0022-1295, 1540-7748},
  doi = {10.1085/jgp.59.6.734},
  abstract = {A simple encoder model, which is a reasonable idealization from known electrophysiological properties, yields a population in which the variation of the firing rate with time is a perfect replica of the shape of the input stimulus. A population of noise-free encoders which depart even slightly from the simple model yield a very much degraded copy of the input stimulus. The presence of noise improves the performance of such a population. The firing rate of a population of neurons is related to the firing rate of a single member in a subtle way.},
  file = {1972 - Knight - Dynamics of Encoding in a Population of Neurons.pdf},
  journal = {The Journal of General Physiology},
  language = {en},
  number = {6}
}

@article{Knowlton2014,
  title = {Dynamical Estimation of Neuron and Network Properties {{III}}: Network Analysis Using Neuron Spike Times},
  shorttitle = {Dynamical Estimation of Neuron and Network Properties {{III}}},
  author = {Knowlton, Chris and Meliza, C. Daniel and Margoliash, Daniel and Abarbanel, Henry D. I.},
  year = {2014},
  month = jun,
  volume = {108},
  pages = {261--273},
  issn = {0340-1200, 1432-0770},
  doi = {10.1007/s00422-014-0601-y},
  abstract = {Estimating the behavior of a network of neurons requires accurate models of the individual neurons along with accurate characterizations of the connections among them. Whereas for a single cell, measurements of the intracellular voltage are technically feasible and sufficient to characterize a useful model of its behavior, making sufficient numbers of simultaneous intracellular measurements to characterize even small networks is infeasible. This paper builds on prior work on single neurons to explore whether knowledge of the time of spiking of neurons in a network, once the nodes (neurons) have been characterized biophysically, can provide enough information to usefully constrain the functional architecture of the network: the existence of synaptic links among neurons and their strength. Using standardized voltage and synaptic gating variable waveforms associated with a spike, we demonstrate that the functional architecture of a small network of model neurons can be established.},
  file = {2014 - Knowlton et al. - Dynamical estimation of neuron and network properties III Network analysis using neuron spike times.pdf;Knowlton et al. - 2014 - Dynamical estimation of neuron and network propert.pdf},
  journal = {Biological Cybernetics},
  language = {en},
  number = {3}
}

@article{Kobayashi2019,
  title = {Common Neural Code for Reward and Information Value},
  author = {Kobayashi, Kenji and Hsu, Ming},
  year = {2019},
  month = jun,
  volume = {116},
  pages = {13061--13066},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1820145116},
  abstract = {Adaptive information seeking is critical for goal-directed behavior. Growing evidence suggests the importance of intrinsic motives such as curiosity or need for novelty, mediated through dopaminergic valuation systems, in driving information-seeking behavior. However, valuing information for its own sake can be highly suboptimal when agents need to evaluate instrumental benefit of information in a forward-looking manner. Here we show that information-seeking behavior in humans is driven by subjective value that is shaped by both instrumental and noninstrumental motives, and that this subjective value of information (SVOI) shares a common neural code with more basic reward value. Specifically, using a task where subjects could purchase information to reduce uncertainty about outcomes of a monetary lottery, we found information purchase decisions could be captured by a computational model of SVOI incorporating utility of anticipation, a form of noninstrumental motive for information seeking, in addition to instrumental benefits. Neurally, trial-by-trial variation in SVOI was correlated with activity in striatum and ventromedial prefrontal cortex. Furthermore, cross-categorical decoding revealed that, within these regions, SVOI and expected utility of lotteries were represented using a common code. These findings provide support for the common currency hypothesis and shed insight on neurocognitive mechanisms underlying information-seeking behavior.},
  file = {Kobayashi and Hsu - 2019 - Common neural code for reward and information valu.pdf},
  journal = {Proc Natl Acad Sci USA},
  language = {en},
  number = {26}
}

@techreport{Kobor2020,
  title = {Adaptation to Recent Outcomes Attenuates the Lasting Effect of Initial Experience on Risky Decisions},
  author = {K{\'o}bor, Andrea and Kardos, Zs{\'o}fia and Tak{\'a}cs, {\'A}d{\'a}m and {\'E}ltet{\H o}, No{\'e}mi and Janacsek, Karolina and {T{\'o}th-F{\'a}ber}, Eszter and Cs{\'e}pe, Val{\'e}ria and Nemeth, Dezso},
  year = {2020},
  month = sep,
  institution = {{Neuroscience}},
  doi = {10.1101/2020.09.25.313551},
  abstract = {Both primarily and recently encountered information have been shown to influence experience-based risky decision making. The primacy effect predicts that initial experience will influence later choices even if outcome probabilities change and reward is ultimately more or less sparse than primarily experienced. However, it has not been investigated whether extended initial experience would induce a more profound primacy effect upon risky choices than brief experience. Therefore, the present study tested in two experiments whether young adults adjusted their risk-taking behavior in the Balloon Analogue Risk task after an unsignaled and unexpected change point. The change point separated early ``good luck'' or ``bad luck'' trials from subsequent ones. While mostly positive (more reward) or mostly negative (no reward) events characterized the early trials, subsequent trials were unbiased. In Experiment 1, the change point occurred after one-sixth or one-third of the trials (brief vs. extended experience) without intermittence, whereas in Experiment 2, it occurred between separate task phases. In Experiment 1, if negative events characterized the early trials, after the change point, risk-taking behavior increased as compared with the early trials. Conversely, if positive events characterized the early trials, risk-taking behavior decreased after the change point. Although the adjustment of risk-taking behavior occurred due to integrating recent experiences, the impact of initial experience was simultaneously observed. The length of initial experience did not reliably influence the adjustment of behavior. In Experiment 2, participants became more prone to take risks as the task progressed, indicating that the impact of initial experience could be overcome. Altogether, we suggest that initial beliefs about outcome probabilities can be updated by recent experiences to adapt to the continuously changing decision environment.},
  file = {Kóbor et al. - 2020 - Adaptation to recent outcomes attenuates the lasti.pdf},
  language = {en},
  type = {Preprint}
}

@incollection{Kocsis2006,
  title = {Bandit {{Based Monte}}-{{Carlo Planning}}},
  booktitle = {Machine {{Learning}}: {{ECML}} 2006},
  author = {Kocsis, Levente and Szepesv{\'a}ri, Csaba},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard and F{\"u}rnkranz, Johannes and Scheffer, Tobias and Spiliopoulou, Myra},
  year = {2006},
  volume = {4212},
  pages = {282--293},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/11871842_29},
  abstract = {For large state-space Markovian Decision Problems MonteCarlo planning is one of the few viable approaches to find near-optimal solutions. In this paper we introduce a new algorithm, UCT, that applies bandit ideas to guide Monte-Carlo planning. In finite-horizon or discounted MDPs the algorithm is shown to be consistent and finite sample bounds are derived on the estimation error due to sampling. Experimental results show that in several domains, UCT is significantly more efficient than its alternatives.},
  file = {Kocsis and Szepesvári - 2006 - Bandit Based Monte-Carlo Planning.pdf},
  isbn = {978-3-540-45375-8 978-3-540-46056-5},
  language = {en}
}

@article{Kolchinsky2018,
  title = {Semantic Information, Autonomous Agency and Non-Equilibrium Statistical Physics},
  author = {Kolchinsky, Artemy and Wolpert, David H.},
  year = {2018},
  month = dec,
  volume = {8},
  pages = {20180041},
  issn = {2042-8898, 2042-8901},
  doi = {10.1098/rsfs.2018.0041},
  file = {Kolchinsky and Wolpert - 2018 - Semantic information, autonomous agency and non-eq.pdf},
  journal = {Interface Focus},
  language = {en},
  number = {6}
}

@article{Kompella2017,
  title = {Continual Curiosity-Driven Skill Acquisition from High-Dimensional Video Inputs for Humanoid Robots},
  author = {Kompella, Varun Raj and Stollenga, Marijn and Luciw, Matthew and Schmidhuber, Juergen},
  year = {2017},
  month = jun,
  volume = {247},
  pages = {313--335},
  issn = {00043702},
  doi = {10.1016/j.artint.2015.02.001},
  file = {Kompella et al. - 2017 - Continual curiosity-driven skill acquisition from .pdf},
  journal = {Artificial Intelligence},
  language = {en}
}

@incollection{Konidaris2006,
  title = {An {{Adaptive Robot Motivational System}}},
  booktitle = {From {{Animals}} to {{Animats}} 9},
  author = {Konidaris, George and Barto, Andrew},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard and Nolfi, Stefano and Baldassarre, Gianluca and Calabretta, Raffaele and Hallam, John C. T. and Marocco, Davide and Meyer, Jean-Arcady and Miglino, Orazio and Parisi, Domenico},
  year = {2006},
  volume = {4095},
  pages = {346--356},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/11840541_29},
  abstract = {We present a robot motivational system design framework. The framework represents the underlying (possibly conflicting) goals of the robot as a set of drives, while ensuring comparable drive levels and providing a mechanism for drive priority adaptation during the robot's lifetime. The resulting drive reward signals are compatible with existing reinforcement learning methods for balancing multiple reward functions. We illustrate the framework with an experiment that demonstrates some of its benefits.},
  file = {Konidaris and Barto - 2006 - An Adaptive Robot Motivational System.pdf},
  isbn = {978-3-540-38608-7 978-3-540-38615-5},
  language = {en}
}

@article{Kopell2014,
  title = {Beyond the {{Connectome}}: {{The Dynome}}},
  shorttitle = {Beyond the {{Connectome}}},
  author = {Kopell, Nancy J. and Gritton, Howard J. and Whittington, Miles A. and Kramer, Mark A.},
  year = {2014},
  month = sep,
  volume = {83},
  pages = {1319--1328},
  issn = {08966273},
  doi = {10.1016/j.neuron.2014.08.016},
  file = {2014 - Kopell et al. - Perspective Beyond the Connectome The Dynome.pdf;Kopell et al. - 2014 - Beyond the Connectome The Dynome.pdf},
  journal = {Neuron},
  language = {en},
  number = {6}
}

@article{Korcsak-Gorzo2021,
  title = {Cortical Oscillations Implement a Backbone for Sampling-Based Computation in Spiking Neural Networks},
  author = {{Korcsak-Gorzo}, Agnes and M{\"u}ller, Michael G. and Baumbach, Andreas and Leng, Luziwei and Breitwieser, Oliver Julien and {van Albada}, Sacha J. and Senn, Walter and Meier, Karlheinz and Legenstein, Robert and Petrovici, Mihai A.},
  year = {2021},
  month = mar,
  abstract = {Brains need to deal with an uncertain world. O en, this requires visiting multiple interpretations of the available information or multiple solutions to an encountered problem. is gives rise to the so-called mixing problem: since all of these ``valid'' states represent powerful a ractors, but between themselves can be very dissimilar, switching between such states can be di cult. We propose that cortical oscillations can be e ectively used to overcome this challenge. By acting as an e ective temperature, background spiking activity modulates exploration. Rhythmic changes induced by cortical oscillations can then be interpreted as a form of simulated tempering. We provide a rigorous mathematical discussion of this link and study some of its phenomenological implications in computer simulations. is identi es a new computational role of cortical oscillations and connects them to various phenomena in the brain, such as sampling-based probabilistic inference, memory replay, multisensory cue combination and place cell ickering.},
  archiveprefix = {arXiv},
  eprint = {2006.11099},
  eprinttype = {arxiv},
  file = {Korcsak-Gorzo et al. - 2021 - Cortical oscillations implement a backbone for sam.pdf},
  journal = {arXiv:2006.11099 [cs, q-bio]},
  keywords = {Computer Science - Neural and Evolutionary Computing,Quantitative Biology - Neurons and Cognition},
  language = {en},
  primaryclass = {cs, q-bio}
}

@article{Kording2004,
  title = {Bayesian Integration in Sensorimotor Learning},
  author = {K{\"o}rding, Konrad P. and Wolpert, Daniel M.},
  year = {2004},
  month = jan,
  volume = {427},
  pages = {244--247},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature02169},
  file = {Körding and Wolpert - 2004 - Bayesian integration in sensorimotor learning.pdf},
  journal = {Nature},
  language = {en},
  number = {6971}
}

@article{Kosoy2020,
  title = {Exploring {{Exploration}}: {{Comparing Children}} with {{RL Agents}} in {{Unified Environments}}},
  shorttitle = {Exploring {{Exploration}}},
  author = {Kosoy, Eliza and Collins, Jasmine and Chan, David M. and Huang, Sandy and Pathak, Deepak and Agrawal, Pulkit and Canny, John and Gopnik, Alison and Hamrick, Jessica B.},
  year = {2020},
  month = jul,
  abstract = {Research in developmental psychology consistently shows that children explore the world thoroughly and efficiently and that this exploration allows them to learn. In turn, this early learning supports more robust generalization and intelligent behavior later in life. While much work has gone into developing methods for exploration in machine learning, artificial agents have not yet reached the high standard set by their human counterparts. In this work we propose using DeepMind Lab (Beattie et al., 2016) as a platform to directly compare child and agent behaviors and to develop new exploration techniques. We outline two ongoing experiments to demonstrate the effectiveness of a direct comparison, and outline a number of open research questions that we believe can be tested using this methodology.},
  archiveprefix = {arXiv},
  eprint = {2005.02880},
  eprinttype = {arxiv},
  file = {Kosoy et al. - 2020 - Exploring Exploration Comparing Children with RL .pdf},
  journal = {arXiv:2005.02880 [cs]},
  keywords = {Computer Science - Artificial Intelligence},
  language = {en},
  primaryclass = {cs}
}

@article{Kostrikov2020,
  title = {Image {{Augmentation Is All You Need}}: {{Regularizing Deep Reinforcement Learning}} from {{Pixels}}},
  shorttitle = {Image {{Augmentation Is All You Need}}},
  author = {Kostrikov, Ilya and Yarats, Denis and Fergus, Rob},
  year = {2020},
  month = apr,
  abstract = {We propose a simple data augmentation technique that can be applied to standard model-free reinforcement learning algorithms, enabling robust learning directly from pixels without the need for auxiliary losses or pre-training. The approach leverages input perturbations commonly used in computer vision tasks to regularize the value function. Existing model-free approaches, such as Soft Actor-Critic (SAC) [20], are not able to train deep networks effectively from image pixels. However, the addition of our augmentation method dramatically improves SAC's performance, enabling it to reach state-of-the-art performance on the DeepMind control suite, surpassing model-based [21, 31, 22] methods and recently proposed contrastive learning [42]. Our approach can be combined with any model-free reinforcement learning algorithm, requiring only minor modifications. An implementation can be found at https://sites.google.com/view/data-regularized-q.},
  archiveprefix = {arXiv},
  eprint = {2004.13649},
  eprinttype = {arxiv},
  file = {Kostrikov et al. - 2020 - Image Augmentation Is All You Need Regularizing D.pdf},
  journal = {arXiv:2004.13649 [cs, eess, stat]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing,Statistics - Machine Learning},
  language = {en},
  primaryclass = {cs, eess, stat}
}

@article{Kostrikov2020a,
  title = {Image {{Augmentation Is All You Need}}: {{Regularizing Deep Reinforcement Learning}} from {{Pixels}}},
  shorttitle = {Image {{Augmentation Is All You Need}}},
  author = {Kostrikov, Ilya and Yarats, Denis and Fergus, Rob},
  year = {2020},
  month = apr,
  abstract = {We propose a simple data augmentation technique that can be applied to standard model-free reinforcement learning algorithms, enabling robust learning directly from pixels without the need for auxiliary losses or pre-training. The approach leverages input perturbations commonly used in computer vision tasks to regularize the value function. Existing model-free approaches, such as Soft Actor-Critic (SAC) [20], are not able to train deep networks effectively from image pixels. However, the addition of our augmentation method dramatically improves SAC's performance, enabling it to reach state-of-the-art performance on the DeepMind control suite, surpassing model-based [21, 31, 22] methods and recently proposed contrastive learning [42]. Our approach can be combined with any model-free reinforcement learning algorithm, requiring only minor modifications. An implementation can be found at https://sites.google.com/view/data-regularized-q.},
  archiveprefix = {arXiv},
  eprint = {2004.13649},
  eprinttype = {arxiv},
  file = {Kostrikov et al. - 2020 - Image Augmentation Is All You Need Regularizing D 2.pdf},
  journal = {arXiv:2004.13649 [cs, eess, stat]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing,Statistics - Machine Learning},
  language = {en},
  primaryclass = {cs, eess, stat}
}

@article{Kostuk2012,
  title = {Dynamical Estimation of Neuron and Network Properties {{II}}: Path Integral {{Monte Carlo}} Methods},
  shorttitle = {Dynamical Estimation of Neuron and Network Properties {{II}}},
  author = {Kostuk, Mark and Toth, Bryan A. and Meliza, C. Daniel and Margoliash, Daniel and Abarbanel, Henry D. I.},
  year = {2012},
  month = mar,
  volume = {106},
  pages = {155--167},
  issn = {0340-1200, 1432-0770},
  doi = {10.1007/s00422-012-0487-5},
  file = {2012 - Kostuk et al. - Dynamical estimation of neuron and network properties II Path integral Monte Carlo methods.pdf},
  journal = {Biological Cybernetics},
  language = {en},
  number = {3}
}

@article{Koyama2008,
  title = {Spike {{Train Probability Models}} for {{Stimulus}}-{{Driven Leaky Integrate}}-and-{{Fire Neurons}}},
  author = {Koyama, Shinsuke and Kass, Robert E.},
  year = {2008},
  month = jul,
  volume = {20},
  pages = {1776--1795},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/neco.2008.06-07-540},
  file = {2008 - Koyama, Kass - Spike train probability models for stimulus-driven leaky integrate-and-fire neurons.pdf},
  journal = {Neural Computation},
  language = {en},
  number = {7}
}

@article{Kreiss1997,
  title = {The {{Response}} of {{Subthalamic Nucleus Neurons}} to {{Dopamine Receptor Stimulation}} in a {{Rodent Model}} of {{Parkinson}}'s {{Disease}}},
  author = {Kreiss, Deborah S. and Mastropietro, Christopher W. and Rawji, Saima S. and Walters, Judith R.},
  year = {1997},
  month = sep,
  volume = {17},
  pages = {6807--6819},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.17-17-06807.1997},
  file = {1997 - Kreiss et al. - The response of subthalamic nucleus neurons to dopamine receptor stimulation in a rodent model of Parkinson's dis.pdf},
  journal = {The Journal of Neuroscience},
  language = {en},
  number = {17}
}

@article{Kreitz2015,
  title = {Working-Memory Performance Is Related to Spatial Breadth of Attention},
  author = {Kreitz, Carina and Furley, Philip and Memmert, Daniel and Simons, Daniel J.},
  year = {2015},
  month = nov,
  volume = {79},
  pages = {1034--1041},
  issn = {0340-0727, 1430-2772},
  doi = {10.1007/s00426-014-0633-x},
  abstract = {Working memory and attention are closely related constructs. Models of working memory often incorporate an attention component, and some even equate working memory and attentional control. Although some attention-related processes, including inhibitory control of response conflict and interference resolution, are strongly associated with working memory, for other aspects of attention the link is less clear. We examined the association between working-memory performance and attentional breadth, the ability to spread attention spatially. If the link between attention and working memory is broader than inhibitory and interference resolution processes, then working-memory performance might also be associated with other attentional abilities, including attentional breadth. We tested 123 participants on a variety of working-memory and attentional-breadth measures, finding a strong correlation between performances on these two types of tasks. This finding demonstrates that the link between working memory and attention extends beyond inhibitory processes.},
  file = {2014 - Kreitz et al. - Working-memory performance is related to spatial breadth of attention.pdf;Kreitz et al. - 2015 - Working-memory performance is related to spatial b.pdf},
  journal = {Psychological Research},
  language = {en},
  number = {6}
}

@article{Kriegeskorte2007,
  title = {Analyzing for Information, Not Activation, to Exploit High-Resolution {{fMRI}}},
  author = {Kriegeskorte, Nikolaus and Bandettini, Peter},
  year = {2007},
  month = dec,
  volume = {38},
  pages = {649--662},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2007.02.022},
  file = {2007 - Kriegeskorte, Bandettini - Analyzing for information, not activation, to exploit high-resolution fMRI.pdf},
  journal = {NeuroImage},
  language = {en},
  number = {4}
}

@article{Kriegeskorte2007a,
  title = {Combining the Tools: {{Activation}}- and Information-Based {{fMRI}} Analysis},
  shorttitle = {Combining the Tools},
  author = {Kriegeskorte, Nikolaus and Bandettini, Peter},
  year = {2007},
  month = dec,
  volume = {38},
  pages = {666--668},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2007.06.030},
  file = {2007 - Kriegeskorte, Bandettini - Combining the tools activation- and information-based fMRI analysis.pdf},
  journal = {NeuroImage},
  language = {en},
  number = {4}
}

@article{Kriegeskorte2008,
  title = {Representational Similarity Analysis \textendash{} Connecting the Branches of Systems Neuroscience},
  author = {Kriegeskorte, Nikolaus},
  year = {2008},
  issn = {16625137},
  doi = {10.3389/neuro.06.004.2008},
  abstract = {A fundamental challenge for systems neuroscience is to quantitatively relate its three major branches of research: brain-activity measurement, behavioral measurement, and computational modeling. Using measured brain-activity patterns to evaluate computational network models is complicated by the need to define the correspondency between the units of the model and the channels of the brain-activity data, e.g., single-cell recordings or voxels from functional magnetic resonance imaging (fMRI). Similar correspondency problems complicate relating activity patterns between different modalities of brain-activity measurement (e.g., fMRI and invasive or scalp electrophysiology), and between subjects and species. In order to bridge these divides, we suggest abstracting from the activity patterns themselves and computing representational dissimilarity matrices (RDMs), which characterize the information carried by a given representation in a brain or model. Building on a rich psychological and mathematical literature on similarity analysis, we propose a new experimental and data-analytical framework called representational similarity analysis (RSA), in which multi-channel measures of neural activity are quantitatively related to each other and to computational theory and behavior by comparing RDMs. We demonstrate RSA by relating representations of visual objects as measured with fMRI in early visual cortex and the fusiform face area to computational models spanning a wide range of complexities.The RDMs are simultaneously related via second-level application of multidimensional scaling and tested using randomization and bootstrap techniques. We discuss the broad potential of RSA, including novel approaches to experimental design, and argue that these ideas, which have deep roots in psychology and neuroscience, will allow the integrated quantitative analysis of data from all three branches, thus contributing to a more unified systems neuroscience.},
  file = {2008 - Kriegeskorte, Mur, Bandettini - Representational similarity analysis - connecting the branches of systems neuroscience.pdf},
  journal = {Frontiers in Systems Neuroscience},
  language = {en}
}

@article{Kriegeskorte2011,
  title = {Pattern-Information Analysis: {{From}} Stimulus Decoding to Computational-Model Testing},
  shorttitle = {Pattern-Information Analysis},
  author = {Kriegeskorte, Nikolaus},
  year = {2011},
  month = may,
  volume = {56},
  pages = {411--421},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2011.01.061},
  abstract = {Pattern-information analysis has become an important new paradigm in functional imaging. Here I review and compare existing approaches with a focus on the question of what we can learn from them in terms of brain theory. The most popular and widespread method is stimulus decoding by response-pattern classification. This approach addresses the question whether activity patterns in a given region carry information about the stimulus category. Pattern classification uses generic models of the stimulus\textendash response relationship that do not mimic brain information processing and treats the stimulus space as categorical\textemdash a simplification that is often helpful, but also limiting in terms of the questions that can be addressed. We can address the question whether representations are consistent across different stimulus sets or tasks by crossdecoding, where the classifier is trained with one set of stimuli (or task) and tested with another. Beyond pattern classification, a major new direction is the integration of computational models of brain information processing into pattern-information analysis. This approach enables us to address the question to what extent competing computational models are consistent with the stimulus representations in a brain region. Two methods that test computational models are voxel receptive-field modeling and representational similarity analysis. These methods sample the stimulus (or mental-state) space more richly, estimate a separate response pattern for each stimulus, and can generalize from the stimulus sample to a stimulus population. Computational models that mimic brain information processing predict responses from stimuli. The reverse transform can be modeled to reconstruct stimuli from responses. Stimulus reconstruction is a challenging feat of engineering, but the implications of the results for brain theory are not always clear. Exploratory pattern analyses complement the confirmatory approaches mentioned so far and can reveal strong, unexpected effects that might be missed when testing only a restricted set of predefined hypotheses.},
  file = {2011 - Kriegeskorte - Pattern-information analysis from stimulus decoding to computational-model testing.pdf},
  journal = {NeuroImage},
  language = {en},
  number = {2}
}

@article{Kriegeskorte2016,
  title = {Inferring Brain-Computational Mechanisms with Models of Activity Measurements},
  author = {Kriegeskorte, Nikolaus and Diedrichsen, J{\"o}rn},
  year = {2016},
  month = oct,
  volume = {371},
  pages = {20160278},
  issn = {0962-8436, 1471-2970},
  doi = {10.1098/rstb.2016.0278},
  file = {Kriegeskorte and Diedrichsen - 2016 - Inferring brain-computational mechanisms with mode.pdf},
  journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  language = {en},
  number = {1705}
}

@article{Kriegeskorte2018,
  title = {Cognitive Computational Neuroscience},
  author = {Kriegeskorte, Nikolaus and Douglas, Pamela K.},
  year = {2018},
  month = sep,
  volume = {21},
  pages = {1148--1160},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/s41593-018-0210-5},
  file = {Kriegeskorte and Douglas - 2018 - Cognitive computational neuroscience.pdf},
  journal = {Nature Neuroscience},
  language = {en},
  number = {9}
}

@article{Kriegman2020,
  title = {A Scalable Pipeline for Designing Reconfigurable Organisms},
  author = {Kriegman, Sam and Blackiston, Douglas and Levin, Michael and Bongard, Josh},
  year = {2020},
  month = jan,
  volume = {117},
  pages = {1853--1859},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1910837117},
  abstract = {Living systems are more robust, diverse, complex, and supportive of human life than any technology yet created. However, our ability to create novel lifeforms is currently limited to varying existing organisms or bioengineering organoids in vitro. Here we show a scalable pipeline for creating functional novel lifeforms: AI methods automatically design diverse candidate lifeforms in silico to perform some desired function, and transferable designs are then created using a cell-based construction toolkit to realize living systems with the predicted behaviors. Although some steps in this pipeline still require manual intervention, complete automation in future would pave the way to designing and deploying unique, bespoke living systems for a wide range of functions.},
  file = {Kriegman et al. - 2020 - A scalable pipeline for designing reconfigurable o.pdf},
  journal = {Proc Natl Acad Sci USA},
  language = {en},
  number = {4}
}

@article{Kristan2014,
  title = {Behavioral {{Sequencing}}: {{Competitive Queuing}} in the {{Fly CNS}}},
  shorttitle = {Behavioral {{Sequencing}}},
  author = {Kristan, William B.},
  year = {2014},
  month = aug,
  volume = {24},
  pages = {R743-R746},
  issn = {09609822},
  doi = {10.1016/j.cub.2014.06.071},
  file = {Kristan - 2014 - Behavioral Sequencing Competitive Queuing in the .pdf},
  journal = {Current Biology},
  language = {en},
  number = {16}
}

@article{Krnjevic1971,
  title = {The Mechanism of Excitation by Acetylcholine in the Cerebral Cortex},
  author = {Krnjevi{\'c}, K. and Pumain, R. and Renaud, L.},
  year = {1971},
  month = may,
  volume = {215},
  pages = {247--268},
  issn = {00223751},
  doi = {10.1113/jphysiol.1971.sp009467},
  file = {1971 - Krnjevj, Pumain, Renaudt - THE MECHANISM OF EXCITATION BY ACETYLCHOLINE IN THE CEREBRAL CORTEX BY.pdf},
  journal = {The Journal of Physiology},
  language = {en},
  number = {1}
}

@article{Krueger,
  title = {Active {{Reinforcement Learning}}: {{Observing Rewards}} at a {{Cost}}},
  author = {Krueger, David and Leike, Jan and Evans, Owain and Salvatier, John},
  pages = {9},
  abstract = {Active reinforcement learning (ARL) is a variant on reinforcement learning where the agent does not observe the reward unless it chooses to pay a query cost c {$>$} 0. The central question of ARL is how to quantify the long-term value of reward information. Even in multi-armed bandits, computing the value of this information is intractable and we have to rely on heuristics. We propose and evaluate several heuristic approaches for ARL in multi-armed bandits and (tabular) Markov decision processes, and discuss and illustrate some challenging aspects of the ARL problem.},
  file = {Krueger et al. - Active Reinforcement Learning Observing Rewards a.pdf},
  language = {en}
}

@article{Ku2008,
  title = {Comparison of Pattern Recognition Methods in Classifying High-Resolution {{BOLD}} Signals Obtained at High Magnetic Field in Monkeys},
  author = {Ku, Shih-pi and Gretton, Arthur and Macke, Jakob and Logothetis, Nikos K.},
  year = {2008},
  month = sep,
  volume = {26},
  pages = {1007--1014},
  issn = {0730725X},
  doi = {10.1016/j.mri.2008.02.016},
  abstract = {Pattern recognition methods have shown that functional magnetic resonance imaging (fMRI) data can reveal significant information about brain activity. For example, in the debate of how object categories are represented in the brain, multivariate analysis has been used to provide evidence of a distributed encoding scheme [Science 293:5539 (2001) 2425\textendash 2430]. Many follow-up studies have employed different methods to analyze human fMRI data with varying degrees of success [Nature reviews 7:7 (2006) 523\textendash 534]. In this study, we compare four popular pattern recognition methods: correlation analysis, support-vector machines (SVM), linear discriminant analysis (LDA) and Gaussian na\"ive Bayes (GNB), using data collected at high field (7 Tesla) with higher resolution than usual fMRI studies. We investigate prediction performance on single trials and for averages across varying numbers of stimulus presentations. The performance of the various algorithms depends on the nature of the brain activity being categorized: for several tasks, many of the methods work well, whereas for others, no method performs above chance level. An important factor in overall classification performance is careful preprocessing of the data, including dimensionality reduction, voxel selection and outlier elimination.},
  file = {2008 - Ku et al. - Comparison of pattern recognition methods in classifying high-resolution BOLD signals obtained at high magnetic field.pdf},
  journal = {Magnetic Resonance Imaging},
  language = {en},
  number = {7}
}

@article{Kubanek2020,
  title = {Remote, Brain Region\textendash Specific Control of Choice Behavior with Ultrasonic Waves},
  author = {Kubanek, Jan and Brown, Julian and Ye, Patrick and Pauly, Kim Butts and Moore, Tirin and Newsome, William},
  year = {2020},
  month = may,
  volume = {6},
  pages = {eaaz4193},
  issn = {2375-2548},
  doi = {10.1126/sciadv.aaz4193},
  abstract = {The ability to modulate neural activity in specific brain circuits remotely and systematically could revolutionize studies of brain function and treatments of brain disorders. Sound waves of high frequencies (ultrasound) have shown promise in this respect, combining the ability to modulate neuronal activity with sharp spatial focus. Here, we show that the approach can have potent effects on choice behavior. Brief, low-intensity ultrasound pulses delivered noninvasively into specific brain regions of macaque monkeys influenced their decisions regarding which target to choose. The effects were substantial, leading to around a 2:1 bias in choices compared to the default balanced proportion. The effect presence and polarity was controlled by the specific target region. These results represent a critical step towards the ability to influence choice behavior noninvasively, enabling systematic investigations and treatments of brain circuits underlying disorders of choice.},
  file = {Kubanek et al. - 2020 - Remote, brain region–specific control of choice be.pdf},
  journal = {Sci. Adv.},
  language = {en},
  number = {21}
}

@article{Kuchibhotla2017,
  title = {Parallel Processing by Cortical Inhibition Enables Context-Dependent Behavior},
  author = {Kuchibhotla, Kishore V and Gill, Jonathan V and Lindsay, Grace W and Papadoyannis, Eleni S and Field, Rachel E and Sten, Tom A Hindmarsh and Miller, Kenneth D and Froemke, Robert C},
  year = {2017},
  month = jan,
  volume = {20},
  pages = {62--71},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.4436},
  file = {Kuchibhotla et al. - 2017 - Parallel processing by cortical inhibition enables.pdf},
  journal = {Nature Neuroscience},
  language = {en},
  number = {1}
}

@article{Kuczala2016,
  title = {Eigenvalue Spectra of Large Correlated Random Matrices},
  author = {Kuczala, Alexander and Sharpee, Tatyana O.},
  year = {2016},
  month = nov,
  volume = {94},
  issn = {2470-0045, 2470-0053},
  doi = {10.1103/PhysRevE.94.050101},
  file = {Kuczala and Sharpee - 2016 - Eigenvalue spectra of large correlated random matr.pdf},
  journal = {Physical Review E},
  language = {en},
  number = {5}
}

@article{Kuga2011,
  title = {Large-{{Scale Calcium Waves Traveling}} through {{Astrocytic Networks In Vivo}}},
  author = {Kuga, N. and Sasaki, T. and Takahara, Y. and Matsuki, N. and Ikegaya, Y.},
  year = {2011},
  month = feb,
  volume = {31},
  pages = {2607--2614},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.5319-10.2011},
  file = {Kuga et al. - 2011 - Large-Scale Calcium Waves Traveling through Astroc.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {7}
}

@article{Kuhn2006,
  title = {Modulation of Beta Oscillations in the Subthalamic Area during Motor Imagery in {{Parkinson}}'s Disease},
  author = {K{\"u}hn, Andrea A. and Doyle, Louise and Pogosyan, Alek and Yarrow, Kielan and Kupsch, Andreas and Schneider, Gerd-Helge and Hariz, Marwan I. and Trottenberg, Thomas and Brown, Peter},
  year = {2006},
  month = mar,
  volume = {129},
  pages = {695--706},
  issn = {1460-2156, 0006-8950},
  doi = {10.1093/brain/awh715},
  file = {2006 - Kühn et al. - Modulation of beta oscillations in the subthalamic area during motor imagery in Parkinson's disease.pdf},
  journal = {Brain},
  language = {en},
  number = {3}
}

@article{Kulkarni2016,
  title = {Hierarchical {{Deep Reinforcement Learning}}: {{Integrating Temporal Abstraction}} and {{Intrinsic Motivation}}},
  shorttitle = {Hierarchical {{Deep Reinforcement Learning}}},
  author = {Kulkarni, Tejas D. and Narasimhan, Karthik R. and Saeedi, Ardavan and Tenenbaum, Joshua B.},
  year = {2016},
  month = apr,
  abstract = {Learning goal-directed behavior in environments with sparse feedback is a major challenge for reinforcement learning algorithms. The primary difficulty arises due to insufficient exploration, resulting in an agent being unable to learn robust value functions. Intrinsically motivated agents can explore new behavior for its own sake rather than to directly solve problems. Such intrinsic behaviors could eventually help the agent solve tasks posed by the environment. We present hierarchical-DQN (h-DQN), a framework to integrate hierarchical value functions, operating at different temporal scales, with intrinsically motivated deep reinforcement learning. A top-level value function learns a policy over intrinsic goals, and a lower-level function learns a policy over atomic actions to satisfy the given goals. h-DQN allows for flexible goal specifications, such as functions over entities and relations. This provides an efficient space for exploration in complicated environments. We demonstrate the strength of our approach on two problems with very sparse, delayed feedback: (1) a complex discrete stochastic decision process, and (2) the classic ATARI game `Montezuma's Revenge'.},
  archiveprefix = {arXiv},
  eprint = {1604.06057},
  eprinttype = {arxiv},
  file = {Kulkarni et al. - 2016 - Hierarchical Deep Reinforcement Learning Integrat.pdf},
  journal = {arXiv:1604.06057 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  language = {en},
  primaryclass = {cs, stat}
}

@article{Kulkarni2016a,
  title = {Deep {{Successor Reinforcement Learning}}},
  author = {Kulkarni, Tejas D and Saeedi, Ardavan and Gautam, Simanta and Gershman, Samuel J},
  year = {2016},
  volume = {1606.02396v1},
  pages = {1--10},
  file = {Kulkarni et al. - Deep Successor Reinforcement Learning.pdf},
  journal = {Arxiv},
  language = {en}
}

@article{Kulkarni2019,
  title = {Unsupervised {{Learning}} of {{Object Keypoints}} for {{Perception}} and {{Control}}},
  author = {Kulkarni, Tejas and Gupta, Ankush and Ionescu, Catalin and Borgeaud, Sebastian and Reynolds, Malcolm and Zisserman, Andrew and Mnih, Volodymyr},
  year = {2019},
  month = jun,
  abstract = {The study of object representations in computer vision has primarily focused on developing representations that are useful for image classification, object detection, or semantic segmentation as downstream tasks. In this work we aim to learn object representations that are useful for control and reinforcement learning (RL). To this end, we introduce Transporter, a neural network architecture for discovering concise geometric object representations in terms of keypoints or image-space coordinates. Our method learns from raw video frames in a fully unsupervised manner, by transporting learnt image features between video frames using a keypoint bottleneck. The discovered keypoints track objects and object parts across long time-horizons more accurately than recent similar methods. Furthermore, consistent long-term tracking enables two notable results in control domains \textendash{} (1) using the keypoint co-ordinates and corresponding image features as inputs enables highly sample-efficient reinforcement learning; (2) learning to explore by controlling keypoint locations drastically reduces the search space, enabling deep exploration (leading to states unreachable through random action exploration) without any extrinsic rewards.},
  archiveprefix = {arXiv},
  eprint = {1906.11883},
  eprinttype = {arxiv},
  file = {Kulkarni et al. - 2019 - Unsupervised Learning of Object Keypoints for Perc.pdf},
  journal = {arXiv:1906.11883 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  language = {en},
  primaryclass = {cs}
}

@article{Kumar2011,
  title = {The {{Role}} of {{Inhibition}} in {{Generating}} and {{Controlling Parkinson}}?S {{Disease Oscillations}} in the {{Basal Ganglia}}},
  shorttitle = {The {{Role}} of {{Inhibition}} in {{Generating}} and {{Controlling Parkinson}}?},
  author = {Kumar, Arvind and Cardanobile, Stefano and Rotter, Stefan and Aertsen, Ad},
  year = {2011},
  volume = {5},
  issn = {1662-5137},
  doi = {10.3389/fnsys.2011.00086},
  abstract = {Movement disorders in Parkinson's disease (PD) are commonly associated with slow oscillations and increased synchrony of neuronal activity in the basal ganglia. The neural mechanisms underlying this dynamic network dysfunction, however, are only poorly understood. Here, we show that the strength of inhibitory inputs from striatum to globus pallidus external (GPe) is a key parameter controlling oscillations in the basal ganglia. Specifically, the increase in striatal activity observed in PD is sufficient to unleash the oscillations in the basal ganglia. This finding allows us to propose a unified explanation for different phenomena: absence of oscillation in the healthy state of the basal ganglia, oscillations in dopamine-depleted state and quenching of oscillations under deep-brain-stimulation (DBS). These novel insights help us to better understand and optimize the function of DBS protocols. Furthermore, studying the model behavior under transient increase of activity of the striatal neurons projecting to the indirect pathway, we are able to account for both motor impairment in PD patients and for reduced response inhibition in DBS implanted patients.},
  file = {2011 - Kumar et al. - The Role of Inhibition in Generating and Controlling Parkinsons Disease Oscillations in the Basal Ganglia.pdf},
  journal = {Frontiers in Systems Neuroscience},
  language = {en}
}

@article{Kumar2015,
  title = {Ask {{Me Anything}}: {{Dynamic Memory Networks}} for {{Natural Language Processing}}},
  shorttitle = {Ask {{Me Anything}}},
  author = {Kumar, Ankit and Irsoy, Ozan and Ondruska, Peter and Iyyer, Mohit and Bradbury, James and Gulrajani, Ishaan and Zhong, Victor and Paulus, Romain and Socher, Richard},
  year = {2015},
  month = jun,
  abstract = {Most tasks in natural language processing can be cast into question answering (QA) problems over language input. We introduce the dynamic memory network (DMN), a neural network architecture which processes input sequences and questions, forms episodic memories, and generates relevant answers. Questions trigger an iterative attention process which allows the model to condition its attention on the inputs and the result of previous iterations. These results are then reasoned over in a hierarchical recurrent sequence model to generate answers. The DMN can be trained end-to-end and obtains state-of-the-art results on several types of tasks and datasets: question answering (Facebook's bAbI dataset), text classification for sentiment analysis (Stanford Sentiment Treebank) and sequence modeling for part-of-speech tagging (WSJ-PTB). The training for these different tasks relies exclusively on trained word vector representations and input-question-answer triplets.},
  archiveprefix = {arXiv},
  eprint = {1506.07285},
  eprinttype = {arxiv},
  file = {2015 - Kumar et al. - Ask Me Anything Dynamic Memory Networks for Natural Language Processing.pdf;Kumar et al. - 2015 - Ask Me Anything Dynamic Memory Networks for Natur.pdf},
  journal = {arXiv:1506.07285 [cs]},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  language = {en},
  primaryclass = {cs}
}

@article{Kunori2014,
  title = {Voltage-{{Sensitive Dye Imaging}} of {{Primary Motor Cortex Activity Produced}} by {{Ventral Tegmental Area Stimulation}}},
  author = {Kunori, N. and Kajiwara, R. and Takashima, I.},
  year = {2014},
  month = jun,
  volume = {34},
  pages = {8894--8903},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.5286-13.2014},
  file = {Kunori et al. - 2014 - Voltage-Sensitive Dye Imaging of Primary Motor Cor.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {26}
}

@article{Kushnir2017,
  title = {Neural Classifiers with Limited Connectivity and Recurrent Readouts},
  author = {Kushnir, Lyudmila and Fusi, Stefano},
  year = {2017},
  month = dec,
  doi = {10.1101/157289},
  abstract = {For many neural network models that are based on perceptrons, the number of activity patterns that can be classified is limited by the number of plastic connections that each neuron receives, even when the total number of neurons is much larger. This poses the problem of how the biological brain can take advantage of its huge number of neurons given that the connectivity is extremely sparse, especially when long range connections are considered. One possible way to overcome this limitation in the case of feed-forward networks is to combine multiple perceptrons together, as in committee machines. The number of classifiable random patterns would then grow linearly with the number of perceptrons, even when each perceptron has limited connectivity. However, the problem is moved to the downstream readout neurons, which would need a number of connections that is as large as the number of perceptrons. Here we propose a different approach in which the readout is implemented by connecting multiple perceptrons in a recurrent attractor neural network. We show with analytical calculations that the number of random classifiable patterns can grow unboundedly with the number of perceptrons, even when the connectivity of each perceptron remains finite. Most importantly both the recurrent connectivity and the connectivity of a downstream readout are also finite. Our study shows that feed-forward neural classifiers with numerous long range connections connecting different layers can be replaced by networks with sparse long range connectivity and local recurrent connectivity without sacrificing the classification performance. Our strategy could be used in the future to design more general scalable network architectures with limited connectivity, which resemble more closely brain neural circuits dominated by recurrent connectivity.},
  file = {Kushnir and Fusi - 2017 - Neural classifiers with limited connectivity and r.pdf},
  journal = {bioRxiv},
  language = {en}
}

@article{Kuznetsov,
  title = {Elements of {{Applied Bifurcation Theory}}, {{Second Edition}}},
  author = {Kuznetsov, Yuri A},
  pages = {614},
  file = {1998 - Kuznetsov - Elements of Applied Bifurcation Theory.pdf}
}

@article{Kvam2015,
  title = {Interference Effects of Choice on Confidence: {{Quantum}} Characteristics of Evidence Accumulation},
  shorttitle = {Interference Effects of Choice on Confidence},
  author = {Kvam, Peter D. and Pleskac, Timothy J. and Yu, Shuli and Busemeyer, Jerome R.},
  year = {2015},
  month = aug,
  volume = {112},
  pages = {10645--10650},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1500688112},
  file = {Kvam et al. - 2015 - Interference effects of choice on confidence Quan.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {34}
}

@article{Laan2017,
  title = {Echo State Networks with Multiple Read-out Modules},
  author = {Laan, Andres and Vicente, Raul},
  year = {2017},
  month = mar,
  doi = {10.1101/017558},
  abstract = {We propose a new readout architecture for echo state networks where multiple linear readout modules are activated at distinct time points to varying degrees by a separate controller module. The controller module, like the reservoir of the echo state network, can be initialized randomly. All linear readout modules are trained through simple linear regression, which is the only adaptive step in the modified algorithm. The resulting architecture provides modest improvements on a variety of time series processing tasks (between 5 to 50\% in performance metric depending on the task studied). The novel architecture is guaranteed to perform at least as accurately as a conventional linear readout. It can be utilized as a general purpose readout method when augmentations to performance relative to the standard method is needed.},
  file = {2015 - Laan - Echo state networks with multiple read-out modules.pdf;Laan and Vicente - 2017 - Echo state networks with multiple read-out modules.pdf},
  journal = {bioRxiv},
  language = {en}
}

@article{Laconte2005,
  title = {Support Vector Machines for Temporal Classification of Block Design {{fMRI}} Data},
  author = {Laconte, S and Strother, S and Cherkassky, V and Anderson, J and Hu, X},
  year = {2005},
  month = jun,
  volume = {26},
  pages = {317--329},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2005.01.048},
  file = {2005 - LaConte et al. - Support vector machines for temporal classification of block design fMRI data.pdf},
  journal = {NeuroImage},
  language = {en},
  number = {2}
}

@article{LaConte2007,
  title = {Real-Time {{fMRI}} Using Brain-State Classification},
  author = {LaConte, Stephen M. and Peltier, Scott J. and Hu, Xiaoping P.},
  year = {2007},
  month = oct,
  volume = {28},
  pages = {1033--1044},
  issn = {10659471, 10970193},
  doi = {10.1002/hbm.20326},
  abstract = {We have implemented a real-time functional magnetic resonance imaging system based on multivariate classification. This approach is distinctly different from spatially localized real-time implementations, since it does not require prior assumptions about functional localization and individual performance strategies, and has the ability to provide feedback based on intuitive translations of brain state rather than localized fluctuations. Thus this approach provides the capability for a new class of experimental designs in which real-time feedback control of the stimulus is possible\textemdash rather than using a fixed paradigm, experiments can adaptively evolve as subjects receive brain-state feedback. In this report, we describe our implementation and characterize its performance capabilities. We observed \$80\% classification accuracy using whole brain, block-design, motor data. Within both left and right motor task conditions, important differences exist between the initial transient period produced by task switching (changing between rapid left or right index finger button presses) and the subsequent stable period during sustained activity. Further analysis revealed that very high accuracy is achievable during stable task periods, and that the responsiveness of the classifier to changes in task condition can be much faster than signal time-to-peak rates. Finally, we demonstrate the versatility of this implementation with respect to behavioral task, suggesting that our results are applicable across a spectrum of cognitive domains. Beyond basic research, this technology can complement electroencephalography-based brain computer interface research, and has potential applications in the areas of biofeedback rehabilitation, lie detection, learning studies, virtual reality-based training, and enhanced conscious awareness. Hum Brain Mapp 28:1033\textendash 1044, 2007. VC 2006 Wiley-Liss, Inc.},
  file = {2007 - LaConte, Peltier, Hu - Real-time fMRI using brain-state classification.pdf},
  journal = {Human Brain Mapping},
  language = {en},
  number = {10}
}

@article{Lagorce2015,
  title = {{{STICK}}: {{Spike Time Interval Computational Kernel}}, a {{Framework}} for {{General Purpose Computation Using Neurons}}, {{Precise Timing}}, {{Delays}}, and {{Synchrony}}},
  shorttitle = {{{STICK}}},
  author = {Lagorce, Xavier and Benosman, Ryad},
  year = {2015},
  month = nov,
  volume = {27},
  pages = {2261--2317},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/NECO_a_00783},
  file = {2015 - Lagorce, Benosman - STICK Spike Time Interval Computational Kernel, a Framework for General Purpose Computation Using Neurons, Pr.pdf;Lagorce and Benosman - 2015 - STICK Spike Time Interval Computational Kernel, a.pdf},
  journal = {Neural Computation},
  language = {en},
  number = {11}
}

@article{Lahiri,
  title = {A Universal Tradeoff between Power, Precision and Speed in Physical Communication},
  author = {Lahiri, Subhaneil and {Sohl-Dickstein}, Jascha and Ganguli, Surya},
  pages = {6},
  file = {Lahiri et al. - A universal tradeoﬀ between power, precision and s.pdf},
  language = {en}
}

@article{Lai1985,
  title = {Asymptotically Efficient Adaptive Allocation Rules},
  author = {Lai, T.L and Robbins, Herbert},
  year = {1985},
  month = mar,
  volume = {6},
  pages = {4--22},
  issn = {01968858},
  doi = {10.1016/0196-8858(85)90002-8},
  file = {Lai and Robbins - 1985 - Asymptotically efficient adaptive allocation rules.pdf},
  journal = {Advances in Applied Mathematics},
  language = {en},
  number = {1}
}

@article{Laing2006,
  title = {On the Application of ``Equation-Free Modelling'' to Neural Systems},
  author = {Laing, Carlo R.},
  year = {2006},
  month = feb,
  volume = {20},
  pages = {5--23},
  issn = {0929-5313, 1573-6873},
  doi = {10.1007/s10827-006-3843-z},
  abstract = {Equation-free modelling'' is a recentlydeveloped technique for bridging the gap between detailed, microscopic descriptions of systems and macroscopic descriptions of their collective behaviour. It uses short, repeated bursts of simulation of the microscopic dynamics to analyse the effective macroscopic equations, even though such equations are not directly available for evaluation. This paper demonstrates these techniques on a variety of networks of model neurons, and discusses the advantages and limitations of such an approach. New results include an understanding of the effects of including gap junctions in a model capable of sustaining spatially localised ``bumps'' of activity, and an investigation of a network of coupled bursting neurons.},
  file = {2006 - Laing - On the application of equation-free modelling to neural systems.pdf},
  journal = {Journal of Computational Neuroscience},
  language = {en},
  number = {1}
}

@article{Laing2014,
  title = {Numerical {{Bifurcation Theory}} for {{High}}-{{Dimensional Neural Models}}},
  author = {Laing, Carlo R},
  year = {2014},
  volume = {4},
  pages = {13},
  issn = {2190-8567},
  doi = {10.1186/2190-8567-4-13},
  abstract = {Numerical bifurcation theory involves finding and then following certain types of solutions of differential equations as parameters are varied, and determining whether they undergo any bifurcations (qualitative changes in behaviour). The primary technique for doing this is numerical continuation, where the solution of interest satisfies a parametrised set of algebraic equations, and branches of solutions are followed as the parameter is varied. An effective way to do this is with pseudoarclength continuation. We give an introduction to pseudo-arclength continuation and then demonstrate its use in investigating the behaviour of a number of models from the field of computational neuroscience. The models we consider are high dimensional, as they result from the discretisation of neural field models\textemdash nonlocal differential equations used to model macroscopic pattern formation in the cortex. We consider both stationary and moving patterns in one spatial dimension, and then translating patterns in two spatial dimensions. A variety of results from the literature are discussed, and a number of extensions of the technique are given.},
  file = {2014 - Laing - Numerical Bifurcation Theory for High-Dimensional Neural Models.pdf;2014 - Laing - Numerical Bifurcation Theory for High-Dimensional Neural Models(2).pdf;Laing - 2014 - Numerical Bifurcation Theory for High-Dimensional  2.pdf;Laing - 2014 - Numerical Bifurcation Theory for High-Dimensional .pdf},
  journal = {The Journal of Mathematical Neuroscience},
  language = {en},
  number = {1}
}

@article{Lainscsek2015,
  title = {Delay {{Differential Analysis}} of {{Time Series}}},
  author = {Lainscsek, Claudia and Sejnowski, Terrence J.},
  year = {2015},
  month = mar,
  volume = {27},
  pages = {594--614},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/NECO_a_00706},
  file = {2014 - Lehky et al. - Dimensionality of object representations in monkey inferotemporal cortex(2).pdf;Lainscsek and Sejnowski - 2015 - Delay Differential Analysis of Time Series.pdf},
  journal = {Neural Computation},
  language = {en},
  number = {3}
}

@article{Laje2013,
  title = {Robust Timing and Motor Patterns by Taming Chaos in Recurrent Neural Networks},
  author = {Laje, Rodrigo and Buonomano, Dean V},
  year = {2013},
  month = jul,
  volume = {16},
  pages = {925--933},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.3405},
  file = {2013 - Laje, Buonomano - Robust timing and motor patterns by taming chaos in recurrent neural networks.pdf},
  journal = {Nature Neuroscience},
  language = {en},
  number = {7}
}

@article{Lajoie2014,
  title = {Structured Chaos Shapes Spike-Response Noise Entropy in Balanced Neural Networks},
  author = {Lajoie, Guillaume and Thivierge, Jean-Philippe and {Shea-Brown}, Eric},
  year = {2014},
  month = oct,
  volume = {8},
  issn = {1662-5188},
  doi = {10.3389/fncom.2014.00123},
  abstract = {Large networks of sparsely coupled, excitatory and inhibitory cells occur throughout the brain. For many models of these networks, a striking feature is that their dynamics are chaotic and thus, are sensitive to small perturbations. How does this chaos manifest in the neural code? Specifically, how variable are the spike patterns that such a network produces in response to an input signal? To answer this, we derive a bound for a general measure of variability\textemdash spike-train entropy. This leads to important insights on the variability of multi-cell spike pattern distributions in large recurrent networks of spiking neurons responding to fluctuating inputs. The analysis is based on results from random dynamical systems theory and is complemented by detailed numerical simulations. We find that the spike pattern entropy is an order of magnitude lower than what would be extrapolated from single cells. This holds despite the fact that network coupling becomes vanishingly sparse as network size grows\textemdash a phenomenon that depends on ``extensive chaos,'' as previously discovered for balanced networks without stimulus drive. Moreover, we show how spike pattern entropy is controlled by temporal features of the inputs. Our findings provide insight into how neural networks may encode stimuli in the presence of inherently chaotic dynamics.},
  file = {Lajoie et al. - 2014 - Structured chaos shapes spike-response noise entro.pdf},
  journal = {Frontiers in Computational Neuroscience},
  language = {en}
}

@article{Lambert2012,
  title = {Confirmation of Functional Zones within the Human Subthalamic Nucleus: {{Patterns}} of Connectivity and Sub-Parcellation Using Diffusion Weighted Imaging},
  shorttitle = {Confirmation of Functional Zones within the Human Subthalamic Nucleus},
  author = {Lambert, Christian and Zrinzo, Ludvic and Nagy, Zoltan and Lutti, Antoine and Hariz, Marwan and Foltynie, Thomas and Draganski, Bogdan and Ashburner, John and Frackowiak, Richard},
  year = {2012},
  month = mar,
  volume = {60},
  pages = {83--94},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2011.11.082},
  abstract = {The subthalamic nucleus (STN) is a small, glutamatergic nucleus situated in the diencephalon. A critical component of normal motor function, it has become a key target for deep brain stimulation in the treatment of Parkinson's disease. Animal studies have demonstrated the existence of three functional sub-zones but these have never been shown conclusively in humans. In this work, a data driven method with diffusion weighted imaging demonstrated that three distinct clusters exist within the human STN based on brain connectivity profiles. The STN was successfully sub-parcellated into these regions, demonstrating good correspondence with that described in the animal literature. The local connectivity of each sub-region supported the hypothesis of bilateral limbic, associative and motor regions occupying the anterior, mid and posterior portions of the nucleus respectively. This study is the first to achieve in-vivo, non-invasive anatomical parcellation of the human STN into three anatomical zones within normal diagnostic scan times, which has important future implications for deep brain stimulation surgery.},
  file = {2012 - Lambert et al. - Confirmation of functional zones within the human subthalamic nucleus Patterns of connectivity and sub-parcellat.pdf},
  journal = {NeuroImage},
  language = {en},
  number = {1}
}

@article{Lange2013,
  title = {Reduced {{Occipital Alpha Power Indexes Enhanced Excitability Rather}} than {{Improved Visual Perception}}},
  author = {Lange, J. and Oostenveld, R. and Fries, P.},
  year = {2013},
  month = feb,
  volume = {33},
  pages = {3212--3220},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.3755-12.2013},
  file = {2013 - Lange, Oostenveld, Fries - Reduced Occipital Alpha Power Indexes Enhanced Excitability Rather than Improved Visual Perception.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {7}
}

@article{Lansdell2019,
  title = {Spiking Allows Neurons to Estimate Their Causal Effect},
  author = {Lansdell, Benjamin James and Kording, Konrad Paul},
  year = {2019},
  month = feb,
  doi = {10.1101/253351},
  abstract = {Neural plasticity can be seen as ultimately aiming at the maximization of reward. However, the world is complicated and nonlinear and so are neurons' firing properties. A neuron learning to make changes that lead to the maximization of reward is an estimation problem: would there be more reward if the neural activity had been different? Statistically, this is a causal inference problem. Here we show how the spiking discontinuity of neurons can be a tool to estimate the causal influence of a neuron's activity on reward. We show how it can be used to derive a novel learning rule that can operate in the presence of non-linearities and the confounding influence of other neurons. We establish a link between simple learning rules and an existing causal inference method from econometrics.},
  file = {Lansdell and Kording - 2019 - Spiking allows neurons to estimate their causal ef.pdf},
  journal = {bioRxiv},
  language = {en}
}

@article{Latimer2015,
  title = {Single-Trial Spike Trains in Parietal Cortex Reveal Discrete Steps during Decision-Making},
  author = {Latimer, K. W. and Yates, J. L. and Meister, M. L. R. and Huk, A. C. and Pillow, J. W.},
  year = {2015},
  month = jul,
  volume = {349},
  pages = {184--187},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aaa4056},
  abstract = {Neurons in the macaque lateral intraparietal (LIP) area exhibit firing rates that appear to ramp upwards or downwards during decision-making. These ramps are commonly assumed to reflect the gradual accumulation of evidence towards a decision threshold. However, the ramping in trialaveraged responses could instead arise from instantaneous jumps at different times on different trials. We examined single-trial responses in LIP using statistical methods for fitting and comparing latent dynamical spike train models. We compared models with latent spike rates governed by either continuous diffusion-to-bound dynamics or discrete ``stepping'' dynamics. Roughly three-quarters of the choice-selective neurons we recorded were better described by the stepping model. Moreover, the inferred steps carried more information about the animal's choice than spike counts.},
  file = {2015 - Biphenyls - HHS Public Access.pdf;2015 - Latimer et al. - Single-trial spike trains in parietal cortex reveal discrete steps during decision-making.pdf;Latimer et al. - 2015 - Single-trial spike trains in parietal cortex revea 2.pdf;Latimer et al. - 2015 - Single-trial spike trains in parietal cortex revea.pdf},
  journal = {Science},
  language = {en},
  number = {6244}
}

@article{Laversanne-Finot2018,
  title = {Curiosity {{Driven Exploration}} of {{Learned Disentangled Goal Spaces}}},
  author = {{Laversanne-Finot}, Adrien and P{\'e}r{\'e}, Alexandre and Oudeyer, Pierre-Yves},
  year = {2018},
  month = nov,
  abstract = {Intrinsically motivated goal exploration processes enable agents to autonomously sample goals to explore efficiently complex environments with high-dimensional continuous actions. They have been applied successfully to real world robots to discover repertoires of policies producing a wide diversity of effects. Often these algorithms relied on engineered goal spaces but it was recently shown that one can use deep representation learning algorithms to learn an adequate goal space in simple environments. However, in the case of more complex environments containing multiple objects or distractors, an efficient exploration requires that the structure of the goal space reflects the one of the environment. In this paper we show that using a disentangled goal space leads to better exploration performances than an entangled one. We further show that when the representation is disentangled, one can leverage it by sampling goals that maximize learning progress in a modular manner. Finally, we show that the measure of learning progress, used to drive curiosity-driven exploration, can be used simultaneously to discover abstract independently controllable features of the environment. The code used in the experiments is available at https://github.com/flowersteam/ Curiosity\_Driven\_Goal\_Exploration.},
  archiveprefix = {arXiv},
  eprint = {1807.01521},
  eprinttype = {arxiv},
  file = {Laversanne-Finot et al. - 2018 - Curiosity Driven Exploration of Learned Disentangl.pdf},
  journal = {arXiv:1807.01521 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Computer Science - Robotics,Statistics - Machine Learning},
  language = {en},
  primaryclass = {cs, stat}
}

@article{Law2015,
  title = {Data {{Assimilation}}: {{A Mathematical Introduction}}},
  shorttitle = {Data {{Assimilation}}},
  author = {Law, K. J. H. and Stuart, A. M. and Zygalakis, K. C.},
  year = {2015},
  month = jun,
  abstract = {These notes provide a systematic mathematical treatment of the subject of data assimilation.},
  archiveprefix = {arXiv},
  eprint = {1506.07825},
  eprinttype = {arxiv},
  file = {2015 - June - Data Assimilation A Mathematical Introduction.pdf;Law et al. - 2015 - Data Assimilation A Mathematical Introduction.pdf},
  journal = {arXiv:1506.07825 [math, stat]},
  keywords = {Mathematics - Dynamical Systems,Mathematics - Optimization and Control,Statistics - Methodology},
  language = {en},
  primaryclass = {math, stat}
}

@article{Leblois2006,
  title = {Competition between {{Feedback Loops Underlies Normal}} and {{Pathological Dynamics}} in the {{Basal Ganglia}}},
  author = {Leblois, A.},
  year = {2006},
  month = mar,
  volume = {26},
  pages = {3567--3583},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.5050-05.2006},
  file = {2006 - Leblois - Competition between Feedback Loops Underlies Normal and Pathological Dynamics in the Basal Ganglia.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {13}
}

@article{Lebovich2019,
  title = {Idiosyncratic Choice Bias Naturally Emerges from Intrinsic Stochasticity in Neuronal Dynamics},
  author = {Lebovich, Lior and Darshan, Ran and Lavi, Yoni and Hansel, David and Loewenstein, Yonatan},
  year = {2019},
  month = sep,
  issn = {2397-3374},
  doi = {10.1038/s41562-019-0682-7},
  file = {Lebovich et al. - 2019 - Idiosyncratic choice bias naturally emerges from i.pdf},
  journal = {Nat Hum Behav},
  language = {en}
}

@article{Lee2003,
  title = {"{{Gamma}} (40 {{Hz}}) Phase Synchronicity" and Symptom Dimensions in Schizophrenia},
  author = {Lee, Kwang-Hyuk and Williams, Leanne and Haig, Albert and Gordon, Evian},
  year = {2003},
  month = jan,
  volume = {8},
  pages = {57--71},
  issn = {1354-6805, 1464-0619},
  doi = {10.1080/713752240},
  file = {2010 - Lee et al. - Gamma ( 40 Hz ) phase synchronicity and symptom dimensions in schizophrenia.pdf},
  journal = {Cognitive Neuropsychiatry},
  language = {en},
  number = {1}
}

@article{Lee2005,
  title = {Learning and Decision Making in Monkeys during a Rock\textendash Paper\textendash Scissors Game},
  author = {Lee, Daeyeol and McGreevy, Benjamin P. and Barraclough, Dominic J.},
  year = {2005},
  month = oct,
  volume = {25},
  pages = {416--430},
  issn = {09266410},
  doi = {10.1016/j.cogbrainres.2005.07.003},
  abstract = {Game theory provides a solution to the problem of finding a set of optimal decision-making strategies in a group. However, people seldom play such optimal strategies and adjust their strategies based on their experience. Accordingly, many theories postulate a set of variables related to the probabilities of choosing various strategies and describe how such variables are dynamically updated. In reinforcement learning, these value functions are updated based on the outcome of the player's choice, whereas belief learning allows the value functions of all available choices to be updated according to the choices of other players. We investigated the nature of learning process in monkeys playing a competitive game with ternary choices, using a rock \textendash{} paper \textendash{} scissors game. During the baseline condition in which the computer selected its targets randomly, each animal displayed biases towards some targets. When the computer exploited the pattern of animal's choice sequence but not its reward history, the animal's choice was still systematically biased by the previous choice of the computer. This bias was reduced when the computer exploited both the choice and reward histories of the animal. Compared to simple models of reinforcement learning or belief learning, these adaptive processes were better described by a model that incorporated the features of both models. These results suggest that stochastic decision-making strategies in primates during social interactions might be adjusted according to both actual and hypothetical payoffs.},
  file = {2005 - Lee, McGreevy, Barraclough - Learning and decision making in monkeys during a rock-paper-scissors game.pdf},
  journal = {Cognitive Brain Research},
  language = {en},
  number = {2}
}

@article{Lee2011,
  title = {Investigation of Melodic Contour Processing in the Brain Using Multivariate Pattern-Based {{fMRI}}},
  author = {Lee, Yune-Sang and Janata, Petr and Frost, Carlton and Hanke, Michael and Granger, Richard},
  year = {2011},
  month = jul,
  volume = {57},
  pages = {293--300},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2011.02.006},
  abstract = {Music perception generally involves processing the frequency relationships between successive pitches and 34 extraction of the melodic contour. Previous evidence has suggested that the `ups' and `downs' of melodic 35 contour are categorically and automatically processed, but knowledge of the brain regions that discriminate 36 different types of contour is limited. Here, we examined melodic contour discrimination using multivariate 37 pattern analysis (MVPA) of fMRI data. Twelve non-musicians were presented with various ascending and 38 descending melodic sequences while being scanned. Whole-brain MVPA was used to identify regions in 39 which the local pattern of activity accurately discriminated between contour categories. We identified three 40 distinct cortical loci: the right superior temporal sulcus (rSTS), the left inferior parietal lobule (lIPL), and the 41 anterior cingulate cortex (ACC). These results complement previous findings of melodic processing within the 42 rSTS, and extend our understanding of the way in which abstract auditory sequences are categorized by the 43 human brain.},
  file = {2011 - Lee et al. - Investigation of melodic contour processing in the brain using multivariate pattern-based fMRI.pdf},
  journal = {NeuroImage},
  language = {en},
  number = {1}
}

@article{Lee2011a,
  title = {Psychological Models of Human and Optimal Performance in Bandit Problems},
  author = {Lee, Michael D. and Zhang, Shunan and Munro, Miles and Steyvers, Mark},
  year = {2011},
  month = jun,
  volume = {12},
  pages = {164--174},
  issn = {13890417},
  doi = {10.1016/j.cogsys.2010.07.007},
  abstract = {In bandit problems, a decision-maker must choose between a set of alternatives, each of which has a fixed but unknown rate of reward, to maximize their total number of rewards over a sequence of trials. Performing well in these problems requires balancing the need to search for highly-rewarding alternatives, with the need to capitalize on those alternatives already known to be reasonably good. Consistent with this motivation, we develop a new psychological model that relies on switching between latent exploration and exploitation states. We test the model over a range of two-alternative bandit problems, against both human and optimal decision-making data, comparing it to benchmark models from the reinforcement learning literature. By making inferences about the latent states from optimal decision-making behavior, we characterize how people should switch between exploration and exploitation. By making inferences from human data, we begin to characterize how people actually do switch. We discuss the implications of these findings for understanding and measuring the competing demands of exploration and exploitation in sequential decision-making.},
  file = {../../Downloads/Lee et al. - 2011 - Psychological models of human and optimal performa.pdf;2011 - Lee et al. - Psychological models of human and optimal performance in bandit problems.pdf},
  journal = {Cognitive Systems Research},
  language = {en},
  number = {2}
}

@article{Lee2011b,
  title = {Psychological Models of Human and Optimal Performance in Bandit Problems},
  author = {Lee, Michael D. and Zhang, Shunan and Munro, Miles and Steyvers, Mark},
  year = {2011},
  month = jun,
  volume = {12},
  pages = {164--174},
  issn = {13890417},
  doi = {10.1016/j.cogsys.2010.07.007},
  abstract = {In bandit problems, a decision-maker must choose between a set of alternatives, each of which has a fixed but unknown rate of reward, to maximize their total number of rewards over a sequence of trials. Performing well in these problems requires balancing the need to search for highly-rewarding alternatives, with the need to capitalize on those alternatives already known to be reasonably good. Consistent with this motivation, we develop a new psychological model that relies on switching between latent exploration and exploitation states. We test the model over a range of two-alternative bandit problems, against both human and optimal decision-making data, comparing it to benchmark models from the reinforcement learning literature. By making inferences about the latent states from optimal decision-making behavior, we characterize how people should switch between exploration and exploitation. By making inferences from human data, we begin to characterize how people actually do switch. We discuss the implications of these findings for understanding and measuring the competing demands of exploration and exploitation in sequential decision-making.},
  file = {Lee et al. - 2011 - Psychological models of human and optimal performa 2.pdf},
  journal = {Cognitive Systems Research},
  language = {en},
  number = {2}
}

@article{Lee2014,
  title = {Interneuron Subtypes and Orientation Tuning},
  author = {Lee, Seung-Hee and Kwan, Alex C. and Dan, Yang},
  year = {2014},
  month = apr,
  volume = {508},
  pages = {E1-E2},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature13128},
  file = {Lee et al. - 2014 - Interneuron subtypes and orientation tuning.pdf},
  journal = {Nature},
  language = {en},
  number = {7494}
}

@article{Lee2014a,
  title = {Two {{Functionally Distinct Networks}} of {{Gap Junction}}-{{Coupled Inhibitory Neurons}} in the {{Thalamic Reticular Nucleus}}},
  author = {Lee, S.-C. and Patrick, S. L. and Richardson, K. A. and Connors, B. W.},
  year = {2014},
  month = sep,
  volume = {34},
  pages = {13170--13182},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.0562-14.2014},
  file = {Lee et al. - 2014 - Two Functionally Distinct Networks of Gap Junction.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {39}
}

@article{Lee2014b,
  title = {Astrocytes Contribute to Gamma Oscillations and Recognition Memory},
  author = {Lee, H. S. and Ghetti, A. and {Pinto-Duarte}, A. and Wang, X. and Dziewczapolski, G. and Galimi, F. and {Huitron-Resendiz}, S. and {Pina-Crespo}, J. C. and Roberts, A. J. and Verma, I. M. and Sejnowski, T. J. and Heinemann, S. F.},
  year = {2014},
  month = aug,
  volume = {111},
  pages = {E3343-E3352},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1410893111},
  file = {Lee et al. - 2014 - Astrocytes contribute to gamma oscillations and re.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {32}
}

@article{Lee2020,
  title = {Anatomically Segregated Basal Ganglia Pathways Allow Parallel Behavioral Modulation},
  author = {Lee, Jaeeon and Wang, Wengang and Sabatini, Bernardo L.},
  year = {2020},
  month = sep,
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/s41593-020-00712-5},
  file = {Lee et al. - 2020 - Anatomically segregated basal ganglia pathways all.pdf},
  journal = {Nat Neurosci},
  language = {en}
}

@article{Lee2020a,
  title = {Anatomically Segregated Basal Ganglia Pathways Allow Parallel Behavioral Modulation},
  author = {Lee, Jaeeon and Wang, Wengang and Sabatini, Bernardo L.},
  year = {2020},
  month = nov,
  volume = {23},
  pages = {1388--1398},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/s41593-020-00712-5},
  file = {Lee et al. - 2020 - Anatomically segregated basal ganglia pathways all 2.pdf},
  journal = {Nat Neurosci},
  language = {en},
  number = {11}
}

@article{Lefebvre1997,
  title = {Feeding Innovations and Forebrain Size in Birds},
  author = {Lefebvre, Louis and Whittle, Patrick and Lascaris, Evan and Finkelstein, Adam},
  year = {1997},
  month = mar,
  volume = {53},
  pages = {549--560},
  issn = {00033472},
  doi = {10.1006/anbe.1996.0330},
  abstract = {The links between ecology, behavioural plasticity and brain size are often tested via the comparative method. Given the problems in interpretating comparative tests of learning and cognition, however, alternative measures of plasticity need to be developed. From the short notes section of nine ornithological journals, two separate, exhaustive data sets have been collated on opportunistic foraging innovations in birds of North America (1973\textendash 1993; N=196) and the British Isles (1983\textendash 1993; N=126). Both the absolute and relative frequencies (corrected for species number per order) of innovations differ between bird orders in a similar fashion in the two geographical zones. Absolute and relative frequency of innovations per order are also related to two measures of relative forebrain size in the two zones. The study confirms predicted trends linking opportunism, brain size and rate of structural evolution. It also suggests that innovation rate in the field may be a useful measure of behavioural plasticity.},
  file = {Lefebvre et al. - 1997 - Feeding innovations and forebrain size in birds.pdf},
  journal = {Animal Behaviour},
  language = {en},
  number = {3}
}

@article{Lefebvre1997a,
  title = {Feeding Innovations and Forebrain Size in Birds},
  author = {Lefebvre, Louis and Whittle, Patrick and Lascaris, Evan and Finkelstein, Adam},
  year = {1997},
  month = mar,
  volume = {53},
  pages = {549--560},
  issn = {00033472},
  doi = {10.1006/anbe.1996.0330},
  abstract = {The links between ecology, behavioural plasticity and brain size are often tested via the comparative method. Given the problems in interpretating comparative tests of learning and cognition, however, alternative measures of plasticity need to be developed. From the short notes section of nine ornithological journals, two separate, exhaustive data sets have been collated on opportunistic foraging innovations in birds of North America (1973\textendash 1993; N=196) and the British Isles (1983\textendash 1993; N=126). Both the absolute and relative frequencies (corrected for species number per order) of innovations differ between bird orders in a similar fashion in the two geographical zones. Absolute and relative frequency of innovations per order are also related to two measures of relative forebrain size in the two zones. The study confirms predicted trends linking opportunism, brain size and rate of structural evolution. It also suggests that innovation rate in the field may be a useful measure of behavioural plasticity.},
  file = {Lefebvre et al. - 1997 - Feeding innovations and forebrain size in birds 2.pdf},
  journal = {Animal Behaviour},
  language = {en},
  number = {3}
}

@article{Lefebvre1997b,
  title = {Feeding Innovations and Forebrain Size in Birds},
  author = {Lefebvre, Louis and Whittle, Patrick and Lascaris, Evan and Finkelstein, Adam},
  year = {1997},
  month = mar,
  volume = {53},
  pages = {549--560},
  issn = {00033472},
  doi = {10.1006/anbe.1996.0330},
  abstract = {The links between ecology, behavioural plasticity and brain size are often tested via the comparative method. Given the problems in interpretating comparative tests of learning and cognition, however, alternative measures of plasticity need to be developed. From the short notes section of nine ornithological journals, two separate, exhaustive data sets have been collated on opportunistic foraging innovations in birds of North America (1973\textendash 1993; N=196) and the British Isles (1983\textendash 1993; N=126). Both the absolute and relative frequencies (corrected for species number per order) of innovations differ between bird orders in a similar fashion in the two geographical zones. Absolute and relative frequency of innovations per order are also related to two measures of relative forebrain size in the two zones. The study confirms predicted trends linking opportunism, brain size and rate of structural evolution. It also suggests that innovation rate in the field may be a useful measure of behavioural plasticity.},
  file = {Lefebvre et al. - 1997 - Feeding innovations and forebrain size in birds 3.pdf},
  journal = {Animal Behaviour},
  language = {en},
  number = {3}
}

@techreport{Lefebvre2020,
  title = {A Normative Account of Confirmatory Biases during Reinforcement Learning},
  author = {Lefebvre, Germain and Summerfield, Christopher and Bogacz, Rafal},
  year = {2020},
  month = may,
  institution = {{Animal Behavior and Cognition}},
  doi = {10.1101/2020.05.12.090134},
  abstract = {Abstract           Reinforcement learning involves updating estimates of the value of states and actions on the basis of experience. Previous work has shown that in humans, reinforcement learning exhibits a confirmatory bias: when updating the value of a chosen option, estimates are revised more radically following positive than negative reward prediction errors, but the converse is observed when updating the unchosen option value estimate. Here, we simulate performance on a multi-arm bandit task to examine the consequences of a confirmatory bias for reward harvesting. We report a paradoxical finding: that confirmatory biases allow the agent to maximise reward relative to an unbiased updating rule. This principle holds over a wide range of experimental settings and is most influential when decisions are corrupted by noise. We show that this occurs because on average, confirmatory biases overestimate the value of more valuable bandits, and underestimate the value of less valuable bandits, rendering decisions overall more robust in the face of noise. Our results show how apparently suboptimal learning policies can in fact be reward-maximising if decisions are made with finite computational precision.},
  file = {Lefebvre et al. - 2020 - A normative account of confirmatory biases during .pdf},
  language = {en},
  type = {Preprint}
}

@article{Legon2014,
  title = {Transcranial Focused Ultrasound Modulates the Activity of Primary Somatosensory Cortex in Humans},
  author = {Legon, Wynn and Sato, Tomokazu F and Opitz, Alexander and Mueller, Jerel and Barbour, Aaron and Williams, Amanda and Tyler, William J},
  year = {2014},
  month = feb,
  volume = {17},
  pages = {322--329},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.3620},
  file = {2014 - Legon et al. - Transcranial focused ultrasound modulates the activity of primary somatosensory cortex in humans.pdf;Legon et al. - 2014 - Transcranial focused ultrasound modulates the acti.pdf},
  journal = {Nature Neuroscience},
  language = {en},
  number = {2}
}

@article{Legon2016,
  title = {Altered {{Prefrontal Excitation}}/{{Inhibition Balance}} and {{Prefrontal Output}}: {{Markers}} of {{Aging}} in {{Human Memory Networks}}},
  shorttitle = {Altered {{Prefrontal Excitation}}/{{Inhibition Balance}} and {{Prefrontal Output}}},
  author = {Legon, Wynn and Punzell, Steven and Dowlati, Ehsan and Adams, Sarah E. and Stiles, Alexandra B. and Moran, Rosalyn J.},
  year = {2016},
  month = oct,
  volume = {26},
  pages = {4315--4326},
  issn = {1047-3211, 1460-2199},
  doi = {10.1093/cercor/bhv200},
  abstract = {Memory impairments and heightened prefrontal cortical (PFC) activity are hallmarks of cognitive and neurobiological human aging. While structural integrity of PFC gray matter and interregional white matter tracts are thought to impact memory processing, the balance of neurotransmitters within the PFC itself is less well understood. We used fMRI to establish wholebrain networks involved in a memory encoding task and dynamic causal models (DCMs) for fMRI to determine the causal relationships between these areas. These data revealed enhanced connectivity from PFC to medial temporal cortex that negatively correlated with recall ability. To better understand the intrinsic activity within the PFC, DCM for EEG was employed after continuous theta burst transcranial magnetic stimulation (TMS) to the PFC to assess the effect on excitatory/inhibitory (E/I) synaptic ratios and behavior. These data revealed that the young cohort had a stable E/I ratio that was unaffected by the TMS intervention, while the aged cohort exhibited lower E/I ratios driven by a greater intrinsic inhibitory tone. TMS to the aged cohort resulted in decreased intrinsic inhibition and a decrement in memory performance. These results demonstrate increased topdown influence of PFC upon medial temporal lobe in healthy aging that is associated with decreased memory and may be due to unstable local inhibitory tone within the PFC.},
  file = {2015 - Legon et al. - Altered Prefrontal ExcitationInhibition Balance and Prefrontal Output Markers of Aging in Human Memory Networks.pdf;Legon et al. - 2016 - Altered Prefrontal ExcitationInhibition Balance a.pdf},
  journal = {Cerebral Cortex},
  language = {en},
  number = {11}
}

@article{Lehman2008,
  title = {Exploiting {{Open}}-{{Endedness}} to {{Solve Problems Through}} the {{Search}} for {{Novelty}}},
  author = {Lehman, Joel and Stanley, Kenneth O},
  year = {2008},
  pages = {8},
  abstract = {This paper establishes a link between the challenge of solving highly ambitious problems in machine learning and the goal of reproducing the dynamics of open-ended evolution in artificial life. A major problem with the objective function in machine learning is that through deception it may actually prevent the objective from being reached. In a similar way, selection in evolution may sometimes act to discourage increasing complexity. This paper proposes a single idea that both overcomes the obstacle of deception and suggests a simple new approach to open-ended evolution: Instead of either explicitly seeking an objective or modeling a domain to capture the open-endedness of natural evolution, the idea is to simply search for novelty. Even in an objective-based problem, such novelty search ignores the objective and searches for behavioral novelty. Yet because many points in the search space collapse to the same point in behavior space, it turns out that the search for novelty is computationally feasible. Furthermore, because there are only so many simple behaviors, the search for novelty leads to increasing complexity. In fact, on the way up the ladder of complexity, the search is likely to encounter at least one solution. In this way, by decoupling the idea of open-ended search from only artificial life worlds, the raw search for novelty can be applied to real world problems. Counterintuitively, in the deceptive maze navigation task in this paper, novelty search significantly outperforms objective-based search, suggesting a surprising new approach to machine learning.},
  file = {Lehman and Stanley - 2008 - Exploiting Open-Endedness to Solve Problems Throug.pdf},
  language = {en}
}

@inproceedings{Lehman2010,
  title = {Efficiently Evolving Programs through the Search for Novelty},
  booktitle = {Proceedings of the 12th Annual Conference on {{Genetic}} and Evolutionary Computation - {{GECCO}} '10},
  author = {Lehman, Joel and Stanley, Kenneth O.},
  year = {2010},
  pages = {837},
  publisher = {{ACM Press}},
  address = {{Portland, Oregon, USA}},
  doi = {10.1145/1830483.1830638},
  abstract = {A significant challenge in genetic programming is premature convergence to local optima, which often prevents evolution from solving problems. This paper introduces to genetic programming a method that originated in neuroevolution (i.e. the evolution of artificial neural networks) that circumvents the problem of deceptive local optima. The main idea is to search only for behavioral novelty instead of for higher fitness values. Although such novelty search abandons following the gradient of the fitness function, if such gradients are deceptive they may actually occlude paths through the search space towards the objective. Because there are only so many ways to behave, the search for behavioral novelty is often computationally feasible and differs significantly from random search. Counterintuitively, in both a deceptive maze navigation task and the artificial ant benchmark task, genetic programming with novelty search, which ignores the objective, outperforms traditional genetic programming that directly searches for optimal behavior. Additionally, novelty search evolves smaller program trees in every variation of the test domains. Novelty search thus appears less susceptible to bloat, another significant problem in genetic programming. The conclusion is that novelty search is a viable new tool for efficiently solving some deceptive problems in genetic programming.},
  file = {Lehman and Stanley - 2010 - Efficiently evolving programs through the search f.pdf},
  isbn = {978-1-4503-0072-8},
  language = {en}
}

@article{Lehman2011,
  title = {Abandoning {{Objectives}}: {{Evolution Through}} the {{Search}} for {{Novelty Alone}}},
  shorttitle = {Abandoning {{Objectives}}},
  author = {Lehman, Joel and Stanley, Kenneth O.},
  year = {2011},
  month = jun,
  volume = {19},
  pages = {189--223},
  issn = {1063-6560, 1530-9304},
  doi = {10.1162/EVCO_a_00025},
  abstract = {In evolutionary computation, the fitness function normally measures progress towards an objective in the search space, effectively acting as an objective function. Through deception, such objective functions may actually prevent the objective from being reached. While methods exist to mitigate deception, they leave the underlying pathology untreated: Objective functions themselves may actively misdirect search towards dead ends. This paper proposes an approach to circumventing deception that also yields a new perspective on open-ended evolution: Instead of either explicitly seeking an objective or modeling natural evolution to capture open-endedness, the idea is to simply search for behavioral novelty. Even in an objective-based problem, such novelty search ignores the objective. Because many points in the search space collapse to a single behavior, the search for novelty is often feasible. Furthermore, because there are only so many simple behaviors, the search for novelty leads to increasing complexity. By decoupling open-ended search from artificial life worlds, the search for novelty is applicable to real world problems. Counterintuitively, in the maze navigation and biped walking tasks in this paper, novelty search significantly outperforms objective-based search, suggesting the strange conclusion that some problems are best solved by methods that ignore the objective. The main lesson is the inherent limitation of the objective-based paradigm and the unexploited opportunity to guide search through other means.},
  file = {Lehman and Stanley - 2011 - Abandoning Objectives Evolution Through the Searc.pdf},
  journal = {Evolutionary Computation},
  language = {en},
  number = {2}
}

@incollection{Lehman2011a,
  title = {Novelty {{Search}} and the {{Problem}} with {{Objectives}}},
  booktitle = {Genetic {{Programming Theory}} and {{Practice IX}}},
  author = {Lehman, Joel and Stanley, Kenneth O.},
  editor = {Riolo, Rick and Vladislavleva, Ekaterina and Moore, Jason H.},
  year = {2011},
  pages = {37--56},
  publisher = {{Springer New York}},
  address = {{New York, NY}},
  doi = {10.1007/978-1-4614-1770-5_3},
  abstract = {By synthesizing a growing body of work in search processes that are not driven by explicit objectives, this paper advances the hypothesis that there is a fundamental problem with the dominant paradigm of objective-based search in evolutionary computation and genetic programming: Most ambitious objectives do not illuminate a path to themselves. That is, the gradient of improvement induced by ambitious objectives tends to lead not to the objective itself but instead to deadend local optima. Indirectly supporting this hypothesis, great discoveries often are not the result of objective-driven search. For example, the major inspiration for both evolutionary computation and genetic programming, natural evolution, innovates through an open-ended process that lacks a final objective. Similarly, large-scale cultural evolutionary processes, such as the evolution of technology, mathematics, and art, lack a unified fixed goal. In addition, direct evidence for this hypothesis is presented from a recently-introduced search algorithm called novelty search. Though ignorant of the ultimate objective of search, in many instances novelty search has counter-intuitively outperformed searching directly for the objective, including a wide variety of randomly-generated problems introduced in an experiment in this chapter. Thus a new understanding is beginning to emerge that suggests that searching for a fixed objective, which is the reigning paradigm in evolutionary computation and even machine learning as a whole, may ultimately limit what can be achieved. Yet the liberating implication of this hypothesis argued in this paper is that by embracing search processes that are not driven by explicit objectives, the breadth and depth of what is reachable through evolutionary methods such as genetic programming may be greatly expanded.},
  file = {Lehman and Stanley - 2011 - Novelty Search and the Problem with Objectives.pdf},
  isbn = {978-1-4614-1769-9 978-1-4614-1770-5},
  language = {en}
}

@inproceedings{Lehman2013,
  title = {Effective Diversity Maintenance in Deceptive Domains},
  booktitle = {Proceeding of the Fifteenth Annual Conference on {{Genetic}} and Evolutionary Computation Conference - {{GECCO}} '13},
  author = {Lehman, Joel and Stanley, Kenneth O. and Miikkulainen, Risto},
  year = {2013},
  pages = {215},
  publisher = {{ACM Press}},
  address = {{Amsterdam, The Netherlands}},
  doi = {10.1145/2463372.2463393},
  abstract = {Diversity maintenance techniques in evolutionary computation are designed to mitigate the problem of deceptive local optima by encouraging exploration. However, as problems become more difficult, the heuristic of fitness may become increasingly uninformative. Thus, simply encouraging genotypic diversity may fail to much increase the likelihood of evolving a solution. In such cases, diversity needs to be directed towards potentially useful structures. A representative example of such a search process is novelty search, which builds diversity by rewarding behavioral novelty. In this paper the effectiveness of fitness, novelty, and diversity maintenance objectives are compared in two evolutionary robotics domains. In a biped locomotion domain, genotypic diversity maintenance helps evolve biped control policies that travel farther before falling. However, the best method is to optimize a fitness objective and a behavioral novelty objective together. In the more deceptive maze navigation domain, diversity maintenance is ineffective while a novelty objective still increases performance. The conclusion is that while genotypic diversity maintenance works in well-posed domains, a method more directed by phenotypic information, like novelty search, is necessary for highly deceptive ones.},
  file = {Lehman et al. - 2013 - Effective diversity maintenance in deceptive domai.pdf},
  isbn = {978-1-4503-1963-8},
  language = {en}
}

@inproceedings{Lehman2015,
  title = {Enhancing {{Divergent Search}} through {{Extinction Events}}},
  booktitle = {Proceedings of the 2015 on {{Genetic}} and {{Evolutionary Computation Conference}} - {{GECCO}} '15},
  author = {Lehman, Joel and Miikkulainen, Risto},
  year = {2015},
  pages = {951--958},
  publisher = {{ACM Press}},
  address = {{Madrid, Spain}},
  doi = {10.1145/2739480.2754668},
  abstract = {A challenge in evolutionary computation is to create representations as evolvable as those in natural evolution. This paper hypothesizes that extinction events, i.e. mass extinctions, can significantly increase evolvability, but only when combined with a divergent search algorithm, i.e. a search driven towards diversity (instead of optimality). Extinctions amplify diversity-generation by creating unpredictable evolutionary bottlenecks. Persisting through multiple such bottlenecks is more likely for lineages that diversify across many niches, resulting in indirect selection pressure for the capacity to evolve. This hypothesis is tested through experiments in two evolutionary robotics domains. The results show that combining extinction events with divergent search increases evolvability, while combining them with convergent search offers no similar benefit. The conclusion is that extinction events may provide a simple and effective mechanism to enhance performance of divergent search algorithms.},
  file = {Lehman and Miikkulainen - 2015 - Enhancing Divergent Search through Extinction Even.pdf},
  isbn = {978-1-4503-3472-3},
  language = {en}
}

@inproceedings{Lehman2018,
  title = {Safe Mutations for Deep and Recurrent Neural Networks through Output Gradients},
  booktitle = {Proceedings of the {{Genetic}} and {{Evolutionary Computation Conference}} on   - {{GECCO}} '18},
  author = {Lehman, Joel and Chen, Jay and Clune, Jeff and Stanley, Kenneth O.},
  year = {2018},
  pages = {117--124},
  publisher = {{ACM Press}},
  address = {{Kyoto, Japan}},
  doi = {10.1145/3205455.3205473},
  abstract = {While neuroevolution (evolving neural networks) has a successful track record across a variety of domains from reinforcement learning to artificial life, it is rarely applied to large, deep neural networks. A central reason is that while random mutation generally works in low dimensions, a random perturbation of thousands or millions of weights is likely to break existing functionality, providing no learning signal even if some individual weight changes were beneficial. This paper proposes a solution by introducing a family of safe mutation (SM) operators that aim within the mutation operator itself to find a degree of change that does not alter network behavior too much, but still facilitates exploration. Importantly, these SM operators do not require any additional interactions with the environment. The most effective SM variant capitalizes on the intriguing opportunity to scale the degree of mutation of each individual weight according to the sensitivity of the network's outputs to that weight, which requires computing the gradient of outputs with respect to the weights (instead of the gradient of error, as in conventional deep learning). This safe mutation through gradients (SM-G) operator dramatically increases the ability of a simple genetic algorithm-based neuroevolution method to find solutions in high-dimensional domains that require deep and/or recurrent neural networks (which tend to be particularly brittle to mutation), including domains that require processing raw pixels. By improving our ability to evolve deep neural networks, this new safer approach to mutation expands the scope of domains amenable to neuroevolution.},
  file = {Lehman et al. - 2018 - Safe mutations for deep and recurrent neural netwo.pdf},
  isbn = {978-1-4503-5618-3},
  language = {en}
}

@article{Lehman2018a,
  title = {The {{Surprising Creativity}} of {{Digital Evolution}}: {{A Collection}} of {{Anecdotes}} from the {{Evolutionary Computation}} and {{Artificial Life Research Communities}}},
  shorttitle = {The {{Surprising Creativity}} of {{Digital Evolution}}},
  author = {Lehman, Joel and Clune, Jeff and Misevic, Dusan and Adami, Christoph and Altenberg, Lee and Beaulieu, Julie and Bentley, Peter J. and Bernard, Samuel and Beslon, Guillaume and Bryson, David M. and Chrabaszcz, Patryk and Cheney, Nick and Cully, Antoine and Doncieux, Stephane and Dyer, Fred C. and Ellefsen, Kai Olav and Feldt, Robert and Fischer, Stephan and Forrest, Stephanie and Fr{\'e}noy, Antoine and Gagn{\'e}, Christian and Goff, Leni Le and Grabowski, Laura M. and Hodjat, Babak and Hutter, Frank and Keller, Laurent and Knibbe, Carole and Krcah, Peter and Lenski, Richard E. and Lipson, Hod and MacCurdy, Robert and Maestre, Carlos and Miikkulainen, Risto and Mitri, Sara and Moriarty, David E. and Mouret, Jean-Baptiste and Nguyen, Anh and Ofria, Charles and Parizeau, Marc and Parsons, David and Pennock, Robert T. and Punch, William F. and Ray, Thomas S. and Schoenauer, Marc and Shulte, Eric and Sims, Karl and Stanley, Kenneth O. and Taddei, Fran{\c c}ois and Tarapore, Danesh and Thibault, Simon and Weimer, Westley and Watson, Richard and Yosinski, Jason},
  year = {2018},
  month = mar,
  abstract = {Evolution provides a creative fount of complex and subtle adaptations that often surprise the scientists who discover them. However, the creativity of evolution is not limited to the natural world: artificial organisms evolving in computational environments have also elicited surprise and wonder from the researchers studying them. The process of evolution is an algorithmic process that transcends the substrate in which it occurs. Indeed, many researchers in the field of digital evolution can provide examples of how their evolving algorithms and organisms have creatively subverted their expectations or intentions, exposed unrecognized bugs in their code, produced unexpectedly adaptations, or engaged in behaviors and outcomes uncannily convergent with ones found in nature. Such stories routinely reveal surprise and creativity by evolution in these digital worlds, but they rarely fit into the standard scientific narrative. Instead they are often treated as mere obstacles to be overcome, rather than results that warrant study in their own right. Bugs are fixed, experiments are refocused, and one-off surprises are collapsed into a single data point. The stories themselves are traded among researchers through oral tradition, but that mode of information transmission is inefficient and prone to error and outright loss. Moreover, the fact that these stories tend to be shared only among practitioners means that many natural scientists do not realize how interesting and lifelike digital organisms are and how natural their evolution can be. To our knowledge, no collection of such anecdotes has been published before. This paper is the crowd-sourced product of researchers in the fields of artificial life and evolutionary computation who have provided first-hand accounts of such cases. It thus serves as a written, fact-checked collection of scientifically important and even entertaining stories. In doing so we also present here substantial evidence that the existence and importance of evolutionary surprises extends beyond the natural world, and may indeed be a universal property of all complex evolving systems.},
  archiveprefix = {arXiv},
  eprint = {1803.03453},
  eprinttype = {arxiv},
  file = {Lehman et al. - 2018 - The Surprising Creativity of Digital Evolution A .pdf},
  journal = {arXiv:1803.03453 [cs]},
  keywords = {Computer Science - Neural and Evolutionary Computing},
  language = {en},
  primaryclass = {cs}
}

@article{Lehman2018b,
  title = {{{ES Is More Than Just}} a {{Traditional Finite}}-{{Difference Approximator}}},
  author = {Lehman, Joel and Chen, Jay and Clune, Jeff and Stanley, Kenneth O.},
  year = {2018},
  month = may,
  abstract = {An evolution strategy (ES) variant based on a simplification of a natural evolution strategy recently attracted attention because it performs surprisingly well in challenging deep reinforcement learning domains. It searches for neural network parameters by generating perturbations to the current set of parameters, checking their performance, and moving in the aggregate direction of higher reward. Because it resembles a traditional finite-difference approximation of the reward gradient, it can naturally be confused with one. However, this ES optimizes for a different gradient than just reward: It optimizes for the average reward of the entire population, thereby seeking parameters that are robust to perturbation. This difference can channel ES into distinct areas of the search space relative to gradient descent, and also consequently to networks with distinct properties. This unique robustness-seeking property, and its consequences for optimization, are demonstrated in several domains. They include humanoid locomotion, where networks from policy gradient-based reinforcement learning are significantly less robust to parameter perturbation than ES-based policies solving the same task. While the implications of such robustness and robustness-seeking remain open to further study, this work's main contribution is to highlight such differences and their potential importance.},
  archiveprefix = {arXiv},
  eprint = {1712.06568},
  eprinttype = {arxiv},
  file = {Lehman et al. - 2018 - ES Is More Than Just a Traditional Finite-Differen.pdf},
  journal = {arXiv:1712.06568 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Neural and Evolutionary Computing},
  language = {en},
  primaryclass = {cs}
}

@article{Lehnertz2009,
  title = {Synchronization Phenomena in Human Epileptic Brain Networks},
  author = {Lehnertz, Klaus and Bialonski, Stephan and Horstmann, Marie-Therese and Krug, Dieter and Rothkegel, Alexander and Staniek, Matth{\"a}us and Wagner, Tobias},
  year = {2009},
  month = sep,
  volume = {183},
  pages = {42--48},
  issn = {01650270},
  doi = {10.1016/j.jneumeth.2009.05.015},
  abstract = {Epilepsy is a malfunction of the brain that affects over 50 million people worldwide. Epileptic seizures are usually characterized by an abnormal synchronized firing of neurons involved in the epileptic process. In human epilepsy the exact mechanisms underlying seizure generation are still uncertain as are mechanisms underlying seizure spreading and termination. There is now growing evidence that an improved understanding of the epileptic process can be achieved through the analysis of properties of epileptic brain networks and through the analysis of interactions in such networks. In this overview, we summarize recent methodological developments to assess synchronization phenomena in human epileptic brain networks and present findings obtained from analyses of brain electromagnetic signals recorded in epilepsy patients.},
  file = {2009 - Lehnertz et al. - Synchronization phenomena in human epileptic brain networks.pdf},
  journal = {Journal of Neuroscience Methods},
  language = {en},
  number = {1}
}

@article{Lehtinen2018,
  title = {{{Noise2Noise}}: {{Learning Image Restoration}} without {{Clean Data}}},
  shorttitle = {{{Noise2Noise}}},
  author = {Lehtinen, Jaakko and Munkberg, Jacob and Hasselgren, Jon and Laine, Samuli and Karras, Tero and Aittala, Miika and Aila, Timo},
  year = {2018},
  month = oct,
  abstract = {We apply basic statistical reasoning to signal reconstruction by machine learning \textendash{} learning to map corrupted observations to clean signals \textendash{} with a simple and powerful conclusion: it is possible to learn to restore images by only looking at corrupted examples, at performance at and sometimes exceeding training using clean data, without explicit image priors or likelihood models of the corruption. In practice, we show that a single model learns photographic noise removal, denoising synthetic Monte Carlo images, and reconstruction of undersampled MRI scans \textendash{} all corrupted by different processes \textendash{} based on noisy data only.},
  archiveprefix = {arXiv},
  eprint = {1803.04189},
  eprinttype = {arxiv},
  file = {Lehtinen et al. - 2018 - Noise2Noise Learning Image Restoration without Cl.pdf},
  journal = {arXiv:1803.04189 [cs, stat]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryclass = {cs, stat}
}

@article{Leibo,
  title = {Psychlab: {{A Psychology Laboratory}} for {{Deep Reinforcement Learning Agents}}},
  author = {Leibo, Joel Z and Beattie, Charles and Anderson, Keith and Casta{\~n}eda, Antonio Garc{\'i}a and Sanchez, Manuel and Green, Simon and Gruslys, Audrunas and Legg, Shane and Hassabis, Demis and Botvinick, Matthew M},
  pages = {28},
  abstract = {Psychlab is a simulated psychology laboratory inside the first-person 3D game world of DeepMind Lab (Beattie et al., 2016). Psychlab enables implementations of classical laboratory psychological experiments so that they work with both human and artificial agents. Psychlab has a simple and flexible API that enables users to easily create their own tasks. As examples, we are releasing Psychlab implementations of several classical experimental paradigms including visual search, change detection, random dot motion discrimination, and multiple object tracking. We also contribute a study of the visual psychophysics of a specific state-of-the-art deep reinforcement learning agent: UNREAL (Jaderberg et al., 2016). This study leads to the surprising conclusion that UNREAL learns more quickly about larger target stimuli than it does about smaller stimuli. In turn, this insight motivates a specific improvement in the form of a simple model of foveal vision that turns out to significantly boost UNREAL's performance, both on Psychlab tasks, and on standard DeepMind Lab tasks. By open-sourcing Psychlab we hope to facilitate a range of future such studies that simultaneously advance deep reinforcement learning and improve its links with cognitive science.},
  file = {Leibo et al. - Psychlab A Psychology Laboratory for Deep Reinfor.pdf},
  language = {en}
}

@article{Leibold2008,
  title = {Temporal Compression Mediated by Short-Term Synaptic Plasticity},
  author = {Leibold, C. and Gundlfinger, A. and Schmidt, R. and Thurley, K. and Schmitz, D. and Kempter, R.},
  year = {2008},
  month = mar,
  volume = {105},
  pages = {4417--4422},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.0708711105},
  file = {2008 - Leibold et al. - Temporal compression mediated by short-term synaptic plasticity.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {11}
}

@article{Leike2017,
  title = {{{AI Safety Gridworlds}}},
  author = {Leike, Jan and Martic, Miljan and Krakovna, Victoria and Ortega, Pedro A. and Everitt, Tom and Lefrancq, Andrew and Orseau, Laurent and Legg, Shane},
  year = {2017},
  month = nov,
  abstract = {We present a suite of reinforcement learning environments illustrating various safety properties of intelligent agents. These problems include safe interruptibility, avoiding side effects, absent supervisor, reward gaming, safe exploration, as well as robustness to self-modification, distributional shift, and adversaries. To measure compliance with the intended safe behavior, we equip each environment with a performance function that is hidden from the agent. This allows us to categorize AI safety problems into robustness and specification problems, depending on whether the performance function corresponds to the observed reward function. We evaluate A2C and Rainbow, two recent deep reinforcement learning agents, on our environments and show that they are not able to solve them satisfactorily.},
  archiveprefix = {arXiv},
  eprint = {1711.09883},
  eprinttype = {arxiv},
  file = {Leike et al. - 2017 - AI Safety Gridworlds.pdf},
  journal = {arXiv:1711.09883 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  language = {en},
  primaryclass = {cs}
}

@article{LeMasson1993,
  title = {Activity-Dependent Regulation of Conductances in Model Neurons},
  author = {LeMasson, G and Marder, E and Abbott, L.},
  year = {1993},
  month = mar,
  volume = {259},
  pages = {1915--1917},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.8456317},
  file = {1993 - LeMasson, Marder, Abbott - Activity-dependent regulation of conductances in model neurons.pdf},
  journal = {Science},
  language = {en},
  number = {5103}
}

@article{Lempitsky,
  title = {Andrea {{Vedaldi University}} of {{Oxford}}},
  author = {Lempitsky, Victor},
  pages = {10},
  abstract = {Deep convolutional networks have become a popular tool for image generation and restoration. Generally, their excellent performance is imputed to their ability to learn realistic image priors from a large number of example images. In this paper, we show that, on the contrary, the structure of a generator network is sufficient to capture a great deal of low-level image statistics prior to any learning. In order to do so, we show that a randomly-initialized neural network can be used as a handcrafted prior with excellent results in standard inverse problems such as denoising, superresolution, and inpainting. Furthermore, the same prior can be used to invert deep neural representations to diagnose them, and to restore images based on flash-no flash input pairs.},
  file = {Lempitsky - Andrea Vedaldi University of Oxford.pdf},
  language = {en}
}

@article{Leshno1993,
  title = {Multilayer Feedforward Networks with a Nonpolynomial Activation Function Can Approximate Any Function},
  author = {Leshno, Moshe and Lin, Vladimir Ya. and Pinkus, Allan and Schocken, Shimon},
  year = {1993},
  month = jan,
  volume = {6},
  pages = {861--867},
  issn = {08936080},
  doi = {10.1016/S0893-6080(05)80131-5},
  abstract = {Several researchers characterized the activation fimction under which multilayer feedforward networks can act as universal approximators. We show that most of all the characterizations that were reported thus far in the literature are special cases of the following general result: A standard multilayer feedforward network with a locally bounded piecewise continuous activation fimction can approximate an3, continuous function to any degree of accuracy if and only if the network's activation function is not a polynomial. We also emphasize the important role o f the threshold, asserting that without it the last theorem does not hold.},
  file = {Leshno et al. - 1993 - Multilayer feedforward networks with a nonpolynomi.pdf},
  journal = {Neural Networks},
  language = {en},
  number = {6}
}

@article{Leshno1993a,
  title = {Multilayer Feedforward Networks with a Nonpolynomial Activation Function Can Approximate Any Function},
  author = {Leshno, Moshe and Lin, Vladimir Ya. and Pinkus, Allan and Schocken, Shimon},
  year = {1993},
  month = jan,
  volume = {6},
  pages = {861--867},
  issn = {08936080},
  doi = {10.1016/S0893-6080(05)80131-5},
  abstract = {Several researchers characterized the activation fimction under which multilayer feedforward networks can act as universal approximators. We show that most of all the characterizations that were reported thus far in the literature are special cases of the following general result: A standard multilayer feedforward network with a locally bounded piecewise continuous activation fimction can approximate an3, continuous function to any degree of accuracy if and only if the network's activation function is not a polynomial. We also emphasize the important role o f the threshold, asserting that without it the last theorem does not hold.},
  file = {Leshno et al. - 1993 - Multilayer feedforward networks with a nonpolynomi 2.pdf},
  journal = {Neural Networks},
  language = {en},
  number = {6}
}

@article{Levesque2005,
  title = {{{GABAergic}} Interneurons in Human Subthalamic Nucleus},
  author = {L{\'e}vesque, Julie-Christine and Parent, Andr{\'e}},
  year = {2005},
  month = may,
  volume = {20},
  pages = {574--584},
  issn = {0885-3185, 1531-8257},
  doi = {10.1002/mds.20374},
  file = {2005 - Lévesque, André - GABAergic interneurons in human subthalamic nucleus.pdf},
  journal = {Movement Disorders},
  language = {en},
  number = {5}
}

@techreport{Levi-Aharoni2019,
  title = {Surprise Response as a Probe for Compressed Memory States},
  author = {{Levi-Aharoni}, Hadar and Shriki, Oren and Tishby, Naftali},
  year = {2019},
  month = may,
  institution = {{Neuroscience}},
  doi = {10.1101/627133},
  abstract = {The limited capacity of recent memory inevitably leads to partial memory of past stimuli. There is also evidence that behavioral and neural responses to novel or rare stimuli are dependent on one's memory of past stimuli. Thus, these responses may serve as a probe of different individuals' remembering and forgetting characteristics. Here, we utilize two lossy compression models of stimulus sequences that inherently involve forgetting, which in addition to being a necessity under many conditions, also has theoretical and behavioral advantages. One model is based on a simple stimulus counter and the other on the Information Bottleneck (IB) framework. These models are applied to analyze a novelty-detection event-related potential commonly known as the P300.},
  file = {Levi-Aharoni et al. - 2019 - Surprise response as a probe for compressed memory.pdf},
  language = {en},
  type = {Preprint}
}

@article{Levin2020,
  title = {Day-to-{{Day Test}}-{{Retest Reliability}} of {{EEG Profiles}} in {{Children With Autism Spectrum Disorder}} and {{Typical Development}}},
  author = {Levin, April R. and Naples, Adam J. and Scheffler, Aaron Wolfe and Webb, Sara J. and Shic, Frederick and Sugar, Catherine A. and Murias, Michael and Bernier, Raphael A. and Chawarska, Katarzyna and Dawson, Geraldine and Faja, Susan and Jeste, Shafali and Nelson, Charles A. and McPartland, James C. and {\c S}ent{\"u}rk, Damla and {and the Autism Biomarkers Consortium for Clinical Trials}},
  year = {2020},
  month = apr,
  volume = {14},
  pages = {21},
  issn = {1662-5145},
  doi = {10.3389/fnint.2020.00021},
  file = {Levin et al. - 2020 - Day-to-Day Test-Retest Reliability of EEG Profiles.pdf},
  journal = {Front. Integr. Neurosci.},
  language = {en}
}

@article{Levy,
  title = {Galton's {{Two Papers}} on {{Voting}} as {{Robust Estimation}}},
  author = {Levy, David M and Peart, Sandra},
  pages = {10},
  abstract = {The relationship between voting and robust estimation was discussed by Francis Galton in 1907. His two papers in Nature are discussed and reprinted.},
  file = {2002 - Levy, Peart - Galton ’ s two papers on voting as robust estimation ∗.pdf},
  language = {en}
}

@article{Levya,
  title = {The {{Tale}} of {{Galton}}'s {{Mean}}: {{The Influence}} of {{Experts}}},
  author = {Levy, David M and Peart, Sandra J},
  pages = {12},
  file = {2007 - Levy, Peart - The Influence of Experts.pdf},
  language = {en}
}

@article{Lewis-Peacock2008,
  title = {Temporary {{Activation}} of {{Long}}-{{Term Memory Supports Working Memory}}},
  author = {{Lewis-Peacock}, J. A. and Postle, B. R.},
  year = {2008},
  month = aug,
  volume = {28},
  pages = {8765--8771},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.1953-08.2008},
  file = {2008 - Lewis-Peacock, Postle - Temporary activation of long-term memory supports working memory.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {35}
}

@article{Li2007,
  title = {Flexible {{Coding}} for {{Categorical Decisions}} in the {{Human Brain}}},
  author = {Li, S. and Ostwald, D. and Giese, M. and Kourtzi, Z.},
  year = {2007},
  month = nov,
  volume = {27},
  pages = {12321--12330},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.3795-07.2007},
  file = {2007 - Li et al. - Flexible coding for categorical decisions in the human brain.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {45}
}

@article{Li2009,
  title = {Learning {{Shapes}} the {{Representation}} of {{Behavioral Choice}} in the {{Human Brain}}},
  author = {Li, Sheng and Mayhew, Stephen D. and Kourtzi, Zoe},
  year = {2009},
  month = may,
  volume = {62},
  pages = {441--452},
  issn = {08966273},
  doi = {10.1016/j.neuron.2009.03.016},
  abstract = {Making successful decisions under uncertainty due to noisy sensory signals is thought to benefit from previous experience. However, the human brain mechanisms that mediate flexible decisions through learning remain largely unknown. Comparing behavioral choices of human observers with those of a pattern classifier based on multivoxel single-trial fMRI signals, we show that category learning shapes processes related to decision variables in frontal and higher occipitotemporal regions rather than signal detection or response execution in primary visual or motor areas. In particular, fMRI signals in prefrontal regions reflect the observers' behavioral choice according to the learned decision criterion only in the context of the categorization task. In contrast, higher occipitotemporal areas show learning-dependent changes in the representation of perceived categories that are sustained after training independent of the task. These findings demonstrate that learning shapes selective representations of sensory readout signals in accordance with the decision criterion to support flexible decisions.},
  file = {2009 - Li, Mayhew, Kourtzi - Learning shapes the representation of behavioral choice in the human brain.pdf},
  journal = {Neuron},
  language = {en},
  number = {3}
}

@article{Li2010,
  title = {Estimating Coupling Direction between Neuronal Populations with Permutation Conditional Mutual Information},
  author = {Li, Xiaoli and Ouyang, Gaoxiang},
  year = {2010},
  month = aug,
  volume = {52},
  pages = {497--507},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2010.05.003},
  abstract = {To further understand functional connectivity in the brain, we need to identify the coupling direction between neuronal signals recorded from different brain areas. In this paper, we present a novel methodology based on permutation analysis and conditional mutual information for estimation of a directionality index between two neuronal populations. First, the reliability of this method is numerically assessed with a coupled mass neural model; the simulations show that this method is superior to the conditional mutual information method and the Granger causality method for identifying the coupling direction between unidirectional or bidirectional neuronal populations that are generated by the mass neuronal model. The method is also applied to investigate the coupling direction between neuronal populations in CA1 and CA3 in the rat hippocampal tetanus toxin model of focal epilepsy; the propagation direction of the seizure events could be elucidated through this coupling direction estimation method. All together, these results suggest that the permutation conditional mutual information method is a promising technique for estimating directional coupling between mutually interconnected neuronal populations.},
  file = {2010 - Li, Ouyang - Estimating coupling direction between neuronal populations with permutation conditional mutual information.pdf},
  journal = {NeuroImage},
  language = {en},
  number = {2}
}

@article{Li2013,
  title = {Using a Million Cell Simulation of the Cerebellum: {{Network}} Scaling and Task Generality},
  shorttitle = {Using a Million Cell Simulation of the Cerebellum},
  author = {Li, Wen-Ke and Hausknecht, Matthew J. and Stone, Peter and Mauk, Michael D.},
  year = {2013},
  month = nov,
  volume = {47},
  pages = {95--102},
  issn = {08936080},
  doi = {10.1016/j.neunet.2012.11.005},
  abstract = {Several factors combine to make it feasible to build computer simulations of the cerebellum and to test them in biologically realistic ways. These simulations can be used to help understand the computational contributions of various cerebellar components, including the relevance of the enormous number of neurons in the granule cell layer. In previous work we have used a simulation containing 12000 granule cells to develop new predictions and to account for various aspects of eyelid conditioning, a form of motor learning mediated by the cerebellum. Here we demonstrate the feasibility of scaling up this simulation to over one million granule cells using parallel graphics processing unit (GPU) technology. We observe that this increase in number of granule cells requires only twice the execution time of the smaller simulation on the GPU. We demonstrate that this simulation, like its smaller predecessor, can emulate certain basic features of conditioned eyelid responses, with a slight improvement in performance in one measure. We also use this simulation to examine the generality of the computation properties that we have derived from studying eyelid conditioning. We demonstrate that this scaled up simulation can learn a high level of performance in a classic machine learning task, the cart\textendash pole balancing task. These results suggest that this parallel GPU technology can be used to build very large-scale simulations whose connectivity ratios match those of the real cerebellum and that these simulations can be used guide future studies on cerebellar mediated tasks and on machine learning problems.},
  file = {2012 - Li et al. - Using a million cell simulation of the cerebellum Network scaling and task generality.pdf},
  journal = {Neural Networks},
  language = {en}
}

@article{Li2014,
  title = {Chaos-Order Transition in Foraging Behavior of Ants},
  author = {Li, L. and Peng, H. and Kurths, J. and Yang, Y. and Schellnhuber, H. J.},
  year = {2014},
  month = jun,
  volume = {111},
  pages = {8392--8397},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1407083111},
  file = {Li et al. - 2014 - Chaos-order transition in foraging behavior of ant.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {23}
}

@article{Li2016,
  title = {Robust Neuronal Dynamics in Premotor Cortex during Motor Planning},
  author = {Li, Nuo and Daie, Kayvon and Svoboda, Karel and Druckmann, Shaul},
  year = {2016},
  month = apr,
  volume = {532},
  pages = {459--464},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature17643},
  file = {Li et al. - 2016 - Robust neuronal dynamics in premotor cortex during.pdf},
  journal = {Nature},
  language = {en},
  number = {7600}
}

@article{Li2016a,
  title = {Hyperband: {{A Novel Bandit}}-{{Based Approach}} to {{Hyperparameter Optimization}}},
  shorttitle = {Hyperband},
  author = {Li, Lisha and Jamieson, Kevin and DeSalvo, Giulia and Rostamizadeh, Afshin and Talwalkar, Ameet},
  year = {2016},
  month = mar,
  abstract = {Performance of machine learning algorithms depends critically on identifying a good set of hyperparameters. While recent approaches use Bayesian optimization to adaptively select configurations, we focus on speeding up random search through adaptive resource allocation and early-stopping. We formulate hyperparameter optimization as a pure-exploration nonstochastic infinite-armed bandit problem where a predefined resource like iterations, data samples, or features is allocated to randomly sampled configurations. We introduce a novel algorithm, Hyperband, for this framework and analyze its theoretical properties, providing several desirable guarantees. Furthermore, we compare Hyperband with popular Bayesian optimization methods on a suite of hyperparameter optimization problems. We observe that Hyperband can provide over an order-of-magnitude speedup over our competitor set on a variety of deep-learning and kernel-based learning problems.},
  archiveprefix = {arXiv},
  eprint = {1603.06560},
  eprinttype = {arxiv},
  file = {Li et al. - 2016 - Hyperband A Novel Bandit-Based Approach to Hyperp.pdf},
  journal = {arXiv:1603.06560 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryclass = {cs, stat}
}

@article{Li2017,
  title = {Lifting the Veil on the Dynamics of Neuronal Activities Evoked by Transcranial Magnetic Stimulation},
  author = {Li, Bingshuo and Virtanen, Juha P and Oeltermann, Axel and Schwarz, Cornelius and Giese, Martin A and Ziemann, Ulf and Benali, Alia},
  year = {2017},
  month = nov,
  volume = {6},
  issn = {2050-084X},
  doi = {10.7554/eLife.30552},
  abstract = {Transcranial magnetic stimulation (TMS) is a widely used non-invasive tool to study and modulate human brain functions. However, TMS-evoked activity of individual neurons has remained largely inaccessible due to the large TMS-induced electromagnetic fields. Here, we present a general method providing direct in vivo electrophysiological access to TMS-evoked neuronal activity 0.8\textendash 1 ms after TMS onset. We translated human single-pulse TMS to rodents and unveiled time-grained evoked activities of motor cortex layer V neurons that show high-frequency spiking within the first 6 ms depending on TMS-induced current orientation and a multiphasic spike-rhythm alternating between excitation and inhibition in the 6\textendash 300 ms epoch, all of which can be linked to various human TMS responses recorded at the level of spinal cord and muscles. The advance here facilitates a new level of insight into the TMS-brain interaction that is vital for developing this noninvasive tool to purposefully explore and effectively treat the human brain.},
  file = {Li et al. - 2017 - Lifting the veil on the dynamics of neuronal activ.pdf},
  journal = {eLife},
  language = {en}
}

@article{Li2017a,
  title = {Single-Impulse Panoramic Photoacoustic Computed Tomography of Small-Animal Whole-Body Dynamics at High Spatiotemporal Resolution},
  author = {Li, Lei and Zhu, Liren and Ma, Cheng and Lin, Li and Yao, Junjie and Wang, Lidai and Maslov, Konstantin and Zhang, Ruiying and Chen, Wanyi and Shi, Junhui and Wang, Lihong V.},
  year = {2017},
  month = may,
  volume = {1},
  issn = {2157-846X},
  doi = {10.1038/s41551-017-0071},
  file = {Li et al. - 2017 - Single-impulse panoramic photoacoustic computed to.pdf},
  journal = {Nature Biomedical Engineering},
  language = {en},
  number = {5}
}

@article{Liao,
  title = {Theory of {{Deep Learning II}}: {{Landscape}} of the {{Empirical Risk}} in {{Deep Learning}}},
  author = {Liao, Qianli and Poggio, Tomaso},
  pages = {45},
  abstract = {Previous theoretical work on deep learning and neural network optimization tend to focus on avoiding saddle points and local minima. However, the practical observation is that, at least in the case of the most successful Deep Convolutional Neural Networks (DCNNs), practitioners can always increase the network size to fit the training data (an extreme example would be [1]). The most successful DCNNs such as VGG and ResNets are best used with a degree of ``overparametrization''. In this work, we characterize with a mix of theory and experiments, the landscape of the empirical risk of overparametrized DCNNs. We first prove in the regression framework the existence of a large number of degenerate global minimizers with zero empirical error (modulo inconsistent equations). The argument that relies on the use of Bezout theorem is rigorous when the RELUs are replaced by a polynomial nonlinearity (which empirically works as well). As described in our Theory III [2] paper, the same minimizers are degenerate and thus very likely to be found by SGD that will furthermore select with higher probability the most robust zero-minimizer. We further experimentally explored and visualized the landscape of empirical risk of a DCNN on CIFAR-10 during the entire training process and especially the global minima. Finally, based on our theoretical and experimental results, we propose an intuitive model of the landscape of DCNN's empirical loss surface, which might not be as complicated as people commonly believe.},
  file = {Liao and Poggio - Theory of Deep Learning II Landscape of the Empir.pdf},
  language = {en}
}

@article{Liapis,
  title = {Transforming {{Exploratory Creativity}} with {{DeLeNoX}}},
  author = {Liapis, Antonios},
  pages = {8},
  abstract = {We introduce DeLeNoX (Deep Learning Novelty Explorer), a system that autonomously creates artifacts in constrained spaces according to its own evolving interestingness criterion. DeLeNoX proceeds in alternating phases of exploration and transformation. In the exploration phases, a version of novelty search augmented with constraint handling searches for maximally diverse artifacts using a given distance function. In the transformation phases, a deep learning autoencoder learns to compress the variation between the found artifacts into a lower-dimensional space. The newly trained encoder is then used as the basis for a new distance function, transforming the criteria for the next exploration phase. In the current paper, we apply DeLeNoX to the creation of spaceships suitable for use in two-dimensional arcade-style computer games, a representative problem in procedural content generation in games. We also situate DeLeNoX in relation to the distinction between exploratory and transformational creativity, and in relation to Schmidhuber's theory of creativity through the drive for compression progress.},
  file = {Liapis - Transforming Exploratory Creativity with DeLeNoX.pdf},
  language = {en}
}

@article{Lieder,
  title = {When to Use Which Heuristic? {{A}} Rational Solution to the Strategy Selection Problem},
  author = {Lieder, Falk and Griffiths, Tom},
  pages = {1},
  file = {2015 - Lieder, Griffiths - When to use which heuristic A rational solution to the strategy selection problem.pdf;Lieder and Grifﬁths - When to use which heuristic A rational solution t.pdf},
  language = {en}
}

@article{Lien2013,
  title = {Tuned Thalamic Excitation Is Amplified by Visual Cortical Circuits},
  author = {Lien, Anthony D and Scanziani, Massimo},
  year = {2013},
  month = sep,
  volume = {16},
  pages = {1315--1323},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.3488},
  file = {2013 - Lien, Scanziani - Tuned thalamic excitation is amplified by visual cortical circuits.pdf},
  journal = {Nature Neuroscience},
  language = {en},
  number = {9}
}

@article{Lienard2014,
  title = {A Biologically Constrained Model of the Whole Basal Ganglia Addressing the Paradoxes of Connections and Selection},
  author = {Li{\'e}nard, Jean and Girard, Beno{\^i}t},
  year = {2014},
  month = jun,
  volume = {36},
  pages = {445--468},
  issn = {0929-5313, 1573-6873},
  doi = {10.1007/s10827-013-0476-2},
  file = {2014 - Liénard, Girard - A biologically constrained model of the whole basal ganglia addressing the paradoxes of connections and select.pdf;Liénard and Girard - 2014 - A biologically constrained model of the whole basa.pdf},
  journal = {Journal of Computational Neuroscience},
  language = {en},
  number = {3}
}

@article{Lienard2017,
  title = {Beta-{{Band Oscillations}} without {{Pathways}}: The Opposing {{Roles}} of {{D2}} and {{D5 Receptors}}},
  shorttitle = {Beta-{{Band Oscillations}} without {{Pathways}}},
  author = {Lienard, Jean F. and Cos, Ignasi and Girard, Benoit},
  year = {2017},
  month = jul,
  doi = {10.1101/161661},
  abstract = {Parkinson's disease is characterized by the death of dopaminergic neurons and the emergence of strong {$\beta$}-band oscillations throughout the basal ganglia nuclei. According to the mainstream theory, this synchrony is mediated by a dopamine deficit within the striatum creating a functional imbalance between the D1-expressing medium spiny neurons, which project to the internal segment of the globus pallidus, and D2-expressing one, which target its external segment, and ultimately leads to oscillatory activity. However, anatomical evidence gathered in rodents and primates has shown that striatal neurons are for the most part not organized into independent populations differentially targeting the two segments of the globus pallidus, nor alternatively expressing D1 or D2 receptors, thus calling for an alternative mechanism through which the lack of dopamine may cause oscillations. Here we adopt a computational approach in which we investigate a model whose parameters are fit to an extensive set of anatomical and physiological constraints from non-human primates, including axonal transmission delays gathered from eight experimental studies. Investigating the lack of dopamine in this model revealed that in the absence of segregated pathways, {$\beta$}-band oscillations emerge as a consequence of the extra-striate dopaminergic receptors reduced activity. These oscillations are caused by synchronous activity within the external globus pallidus-subthalamic nucleus loop, and their frequency are modulated by the transmission delays between these nuclei. Our model delivers a parsimonious explanation of oscillations that does not require any external driving influence from cortex, nor specific medium spiny neuron properties.},
  file = {Lienard et al. - 2017 - Beta-Band Oscillations without Pathways the oppos 2.pdf;Lienard et al. - 2017 - Beta-Band Oscillations without Pathways the oppos.pdf},
  journal = {bioRxiv},
  language = {en}
}

@article{Lillicrap2019,
  title = {Continuous Control with Deep Reinforcement Learning},
  author = {Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  year = {2019},
  month = jul,
  abstract = {We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies ``end-to-end'': directly from raw pixel inputs.},
  archiveprefix = {arXiv},
  eprint = {1509.02971},
  eprinttype = {arxiv},
  file = {Lillicrap et al. - 2019 - Continuous control with deep reinforcement learnin.pdf},
  journal = {arXiv:1509.02971 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryclass = {cs, stat}
}

@article{Lin,
  title = {Self-Improving Reactive Agents Based on Reinforcement Learning, Planning and Teaching},
  author = {Lin, Long-Ji},
  pages = {29},
  abstract = {To date, reinforcement learning has mostly been studied solving simple learning tasks. Reinforcement learning methods that have been studied so far typically converge slowly. The purpose of this work is thus twofold: 1) to investigate the utility of reinforcement learning in solving much more complicated learning tasks than previously studied, and 2) to investigate methods that will speed up reinforcement learning.},
  file = {1992 - Lin - Self-Improving Reactive Agents Based on Reinforcement Learning, Planning and Teaching.pdf},
  language = {en}
}

@article{Lin2002,
  title = {Modulation of Synaptic Delay during Synaptic Plasticity},
  author = {Lin, Jen-Wei and Faber, Donald S.},
  year = {2002},
  month = sep,
  volume = {25},
  pages = {449--455},
  issn = {01662236},
  doi = {10.1016/S0166-2236(02)02212-9},
  file = {2002 - Lin, Faber - Modulation of synaptic delay during synaptic plasticity.pdf},
  journal = {Trends in Neurosciences},
  language = {en},
  number = {9}
}

@article{Lin2015,
  title = {The {{Nature}} of {{Shared Cortical Variability}}},
  author = {Lin, I-Chun and Okun, Michael and Carandini, Matteo and Harris, Kenneth D.},
  year = {2015},
  month = aug,
  volume = {87},
  pages = {644--656},
  issn = {08966273},
  doi = {10.1016/j.neuron.2015.06.035},
  abstract = {Neuronal responses of sensory cortex are highly variable, and this variability is correlated across neurons. To assess how variability reflects factors shared across a neuronal population, we analyzed the activity of many simultaneously recorded neurons in visual cortex. We developed a simple model that comprises two sources of shared variability: a multiplicative gain, which uniformly scales each neuron's sensory drive, and an additive offset, which affects different neurons to different degrees. This model captured the variability of spike counts and reproduced the dependence of pairwise correlations on neuronal tuning and stimulus orientation. The relative contributions of the additive and multiplicative fluctuations could vary over time and had marked impact on population coding. These observations indicate that shared variability of neuronal populations in sensory cortex can be largely explained by two factors that modulate the whole population.},
  file = {Lin et al. - 2015 - The Nature of Shared Cortical Variability.pdf},
  journal = {Neuron},
  language = {en},
  number = {3}
}

@article{Lin2018,
  title = {All-Optical Machine Learning Using Diffractive Deep Neural Networks},
  author = {Lin, Xing and Rivenson, Yair and Yardimci, Nezih T. and Veli, Muhammed and Luo, Yi and Jarrahi, Mona and Ozcan, Aydogan},
  year = {2018},
  month = sep,
  volume = {361},
  pages = {1004--1008},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aat8084},
  file = {Lin et al. - 2018 - All-optical machine learning using diffractive dee.pdf},
  journal = {Science},
  language = {en},
  number = {6406}
}

@article{Lin2018a,
  title = {Deep {{Gradient Compression}}: {{Reducing}} the {{Communication Bandwidth}} for {{Distributed Training}}},
  shorttitle = {Deep {{Gradient Compression}}},
  author = {Lin, Yujun and Han, Song and Mao, Huizi and Wang, Yu and Dally, William J.},
  year = {2018},
  month = feb,
  abstract = {Large-scale distributed training requires significant communication bandwidth for gradient exchange that limits the scalability of multi-node training, and requires expensive high-bandwidth network infrastructure. The situation gets even worse with distributed training on mobile devices (federated learning), which suffers from higher latency, lower throughput, and intermittent poor connections. In this paper, we find 99.9\% of the gradient exchange in distributed SGD are redundant, and propose Deep Gradient Compression (DGC) to greatly reduce the communication bandwidth. To preserve accuracy during this compression, DGC employs four methods: momentum correction, local gradient clipping, momentum factor masking, and warm-up training. We have applied Deep Gradient Compression to image classification, speech recognition, and language modeling with multiple datasets including Cifar10, ImageNet, Penn Treebank, and Librispeech Corpus. On these scenarios, Deep Gradient Compression achieves a gradient compression ratio from 270\texttimes{} to 600\texttimes{} without losing accuracy, cutting the gradient size of ResNet-50 from 97MB to 0.35MB, and for DeepSpeech from 488MB to 0.74MB. Deep gradient compression enables large-scale distributed training on inexpensive commodity 1Gbps Ethernet and facilitates distributed training on mobile.},
  archiveprefix = {arXiv},
  eprint = {1712.01887},
  eprinttype = {arxiv},
  file = {Lin et al. - 2018 - Deep Gradient Compression Reducing the Communicat.pdf},
  journal = {arXiv:1712.01887 [cs, stat]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Distributed; Parallel; and Cluster Computing,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryclass = {cs, stat}
}

@article{Lina,
  title = {{{ResNet}} with One-Neuron Hidden Layers Is a {{Universal Approximator}}},
  author = {Lin, Hongzhou and Jegelka, Stefanie},
  pages = {10},
  abstract = {We demonstrate that a very deep ResNet with stacked modules that have one neuron per hidden layer and ReLU activation functions can uniformly approximate any Lebesgue integrable function in d dimensions, i.e. {$\mathscr{l}$}1(Rd). Due to the identity mapping inherent to ResNets, our network has alternating layers of dimension one and d. This stands in sharp contrast to fully connected networks, which are not universal approximators if their width is the input dimension d [21, 11]. Hence, our result implies an increase in representational power for narrow deep networks by the ResNet architecture.},
  file = {Lin and Jegelka - ResNet with one-neuron hidden layers is a Universa.pdf},
  language = {en}
}

@article{Linb,
  title = {{{ResNet}} with One-Neuron Hidden Layers Is a {{Universal Approximator}}},
  author = {Lin, Hongzhou and Jegelka, Stefanie},
  pages = {10},
  abstract = {We demonstrate that a very deep ResNet with stacked modules that have one neuron per hidden layer and ReLU activation functions can uniformly approximate any Lebesgue integrable function in d dimensions, i.e. {$\mathscr{l}$}1(Rd). Due to the identity mapping inherent to ResNets, our network has alternating layers of dimension one and d. This stands in sharp contrast to fully connected networks, which are not universal approximators if their width is the input dimension d [21, 11]. Hence, our result implies an increase in representational power for narrow deep networks by the ResNet architecture.},
  file = {Lin and Jegelka - ResNet with one-neuron hidden layers is a Universa 2.pdf},
  language = {en}
}

@article{Lindahl2016,
  title = {Untangling {{Basal Ganglia Network Dynamics}} and {{Function}}: {{Role}} of {{Dopamine Depletion}} and {{Inhibition Investigated}} in a {{Spiking Network Model}}},
  shorttitle = {Untangling {{Basal Ganglia Network Dynamics}} and {{Function}}},
  author = {Lindahl, Mikael and Hellgren Kotaleski, Jeanette},
  year = {2016},
  volume = {3},
  pages = {ENEURO.0156-16.2016},
  issn = {2373-2822},
  doi = {10.1523/ENEURO.0156-16.2016},
  file = {Lindahl and Hellgren Kotaleski - 2016 - Untangling Basal Ganglia Network Dynamics and Func.pdf},
  journal = {eneuro},
  language = {en},
  number = {6}
}

@article{Linderman2016,
  title = {Recurrent Switching Linear Dynamical Systems},
  author = {Linderman, Scott W. and Miller, Andrew C. and Adams, Ryan P. and Blei, David M. and Paninski, Liam and Johnson, Matthew J.},
  year = {2016},
  month = oct,
  abstract = {Many natural systems, such as neurons firing in the brain or basketball teams traversing a court, give rise to time series data with complex, nonlinear dynamics. We can gain insight into these systems by decomposing the data into segments that are each explained by simpler dynamic units. Building on switching linear dynamical systems (SLDS), we present a new model class that not only discovers these dynamical units, but also explains how their switching behavior depends on observations or continuous latent states. These ``recurrent'' switching linear dynamical systems provide further insight by discovering the conditions under which each unit is deployed, something that traditional SLDS models fail to do. We leverage recent algorithmic advances in approximate inference to make Bayesian inference in these models easy, fast, and scalable.},
  archiveprefix = {arXiv},
  eprint = {1610.08466},
  eprinttype = {arxiv},
  file = {Linderman et al. - 2016 - Recurrent switching linear dynamical systems.pdf},
  journal = {arXiv:1610.08466 [stat]},
  keywords = {Statistics - Machine Learning},
  language = {en},
  primaryclass = {stat}
}

@article{Lindsley1939,
  title = {A {{Longitudinal Study}} of the {{Occipital Alpha Rhythm}} in {{Normal Children}}: {{Frequency}} and {{Amplitude Standards}}},
  shorttitle = {A {{Longitudinal Study}} of the {{Occipital Alpha Rhythm}} in {{Normal Children}}},
  author = {Lindsley, Donald B.},
  year = {1939},
  month = sep,
  volume = {55},
  pages = {197--213},
  issn = {0885-6559},
  doi = {10.1080/08856559.1939.10533190},
  file = {Lindsley - 1939 - A Longitudinal Study of the Occipital Alpha Rhythm.pdf},
  journal = {The Pedagogical Seminary and Journal of Genetic Psychology},
  language = {en},
  number = {1}
}

@article{Linsley1943,
  title = {Attraction of {{Melanophila Beetles}} by {{Fire}} and {{Smoke}}},
  author = {Linsley, Gorton E.},
  year = {1943},
  month = apr,
  volume = {36},
  pages = {341--342},
  issn = {1938-291X, 0022-0493},
  doi = {10.1093/jee/36.2.341},
  file = {Linsley - 1943 - Attraction of Melanophila Beetles by Fire and Smok.pdf},
  journal = {Journal of Economic Entomology},
  language = {en},
  number = {2}
}

@article{Lintas2012,
  title = {Dopamine Deficiency Increases Synchronized Activity in the Rat Subthalamic Nucleus},
  author = {Lintas, Alessandra and Silkis, Isabella G. and Alb{\'e}ri, Lavinia and Villa, Alessandro E.P.},
  year = {2012},
  month = jan,
  volume = {1434},
  pages = {142--151},
  issn = {00068993},
  doi = {10.1016/j.brainres.2011.09.005},
  abstract = {Abnormal neuronal activity in the subthalamic nucleus (STN) plays a crucial role in the pathophysiology of Parkinson's disease (PD). In this study we investigated changes in rat STN neuronal activity after 28 days following the injection of 6-OHDA in the substantia nigra pars compacta (SNc). This drug provoked a lesion of SNc that induced a dopamine (DA) depletion assessed by changes in rotating capacity in response to apomorphine injection and by histological analysis. By means of extracellular recordings and waveshape spike sorting it was possible to analyze simultaneous spike trains and compute the crosscorrelations. Based on the analysis of the autocorrelograms we classified four types of firing patterns: regular (Poissonian-like), oscillatory (in the range 4\textendash 12 Hz), bursty and cells characterized by a long refractoriness. The distribution of unit types in the control (n = 61) and lesioned (n = 83) groups was similar, as well as the firing rate. In 6-OHDA treated rats we observed a significant increase (from 26\% to 48\%) in the number of pairs with synchronous firing. These data suggest that the synchronous activity of STN cells, provoked by loss of DA cells in SNc, is likely to be among the most significant dysfunctions in the basal ganglia of Parkinsonian patients. We raise the hypothesis that in normal conditions, DA maintains a balance between funneling information via the hyperdirect cortico-subthalamic pathway and parallel processing through the parallel cortico-basal ganglia-subthalamic pathways, both of which are necessary for selected motor behaviors.},
  file = {2012 - Lintas et al. - Dopamine deficiency increases synchronized activity in the rat subthalamic nucleus.pdf},
  journal = {Brain Research},
  language = {en}
}

@article{Lipshutz2015,
  title = {Existence, {{Uniqueness}}, and {{Stability}} of {{Slowly Oscillating Periodic Solutions}} for {{Delay Differential Equations}} with {{Nonnegativity Constraints}}},
  author = {Lipshutz, David and Williams, Ruth J.},
  year = {2015},
  month = jan,
  volume = {47},
  pages = {4467--4535},
  issn = {0036-1410, 1095-7154},
  doi = {10.1137/140980806},
  abstract = {Deterministic dynamical system models with delayed feedback and nonnegativity constraints arise in a variety of applications in science and engineering. Under certain conditions oscillatory behavior has been observed and it is of interest to know when this behavior is periodic. Here we consider one-dimensional delay differential equations with nonnegativity constraints as prototypes for such models. We obtain sufficient conditions for the existence of slowly oscillating periodic solutions (SOPS) of such equations when the delay/lag interval is long and the dynamics depend only on the current and delayed state. Under further assumptions, including possibly longer delay intervals and restricting the dynamics to depend only on the delayed state, we prove uniqueness and exponential stability for such solutions. To prove these results, we develop a theory for studying perturbations of these constrained SOPS. We illustrate our results with simple examples of biochemical reaction network models and an Internet rate control model.},
  file = {2015 - Lipshutz, Williams - EXISTENCE, UNIQUENESS, AND STABILITY OF SLOWLY OSCILLATING PERIODIC SOLUTIONS FOR DELAY DIFFERENTIAL EQUATIO.pdf;Lipshutz and Williams - 2015 - Existence, Uniqueness, and Stability of Slowly Osc 2.pdf;Lipshutz and Williams - 2015 - Existence, Uniqueness, and Stability of Slowly Osc.pdf},
  journal = {SIAM Journal on Mathematical Analysis},
  language = {en},
  number = {6}
}

@article{Lipton,
  title = {Combating {{Reinforcement Learning}}'s {{Sisyphean Curse}} with {{Intrinsic Fear}}},
  author = {Lipton, Zachary C and Azizzadenesheli, Kamyar and Kumar, Abhishek and Li, Lihong and Gao, Jianfeng and Deng, Li},
  pages = {14},
  abstract = {Many practical environments contain catastrophic states that an optimal agent would visit infrequently or never. Even on toy problems, Deep Reinforcement Learning (DRL) agents tend to periodically revisit these states upon forgetting their existence under a new policy. We introduce intrinsic fear (IF), a learned reward shaping that guards DRL agents against periodic catastrophes. IF agents possess a fear model trained to predict the probability of imminent catastrophe. This score is then used to penalize the Qlearning objective. Our theoretical analysis bounds the reduction in average return due to learning on the perturbed objective. We also prove robustness to classi cation errors. As a bonus, IF models tend to learn faster, owing to reward shaping. Experiments demonstrate that intrinsic-fear DQNs solve otherwise pathological environments and improve on several Atari games.},
  file = {Lipton et al. - Combating Reinforcement Learning’s Sisyphean Curse.pdf},
  language = {en}
}

@article{Lisman2013,
  title = {The {{Theta}}-{{Gamma Neural Code}}},
  author = {Lisman, John E. and Jensen, Ole},
  year = {2013},
  month = mar,
  volume = {77},
  pages = {1002--1016},
  issn = {08966273},
  doi = {10.1016/j.neuron.2013.03.007},
  file = {2013 - Lisman, Jensen - The θ-γ neural code.pdf},
  journal = {Neuron},
  language = {en},
  number = {6}
}

@article{Litwin-Kumar2014,
  title = {Formation and Maintenance of Neuronal Assemblies through Synaptic Plasticity},
  author = {{Litwin-Kumar}, Ashok and Doiron, Brent},
  year = {2014},
  month = dec,
  volume = {5},
  issn = {2041-1723},
  doi = {10.1038/ncomms6319},
  file = {Litwin-Kumar and Doiron - 2014 - Formation and maintenance of neuronal assemblies t.pdf},
  journal = {Nature Communications},
  language = {en},
  number = {1}
}

@article{Litwin-Kumar2019,
  title = {Constraining Computational Models Using Electron Microscopy Wiring Diagrams},
  author = {{Litwin-Kumar}, Ashok and Turaga, Srinivas C},
  year = {2019},
  month = oct,
  volume = {58},
  pages = {94--100},
  issn = {09594388},
  doi = {10.1016/j.conb.2019.07.007},
  file = {Litwin-Kumar and Turaga - 2019 - Constraining computational models using electron m.pdf},
  journal = {Current Opinion in Neurobiology},
  language = {en}
}

@article{Litwin-Kumar2019a,
  title = {Constraining Computational Models Using Electron Microscopy Wiring Diagrams},
  author = {{Litwin-Kumar}, Ashok and Turaga, Srinivas C},
  year = {2019},
  month = oct,
  volume = {58},
  pages = {94--100},
  issn = {09594388},
  doi = {10.1016/j.conb.2019.07.007},
  file = {Litwin-Kumar and Turaga - 2019 - Constraining computational models using electron m 2.pdf},
  journal = {Current Opinion in Neurobiology},
  language = {en}
}

@article{Litwin-Kumar2019b,
  title = {Constraining Computational Models Using Electron Microscopy Wiring Diagrams},
  author = {{Litwin-Kumar}, Ashok and Turaga, Srinivas C},
  year = {2019},
  month = oct,
  volume = {58},
  pages = {94--100},
  issn = {09594388},
  doi = {10.1016/j.conb.2019.07.007},
  file = {Litwin-Kumar and Turaga - 2019 - Constraining computational models using electron m 3.pdf},
  journal = {Current Opinion in Neurobiology},
  language = {en}
}

@article{Liu,
  title = {{{HIERARCHICAL REPRESENTATIONS FOR EFFICIENT ARCHITECTURE SEARCH}}},
  author = {Liu, Hanxiao and Simonyan, Karen and Vinyals, Oriol and Fernando, Chrisantha and Kavukcuoglu, Koray},
  pages = {13},
  abstract = {We explore efficient neural architecture search methods and present a simple yet powerful evolutionary algorithm that can discover new architectures achieving state of the art results. Our approach combines a novel hierarchical genetic representation scheme that imitates the modularized design pattern commonly adopted by human experts, and an expressive search space that supports complex topologies. Our algorithm efficiently discovers architectures that outperform a large number of manually designed models for image classification, obtaining top-1 error of 3.6\% on CIFAR-10 and 20.3\% when transferred to ImageNet, which is competitive with the best existing neural architecture search approaches and represents the new state of the art for evolutionary strategies on this task. We also present results using random search, achieving 0.3\% less top-1 accuracy on CIFAR-10 and 0.1\% less on ImageNet whilst reducing the architecture search time from 36 hours down to 1 hour.},
  file = {Liu et al. - HIERARCHICAL REPRESENTATIONS FOR EFFICIENT ARCHITE.pdf},
  language = {en}
}

@article{Liu1998,
  title = {A {{Model Neuron}} with {{Activity}}-{{Dependent Conductances Regulated}} by {{Multiple Calcium Sensors}}},
  author = {Liu, Zheng and Golowasch, Jorge and Marder, Eve and Abbott, L. F.},
  year = {1998},
  month = apr,
  volume = {18},
  pages = {2309--2320},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.18-07-02309.1998},
  file = {1998 - Liu et al. - A model neuron with activity-dependent conductances regulated by multiple calcium sensors.pdf},
  journal = {The Journal of Neuroscience},
  language = {en},
  number = {7}
}

@article{Liu2009,
  title = {Embedding {{Multiple Trajectories}} in {{Simulated Recurrent Neural Networks}} in a {{Self}}-{{Organizing Manner}}},
  author = {Liu, J. K. and Buonomano, D. V.},
  year = {2009},
  month = oct,
  volume = {29},
  pages = {13172--13181},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.2358-09.2009},
  file = {2009 - Liu, Buonomano - Embedding Multiple Trajectories in Simulated Recurrent Neural Networks in a Self-Organizing Manner.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {42}
}

@article{Liu2019,
  title = {Mouse Navigation Strategies for Odor Source Localization},
  author = {Liu, Annie and Papale, Andrew and Hengenius, James and Patel, Khusbu and Ermentrout, Bard and Urban, Nathaniel},
  year = {2019},
  month = feb,
  doi = {10.1101/558643},
  abstract = {Navigating an odor landscape is a critical behavior for the survival of many species, including mice. One ethologically relevant mouse behavior is locating food using odor concentration gradients. To model this behavior, we use a naturalistic open field odor-based spot-finding task, examining navigation strategies as mice search for and approach an odor source. Mice were trained to navigate to odor sources paired with food reward. We detected behavioral changes consistent with localization of the odor source when mice were 10cm away from the source. These behaviors included both orientation towards the source and increased exploration time. We found that the amplitude of "casting," lateral back and forth head movement, increased exponentially with proximity to the source. We then created concentration-dependent models to simulate mouse behavior, which provided evidence for a serial-sniffing strategy (sampling concentration, moving in space, then sampling again) and a stereo-sniffing strategy (inter-nostril comparison of concentration in a single sniff). Together, these results elucidate key components of behavioral strategies for odor-based navigation.},
  file = {Liu et al. - 2019 - Mouse navigation strategies for odor source locali.pdf;Liu et al. - 2019 - Mouse navigation strategies for odor source locali.pdf},
  journal = {bioRxiv},
  language = {en}
}

@article{Lizbinski2018,
  title = {Intrinsic and {{Extrinsic Neuromodulation}} of {{Olfactory Processing}}},
  author = {Lizbinski, Kristyn M. and Dacks, Andrew M.},
  year = {2018},
  month = jan,
  volume = {11},
  issn = {1662-5102},
  doi = {10.3389/fncel.2017.00424},
  abstract = {Neuromodulation is a ubiquitous feature of neural systems, allowing flexible, context specific control over network dynamics. Neuromodulation was first described in invertebrate motor systems and early work established a basic dichotomy for neuromodulation as having either an intrinsic origin (i.e., neurons that participate in network coding) or an extrinsic origin (i.e., neurons from independent networks). In this conceptual dichotomy, intrinsic sources of neuromodulation provide a ``memory'' by adjusting network dynamics based upon previous and ongoing activation of the network itself, while extrinsic neuromodulators provide the context of ongoing activity of other neural networks. Although this dichotomy has been thoroughly considered in motor systems, it has received far less attention in sensory systems. In this review, we discuss intrinsic and extrinsic modulation in the context of olfactory processing in invertebrate and vertebrate model systems. We begin by discussing presynaptic modulation of olfactory sensory neurons by local interneurons (LNs) as a mechanism for gain control based on ongoing network activation. We then discuss the cell-class specific effects of serotonergic centrifugal neurons on olfactory processing. Finally, we briefly discuss the integration of intrinsic and extrinsic neuromodulation (metamodulation) as an effective mechanism for exerting global control over olfactory network dynamics. The heterogeneous nature of neuromodulation is a recurring theme throughout this review as the effects of both intrinsic and extrinsic modulation are generally non-uniform.},
  file = {Lizbinski and Dacks - 2018 - Intrinsic and Extrinsic Neuromodulation of Olfacto.pdf},
  journal = {Frontiers in Cellular Neuroscience},
  language = {en}
}

@article{Lobacheva2020,
  title = {On {{Power Laws}} in {{Deep Ensembles}}},
  author = {Lobacheva, Ekaterina and Chirkova, Nadezhda and Kodryan, Maxim and Vetrov, Dmitry},
  year = {2020},
  month = jul,
  abstract = {Ensembles of deep neural networks are known to achieve state-of-the-art performance in uncertainty estimation and lead to accuracy improvement. In this work, we focus on a classification problem and investigate the behavior of both noncalibrated and calibrated negative log-likelihood (CNLL) of a deep ensemble as a function of the ensemble size and the member network size. We indicate the conditions under which CNLL follows a power law w. r. t. ensemble size or member network size, and analyze the dynamics of the parameters of the discovered power laws. Our important practical finding is that one large network may perform worse than an ensemble of several medium-size networks with the same total number of parameters (we call this ensemble a memory split). Using the detected power law-like dependencies, we can predict (1) the possible gain from the ensembling of networks with given structure, (2) the optimal memory split given a memory budget, based on a relatively small number of trained networks.},
  archiveprefix = {arXiv},
  eprint = {2007.08483},
  eprinttype = {arxiv},
  file = {Lobacheva et al. - 2020 - On Power Laws in Deep Ensembles.pdf},
  journal = {arXiv:2007.08483 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryclass = {cs, stat}
}

@article{Loewenstein1994,
  title = {The Psychology of Curiosity: {{A}} Review and Reinterpretation.},
  author = {Loewenstein, George},
  year = {1994},
  volume = {116},
  pages = {75},
  file = {Loewenstein - 1994 - The psychology of curiosity A review and reinterp.pdf},
  journal = {Psychological bulletin},
  number = {1}
}

@article{Loewenstein2001,
  title = {Risk as {{Feelings}}},
  author = {Loewenstein, George and Weber, Elle and Hsee, Christpher and Welch, Ned},
  year = {2001},
  volume = {127},
  pages = {44},
  abstract = {Virtually all current theories of choice under risk or uncertainty are cognitive and consequentialist. They assume that people assess the desirability and likelihood of possible outcomes of choice alternatives and integrate this information through some type of expectation-based calculus to arrive at a decision. The authors propose an alternative theoretical perspective, the risk-as-feelings hypothesis, that highlights the role of affect experienced at the moment of decision making. Drawing on research from clinical, physiological, and other subfields of psychology, they show that emotional reactions to risky situations often diverge from cognitive assessments of those risks. When such divergence occurs, emotional reactions often drive behavior. The risk-as-feelings hypothesis is shown to explain a wide range of phenomena that have resisted interpretation in cognitive\textendash consequentialist terms.},
  file = {Loewenstein et al. - 2001 - Risk as Feelings.pdf},
  journal = {Psychological Bulletin},
  language = {en},
  number = {2}
}

@article{Logan2002,
  title = {An Instance Theory of Attention and Memory.},
  author = {Logan, Gordon D.},
  year = {2002},
  volume = {109},
  pages = {376--400},
  issn = {0033-295X},
  doi = {10.1037//0033-295X.109.2.376},
  file = {2002 - Logan - An instance theory of attention and memory.pdf},
  journal = {Psychological Review},
  language = {en},
  number = {2}
}

@article{Lomholt2008,
  title = {Levy Strategies in Intermittent Search Processes Are Advantageous},
  author = {Lomholt, M. A. and Tal, K. and Metzler, R. and Joseph, K.},
  year = {2008},
  month = aug,
  volume = {105},
  pages = {11055--11059},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.0803117105},
  file = {Lomholt et al. - 2008 - Levy strategies in intermittent search processes a.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {32}
}

@article{London2010,
  title = {Sensitivity to Perturbations in Vivo Implies High Noise and Suggests Rate Coding in Cortex},
  author = {London, Michael and Roth, Arnd and Beeren, Lisa and H{\"a}usser, Michael and Latham, Peter E.},
  year = {2010},
  month = jul,
  volume = {466},
  pages = {123--127},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature09086},
  abstract = {It is well known that neural activity exhibits variability, in the sense that identical sensory stimuli produce different responses, but it has been difficult to determine what this variability means. Is it noise, or does it carry important information \textendash{} about, for example, the internal state of the organism? We address this issue from the bottom up, by asking whether small perturbations to activity in cortical networks are amplified. Based on in vivo whole-cell recordings in rat barrel cortex, we find that a perturbation consisting of a single extra spike in one neuron produces \textasciitilde 28 additional spikes in its postsynaptic targets, and we show, using simultaneous intra- and extracellular recordings, that a single spike produces a detectable increase in firing rate in the local network. Theoretical analysis indicates that this amplification leads to intrinsic, stimulusindependent variations in membrane potential on the order of {$\pm$}2.2 - 4.5 mV \textendash{} variations that are pure noise, and so carry no information at all. Therefore, for the brain to perform reliable computations, it must either use a rate code, or generate very large, fast depolarizing events, such as those proposed by the theory of synfire chains \textendash{} yet in our in vivo recordings, we found that such events were very rare. Our findings are consistent with the idea that cortex is likely to use primarily a rate code.},
  file = {2011 - Manuscript - NIH Public Access.pdf},
  journal = {Nature},
  language = {en},
  number = {7302}
}

@article{Lopes2012,
  title = {Exploration in {{Model}}-Based {{Reinforcement Learning}} by {{Empirically Estimating Learning Progress}}},
  author = {Lopes, Manuel and Lang, Tobias and Toussaint, Marc and Oudeyer, Pierre-yves},
  year = {2012},
  volume = {25},
  pages = {1--9},
  abstract = {Formal exploration approaches in model-based reinforcement learning estimate the accuracy of the currently learned model without consideration of the empirical prediction error. For example, PAC-MDP approaches such as R-MAX base their model certainty on the amount of collected data, while Bayesian approaches assume a prior over the transition dynamics. We propose extensions to such approaches which drive exploration solely based on empirical estimates of the learner's accuracy and learning progress. We provide a ``sanity check'' theoretical analysis, discussing the behavior of our extensions in the standard stationary finite state-action case. We then provide experimental studies demonstrating the robustness of these exploration measures in cases of non-stationary environments or where original approaches are misled by wrong domain assumptions.},
  file = {Lopes et al. - Exploration in Model-based Reinforcement Learning .pdf},
  journal = {NIPS},
  language = {en}
}

@article{LopesdaSilva1973,
  title = {Organization of Thalamic and Cortical Alpha Rhythms: {{Spectra}} and Coherences},
  shorttitle = {Organization of Thalamic and Cortical Alpha Rhythms},
  author = {{Lopes da Silva}, F.H and {van Lierop}, T.H.M.T and Schrijer, C.F and {Storm van Leeuwen}, W},
  year = {1973},
  month = dec,
  volume = {35},
  pages = {627--639},
  issn = {00134694},
  doi = {10.1016/0013-4694(73)90216-2},
  file = {1973 - Lierop, Schrijer, Leeuwen - 1973b). Notwithstanding recent advances on physio- logical data on thalamic rhythmic activity in anim.pdf;Lopes da Silva et al. - 1973 - Organization of thalamic and cortical alpha rhythm.pdf},
  journal = {Electroencephalography and Clinical Neurophysiology},
  language = {en},
  number = {6}
}

@article{LopesdaSilva1974,
  title = {Model of Brain Rhythmic Activity: {{The}} Alpha-Rhythm of the Thalamus},
  shorttitle = {Model of Brain Rhythmic Activity},
  author = {{Lopes da Silva}, F. H. and Hoeks, A. and Smits, H. and Zetterberg, L. H.},
  year = {1974},
  volume = {15},
  pages = {27--37},
  issn = {0340-1200, 1432-0770},
  doi = {10.1007/BF00270757},
  abstract = {I. A model of a neuronal network has been set up in a digital computer based on histological and biophysical data experimentally obtained from the thalamus; the model includes two populations of neurons interconnected by means of negative feedback; in the model allowance is also made for other sort of interactions.},
  file = {1974 - Lopes da Silva, Smits, Health - The Alpha-Rhythm of the Thalamus.pdf},
  journal = {Kybernetik},
  language = {en},
  number = {1}
}

@incollection{LopesdaSilva1976,
  title = {Models of {{Neuronal Populations}}: {{The Basic Mechanisms}} of {{Rhythmicity}}},
  shorttitle = {Models of {{Neuronal Populations}}},
  booktitle = {Progress in {{Brain Research}}},
  author = {{Lopes da Silva}, F.H. and {van Rotterdam}, A. and Barts, P. and {van Heusden}, E. and Burr, W.},
  year = {1976},
  volume = {45},
  pages = {281--308},
  publisher = {{Elsevier}},
  doi = {10.1016/S0079-6123(08)60995-4},
  file = {1976 - Lopes da Silva et al. - Models of neuronal populations the basic mechanisms of rhythmicity.pdf},
  isbn = {978-0-444-41457-1},
  language = {en}
}

@article{Lopez-Azcarate2010,
  title = {Coupling between {{Beta}} and {{High}}-{{Frequency Activity}} in the {{Human Subthalamic Nucleus May Be}} a {{Pathophysiological Mechanism}} in {{Parkinson}}'s {{Disease}}},
  author = {{Lopez-Azcarate}, J. and Tainta, M. and {Rodriguez-Oroz}, M. C. and Valencia, M. and Gonzalez, R. and Guridi, J. and Iriarte, J. and Obeso, J. A. and Artieda, J. and Alegre, M.},
  year = {2010},
  month = may,
  volume = {30},
  pages = {6667--6677},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.5459-09.2010},
  file = {2010 - Lopez-Azcarate et al. - Coupling between Beta and High-Frequency Activity in the Human Subthalamic Nucleus May Be a Pathophysiolo.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {19}
}

@article{Lopez-Fidalgo2007,
  title = {An Optimal Experimental Design Criterion for Discriminating between Non-Normal Models},
  author = {{L{\'o}pez-Fidalgo}, J. and Tommasi, C. and Trandafir, P. C.},
  year = {2007},
  month = apr,
  volume = {69},
  pages = {231--242},
  issn = {1369-7412, 1467-9868},
  doi = {10.1111/j.1467-9868.2007.00586.x},
  abstract = {Typically T -optimality is used to obtain optimal designs to discriminate between homoscedastic models with normally distributed observations. Some extensions of this criterion have been made for the heteroscedastic case and binary response models in the literature. In this paper, a new criterion based on the Kullback\textendash Leibler distance is proposed to discriminate between rival models with non-normally distributed observations. The criterion is coherent with the approaches mentioned above. An equivalence theorem is provided for this criterion and an algorithm to compute optimal designs is developed. The criterion is applied to discriminate between the popular Michaelis\textendash Menten model and a typical extension of it under the log-normal and the gamma distributions.},
  file = {2007 - López-Fidalgo, Tommasi, Trandafir - An optimal experimental design criterion for discriminating between non-normal models.pdf},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  language = {en},
  number = {2}
}

@incollection{Loreto2016,
  title = {Dynamics on {{Expanding Spaces}}: {{Modeling}} the {{Emergence}} of {{Novelties}}},
  shorttitle = {Dynamics on {{Expanding Spaces}}},
  booktitle = {Creativity and {{Universality}} in {{Language}}},
  author = {Loreto, Vittorio and Servedio, Vito D. P. and Strogatz, Steven H. and Tria, Francesca},
  editor = {Degli Esposti, Mirko and Altmann, Eduardo G. and Pachet, Fran{\c c}ois},
  year = {2016},
  pages = {59--83},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-24403-7_5},
  abstract = {Novelties are part of our daily lives. We constantly adopt new technologies, conceive new ideas, meet new people, and experiment with new situations. Occasionally, we as individual, in a complicated cognitive and sometimes fortuitous process, come up with something that is not only new to us, but to our entire society so that what is a personal novelty can turn into an innovation at a global level. Innovations occur throughout social, biological, and technological systems and, though we perceive them as a very natural ingredient of our human experience, little is known about the processes determining their emergence. Still the statistical occurrence of innovations shows striking regularities that represent a starting point to get a deeper insight in the whole phenomenology. This paper represents a small step in that direction, focusing on reviewing the scientific attempts to effectively model the emergence of the new and its regularities, with an emphasis on more recent contributions: from the plain Simon's model tracing back to the 1950s, to the newest model of Polya's urn with triggering of one novelty by another. What seems to be key in the successful modeling schemes proposed so far is the idea of looking at evolution as a path in a complex space, physical, conceptual, biological, and technological, whose structure and topology get continuously reshaped and expanded by the occurrence of the new. Mathematically, it is very interesting to look at the consequences of the interplay between the ``actual'' and the ``possible'' and this is the aim of this short review.},
  file = {Loreto et al. - 2016 - Dynamics on Expanding Spaces Modeling the Emergen.pdf},
  isbn = {978-3-319-24401-3 978-3-319-24403-7},
  language = {en}
}

@article{Lotfi2014,
  title = {A {{Novel Single Neuron Perceptron}} with {{Universal Approximation}} and {{XOR Computation Properties}}},
  author = {Lotfi, Ehsan and {Akbarzadeh-T}, M.-R.},
  year = {2014},
  volume = {2014},
  pages = {1--6},
  issn = {1687-5265, 1687-5273},
  doi = {10.1155/2014/746376},
  abstract = {We propose a biologically motivated brain-inspired single neuron perceptron (SNP) with universal approximation and XOR computation properties. This computational model extends the input pattern and is based on the excitatory and inhibitory learning rules inspired from neural connections in the human brain's nervous system. The resulting architecture of SNP can be trained by supervised excitatory and inhibitory online learning rules. The main features of proposed single layer perceptron are universal approximation property and low computational complexity. The method is tested on 6 UCI (University of California, Irvine) pattern recognition and classification datasets. Various comparisons with multilayer perceptron (MLP) with gradient decent backpropagation (GDBP) learning algorithm indicate the superiority of the approach in terms of higher accuracy, lower time, and spatial complexity, as well as faster training. Hence, we believe the proposed approach can be generally applicable to various problems such as in pattern recognition and classification.},
  file = {Lotfi and Akbarzadeh-T - 2014 - A Novel Single Neuron Perceptron with Universal Ap.pdf},
  journal = {Computational Intelligence and Neuroscience},
  language = {en}
}

@article{Lourens2015,
  title = {Exploiting Pallidal Plasticity for Stimulation in {{Parkinson}}'s Disease},
  author = {Lourens, Marcel A J and Schwab, Bettina C and Nirody, Jasmine A and Meijer, Hil G E and {van Gils}, Stephan A},
  year = {2015},
  month = apr,
  volume = {12},
  pages = {026005},
  issn = {1741-2560, 1741-2552},
  doi = {10.1088/1741-2560/12/2/026005},
  abstract = {Objective. Continuous application of high-frequency deep brain stimulation (DBS) often effectively reduces motor symptoms of Parkinson's disease patients. While there is a growing need for more effective and less traumatic stimulation, the exact mechanism of DBS is still unknown. Here, we present a methodology to exploit the plasticity of GABAergic synapses inside the external globus pallidus (GPe) for the optimization of DBS. Approach. Assuming the existence of spike-timing-dependent plasticity (STDP) at GABAergic GPe\textendash GPe synapses, we simulate neural activity in a network model of the subthalamic nucleus and GPe. In particular, we test different DBS protocols in our model and quantify their influence on neural synchrony. Main results. In an exemplary set of biologically plausible model parameters, we show that STDP in the GPe has a direct influence on neural activity and especially the stability of firing patterns. STDP stabilizes both uncorrelated firing in the healthy state and correlated firing in the parkinsonian state. Alternative stimulation protocols such as coordinated reset stimulation can clearly profit from the stabilizing effect of STDP. These results are widely independent of the STDP learning rule. Significance. Once the model settings, e.g., connection architectures, have been described experimentally, our model can be adjusted and directly applied in the development of novel stimulation protocols. More efficient stimulation leads to both minimization of side effects and savings in battery power.},
  file = {2015 - Lourens et al. - Exploiting pallidal plasticity for stimulation in Parkinson's disease.pdf;Lourens et al. - 2015 - Exploiting pallidal plasticity for stimulation in .pdf},
  journal = {Journal of Neural Engineering},
  language = {en},
  number = {2}
}

@article{Lowet2015,
  title = {Input-{{Dependent Frequency Modulation}} of {{Cortical Gamma Oscillations Shapes Spatial Synchronization}} and {{Enables Phase Coding}}},
  author = {Lowet, Eric and Roberts, Mark and Hadjipapas, Avgis and Peter, Alina and {van der Eerden}, Jan and De Weerd, Peter},
  editor = {Ermentrout, Bard},
  year = {2015},
  month = feb,
  volume = {11},
  pages = {e1004072},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1004072},
  file = {Lowet et al. - 2015 - Input-Dependent Frequency Modulation of Cortical G.pdf},
  journal = {PLOS Computational Biology},
  language = {en},
  number = {2}
}

@article{Lowet2016,
  title = {Neuronal Gamma-Band Synchronization Regulated by Instantaneous Modulations of the Oscillation Frequency},
  author = {Lowet, Eric and Roberts, Mark and Peter, Alina and Gips, Bart and De Weerd, Peter},
  year = {2016},
  month = sep,
  doi = {10.1101/070672},
  abstract = {Neuronal gamma-band synchronization shapes information flow during sensory and cognitive processing. A common view is that a stable and shared frequency over time is required for robust and functional synchronization. To the contrary, we found that non-stationary instantaneous frequency modulations were essential for synchronization. First, we recorded gamma rhythms in monkey visual area V1, and found that they synchronized by continuously modulating their frequency difference in a phase-dependent manner. The frequency modulation properties regulated both the phase-locking and the preferred phase-relation between gamma rhythms. Second, our experimental observations were in agreement with a biophysical model of gamma rhythms and were accurately predicted by the theory of weakly coupled oscillators revealing the underlying theoretical principles that govern gamma synchronization. Thus, synchronization through instantaneous frequency modulations represents a fundamental principle of gamma-band neural coordination that is likely generalizable to other brain rhythms.},
  file = {Lowet et al. - 2016 - Neuronal gamma-band synchronization regulated by i.pdf},
  journal = {bioRxiv},
  language = {en}
}

@article{Lu2017,
  title = {The {{Expressive Power}} of {{Neural Networks}}: {{A View}} from the {{Width}}},
  author = {Lu, Zhou and Pu, Hongming and Wang, Feicheng and Hu, Zhiqiang and Wang, Liwei},
  year = {2017},
  pages = {9},
  abstract = {The expressive power of neural networks is important for understanding deep learning. Most existing works consider this problem from the view of the depth of a network. In this paper, we study how width affects the expressiveness of neural networks. Classical results state that depth-bounded (e.g. depth-2) networks with suitable activation functions are universal approximators. We show a universal approximation theorem for width-bounded ReLU networks: width-(n + 4) ReLU networks, where n is the input dimension, are universal approximators. Moreover, except for a measure zero set, all functions cannot be approximated by width-n ReLU networks, which exhibits a phase transition. Several recent works demonstrate the benefits of depth by proving the depth-efficiency of neural networks. That is, there are classes of deep networks which cannot be realized by any shallow network whose size is no more than an exponential bound. Here we pose the dual question on the width-efficiency of ReLU networks: Are there wide networks that cannot be realized by narrow networks whose size is not substantially larger? We show that there exist classes of wide networks which cannot be realized by any narrow network whose depth is no more than a polynomial bound. On the other hand, we demonstrate by extensive experiments that narrow networks whose size exceed the polynomial bound by a constant factor can approximate wide and shallow network with high accuracy. Our results provide more comprehensive evidence that depth may be more effective than width for the expressiveness of ReLU networks.},
  file = {Lu et al. - The Expressive Power of Neural Networks A View fr.pdf},
  journal = {NIPS},
  language = {en}
}

@article{Lu2017a,
  title = {The {{Expressive Power}} of {{Neural Networks}}: {{A View}} from the {{Width}}},
  shorttitle = {The {{Expressive Power}} of {{Neural Networks}}},
  author = {Lu, Zhou and Pu, Hongming and Wang, Feicheng and Hu, Zhiqiang and Wang, Liwei},
  year = {2017},
  month = nov,
  abstract = {The expressive power of neural networks is important for understanding deep learning. Most existing works consider this problem from the view of the depth of a network. In this paper, we study how width affects the expressiveness of neural networks. Classical results state that depth-bounded (e.g. depth-2) networks with suitable activation functions are universal approximators. We show a universal approximation theorem for width-bounded ReLU networks: width-(n + 4) ReLU networks, where n is the input dimension, are universal approximators. Moreover, except for a measure zero set, all functions cannot be approximated by width-n ReLU networks, which exhibits a phase transition. Several recent works demonstrate the benefits of depth by proving the depth-efficiency of neural networks. That is, there are classes of deep networks which cannot be realized by any shallow network whose size is no more than an exponential bound. Here we pose the dual question on the width-efficiency of ReLU networks: Are there wide networks that cannot be realized by narrow networks whose size is not substantially larger? We show that there exist classes of wide networks which cannot be realized by any narrow network whose depth is no more than a polynomial bound. On the other hand, we demonstrate by extensive experiments that narrow networks whose size exceed the polynomial bound by a constant factor can approximate wide and shallow network with high accuracy. Our results provide more comprehensive evidence that depth may be more effective than width for the expressiveness of ReLU networks.},
  archiveprefix = {arXiv},
  eprint = {1709.02540},
  eprinttype = {arxiv},
  file = {Lu et al. - 2017 - The Expressive Power of Neural Networks A View fr.pdf},
  journal = {arXiv:1709.02540 [cs]},
  keywords = {Computer Science - Machine Learning},
  language = {en},
  primaryclass = {cs}
}

@article{Lucken2013,
  title = {Desynchronization Boost by Non-Uniform Coordinated Reset Stimulation in Ensembles of Pulse-Coupled Neurons},
  author = {L{\"u}cken, Leonhard and Yanchuk, Serhiy and Popovych, Oleksandr V. and Tass, Peter A.},
  year = {2013},
  volume = {7},
  issn = {1662-5188},
  doi = {10.3389/fncom.2013.00063},
  abstract = {Several brain diseases are characterized by abnormal neuronal synchronization. Desynchronization of abnormal neural synchrony is theoretically compelling because of the complex dynamical mechanisms involved. We here present a novel type of coordinated reset (CR) stimulation. CR means to deliver phase resetting stimuli at different neuronal sub-populations sequentially, i.e., at times equidistantly distributed in a stimulation cycle. This uniform timing pattern seems to be intuitive and actually applies to the neural network models used for the study of CR so far. CR resets the population to an unstable cluster state from where it passes through a desynchronized transient, eventually resynchronizing if left unperturbed. In contrast, we show that the optimal stimulation times are non-uniform. Using the model of weakly pulse-coupled neurons with phase response curves, we provide an approach that enables to determine optimal stimulation timing patterns that substantially maximize the desynchronized transient time following the application of CR stimulation. This approach includes an optimization search for clusters in a low-dimensional pulse coupled map. As a consequence, model-specific non-uniformly spaced cluster states cause considerably longer desynchronization transients. Intriguingly, such a desynchronization boost with non-uniform CR stimulation can already be achieved by only slight modifications of the uniform CR timing pattern. Our results suggest that the non-uniformness of the stimulation times can be a medically valuable parameter in the calibration procedure for CR stimulation, where the latter has successfully been used in clinical and pre-clinical studies for the treatment of Parkinson's disease and tinnitus.},
  file = {2013 - Lücken et al. - Desynchronization boost by non-uniform coordinated reset stimulation in ensembles of pulse-coupled neurons.pdf},
  journal = {Frontiers in Computational Neuroscience},
  language = {en}
}

@article{Luczak2009,
  title = {Spontaneous {{Events Outline}} the {{Realm}} of {{Possible Sensory Responses}} in {{Neocortical Populations}}},
  author = {Luczak, Artur and Barth{\'o}, Peter and Harris, Kenneth D.},
  year = {2009},
  month = may,
  volume = {62},
  pages = {413--425},
  issn = {08966273},
  doi = {10.1016/j.neuron.2009.03.014},
  abstract = {Neocortical assemblies produce complex activity patterns both in response to sensory stimuli and spontaneously without sensory input. To investigate the structure of these patterns, we recorded from populations of 40\textendash 100 neurons in auditory and somatosensory cortices of anesthetized and awake rats using silicon microelectrodes. Population spike time patterns were broadly conserved across multiple sensory stimuli and spontaneous events. Although individual neurons showed timing variations between stimuli, these were not sufficient to disturb a generally conserved sequential organization observed at the population level, lasting for approximately 100 ms with spiking reliability decaying progressively after event onset. Preserved constraints were also seen in population firing rate vectors, with vectors evoked by individual stimuli occupying subspaces of a larger but still constrained space outlined by the set of spontaneous events. These results suggest that population spike patterns are drawn from a limited ``vocabulary,'' sampled widely by spontaneous events but more narrowly by sensory responses.},
  file = {2009 - Luczak, Barth, Harris - Spontaneous Events Outline the Realm of Possible Sensory Responses in Neocortical Populations.pdf},
  journal = {Neuron},
  language = {en},
  number = {3}
}

@article{Lundqvist2010,
  title = {Bistable, {{Irregular Firing}} and {{Population Oscillations}} in a {{Modular Attractor Memory Network}}},
  author = {Lundqvist, Mikael and Compte, Albert and Lansner, Anders},
  editor = {Morrison, Abigail},
  year = {2010},
  month = jun,
  volume = {6},
  pages = {e1000803},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1000803},
  abstract = {Attractor neural networks are thought to underlie working memory functions in the cerebral cortex. Several such models have been proposed that successfully reproduce firing properties of neurons recorded from monkeys performing working memory tasks. However, the regular temporal structure of spike trains in these models is often incompatible with experimental data. Here, we show that the in vivo observations of bistable activity with irregular firing at the single cell level can be achieved in a large-scale network model with a modular structure in terms of several connected hypercolumns. Despite high irregularity of individual spike trains, the model shows population oscillations in the beta and gamma band in ground and active states, respectively. Irregular firing typically emerges in a high-conductance regime of balanced excitation and inhibition. Population oscillations can produce such a regime, but in previous models only a non-coding ground state was oscillatory. Due to the modular structure of our network, the oscillatory and irregular firing was maintained also in the active state without fine-tuning. Our model provides a novel mechanistic view of how irregular firing emerges in cortical populations as they go from beta to gamma oscillations during memory retrieval.},
  file = {2010 - Lundqvist, Compte, Lansner - Bistable, irregular firing and population oscillations in a modular attractor memory network.pdf},
  journal = {PLoS Computational Biology},
  language = {en},
  number = {6}
}

@article{Lundqvist2016,
  title = {Gamma and {{Beta Bursts Underlie Working Memory}}},
  author = {Lundqvist, Mikael and Rose, Jonas and Herman, Pawel and Brincat, Scott L. and Buschman, Timothy J. and Miller, Earl K.},
  year = {2016},
  month = apr,
  volume = {90},
  pages = {152--164},
  issn = {08966273},
  doi = {10.1016/j.neuron.2016.02.028},
  abstract = {Working memory is thought to result from sustained neuron spiking. However, computational models suggest complex dynamics with discrete oscillatory bursts. We analyzed local field potential (LFP) and spiking from the prefrontal cortex (PFC) of monkeys performing a working memory task. There were brief bursts of narrow-band gamma oscillations (45\textendash 100 Hz), varied in time and frequency, accompanying encoding and re-activation of sensory information. They appeared at a minority of recording sites associated with spiking reflecting the to-beremembered items. Beta oscillations (20\textendash 35 Hz) also occurred in brief, variable bursts but reflected a default state interrupted by encoding and decoding. Only activity of neurons reflecting encoding/ decoding correlated with changes in gamma burst rate. Thus, gamma bursts could gate access to, and prevent sensory interference with, working memory. This supports the hypothesis that working memory is manifested by discrete oscillatory dynamics and spiking, not sustained activity.},
  file = {Lundqvist et al. - 2016 - Gamma and Beta Bursts Underlie Working Memory.pdf},
  journal = {Neuron},
  language = {en},
  number = {1}
}

@article{Lundqvist2016a,
  title = {Gamma and {{Beta Bursts Underlie Working Memory}}},
  author = {Lundqvist, Mikael and Rose, Jonas and Herman, Pawel and Brincat, Scott L. and Buschman, Timothy J. and Miller, Earl K.},
  year = {2016},
  month = apr,
  volume = {90},
  pages = {152--164},
  issn = {08966273},
  doi = {10.1016/j.neuron.2016.02.028},
  abstract = {Working memory is thought to result from sustained neuron spiking. However, computational models suggest complex dynamics with discrete oscillatory bursts. We analyzed local field potential (LFP) and spiking from the prefrontal cortex (PFC) of monkeys performing a working memory task. There were brief bursts of narrow-band gamma oscillations (45\textendash 100 Hz), varied in time and frequency, accompanying encoding and re-activation of sensory information. They appeared at a minority of recording sites associated with spiking reflecting the to-beremembered items. Beta oscillations (20\textendash 35 Hz) also occurred in brief, variable bursts but reflected a default state interrupted by encoding and decoding. Only activity of neurons reflecting encoding/ decoding correlated with changes in gamma burst rate. Thus, gamma bursts could gate access to, and prevent sensory interference with, working memory. This supports the hypothesis that working memory is manifested by discrete oscillatory dynamics and spiking, not sustained activity.},
  file = {Lundqvist et al. - 2016 - Gamma and Beta Bursts Underlie Working Memory 2.pdf},
  journal = {Neuron},
  language = {en},
  number = {1}
}

@article{Lupyan2013,
  title = {The Difficulties of Executing Simple Algorithms: {{Why}} Brains Make Mistakes Computers Don't},
  shorttitle = {The Difficulties of Executing Simple Algorithms},
  author = {Lupyan, Gary},
  year = {2013},
  month = dec,
  volume = {129},
  pages = {615--636},
  issn = {00100277},
  doi = {10.1016/j.cognition.2013.08.015},
  abstract = {It is shown that educated adults routinely make errors in placing stimuli into familiar, welldefined categories such as TRIANGLE and ODD NUMBER. Scalene triangles are often rejected as instances of triangles and 798 is categorized by some as an odd number. These patterns are observed both in timed and untimed tasks, hold for people who can fully express the necessary and sufficient conditions for category membership, and for individuals with varying levels of education. A sizeable minority of people believe that 400 is more even than 798 and that an equilateral triangle is the most ``trianglest'' of triangles. Such beliefs predict how people instantiate other categories with necessary and sufficient conditions, e.g., GRANDMOTHER. I argue that the distributed and graded nature of mental representations means that human algorithms, unlike conventional computer algorithms, only approximate rule-based classification and never fully abstract from the specifics of the input. This input-sensitivity is critical to obtaining the kind of cognitive flexibility at which humans excel, but comes at the cost of generally poor abilities to perform context-free computations. If human algorithms cannot be trusted to produce unfuzzy representations of odd numbers, triangles, and grandmothers, the idea that they can be trusted to do the heavy lifting of moment-to-moment cognition that is inherent in the metaphor of mind as digital computer still common in cognitive science, needs to be seriously reconsidered.},
  file = {2013 - Lupyan - The difficulties of executing simple algorithms Why brains make mistakes computers don't.pdf},
  journal = {Cognition},
  language = {en},
  number = {3}
}

@article{Luzzana,
  title = {The Regulation of Oxygen Affinity of Human Haemoglobin},
  author = {Luzzana, M},
  pages = {2},
  file = {1972 - Brindley, Cgraggs - The electrical activity in the motor cortex that accompanices volentary movement.pdf},
  language = {en}
}

@article{Ly2017,
  title = {A {{Tutorial}} on {{Fisher Information}}},
  author = {Ly, Alexander and Marsman, Maarten and Verhagen, Josine and Grasman, Raoul and Wagenmakers, Eric-Jan},
  year = {2017},
  month = may,
  abstract = {In many statistical applications that concern mathematical psychologists, the concept of Fisher information plays an important role. In this tutorial we clarify the concept of Fisher information as it manifests itself across three different statistical paradigms. First, in the frequentist paradigm, Fisher information is used to construct hypothesis tests and confidence intervals using maximum likelihood estimators; second, in the Bayesian paradigm, Fisher information is used to define a default prior; finally, in the minimum description length paradigm, Fisher information is used to measure model complexity.},
  archiveprefix = {arXiv},
  eprint = {1705.01064},
  eprinttype = {arxiv},
  file = {2014 - Ly et al. - A Tutorial on Fisher Information.pdf;Ly et al. - 2017 - A Tutorial on Fisher Information.pdf},
  journal = {arXiv:1705.01064 [math, stat]},
  keywords = {62-01; 62B10 (Primary); 62F03; 62F12; 62F15; 62B10 (Secondary),Mathematics - Statistics Theory},
  language = {en},
  primaryclass = {math, stat}
}

@article{Lydon-Staley2021,
  title = {Hunters, Busybodies and the Knowledge Network Building Associated with Deprivation Curiosity},
  author = {{Lydon-Staley}, David M. and Zhou, Dale and Blevins, Ann Sizemore and Zurn, Perry and Bassett, Danielle S.},
  year = {2021},
  month = mar,
  volume = {5},
  pages = {327--336},
  issn = {2397-3374},
  doi = {10.1038/s41562-020-00985-7},
  file = {Lydon-Staley et al. - 2021 - Hunters, busybodies and the knowledge network buil.pdf},
  journal = {Nat Hum Behav},
  language = {en},
  number = {3}
}

@article{Lydon-Staley2021a,
  title = {Hunters, Busybodies and the Knowledge Network Building Associated with Deprivation Curiosity},
  author = {{Lydon-Staley}, David M. and Zhou, Dale and Blevins, Ann Sizemore and Zurn, Perry and Bassett, Danielle S.},
  year = {2021},
  month = mar,
  volume = {5},
  pages = {327--336},
  issn = {2397-3374},
  doi = {10.1038/s41562-020-00985-7},
  file = {Lydon-Staley et al. - 2021 - Hunters, busybodies and the knowledge network buil 2.pdf},
  journal = {Nat Hum Behav},
  language = {en},
  number = {3}
}

@article{Lynch,
  title = {Computational {{Tradeoffs}} in {{Biological Neural Networks}}: {{Self}}-{{Stabilizing Winner}}-{{Take}}-{{All Networks}}},
  author = {Lynch, Nancy and Musco, Cameron and Parter, Merav},
  pages = {43},
  abstract = {We initiate a line of investigation into biological neural networks from an algorithmic perspective. We develop a simplified but biologically plausible model for distributed computation in stochastic spiking neural networks and study tradeoffs between computation time and network complexity in this model. Our aim is to abstract real neural networks in a way that, while not capturing all interesting features, preserves high-level behavior and allows us to make biologically relevant conclusions.},
  file = {Lynch et al. - Computational Tradeoﬀs in Biological Neural Networ.pdf},
  language = {en}
}

@techreport{Lyu2019,
  title = {Scenes That Produce More Consistent Fixation Maps Are More Memorable},
  author = {Lyu, Muxuan and Choe, Kyoung Whan and Kardan, Omid and Kotabe, Hiroki and Henderson, John M. and Berman, Marc},
  year = {2019},
  month = jul,
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/3e8qm},
  abstract = {Studying factors that contribute to scene memorability is important for understanding human vision and memory. Here we demonstrated in two different eye-tracking datasets that the higher the fixation map consistency (also called inter-observer congruency of fixation maps) of a scene, the higher its memorability is. To provide a mechanistic explanation for how a scene can produce more or less consistent fixation maps across viewers, we created a simple computational model by assuming some high signal regions in a scene that will attract more fixations than other regions (ambient noise). We then varied the amplitude of the signal relative to noise (SNR) to examine the relationship between SNR and fixation map consistency. Our model showed that the higher a scene's SNR, the higher its fixation map consistency, suggesting that fixation map consistency reflects the SNR of a scene, an intrinsic scene property that can affect human vision and memory.},
  file = {Lyu et al. - 2019 - Scenes that produce more consistent fixation maps .pdf},
  language = {en},
  type = {Preprint}
}

@article{Ma2019,
  title = {Sampling Can Be Faster than Optimization},
  author = {Ma, Yi-An and Chen, Yuansi and Jin, Chi and Flammarion, Nicolas and Jordan, Michael I.},
  year = {2019},
  month = oct,
  volume = {116},
  pages = {20881--20885},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1820003116},
  abstract = {Optimization algorithms and Monte Carlo sampling algorithms have provided the computational foundations for the rapid growth in applications of statistical machine learning in recent years. There is, however, limited theoretical understanding of the relationships between these 2 kinds of methodology, and limited understanding of relative strengths and weaknesses. Moreover, existing results have been obtained primarily in the setting of convex functions (for optimization) and log-concave functions (for sampling). In this setting, where local properties determine global properties, optimization algorithms are unsurprisingly more efficient computationally than sampling algorithms. We instead examine a class of nonconvex objective functions that arise in mixture modeling and multistable systems. In this nonconvex setting, we find that the computational complexity of sampling algorithms scales linearly with the model dimension while that of optimization algorithms scales exponentially.},
  file = {Ma et al. - 2019 - Sampling can be faster than optimization.pdf},
  journal = {Proc Natl Acad Sci USA},
  language = {en},
  number = {42}
}

@incollection{Maass1994,
  title = {A {{Comparison}} of the {{Computational Power}} of {{Sigmoid}} and {{Boolean Threshold Circuits}}},
  booktitle = {Theoretical {{Advances}} in {{Neural Computation}} and {{Learning}}},
  author = {Maass, W. and Schnitger, G. and Sontag, E. D.},
  editor = {Roychowdhury, Vwani and Siu, Kai-Yeung and Orlitsky, Alon},
  year = {1994},
  pages = {127--151},
  publisher = {{Springer US}},
  address = {{Boston, MA}},
  doi = {10.1007/978-1-4615-2696-4_4},
  abstract = {We examine the power of constant depth circuits with sigmoid (i.e. smooth) threshold gates for computing boolean functions. It is shown that, for depth 2, constant size circuits of this type are strictly more powerful than constant size boolean threshold circuits (i.e. circuits with linear threshold gates). On the other hand it turns out that, for any constant depth d, polynomial size sigmoid threshold circuits with polynomially bounded weights compute exactly the same boolean functions as the corresponding circuits with linear threshold gates.},
  file = {1994 - Maass, Schnitger, Sontag - A comparison of the computational power of sigmoid and Boolean threshold circuits.pdf},
  isbn = {978-1-4613-6160-2 978-1-4615-2696-4},
  language = {en}
}

@article{Maass2002,
  title = {Real-{{Time Computing Without Stable States}}: {{A New Framework}} for {{Neural Computation Based}} on {{Perturbations}}},
  shorttitle = {Real-{{Time Computing Without Stable States}}},
  author = {Maass, Wolfgang and Natschl{\"a}ger, Thomas and Markram, Henry},
  year = {2002},
  month = nov,
  volume = {14},
  pages = {2531--2560},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/089976602760407955},
  file = {2002 - Maass, Natschläger, Markram - Real-Time Computing Without Stable States A New Framework for Neural Computation Based on Perturba.pdf},
  journal = {Neural Computation},
  language = {en},
  number = {11}
}

@inbook{Maass2011,
  title = {Liquid {{State Machines}}: {{Motivation}}, {{Theory}}, and {{Applications}}},
  shorttitle = {Liquid {{State Machines}}},
  booktitle = {Computability in {{Context}}},
  author = {Maass, Wolfgang},
  year = {2011},
  month = feb,
  pages = {275--296},
  publisher = {{IMPERIAL COLLEGE PRESS}},
  doi = {10.1142/9781848162778_0008},
  collaborator = {Cooper, S Barry and Sorbi, Andrea},
  file = {2010 - Maass - Liquid State Machines Motivation, Theory, and Applications.pdf},
  isbn = {978-1-84816-245-7 978-1-84816-277-8},
  language = {en}
}

@article{Mace2011,
  title = {Functional Ultrasound Imaging of the Brain},
  author = {Mac{\'e}, Emilie and Montaldo, Gabriel and Cohen, Ivan and Baulac, Michel and Fink, Mathias and Tanter, Mickael},
  year = {2011},
  month = aug,
  volume = {8},
  pages = {662--664},
  issn = {1548-7091, 1548-7105},
  doi = {10.1038/nmeth.1641},
  file = {2011 - Macé et al. - Functional ultrasound imaging of the brain.pdf},
  journal = {Nature Methods},
  language = {en},
  number = {8}
}

@article{Machta2013,
  title = {Parameter {{Space Compression Underlies Emergent Theories}} and {{Predictive Models}}},
  author = {Machta, Benjamin B. and Chachra, Ricky and Transtrum, Mark K. and Sethna, James P.},
  year = {2013},
  month = nov,
  volume = {342},
  pages = {604--607},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1238723},
  abstract = {We report a similarity between the microscopic parameter dependance of emergent theories in physics and that of multiparameter models common in other areas of science. In both cases, predictions are possible despite large uncertainties in the microscopic parameters because these details are compressed into just a few governing parameters that are sufficient to describe relevant observables. We make this commonality explicit by examining parameter sensitivity in a hopping model of diffusion and a generalized Ising model of ferromagnetism. We trace the emergence of a smaller effective model to the development of a hierarchy of parameter importance quantified by the eigenvalues of the Fisher Information Matrix. Strikingly, the same hierarchy appears ubiquitously in models taken from diverse areas of science. We conclude that the emergence of effective continuum and universal theories in physics is due to the same parameter space hierarchy that underlies predictive modeling in other areas of science.},
  archiveprefix = {arXiv},
  eprint = {1303.6738},
  eprinttype = {arxiv},
  file = {2013 - Machta et al. - Parameter space compression underlies emergent theories and predictive models.pdf},
  journal = {Science},
  keywords = {Condensed Matter - Statistical Mechanics,Mathematical Physics,Physics - Data Analysis; Statistics and Probability,Quantitative Biology - Other Quantitative Biology},
  language = {en},
  number = {6158}
}

@article{MacIver2010,
  title = {Energy-{{Information Trade}}-{{Offs}} between {{Movement}} and {{Sensing}}},
  author = {MacIver, Malcolm A. and Patankar, Neelesh A. and Shirgaonkar, Anup A.},
  editor = {Friston, Karl J.},
  year = {2010},
  month = may,
  volume = {6},
  pages = {e1000769},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1000769},
  abstract = {While there is accumulating evidence for the importance of the metabolic cost of information in sensory systems, how these costs are traded-off with movement when sensing is closely linked to movement is poorly understood. For example, if an animal needs to search a given amount of space beyond the range of its vision system, is it better to evolve a higher acuity visual system, or evolve a body movement system that can more rapidly move the body over that space? How is this tradeoff dependent upon the three-dimensional shape of the field of sensory sensitivity (hereafter, sensorium)? How is it dependent upon sensorium mobility, either through rotation of the sensorium via muscles at the base of the sense organ (e.g., eye or pinna muscles) or neck rotation, or by whole body movement through space? Here we show that in an aquatic model system, the electric fish, a choice to swim in a more inefficient manner during prey search results in a higher prey encounter rate due to better sensory performance. The increase in prey encounter rate more than counterbalances the additional energy expended in swimming inefficiently. The reduction of swimming efficiency for improved sensing arises because positioning the sensory receptor surface to scan more space per unit time results in an increase in the area of the body pushing through the fluid, increasing wasteful body drag forces. We show that the improvement in sensory performance that occurs with the costly repositioning of the body depends upon having an elongated sensorium shape. Finally, we show that if the fish was able to reorient their sensorium independent of body movement, as fish with movable eyes can, there would be significant energy savings. This provides insight into the ubiquity of sensory organ mobility in animal design. This study exposes important links between the morphology of the sensorium, sensorium mobility, and behavioral strategy for maximally extracting energy from the environment. An ``infomechanical'' approach to complex behavior helps to elucidate how animals distribute functions across sensory systems and movement systems with their diverse energy loads.},
  file = {MacIver et al. - 2010 - Energy-Information Trade-Offs between Movement and.pdf},
  journal = {PLoS Comput Biol},
  language = {en},
  number = {5}
}

@book{MacKay2003,
  title = {Information {{Theory}}, {{Inference}}, and {{Learning Algorithms}}},
  author = {MacKay, David J C},
  year = {2003},
  edition = {Second},
  file = {2003 - Mackay - Information Theory , Inference , and Learning Algorithms.pdf},
  language = {en}
}

@article{MacNulty2014,
  title = {Influence of {{Group Size}} on the {{Success}} of {{Wolves Hunting Bison}}},
  author = {MacNulty, Daniel R and Tallian, Aimee and Stahler, Daniel R and Smith, Douglas W},
  year = {2014},
  volume = {9},
  pages = {8},
  abstract = {An intriguing aspect of social foraging behaviour is that large groups are often no better at capturing prey than are small groups, a pattern that has been attributed to diminished cooperation (i.e., free riding) in large groups. Although this suggests the formation of large groups is unrelated to prey capture, little is known about cooperation in large groups that hunt hard-to-catch prey. Here, we used direct observations of Yellowstone wolves (Canis lupus) hunting their most formidable prey, bison (Bison bison), to test the hypothesis that large groups are more cooperative when hunting difficult prey. We quantified the relationship between capture success and wolf group size, and compared it to previously reported results for Yellowstone wolves hunting elk (Cervus elaphus), a prey that was, on average, 3 times easier to capture than bison. Whereas improvement in elk capture success levelled off at 2\textendash 6 wolves, bison capture success levelled off at 9\textendash 13 wolves with evidence that it continued to increase beyond 13 wolves. These results are consistent with the hypothesis that hunters in large groups are more cooperative when hunting more formidable prey. Improved ability to capture formidable prey could therefore promote the formation and maintenance of large predator groups, particularly among predators that specialize on such prey.},
  file = {MacNulty et al. - 2014 - Influence of Group Size on the Success of Wolves H.PDF},
  journal = {PLOS ONE},
  language = {en},
  number = {11}
}

@article{Macy,
  title = {Learning Dynamics in Social Dilemmas},
  author = {Macy, Michael W and Flache, Andreas},
  pages = {8},
  file = {2002 - Macy, Flache - Learning dynamics in social dilemmas.pdf},
  language = {en}
}

@article{Mahon2010,
  title = {Judging Semantic Similarity: An Event-Related {{fMRI}} Study with Auditory Word Stimuli},
  shorttitle = {Judging Semantic Similarity},
  author = {Mahon, B.Z. and Caramazza, A.},
  year = {2010},
  month = aug,
  volume = {169},
  pages = {279--286},
  issn = {03064522},
  doi = {10.1016/j.neuroscience.2010.04.029},
  abstract = {Much of mental life consists in thinking about object concepts that are not currently within the scope of perception. The general system that enables multiple representations to be maintained and compared is referred to as ``working memory'' [Repov\v{s} G, Baddeley A (2006) Neuroscience 139:5\textendash 21], and involves regions in medial and lateral parietal and frontal cortex [e.g., Smith EE, Jonides J (1999) Science 283:1657\textendash 1661]. It has been assumed that the contents of working memory index information in regions of the brain that are critical for processing and storing object knowledge. To study the processes involved in thinking about common object concepts, we used event related fMRI to study BOLD activity while participants made judgments of conceptual similarity over pairs of sequentially presented auditory words. Through a combination of conventional fMRI analysis approaches and multi-voxel pattern analysis (MVPA), we show that the brain responses associated with the second word in a pair carry information about the conceptual similarity between the two members of the pair. This was the case in frontal and parietal regions involved in the working memory and decision components of the task for both analysis approaches. However, in other regions of the brain, including early visual regions, MVPA permitted classification of semantic distance relationships where conventional averaging approaches failed to show a difference. These findings suggest that diffuse and statistically sub-threshold ``scattering'' of BOLD activity in some regions may carry substantial information about the contents of mental representations. \textcopyright{} 2010 IBRO. Published by Elsevier Ltd. All rights reserved.},
  file = {2010 - Mahon, Caramazza - Judging semantic similarity an event-related fMRI study with auditory word stimuli.pdf},
  journal = {Neuroscience},
  language = {en},
  number = {1}
}

@article{Maia2017,
  title = {An {{Integrative Perspective}} on the {{Role}} of {{Dopamine}} in {{Schizophrenia}}},
  author = {Maia, Tiago V. and Frank, Michael J.},
  year = {2017},
  month = jan,
  volume = {81},
  pages = {52--66},
  issn = {00063223},
  doi = {10.1016/j.biopsych.2016.05.021},
  abstract = {We propose that schizophrenia involves a combination of decreased phasic dopamine responses for relevant stimuli and increased spontaneous phasic dopamine release. Using insights from computational reinforcement-learning models and basic-science studies of the dopamine system, we show that each of these two disturbances contributes to a specific symptom domain and explains a large set of experimental findings associated with that domain. Reduced phasic responses for relevant stimuli help to explain negative symptoms and provide a unified explanation for the following experimental findings in schizophrenia, most of which have been shown to correlate with negative symptoms: reduced learning from rewards; blunted activation of the ventral striatum, midbrain, and other limbic regions for rewards and positive prediction errors; blunted activation of the ventral striatum during reward anticipation; blunted autonomic responding for relevant stimuli; blunted neural activation for aversive outcomes and aversive prediction errors; reduced willingness to expend effort for rewards; and psychomotor slowing. Increased spontaneous phasic dopamine release helps to explain positive symptoms and provides a unified explanation for the following experimental findings in schizophrenia, most of which have been shown to correlate with positive symptoms: aberrant learning for neutral cues (assessed with behavioral and autonomic responses), and aberrant, increased activation of the ventral striatum, midbrain, and other limbic regions for neutral cues, neutral outcomes, and neutral prediction errors. Taken together, then, these two disturbances explain many findings in schizophrenia. We review evidence supporting their co-occurrence and consider their differential implications for the treatment of positive and negative symptoms.},
  file = {Maia and Frank - 2017 - An Integrative Perspective on the Role of Dopamine.pdf},
  journal = {Biological Psychiatry},
  language = {en},
  number = {1}
}

@article{Maimon2009,
  title = {Beyond {{Poisson}}: {{Increased Spike}}-{{Time Regularity}} across {{Primate Parietal Cortex}}},
  shorttitle = {Beyond {{Poisson}}},
  author = {Maimon, Gaby and Assad, John A.},
  year = {2009},
  month = may,
  volume = {62},
  pages = {426--440},
  issn = {08966273},
  doi = {10.1016/j.neuron.2009.03.021},
  abstract = {Cortical areas differ in their patterns of connectivity, cellular composition, and functional architecture. Spike trains, on the other hand, are commonly assumed to follow similarly irregular dynamics across neocortex. We examined spike-time statistics in four parietal areas using a method that accounts for nonstationarities in firing rate. We found that, whereas neurons in visual areas fire irregularly, many cells in association and motor-like parietal regions show increasingly regular spike trains by comparison. Regularity was evident both in the shape of interspike interval distributions and in spike-count variability across trials. Thus, Poisson-like randomness is not a universal feature of neocortex. Rather, many parietal cells have reduced trial-to-trial variability in spike counts that could provide for more reliable firing-rate signals. These results suggest that spiking dynamics may play different roles in different cortical areas and should not be assumed to arise from fundamentally irreducible noise sources.},
  file = {2009 - Maimon, Assad - Beyond Poisson Increased Spike-Time Regularity across Primate Parietal Cortex.pdf},
  journal = {Neuron},
  language = {en},
  number = {3}
}

@article{Mainen1995,
  title = {Reliability of {{Spike Timing}} in {{Neocortical Neurons}}},
  author = {Mainen, Zachary F. and Sejnowski, Terrence J.},
  year = {1995},
  volume = {268},
  pages = {1503--1506},
  file = {2009 - Mainen, Sejnowski - Reliability of Spike Timing in Neocortical Neurons.pdf},
  journal = {Science, New Series},
  number = {5216}
}

@article{Mainen1995a,
  title = {Reliability of Spike Timing in Neocortical Neurons},
  author = {Mainen, Z. and Sejnowski, T.},
  year = {1995},
  month = jun,
  volume = {268},
  pages = {1503--1506},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.7770778},
  file = {Mainen and Sejnowski - 1995 - Reliability of spike timing in neocortical neurons.pdf},
  journal = {Science},
  language = {en},
  number = {5216}
}

@article{Mainen1996,
  title = {Influence of Dendritic Structure on Firing Pattern in Model Neocortical Neurons},
  author = {Mainen, Zachary F. and Sejnowski, Terrence J.},
  year = {1996},
  month = jul,
  volume = {382},
  pages = {363--366},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/382363a0},
  file = {1996 - Mainen, Sejnowski - Influence of dendritic structure on firing pattern in model neocortical neurons(2).pdf},
  journal = {Nature},
  language = {en},
  number = {6589}
}

@article{Majumder2005,
  title = {Enhanced Flow in Carbon Nanotubes},
  author = {Majumder, Mainak and Chopra, Nitin and Andrews, Rodney and Hinds, Bruce J.},
  year = {2005},
  month = nov,
  volume = {438},
  pages = {44--44},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/438044a},
  file = {2005 - Strogatz et al. - Crowd synchrony on the Millennium Bridge.pdf},
  journal = {Nature},
  language = {en},
  number = {7064}
}

@article{Mallet2007,
  title = {Stimulation of Subterritories of the Subthalamic Nucleus Reveals Its Role in the Integration of the Emotional and Motor Aspects of Behavior},
  author = {Mallet, L. and Schupbach, M. and N'Diaye, K. and Remy, P. and Bardinet, E. and Czernecki, V. and Welter, M.-L. and Pelissolo, A. and Ruberg, M. and Agid, Y. and Yelnik, J.},
  year = {2007},
  month = jun,
  volume = {104},
  pages = {10661--10666},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.0610849104},
  file = {2007 - Mallet et al. - Stimulation of subterritories of the subthalamic nucleus reveals its role in the integration of the emotional and.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {25}
}

@article{Mallet2012,
  title = {Dichotomous {{Organization}} of the {{External Globus Pallidus}}},
  author = {Mallet, Nicolas and Micklem, Benjamin R. and Henny, Pablo and Brown, Matthew T. and Williams, Claire and Bolam, J. Paul and Nakamura, Kouichi C. and Magill, Peter J.},
  year = {2012},
  month = jun,
  volume = {74},
  pages = {1075--1086},
  issn = {08966273},
  doi = {10.1016/j.neuron.2012.04.027},
  abstract = {Different striatal projection neurons are the origin of a dual organization essential for basal ganglia function. We have defined an analogous division of labor in the external globus pallidus (GPe) of Parkinsonian rats, showing that the distinct temporal activities of two populations of GPe neuron in vivo are underpinned by distinct molecular profiles and axonal connectivities. A first population of prototypic GABAergic GPe neurons fire antiphase to subthalamic nucleus (STN) neurons, often express parvalbumin, and target downstream basal ganglia nuclei, including STN. In contrast, a second population (arkypallidal neurons) fire in-phase with STN neurons, express preproenkephalin, and only innervate the striatum. This novel cell type provides the largest extrinsic GABAergic innervation of striatum, targeting both projection neurons and interneurons. We conclude that GPe exhibits several core components of a dichotomous organization as fundamental as that in striatum. Thus, two populations of GPe neuron together orchestrate activities across all basal ganglia nuclei in a cell-type-specific manner.},
  file = {2012 - Mallet et al. - Dichotomous Organization of the External Globus Pallidus.pdf},
  journal = {Neuron},
  language = {en},
  number = {6}
}

@article{Mamou,
  title = {Emergence of {{Separable Manifolds}} in {{Deep Language Representations}}},
  author = {Mamou, Jonathan and Le, Hang and Rio, Miguel A Del and Stephenson, Cory and Tang, Hanlin and Kim, Yoon and Chung, SueYeon},
  pages = {11},
  file = {Mamou et al. - Emergence of Separable Manifolds in Deep Language .pdf},
  language = {en}
}

@article{Mandelbrot1963,
  title = {New {{Methods}} in {{Statistical Economics}}},
  author = {Mandelbrot, Benoit},
  year = {1963},
  volume = {71},
  pages = {421--440},
  file = {Mandelbrot - 1963 - New Methods in Statistical Economics.pdf},
  journal = {Journal of Political Economy},
  language = {en},
  number = {5}
}

@article{Mandelbrot1968,
  title = {Fractional {{Brownian Motions}}, {{Fractional Noises}} and {{Applications}}},
  author = {Mandelbrot, Benoit B. and Ness, John W. Van},
  year = {1968},
  volume = {10},
  pages = {422--437},
  file = {1968 - Mandelbrot, Van Ness - Fractional Brownian Motions, Fractional Noises and Applications.pdf},
  journal = {SIAM Review},
  language = {en},
  number = {4}
}

@article{Mangalam2021,
  title = {Point Estimates, {{Simpson}}'s Paradox, and Nonergodicity in Biological Sciences},
  author = {Mangalam, Madhur and {Kelty-Stephen}, Damian G.},
  year = {2021},
  month = jun,
  volume = {125},
  pages = {98--107},
  issn = {01497634},
  doi = {10.1016/j.neubiorev.2021.02.017},
  abstract = {Modern biomedical, behavioral and psychological inference about cause-effect relationships respects an ergodic assumption, that is, that mean response of representative samples allow predictions about individual members of those samples. Recent empirical evidence in all of the same fields indicates systematic violations of the ergodic assumption. Indeed, violation of ergodicity in biomedical, behavioral and psychological causes is precisely the inspiration behind our research inquiry. Here, we review the long term costs to scientific progress in these do\- mains and a practical way forward. Specifically, we advocate using statistical measures that can themselves encode the degree and type of nonergodicity in measurements. Taking such steps will lead to a paradigm shift, allowing researchers to investigate the nonstationary, far-from-equilibrium processes that characterize the creativity and emergence of biological and psychological behavior.},
  file = {Mangalam and Kelty-Stephen - 2021 - Point estimates, Simpson’s paradox, and nonergodic.pdf},
  journal = {Neuroscience \& Biobehavioral Reviews},
  language = {en}
}

@article{Mania,
  title = {Simple Random Search Provides a Competitive Approach to Reinforcement Learning},
  author = {Mania, Horia and Guy, Aurelia and Recht, Benjamin},
  pages = {22},
  abstract = {A common belief in model-free reinforcement learning is that methods based on random search in the parameter space of policies exhibit significantly worse sample complexity than those that explore the space of actions. We dispel such beliefs by introducing a random search method for training static, linear policies for continuous control problems, matching state-ofthe-art sample efficiency on the benchmark MuJoCo locomotion tasks. Our method also finds a nearly optimal controller for a challenging instance of the Linear Quadratic Regulator, a classical problem in control theory, when the dynamics are not known. Computationally, our random search algorithm is at least 15 times more efficient than the fastest competing model-free methods on these benchmarks. We take advantage of this computational efficiency to evaluate the performance of our method over hundreds of random seeds and many different hyperparameter configurations for each benchmark task. Our simulations highlight a high variability in performance in these benchmark tasks, suggesting that commonly used estimations of sample efficiency do not adequately evaluate the performance of RL algorithms.},
  file = {Mania et al. - Simple random search provides a competitive approa.pdf},
  language = {en}
}

@article{Mante2013,
  title = {Context-Dependent Computation by Recurrent Dynamics in Prefrontal Cortex},
  author = {Mante, Valerio and Sussillo, David and Shenoy, Krishna V. and Newsome, William T.},
  year = {2013},
  month = nov,
  volume = {503},
  pages = {78--84},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature12742},
  file = {2013 - Mante et al. - Context-dependent computation by recurrent dynamics in prefrontal cortex.pdf},
  journal = {Nature},
  language = {en},
  number = {7474}
}

@article{Manwani1999,
  title = {Detecting and {{Estimating Signals}} in {{Noisy Cable Structures}}, {{II}}: {{Information Theoretical Analysis}}},
  shorttitle = {Detecting and {{Estimating Signals}} in {{Noisy Cable Structures}}, {{II}}},
  author = {Manwani, Amit and Koch, Christof},
  year = {1999},
  month = nov,
  volume = {11},
  pages = {1831--1873},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/089976699300015981},
  file = {1999 - Manwani, Koch - Detecting and estimating signals in noisy cable structures, II information theoretical analysis.pdf},
  journal = {Neural Computation},
  language = {en},
  number = {8}
}

@article{Mao,
  title = {Modeling the {{Role}} of the {{Basal Ganglia}} in {{Motor Control}} and {{Motor Programming}}},
  author = {Mao, Zhi-Hong},
  pages = {166},
  abstract = {The basal ganglia (BG) are a group of highly interconnected nuclei buried deep in the brain. They are involved in an important range of brain functions, including both lower-level movement control and higher-level cognitive decision making. Dysfunction of the BG has been linked to the human neurological disorders such as Parkinson's disease, Huntington's disease, and schizophrenia.},
  file = {2005 - Mao - Modeling the Role of the Basal Ganglia in Motor Control and Motor Programming.pdf},
  language = {en}
}

@article{Marblestone2016,
  title = {Towards an Integration of Deep Learning and Neuroscience},
  author = {Marblestone, Adam Henry and Wayne, Greg and Kording, Konrad P},
  year = {2016},
  month = aug,
  doi = {10.1101/058545},
  abstract = {Neuroscience has focused on the detailed implementation of computation, studying neural codes, dynamics and circuits. In machine learning, however, artificial neural networks tend to eschew precisely designed codes, dynamics or circuits in favor of brute force optimization of a cost function, often using simple and relatively uniform initial architectures. Two recent developments have emerged within machine learning that create an opportunity to connect these seemingly divergent perspectives. First, structured architectures are used, including dedicated systems for attention, recursion and various forms of short- and long-term memory storage. Second, cost functions and training procedures have become more complex and are varied across layers and over time. Here we think about the brain in terms of these ideas. We hypothesize that (1) the brain optimizes cost functions, (2) these cost functions are diverse and differ across brain locations and over development, and (3) optimization operates within a pre-structured architecture matched to the computational problems posed by behavior. Such a heterogeneously optimized system, enabled by a series of interacting cost functions, serves to make learning data-efficient and precisely targeted to the needs of the organism. We suggest directions by which neuroscience could seek to refine and test these hypotheses.},
  file = {Marblestone et al. - 2016 - Towards an integration of deep learning and neuros.pdf},
  journal = {bioRxiv},
  language = {en}
}

@article{Marchiori2008,
  title = {Predicting {{Human Interactive Learning}} by {{Regret}}-{{Driven Neural Networks}}},
  author = {Marchiori, D. and Warglien, M.},
  year = {2008},
  month = feb,
  volume = {319},
  pages = {1111--1113},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1151185},
  file = {2008 - Marchiori, Warglien - Predicting human interactive learning by regret-driven neural networks.pdf},
  journal = {Science},
  language = {en},
  number = {5866}
}

@article{Marcus2014,
  title = {The Atoms of Neural Computation},
  author = {Marcus, G. and Marblestone, A. and Dean, T.},
  year = {2014},
  month = oct,
  volume = {346},
  pages = {551--552},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1261661},
  file = {Marcus et al. - 2014 - The atoms of neural computation.pdf},
  journal = {Science},
  language = {en},
  number = {6209}
}

@article{Marcus2014a,
  title = {The Atoms of Neural Computation},
  author = {Marcus, G. and Marblestone, A. and Dean, T.},
  year = {2014},
  month = oct,
  volume = {346},
  pages = {551--552},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1261661},
  file = {Marcus et al. - 2014 - The atoms of neural computation 2.pdf},
  journal = {Science},
  language = {en},
  number = {6209}
}

@article{Marder2007,
  title = {Understanding {{Circuit Dynamics Using}} the {{Stomatogastric Nervous System}} of {{Lobsters}} and {{Crabs}}},
  author = {Marder, Eve and Bucher, Dirk},
  year = {2007},
  month = mar,
  volume = {69},
  pages = {291--316},
  issn = {0066-4278, 1545-1585},
  doi = {10.1146/annurev.physiol.69.031905.161516},
  abstract = {Studies of the stomatogastric nervous systems of lobsters and crabs have led to numerous insights into the cellular and circuit mechanisms that generate rhythmic motor patterns. The small number of easily identifiable neurons allowed the establishment of connectivity diagrams among the neurons of the stomatogastric ganglion. We now know that (a) neuromodulatory substances reconfigure circuit dynamics by altering synaptic strength and voltage-dependent conductances and (b) individual neurons can switch among different functional circuits. Computational and experimental studies of single-neuron and network homeostatic regulation have provided insight into compensatory mechanisms that can underlie stable network performance. Many of the observations first made using the stomatogastric nervous system can be generalized to other invertebrate and vertebrate circuits.},
  file = {Marder and Bucher - 2007 - Understanding Circuit Dynamics Using the Stomatoga.pdf},
  journal = {Annual Review of Physiology},
  language = {en},
  number = {1}
}

@article{Marder2014,
  title = {Neuromodulation of {{Circuits}} with {{Variable Parameters}}: {{Single Neurons}} and {{Small Circuits Reveal Principles}} of {{State}}-{{Dependent}} and {{Robust Neuromodulation}}},
  shorttitle = {Neuromodulation of {{Circuits}} with {{Variable Parameters}}},
  author = {Marder, Eve and O'Leary, Timothy and Shruti, Sonal},
  year = {2014},
  month = jul,
  volume = {37},
  pages = {329--346},
  issn = {0147-006X, 1545-4126},
  doi = {10.1146/annurev-neuro-071013-013958},
  file = {2014 - Marder, O'Leary, Shruti - Neuromodulation of Circuits with Variable Parameters Single Neurons and Small Circuits Reveal Principle.pdf;Marder et al. - 2014 - Neuromodulation of Circuits with Variable Paramete.pdf},
  journal = {Annual Review of Neuroscience},
  language = {en},
  number = {1}
}

@article{Marder2015,
  title = {Robust Circuit Rhythms in Small Circuits Arise from Variable Circuit Components and Mechanisms},
  author = {Marder, Eve and Goeritz, Marie L and Otopalik, Adriane G},
  year = {2015},
  month = apr,
  volume = {31},
  pages = {156--163},
  issn = {09594388},
  doi = {10.1016/j.conb.2014.10.012},
  file = {Marder et al. - 2015 - Robust circuit rhythms in small circuits arise fro 2.pdf;Marder et al. - 2015 - Robust circuit rhythms in small circuits arise fro.pdf},
  journal = {Current Opinion in Neurobiology},
  language = {en}
}

@article{Marinelli2014,
  title = {Heterogeneity of Dopamine Neuron Activity across Traits and States},
  author = {Marinelli, M. and McCutcheon, J.E.},
  year = {2014},
  month = dec,
  volume = {282},
  pages = {176--197},
  issn = {03064522},
  doi = {10.1016/j.neuroscience.2014.07.034},
  abstract = {Midbrain dopamine neurons fire irregularly, with interspersed clusters of high-frequency spikes, commonly called `bursts'. In this review we examine such heterogeneity in activity, and provide insight into how it can participate in psychiatric conditions such as drug addiction. We first describe several techniques used to evaluate dopamine neuron activity, and comment on the different measures that each provides. We next describe the activity of dopamine neurons in `basal' conditions. Specifically, we discuss how the use of anesthesia and reduced preparations may alter aspects of dopamine cell activity, and how there is heterogeneity across species and regions. We also describe how dopamine cell firing changes throughout the peri-adolescent period and how dopamine neuron activity differs across the population. In the final section, we discuss how dopamine neuron activity changes in response to life events. First, we focus attention on drugs of abuse. Drugs themselves change firing activity through a variety of mechanisms, with effects on firing while drug is present differing from those seen after drug discontinuation. We then review how stimuli that are rewarding, aversive, or salient can evoke changes in firing rate and discharge pattern of dopamine neurons, and provide behavioral relevance of dopamine signaling. Finally, we discuss how stress can modulate dopamine neuron firing and how this may contribute to the role that stressful experiences play in psychiatric disorders such as addiction and depression.},
  file = {Marinelli and McCutcheon - 2014 - Heterogeneity of dopamine neuron activity across t.pdf},
  journal = {Neuroscience},
  language = {en}
}

@article{Markowitz2008,
  title = {Rate-Specific Synchrony: {{Using}} Noisy Oscillations to Detect Equally Active Neurons},
  shorttitle = {Rate-Specific Synchrony},
  author = {Markowitz, D. A. and Collman, F. and Brody, C. D. and Hopfield, J. J. and Tank, D. W.},
  year = {2008},
  month = jun,
  volume = {105},
  pages = {8422--8427},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.0803183105},
  file = {2008 - Markowitz et al. - Rate-specific synchrony using noisy oscillations to detect equally active neurons.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {24}
}

@article{Markram1997,
  title = {Regulation of {{Synaptic Efficacy}} by {{Coincidence}} of {{Postsynaptic APs}} and {{EPSPs}}},
  author = {Markram, H.},
  year = {1997},
  month = jan,
  volume = {275},
  pages = {213--215},
  issn = {00368075, 10959203},
  doi = {10.1126/science.275.5297.213},
  file = {1997 - Markram et al. - Regulation of synaptic efficacy by coincidence of postsynaptic APs and EPSPs.pdf},
  journal = {Science},
  language = {en},
  number = {5297}
}

@article{Markram2015,
  title = {Reconstruction and {{Simulation}} of {{Neocortical Microcircuitry}}},
  author = {Markram, Henry and Muller, Eilif and Ramaswamy, Srikanth and Reimann, Michael W. and Abdellah, Marwan and Sanchez, Carlos Aguado and Ailamaki, Anastasia and {Alonso-Nanclares}, Lidia and Antille, Nicolas and Arsever, Selim and Kahou, Guy Antoine Atenekeng and Berger, Thomas K. and Bilgili, Ahmet and Buncic, Nenad and Chalimourda, Athanassia and Chindemi, Giuseppe and Courcol, Jean-Denis and Delalondre, Fabien and Delattre, Vincent and Druckmann, Shaul and Dumusc, Raphael and Dynes, James and Eilemann, Stefan and Gal, Eyal and Gevaert, Michael Emiel and Ghobril, Jean-Pierre and Gidon, Albert and Graham, Joe W. and Gupta, Anirudh and Haenel, Valentin and Hay, Etay and Heinis, Thomas and Hernando, Juan B. and Hines, Michael and Kanari, Lida and Keller, Daniel and Kenyon, John and Khazen, Georges and Kim, Yihwa and King, James G. and Kisvarday, Zoltan and Kumbhar, Pramod and Lasserre, S{\'e}bastien and Le B{\'e}, Jean-Vincent and Magalh{\~a}es, Bruno R.C. and {Merch{\'a}n-P{\'e}rez}, Angel and Meystre, Julie and Morrice, Benjamin Roy and Muller, Jeffrey and {Mu{\~n}oz-C{\'e}spedes}, Alberto and Muralidhar, Shruti and Muthurasa, Keerthan and Nachbaur, Daniel and Newton, Taylor H. and Nolte, Max and Ovcharenko, Aleksandr and Palacios, Juan and Pastor, Luis and Perin, Rodrigo and Ranjan, Rajnish and Riachi, Imad and Rodr{\'i}guez, Jos{\'e}-Rodrigo and Riquelme, Juan Luis and R{\"o}ssert, Christian and Sfyrakis, Konstantinos and Shi, Ying and Shillcock, Julian C. and Silberberg, Gilad and Silva, Ricardo and Tauheed, Farhan and Telefont, Martin and {Toledo-Rodriguez}, Maria and Tr{\"a}nkler, Thomas and Van Geit, Werner and D{\'i}az, Jafet Villafranca and Walker, Richard and Wang, Yun and Zaninetta, Stefano M. and DeFelipe, Javier and Hill, Sean L. and Segev, Idan and Sch{\"u}rmann, Felix},
  year = {2015},
  month = oct,
  volume = {163},
  pages = {456--492},
  issn = {00928674},
  doi = {10.1016/j.cell.2015.09.029},
  file = {2015 - Markram - Reconstruction and Simulation of Neocortical Microcircuitry.pdf;Markram et al. - 2015 - Reconstruction and Simulation of Neocortical Micro.pdf},
  journal = {Cell},
  language = {en},
  number = {2}
}

@article{Marquand2011,
  title = {Pattern {{Classification}} of {{Working Memory Networks Reveals Differential Effects}} of {{Methylphenidate}}, {{Atomoxetine}} and {{Placebo}} in {{Healthy Volunteers}}},
  author = {Marquand, Andre F and De Simoni, Sara and O'Daly, Owen G and Williams, Steven CR and {Mour{\~a}o-Miranda}, Janaina and Mehta, Mitul A},
  year = {2011},
  month = may,
  volume = {36},
  pages = {1237--1247},
  issn = {0893-133X, 1740-634X},
  doi = {10.1038/npp.2011.9},
  file = {2011 - Marquand et al. - Pattern classification of working memory networks reveals differential effects of methylphenidate, atomoxetine,.pdf},
  journal = {Neuropsychopharmacology},
  language = {en},
  number = {6}
}

@article{Marques2019,
  title = {Internal State Dynamics Shape Brainwide Activity and Foraging Behaviour},
  author = {Marques, Joao and Meng, Li and Schaak, Diane and Robson, Drew and Li, Jennifer},
  year = {2019},
  pages = {27},
  file = {Joao et al - 2019 - Internal state dynamics shape brainwide activity a.pdf},
  journal = {Nature},
  language = {en}
}

@article{Marques2020,
  title = {Internal State Dynamics Shape Brainwide Activity and Foraging Behaviour},
  author = {Marques, Jo{\~a}o C. and Li, Meng and Schaak, Diane and Robson, Drew N. and Li, Jennifer M.},
  year = {2020},
  month = jan,
  volume = {577},
  pages = {239--243},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/s41586-019-1858-z},
  file = {Marques et al. - 2020 - Internal state dynamics shape brainwide activity a.pdf},
  journal = {Nature},
  language = {en},
  number = {7789}
}

@article{Marreiros2009,
  title = {Population Dynamics under the {{Laplace}} Assumption},
  author = {Marreiros, Andr{\'e} C. and Kiebel, Stefan J. and Daunizeau, Jean and Harrison, Lee M. and Friston, Karl J.},
  year = {2009},
  month = feb,
  volume = {44},
  pages = {701--714},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2008.10.008},
  abstract = {In this paper, we describe a generic approach to modelling dynamics in neuronal populations. This approach models a full density on the states of neuronal populations but finesses this high-dimensional problem by reformulating density dynamics in terms of ordinary differential equations on the sufficient statistics of the densities considered (c.f., the method of moments). The particular form for the population density we adopt is a Gaussian density (c.f., the Laplace assumption). This means population dynamics are described by equations governing the evolution of the population's mean and covariance. We derive these equations from the Fokker-Planck formalism and illustrate their application to a conductance-based model of neuronal exchanges. One interesting aspect of this formulation is that we can uncouple the mean and covariance to furnish a neural-mass model, which rests only on the populations mean. This enables us to compare equivalent mean-field and neural-mass models of the same populations and evaluate, quantitatively, the contribution of population variance to the expected dynamics. The mean-field model presented here will form the basis of a dynamic causal model of observed electromagnetic signals in future work.},
  file = {2009 - Marreiros et al. - Population dynamics under the Laplace assumption.pdf},
  journal = {NeuroImage},
  language = {en},
  number = {3}
}

@article{Martens2009,
  title = {Exact Results for the {{Kuramoto}} Model with a Bimodal Frequency Distribution},
  author = {Martens, E. A. and Barreto, E. and Strogatz, S. H. and Ott, E. and So, P. and Antonsen, T. M.},
  year = {2009},
  month = feb,
  volume = {79},
  issn = {1539-3755, 1550-2376},
  doi = {10.1103/PhysRevE.79.026204},
  file = {2009 - Martens et al. - Exact results for the Kuramoto model with a bimodal frequency distribution.pdf},
  journal = {Physical Review E},
  language = {en},
  number = {2}
}

@article{Marti2018,
  title = {Correlations between Synapses in Pairs of Neurons Slow down Dynamics in Randomly Connected Neural Networks},
  author = {Mart{\'i}, Daniel and Brunel, Nicolas and Ostojic, Srdjan},
  year = {2018},
  month = jun,
  volume = {97},
  issn = {2470-0045, 2470-0053},
  doi = {10.1103/PhysRevE.97.062314},
  file = {Martí et al. - 2018 - Correlations between synapses in pairs of neurons .pdf},
  journal = {Physical Review E},
  language = {en},
  number = {6}
}

@article{Martin2018,
  title = {Differential Contributions of Subthalamic Beta Rhythms and 1/f Broadband Activity to Motor Symptoms in {{Parkinson}}'s Disease},
  author = {Martin, Stephanie and Iturrate, I{\~n}aki and Chavarriaga, Ricardo and Leeb, Robert and Sobolewski, Aleksander and Li, Andrew M. and Zaldivar, Julien and {Peciu-Florianu}, Iulia and Pralong, Etienne and {Castro-Jim{\'e}nez}, Mayte and Benninger, David and Vingerhoets, Fran{\c c}ois and Knight, Robert T. and Bloch, Jocelyne and Mill{\'a}n, Jos{\'e} del R.},
  year = {2018},
  month = dec,
  volume = {4},
  issn = {2373-8057},
  doi = {10.1038/s41531-018-0068-y},
  file = {Martin et al. - 2018 - Differential contributions of subthalamic beta rhy.pdf},
  journal = {npj Parkinson's Disease},
  language = {en},
  number = {1}
}

@article{Martinez-Ramon2006,
  title = {{{fMRI}} Pattern Classification Using Neuroanatomically Constrained Boosting},
  author = {{Mart{\'i}nez-Ram{\'o}n}, Manel and Koltchinskii, Vladimir and Heileman, Gregory L. and Posse, Stefan},
  year = {2006},
  month = jul,
  volume = {31},
  pages = {1129--1141},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2006.01.022},
  file = {2006 - Martínez-Ramón et al. - fMRI pattern classification using neuroanatomically constrained boosting.pdf},
  journal = {NeuroImage},
  language = {en},
  number = {3}
}

@article{Marvel2009,
  title = {Energy {{Landscape}} of {{Social Balance}}},
  author = {Marvel, Seth A. and Strogatz, Steven H. and Kleinberg, Jon M.},
  year = {2009},
  month = nov,
  volume = {103},
  issn = {0031-9007, 1079-7114},
  doi = {10.1103/PhysRevLett.103.198701},
  file = {2009 - Marvel, Strogatz, Kleinberg - Energy landscape of social balance.pdf},
  journal = {Physical Review Letters},
  language = {en},
  number = {19}
}

@article{Marvel2011,
  title = {Continuous-Time Model of Structural Balance},
  author = {Marvel, Seth A. and Kleinberg, Jon and Kleinberg, Robert D. and Strogatz, Steven H.},
  year = {2011},
  month = feb,
  volume = {108},
  pages = {1771--1776},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1013213108},
  file = {2011 - Marvel et al. - Continuous-time model of structural balance.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {5}
}

@article{Marvel2012,
  title = {Encouraging {{Moderation}}: {{Clues}} from a {{Simple Model}} of {{Ideological Conflict}}},
  shorttitle = {Encouraging {{Moderation}}},
  author = {Marvel, Seth A. and Hong, Hyunsuk and Papush, Anna and Strogatz, Steven H.},
  year = {2012},
  month = sep,
  volume = {109},
  issn = {0031-9007, 1079-7114},
  doi = {10.1103/PhysRevLett.109.118702},
  file = {2012 - Marvel et al. - Encouraging moderation Clues from a simple model of ideological conflict.pdf},
  journal = {Physical Review Letters},
  language = {en},
  number = {11}
}

@article{Marzen2015,
  title = {Time {{Resolution Dependence}} of {{Information Measures}} for {{Spiking Neurons}}: {{Atoms}}, {{Scaling}}, and {{Universality}}},
  shorttitle = {Time {{Resolution Dependence}} of {{Information Measures}} for {{Spiking Neurons}}},
  author = {Marzen, Sarah E. and DeWeese, Michael R. and Crutchfield, James P.},
  year = {2015},
  month = apr,
  abstract = {The mutual information between stimulus and spike-train response is commonly used to monitor neural coding efficiency, but neuronal computation broadly conceived requires more refined and targeted information measures of input-output joint processes. A first step towards that larger goal is to develop information measures for individual output processes, including information generation (entropy rate), stored information (statistical complexity), predictable information (excess entropy), and active information accumulation (bound information rate). We calculate these for spike trains generated by a variety of noise-driven integrate-and-fire neurons as a function of time resolution and for alternating renewal processes. We show that their time-resolution dependence reveals coarsegrained structural properties of interspike interval statistics; e.g., {$\tau$} -entropy rates that diverge less quickly than the firing rate indicate interspike interval correlations. We also find evidence that the excess entropy and regularized statistical complexity of different types of integrate-and-fire neurons are universal in the continuous-time limit in the sense that they do not depend on mechanism details. This suggests a surprising simplicity in the spike trains generated by these model neurons. Interestingly, neurons with gamma-distributed ISIs and neurons whose spike trains are alternating renewal processes do not fall into the same universality class. These results lead to two conclusions. First, the dependence of information measures on time resolution reveals mechanistic details about spike train generation. Second, information measures can be used as model selection tools for analyzing spike train processes.},
  archiveprefix = {arXiv},
  eprint = {1504.04756},
  eprinttype = {arxiv},
  file = {2015 - Marzen, DeWeese, Crutchfield - Time resolution dependence of information measures for spiking neurons scaling and universality.pdf;Marzen et al. - 2015 - Time Resolution Dependence of Information Measures.pdf},
  journal = {arXiv:1504.04756 [cond-mat, physics:nlin, q-bio]},
  keywords = {Computer Science - Neural and Evolutionary Computing,Condensed Matter - Disordered Systems and Neural Networks,Mathematics - Probability,Nonlinear Sciences - Chaotic Dynamics,Quantitative Biology - Neurons and Cognition},
  language = {en},
  primaryclass = {cond-mat, physics:nlin, q-bio}
}

@article{Marzen2016,
  title = {Weak Universality in Sensory Tradeoffs},
  author = {Marzen, Sarah and DeDeo, Simon},
  year = {2016},
  month = dec,
  volume = {94},
  issn = {2470-0045, 2470-0053},
  doi = {10.1103/PhysRevE.94.060101},
  file = {Marzen and DeDeo - 2016 - Weak universality in sensory tradeoffs.pdf},
  journal = {Physical Review E},
  language = {en},
  number = {6}
}

@article{Massey2012,
  title = {High Resolution {{MR}} Anatomy of the Subthalamic Nucleus: {{Imaging}} at 9.{{4T}} with Histological Validation},
  shorttitle = {High Resolution {{MR}} Anatomy of the Subthalamic Nucleus},
  author = {Massey, L.A. and Miranda, M.A. and Zrinzo, L. and {Al-Helli}, O. and Parkes, H.G. and Thornton, J.S. and So, P.-W. and White, M.J. and Mancini, L. and Strand, C. and Holton, J.L. and Hariz, M.I. and Lees, A.J. and Revesz, T. and Yousry, T.A.},
  year = {2012},
  month = feb,
  volume = {59},
  pages = {2035--2044},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2011.10.016},
  abstract = {Using conventional MRI the subthalamic nucleus (STN) is not clearly defined. Our objective was to define the anatomy of the STN using 9.4 T MRI of post mortem tissue with histological validation. Spin-echo (SE) and 3D gradient-echo (GE) images were obtained at 9.4 T in 8 post mortem tissue blocks and compared directly with corresponding histological slides prepared with Luxol Fast Blue/Cresyl Violet (LFB/CV) in 4 cases and Perl stain in 3. The variability of the STN anatomy was studied using internal reference points. The anatomy of the STN and surrounding structures was demonstrated in all three anatomical planes using 9.4 T MR images in concordance with LFB/CV stained histological sections. Signal hypointensity was seen in 6/8 cases in the anterior and medial STN that corresponded with regions of more intense Perl staining. There was significant variability in the volume, shape and location of the borders of the STN. Using 9.4 T MRI, the internal signal characteristics and borders of the STN are clearly defined and significant anatomical variability is apparent. Direct visualisation of the STN is possible using high field MRI and this is particularly relevant, given its anatomical variability, for planning deep brain stimulation.},
  file = {2012 - Massey et al. - High resolution MR anatomy of the subthalamic nucleus Imaging at 9.4T with histological validation.pdf},
  journal = {NeuroImage},
  language = {en},
  number = {3}
}

@article{Mastro2017,
  title = {Cell-Specific Pallidal Intervention Induces Long-Lasting Motor Recovery in Dopamine-Depleted Mice},
  author = {Mastro, Kevin J and Zitelli, Kevin T and Willard, Amanda M and Leblanc, Kimberly H and Kravitz, Alexxai V and Gittis, Aryn H},
  year = {2017},
  month = jun,
  volume = {20},
  pages = {815--823},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.4559},
  file = {Mastro et al. - 2017 - Cell-specific pallidal intervention induces long-l.pdf},
  journal = {Nature Neuroscience},
  language = {en},
  number = {6}
}

@article{Mather,
  title = {Exploration, {{Play}}, and {{Habituation}} in {{Octopuses}} ({{Octopus}} Dofleini)},
  author = {Mather, Jennifer A and Anderson, Roland C},
  pages = {6},
  file = {Mather and Anderson - Exploration, Play, and Habituation in Octopuses (O.pdf},
  language = {en}
}

@article{Mathera,
  title = {What Is in an Octopus's Mind?},
  author = {Mather, Jennifer},
  pages = {29},
  abstract = {It is difficult to imagine what an animal as different from us as the octopus `thinks', but we can make some progress. In the Umwelt or perceptual world of an octopus, what the lateralized monocular eyes perceive is not color but the plane of polarization of light. Information is processed by a bilateral brain but manipulation is done by a radially symmetrical set of eight arms. Octopuses do not self-monitor by vision. Their skin pattern system, used for excellent camouflage, is open loop. The output of the motor system of the eight arms is organized at several levels \textemdash{} brain, intrabrachial commissure and local brachial ganglia. Octopuses may be motivated by a combination of fear and exploration. Several actions \textemdash{} a head bob for motion parallax, a `Passing Cloud' skin display to startle prey, and particularly exploration by their arms \textemdash{} demonstrate the presence of a controlling mind, motivated to gather information. Yet most octopuses are solitary and many are cannibalistic, so they must always be on guard, even against conspecifics. The actions of octopuses can be domain general, with flexible problem-solving strategies, enabling them to survive ``by their wits'' in a challenging and variable environment.},
  file = {Mather - What is in an octopus’s mind.pdf},
  language = {en}
}

@article{Mathewson2009,
  title = {To {{See}} or {{Not}} to {{See}}: {{Prestimulus Phase Predicts Visual Awareness}}},
  shorttitle = {To {{See}} or {{Not}} to {{See}}},
  author = {Mathewson, K. E. and Gratton, G. and Fabiani, M. and Beck, D. M. and Ro, T.},
  year = {2009},
  month = mar,
  volume = {29},
  pages = {2725--2732},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.3963-08.2009},
  file = {2009 - Mathewson et al. - To See or Not to See Prestimulus α Phase Predicts Visual Awareness.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {9}
}

@article{Mathewson2017,
  title = {High and Dry? {{Comparing}} Active Dry {{EEG}} Electrodes to Active and Passive Wet Electrodes: {{Active}} Dry vs. Active \& Passive Wet {{EEG}} Electrodes},
  shorttitle = {High and Dry?},
  author = {Mathewson, Kyle E. and Harrison, Tyler J. L. and Kizuk, Sayeed A. D.},
  year = {2017},
  month = jan,
  volume = {54},
  pages = {74--82},
  issn = {00485772},
  doi = {10.1111/psyp.12536},
  abstract = {Dry electrodes are becoming popular for both lab-based and consumer-level electrophysiological-recording technologies because they better afford the ability to move traditional lab-based research into the real world. It is unclear, however, how dry electrodes compare in data quality to traditional electrodes. The current study compared three EEG electrode types: (a) passive-wet electrodes with no onboard amplification, (b) actively amplified, wet electrodes with moderate impedance levels, and low impedance levels, and (c) active-dry electrodes with very high impedance. Participants completed a classic P3 auditory oddball task to elicit characteristic EEG signatures and eventrelated potentials (ERPs). Across the three electrode types, we compared single-trial noise, average ERPs, scalp topographies, ERP noise, and ERP statistical power as a function of number of trials. We extended past work showing active electrodes' insensitivity to moderate levels of interelectrode impedance when compared to passive electrodes in the same amplifier. Importantly, the new dry electrode system could reliably measure EEG spectra and ERP components comparable to traditional electrode types. As expected, however, dry active electrodes with very high interelectrode impedance exhibited marked increases in single-trial and average noise levels, which decreased statistical power, requiring more trials to detect significant effects. This power decrease must be considered as a tradeoff with the ease of application and long-term use. The current results help set constraints on experimental design with novel dry electrodes, and provide important evidence needed to measure brain activity in novel settings and situations.},
  file = {Mathewson et al. - 2017 - High and dry Comparing active dry EEG electrodes .pdf},
  journal = {Psychophysiology},
  language = {en},
  number = {1}
}

@article{Maya2017,
  title = {On Salesmen and Tourists: {{Two}}-Step Optimization in Deterministic Foragers},
  shorttitle = {On Salesmen and Tourists},
  author = {Maya, Miguel and Miramontes, Octavio and Boyer, Denis},
  year = {2017},
  month = feb,
  volume = {226},
  pages = {391--400},
  issn = {1951-6355, 1951-6401},
  doi = {10.1140/epjst/e2016-60195-6},
  abstract = {We explore a two-step optimization problem in random environments, the so-called restaurant-coffee shop problem, where a walker aims at visiting the nearest and better restaurant in an area and then move to the nearest and better coffee-shop. This is an extension of the Tourist Problem, a one-step optimization dynamics that can be viewed as a deterministic walk in a random medium. A certain amount of heterogeneity in the values of the resources to be visited causes the emergence of power-laws distributions for the steps performed by the walker, similarly to a L\textasciiacute evy flight. The fluctuations of the step lengths tend to decrease as a consequence of multiple-step planning, thus reducing the foraging uncertainty. We find that the first and second steps of each planned movement play very different roles in heterogeneous environments. The two-step process improves only slightly the foraging efficiency compared to the one-step optimization, at a much higher computational cost. We discuss the implications of these findings for animal and human mobility, in particular in relation to the computational effort that informed agents should deploy to solve search problems.},
  file = {Maya et al. - 2017 - On salesmen and tourists Two-step optimization in.pdf},
  journal = {Eur. Phys. J. Spec. Top.},
  language = {en},
  number = {3}
}

@article{Mazzoni2015,
  title = {Computing the {{Local Field Potential}} ({{LFP}}) from {{Integrate}}-and-{{Fire Network Models}}},
  author = {Mazzoni, Alberto and Lind{\'e}n, Henrik and Cuntz, Hermann and Lansner, Anders and Panzeri, Stefano and Einevoll, Gaute T.},
  editor = {Roth, Arnd},
  year = {2015},
  month = dec,
  volume = {11},
  pages = {e1004584},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1004584},
  abstract = {Leaky integrate-and-fire (LIF) network models are commonly used to study how the spiking dynamics of neural networks changes with stimuli, tasks or dynamic network states. However, neurophysiological studies in vivo often rather measure the mass activity of neuronal microcircuits with the local field potential (LFP). Given that LFPs are generated by spatially separated currents across the neuronal membrane, they cannot be computed directly from quantities defined in models of point-like LIF neurons. Here, we explore the best approximation for predicting the LFP based on standard output from point-neuron LIF networks. To search for this best ``LFP proxy'', we compared LFP predictions from candidate proxies based on LIF network output (e.g, firing rates, membrane potentials, synaptic currents) with ``ground-truth'' LFP obtained when the LIF network synaptic input currents were injected into an analogous three-dimensional (3D) network model of multi-compartmental neurons with realistic morphology, spatial distributions of somata and synapses. We found that a specific fixed linear combination of the LIF synaptic currents provided an accurate LFP proxy, accounting for most of the variance of the LFP time course observed in the 3D network for all recording locations. This proxy performed well over a broad set of conditions, including substantial variations of the neuronal morphologies. Our results provide a simple formula for estimating the time course of the LFP from LIF network simulations in cases where a single pyramidal population dominates the LFP generation, and thereby facilitate quantitative comparison between computational models and experimental LFP recordings in vivo.},
  file = {2015 - Mazzoni et al. - Computing the Local Field Potential (LFP) from Integrate-and-Fire Network Models.pdf;Mazzoni et al. - 2015 - Computing the Local Field Potential (LFP) from Int.pdf},
  journal = {PLOS Computational Biology},
  language = {en},
  number = {12}
}

@article{McCarthy2011,
  title = {Striatal Origin of the Pathologic Beta Oscillations in {{Parkinson}}'s Disease},
  author = {McCarthy, M. M. and {Moore-Kochlacs}, C. and Gu, X. and Boyden, E. S. and Han, X. and Kopell, N.},
  year = {2011},
  month = jul,
  volume = {108},
  pages = {11620--11625},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1107748108},
  file = {2011 - McCarthy et al. - Striatal origin of the pathologic beta oscillations in Parkinson's disease.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {28}
}

@article{Mcculloch,
  title = {A {{LOGICAL CALCULUS OF THE IDEAS IMMANENT IN NERVOUS ACTIVITY}}},
  author = {Mcculloch, Warren S and Pitts, Walter},
  pages = {17},
  file = {1990 - Mcculloch, Pitts - A logical calculus nervous activity.pdf},
  language = {en}
}

@article{McDuff2009,
  title = {Multivoxel {{Pattern Analysis Reveals Increased Memory Targeting}} and {{Reduced Use}} of {{Retrieved Details}} during {{Single}}-{{Agenda Source Monitoring}}},
  author = {McDuff, S. G. R. and Frankel, H. C. and Norman, K. A.},
  year = {2009},
  month = jan,
  volume = {29},
  pages = {508--516},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.3587-08.2009},
  file = {2009 - McDuff, Frankel, Norman - Multivoxel pattern analysis reveals increased memory targeting and reduced use of retrieved details dur.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {2}
}

@article{McGill1978,
  title = {Variations of {{Box Plots}}},
  author = {McGill, Robert and Tukey, John W. and Larsen, Wayne A.},
  year = {1978},
  month = feb,
  volume = {32},
  pages = {12},
  issn = {00031305},
  doi = {10.2307/2683468},
  file = {1978 - McGill, Tukey, Larsen - Variations of box plots.pdf;McGill et al. - 1978 - Variations of Box Plots.pdf},
  journal = {The American Statistician},
  language = {en},
  number = {1}
}

@article{McIntosh,
  title = {Deep {{Learning Models}} of the {{Retinal Response}} to {{Natural Scenes}}},
  author = {McIntosh, Lane and Maheswaranathan, Niru and Nayebi, Aran and Ganguli, Surya and Baccus, Stephen},
  pages = {9},
  abstract = {A central challenge in sensory neuroscience is to understand neural computations and circuit mechanisms that underlie the encoding of ethologically relevant, natural stimuli. In multilayered neural circuits, nonlinear processes such as synaptic transmission and spiking dynamics present a significant obstacle to the creation of accurate computational models of responses to natural stimuli. Here we demonstrate that deep convolutional neural networks (CNNs) capture retinal responses to natural scenes nearly to within the variability of a cell's response, and are markedly more accurate than linear-nonlinear (LN) models and Generalized Linear Models (GLMs). Moreover, we find two additional surprising properties of CNNs: they are less susceptible to overfitting than their LN counterparts when trained on small amounts of data, and generalize better when tested on stimuli drawn from a different distribution (e.g. between natural scenes and white noise). An examination of the learned CNNs reveals several properties. First, a richer set of feature maps is necessary for predicting the responses to natural scenes compared to white noise. Second, temporally precise responses to slowly varying inputs originate from feedforward inhibition, similar to known retinal mechanisms. Third, the injection of latent noise sources in intermediate layers enables our model to capture the sub-Poisson spiking variability observed in retinal ganglion cells. Fourth, augmenting our CNNs with recurrent lateral connections enables them to capture contrast adaptation as an emergent property of accurately describing retinal responses to natural scenes. These methods can be readily generalized to other sensory modalities and stimulus ensembles. Overall, this work demonstrates that CNNs not only accurately capture sensory circuit responses to natural scenes, but also can yield information about the circuit's internal structure and function.},
  file = {McIntosh et al. - Deep Learning Models of the Retinal Response to Na.pdf},
  language = {en}
}

@article{McIntosh2017,
  title = {Deep {{Learning Models}} of the {{Retinal Response}} to {{Natural Scenes}}},
  author = {McIntosh, Lane T. and Maheswaranathan, Niru and Nayebi, Aran and Ganguli, Surya and Baccus, Stephen A.},
  year = {2017},
  month = feb,
  abstract = {A central challenge in sensory neuroscience is to understand neural computations and circuit mechanisms that underlie the encoding of ethologically relevant, natural stimuli. In multilayered neural circuits, nonlinear processes such as synaptic transmission and spiking dynamics present a significant obstacle to the creation of accurate computational models of responses to natural stimuli. Here we demonstrate that deep convolutional neural networks (CNNs) capture retinal responses to natural scenes nearly to within the variability of a cell's response, and are markedly more accurate than linear-nonlinear (LN) models and Generalized Linear Models (GLMs). Moreover, we find two additional surprising properties of CNNs: they are less susceptible to overfitting than their LN counterparts when trained on small amounts of data, and generalize better when tested on stimuli drawn from a different distribution (e.g. between natural scenes and white noise). An examination of the learned CNNs reveals several properties. First, a richer set of feature maps is necessary for predicting the responses to natural scenes compared to white noise. Second, temporally precise responses to slowly varying inputs originate from feedforward inhibition, similar to known retinal mechanisms. Third, the injection of latent noise sources in intermediate layers enables our model to capture the sub-Poisson spiking variability observed in retinal ganglion cells. Fourth, augmenting our CNNs with recurrent lateral connections enables them to capture contrast adaptation as an emergent property of accurately describing retinal responses to natural scenes. These methods can be readily generalized to other sensory modalities and stimulus ensembles. Overall, this work demonstrates that CNNs not only accurately capture sensory circuit responses to natural scenes, but also can yield information about the circuit's internal structure and function.},
  archiveprefix = {arXiv},
  eprint = {1702.01825},
  eprinttype = {arxiv},
  file = {McIntosh et al. - 2017 - Deep Learning Models of the Retinal Response to Na.pdf},
  journal = {arXiv:1702.01825 [q-bio, stat]},
  keywords = {Quantitative Biology - Neurons and Cognition,Statistics - Machine Learning},
  language = {en},
  primaryclass = {q-bio, stat}
}

@article{McIntyre2002,
  title = {Extracellular {{Stimulation}} of {{Central Neurons}}: {{Influence}} of {{Stimulus Waveform}} and {{Frequency}} on {{Neuronal Output}}},
  shorttitle = {Extracellular {{Stimulation}} of {{Central Neurons}}},
  author = {McIntyre, Cameron C. and Grill, Warren M.},
  year = {2002},
  month = oct,
  volume = {88},
  pages = {1592--1604},
  issn = {0022-3077, 1522-1598},
  doi = {10.1152/jn.2002.88.4.1592},
  file = {2002 - McIntyre, Grill - Extracellular Stimulation of Central Neurons Influence of Stimulus Waveform and Frequency on Neuronal Output.pdf},
  journal = {Journal of Neurophysiology},
  language = {en},
  number = {4}
}

@article{McKeown2003,
  title = {Independent Component Analysis of Functional {{MRI}}: What Is Signal and What Is Noise?},
  shorttitle = {Independent Component Analysis of Functional {{MRI}}},
  author = {McKeown, M},
  year = {2003},
  month = oct,
  volume = {13},
  pages = {620--629},
  issn = {09594388},
  doi = {10.1016/j.conb.2003.09.012},
  abstract = {Many sources of fluctuation contribute to the functional magnetic resonance imaging (fMRI) signal, complicating attempts to infer those changes that are truly related to brain activation. Unlike methods of analysis of fMRI data that test the time course of each voxel against a hypothesized waveform, data-driven methods, such as independent component analysis and clustering, attempt to find common features within the data. This exploratory approach can be revealing when the brain activation is difficult to predict beforehand, such as with complex stimuli and internal shifts of activation that are not time-locked to an easily specified sensory or motor event. These methods can be further improved by incorporating prior knowledge regarding the temporal and spatial extent of brain activation.},
  file = {2003 - McKeown, Hansen, Sejnowski - Independent component analysis of functional MRI what is signal and what is noise.pdf},
  journal = {Current Opinion in Neurobiology},
  language = {en},
  number = {5}
}

@article{McMahan2016,
  title = {Communication-{{Efficient Learning}} of {{Deep Networks}} from {{Decentralized Data}}},
  author = {McMahan, H. Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and y Arcas, Blaise Ag{\"u}era},
  year = {2016},
  month = feb,
  abstract = {Modern mobile devices have access to a wealth of data suitable for learning models, which in turn can greatly improve the user experience on the device. For example, language models can improve speech recognition and text entry, and image models can automatically select good photos. However, this rich data is often privacy sensitive, large in quantity, or both, which may preclude logging to the data center and training there using conventional approaches. We advocate an alternative that leaves the training data distributed on the mobile devices, and learns a shared model by aggregating locally-computed updates. We term this decentralized approach Federated Learning. We present a practical method for the federated learning of deep networks based on iterative model averaging, and conduct an extensive empirical evaluation, considering five different model architectures and four datasets. These experiments demonstrate the approach is robust to the unbalanced and non-IID data distributions that are a defining characteristic of this setting. Communication costs are the principal constraint, and we show a reduction in required communication rounds by 10\textendash 100\texttimes{} as compared to synchronized stochastic gradient descent.},
  archiveprefix = {arXiv},
  eprint = {1602.05629},
  eprinttype = {arxiv},
  file = {McMahan et al. - 2016 - Communication-Efficient Learning of Deep Networks .pdf},
  journal = {arXiv:1602.05629 [cs]},
  keywords = {Computer Science - Machine Learning},
  language = {en},
  primaryclass = {cs}
}

@article{McNeal1976,
  title = {Analysis of a {{Model}} for {{Excitation}} of {{Myelinated Nerve}}},
  author = {McNeal, Donald R.},
  year = {1976},
  month = jul,
  volume = {BME-23},
  pages = {329--337},
  issn = {0018-9294},
  doi = {10.1109/TBME.1976.324593},
  abstract = {Excellent models have been presented in the literature which relate membrane potential to transverse membrane current and which describe the propagation of action potentials along the axon, for both myelinated and nonmyelinated fibers. There is not, however, an adequate model for nerve excitation which allows one to compute the threshold of a nerve fiber for pulses of finite duration using electrodes that are not in direct contact with the fiber. This paper considers this problem and presents a model of the electrical properties of myelinated nerve which describes the time course of events following stimulus application up to the initiation of the action potential. The time-varying current and potential at all nodes can be computed from the model, and the strength-duration curve can be determined for arbitrary electrode geometries, although only the case of a monopolar electrode is considered in this paper. It is shown that even when the stimulus is a constant-current pulse, the membrane current at the nodes varies considerably with time. The strength-duration curve calculated from the model is consistent with previously published experimental data, and the model provides a quantitative relationship between threshold and fiber diameter which shows there is less selectivity among fibers of large diameter than those of small diameter.},
  file = {1976 - Mcneal - Analysis of a Model for Excitation of Myelinated Nerve.pdf;McNeal - 1976 - Analysis of a Model for Excitation of Myelinated N.pdf},
  journal = {IEEE Transactions on Biomedical Engineering},
  language = {en},
  number = {4}
}

@article{Meder,
  title = {Ergodicity-Breaking Reveals Time Optimal Decision Making in Humans},
  author = {Meder, David and Rabe, Finn and Morville, Tobias and Madsen, Kristoffer H and Koudahl, Magnus T and Dolan, Ray J and Hulme, Oliver J and Dolan, R J and Siebner, H R and Hulme, O J},
  pages = {43},
  file = {Meder et al. - Ergodicity-breaking reveals time optimal decision .pdf},
  language = {en}
}

@article{Megias2001,
  title = {Total Number and Distribution of Inhibitory and Excitatory Synapses on Hippocampal {{CA1}} Pyramidal Cells},
  author = {Meg{\'{\i}}as, M and Emri, Zs and Freund, T.F and Guly{\'a}s, A.I},
  year = {2001},
  month = feb,
  volume = {102},
  pages = {527--540},
  issn = {03064522},
  doi = {10.1016/S0306-4522(00)00496-6},
  abstract = {The integrative properties of neurons depend strongly on the number, proportions and distribution of excitatory and inhibitory synaptic inputs they receive. In this study the three-dimensional geometry of dendritic trees and the density of symmetrical and asymmetrical synapses on different cellular compartments of rat hippocampal CA I area pyramidal cells was measured to calculate the total number and distribution of excitatory and inhibitory inputs on a single cell.},
  file = {2001 - Megías et al. - Total number and distribution of inhibitory and excitatory synapses on hippocampal CA1 pyramidal cells.pdf},
  journal = {Neuroscience},
  language = {en},
  number = {3}
}

@article{Mehaffey2005,
  title = {Deterministic {{Multiplicative Gain Control}} with {{Active Dendrites}}},
  author = {Mehaffey, W. H.},
  year = {2005},
  month = oct,
  volume = {25},
  pages = {9968--9977},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.2682-05.2005},
  file = {Mehaffey - 2005 - Deterministic Multiplicative Gain Control with Act.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {43}
}

@article{Mehlhorn2015,
  title = {Unpacking the Exploration\textendash Exploitation Tradeoff: {{A}} Synthesis of Human and Animal Literatures.},
  shorttitle = {Unpacking the Exploration\textendash Exploitation Tradeoff},
  author = {Mehlhorn, Katja and Newell, Ben R. and Todd, Peter M. and Lee, Michael D. and Morgan, Kate and Braithwaite, Victoria A. and Hausmann, Daniel and Fiedler, Klaus and Gonzalez, Cleotilde},
  year = {2015},
  month = jul,
  volume = {2},
  pages = {191--215},
  issn = {2325-9973, 2325-9965},
  doi = {10.1037/dec0000033},
  file = {Mehlhorn et al. - 2015 - Unpacking the exploration–exploitation tradeoff A.pdf},
  journal = {Decision},
  language = {en},
  number = {3}
}

@article{Mejias2014,
  title = {Differential Effects of Excitatory and Inhibitory Heterogeneity on the Gain and Asynchronous State of Sparse Cortical Networks},
  author = {Mejias, Jorge F. and Longtin, Andr{\~A}{\textcopyright}},
  year = {2014},
  month = sep,
  volume = {8},
  issn = {1662-5188},
  doi = {10.3389/fncom.2014.00107},
  abstract = {Recent experimental and theoretical studies have highlighted the importance of cell-to-cell differences in the dynamics and functions of neural networks, such as in different types of neural coding or synchronization. It is still not known, however, how neural heterogeneity can affect cortical computations, or impact the dynamics of typical cortical circuits constituted of sparse excitatory and inhibitory networks. In this work, we analytically and numerically study the dynamics of a typical cortical circuit with a certain level of neural heterogeneity. Our circuit includes realistic features found in real cortical populations, such as network sparseness, excitatory, and inhibitory subpopulations of neurons, and different cell-to-cell heterogeneities for each type of population in the system. We find highly differentiated roles for heterogeneity, depending on the subpopulation in which it is found. In particular, while heterogeneity among excitatory neurons non-linearly increases the mean firing rate and linearizes the f-I curves, heterogeneity among inhibitory neurons may decrease the network activity level and induces divisive gain effects in the f-I curves of the excitatory cells, providing an effective gain control mechanism to influence information flow. In addition, we compute the conditions for stability of the network activity, finding that the synchronization onset is robust to inhibitory heterogeneity, but it shifts to lower input levels for higher excitatory heterogeneity. Finally, we provide an extension of recently reported heterogeneity-induced mechanisms for signal detection under rate coding, and we explore the validity of our findings when multiple sources of heterogeneity are present. These results allow for a detailed characterization of the role of neural heterogeneity in asynchronous cortical networks.},
  file = {Mejias and Longtin - 2014 - Differential effects of excitatory and inhibitory .pdf},
  journal = {Frontiers in Computational Neuroscience},
  language = {en}
}

@article{Mejias2016,
  title = {Feedforward and Feedback Frequency-Dependent Interactions in a Large-Scale Laminar Network of the Primate Cortex},
  author = {Mejias, Jorge F and Murray, John D and Kennedy, Henry and Wang, Xiao-Jing},
  year = {2016},
  volume = {2},
  pages = {e1601335},
  abstract = {Interactions between top-down and bottom-up processes in the cerebral cortex hold the key to understanding predictive coding, executive control and a gamut of other brain functions. The underlying circuit mechanism, however, remains poorly understood and represents a major challenge in neuroscience. In the present work we tackled this problem using a large-scale computational model of the primate cortex constrained by new directed and weighted connectivity data. In our model, the interplay between feedforward and feedback signaling depends on the cortical laminar structure and involves complex dynamics across multiple (intra-laminar, inter-laminar, inter-areal and whole cortex) scales. The model was tested by reproducing, and shedding insights into, a wide range of neurophysiological findings about frequency-dependent interactions between visual cortical areas: feedforward pathways are associated with enhanced gamma (30-70 Hz) oscillations, whereas feedback projections selectively modulate alpha/low beta (8-15 Hz) oscillations. We found that in order for the model to account for the experimental observations, the feedback projection needs to predominantly target infragranular layers in a target area, which leads to a proposed circuit substrate for predictive coding. The model reproduces a functional hierarchy based on frequency-dependent Granger causality analysis of inter-areal signaling, as reported in recent monkey and human experiments. Taken together, this work highlights the importance of multi-scale approaches and provides a modeling platform for studies of large-scale brain circuit dynamics and functions.},
  file = {Mejias et al. - Feedforward and feedback frequency-dependent inter.pdf},
  journal = {Science Advances},
  language = {en},
  number = {11}
}

@article{Mel2004,
  title = {On the {{Fight Between Excitation}} and {{Inhibition}}: {{Location Is Everything}}},
  shorttitle = {On the {{Fight Between Excitation}} and {{Inhibition}}},
  author = {Mel, B. W. and Schiller, J.},
  year = {2004},
  month = sep,
  volume = {2004},
  pages = {pe44-pe44},
  issn = {1945-0877, 1937-9145},
  doi = {10.1126/stke.2502004pe44},
  file = {2004 - Mel, Schiller - On the fight between excitation and inhibition location is everything.pdf},
  journal = {Science Signaling},
  language = {en},
  number = {250}
}

@article{Meng2014,
  title = {A {{Unified Approach}} to {{Linking Experimental}}, {{Statistical}} and {{Computational Analysis}} of {{Spike Train Data}}},
  author = {Meng, Liang and Kramer, Mark A. and Middleton, Steven J. and Whittington, Miles A. and Eden, Uri T.},
  editor = {Chacron, Maurice J.},
  year = {2014},
  month = jan,
  volume = {9},
  pages = {e85269},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0085269},
  abstract = {A fundamental issue in neuroscience is how to identify the multiple biophysical mechanisms through which neurons generate observed patterns of spiking activity. In previous work, we proposed a method for linking observed patterns of spiking activity to specific biophysical mechanisms based on a state space modeling framework and a sequential Monte Carlo, or particle filter, estimation algorithm. We have shown, in simulation, that this approach is able to identify a space of simple biophysical models that were consistent with observed spiking data (and included the model that generated the data), but have yet to demonstrate the application of the method to identify realistic currents from real spike train data. Here, we apply the particle filter to spiking data recorded from rat layer V cortical neurons, and correctly identify the dynamics of an slow, intrinsic current. The underlying intrinsic current is successfully identified in four distinct neurons, even though the cells exhibit two distinct classes of spiking activity: regular spiking and bursting. This approach \textendash{} linking statistical, computational, and experimental neuroscience \textendash{} provides an effective technique to constrain detailed biophysical models to specific mechanisms consistent with observed spike train data.},
  file = {2014 - Meng et al. - A unified approach to linking experimental, statistical and computational analysis of spike train data.pdf;Meng et al. - 2014 - A Unified Approach to Linking Experimental, Statis.pdf},
  journal = {PLoS ONE},
  language = {en},
  number = {1}
}

@article{Mensch,
  title = {Learning {{Neural Representations}} of {{Human Cognition}} across {{Many fMRI Studies}}},
  author = {Mensch, Arthur and Mairal, Julien and Bzdok, Danilo and Thirion, Bertrand and Varoquaux, Ga{\"e}l},
  pages = {14},
  abstract = {Cognitive neuroscience is enjoying rapid increase in extensive public brain-imaging datasets. It opens the door to large-scale statistical models. Finding a unified perspective for all available data calls for scalable and automated solutions to an old challenge: how to aggregate heterogeneous information on brain function into a universal cognitive system that relates mental operations/cognitive processes/psychological tasks to brain networks? We cast this challenge in a machine-learning approach to predict conditions from statistical brain maps across different studies. For this, we leverage multi-task learning and multi-scale dimension reduction to learn low-dimensional representations of brain images that carry cognitive information and can be robustly associated with psychological stimuli. Our multi-dataset classification model achieves the best prediction performance on several large reference datasets, compared to models without cognitive-aware low-dimension representations; it brings a substantial performance boost to the analysis of small datasets, and can be introspected to identify universal template cognitive concepts.},
  file = {Mensch et al. - Learning Neural Representations of Human Cognition.pdf},
  language = {en}
}

@article{Mensch2018,
  title = {Stochastic {{Subsampling}} for {{Factorizing Huge Matrices}}},
  author = {Mensch, Arthur and Mairal, Julien and Thirion, Bertrand and Varoquaux, Gael},
  year = {2018},
  month = jan,
  volume = {66},
  pages = {113--128},
  issn = {1053-587X, 1941-0476},
  doi = {10.1109/TSP.2017.2752697},
  abstract = {We present a matrix-factorization algorithm that scales to input matrices with both huge number of rows and columns. Learned factors may be sparse or dense and/or non-negative, which makes our algorithm suitable for dictionary learning, sparse component analysis, and non-negative matrix factorization. Our algorithm streams matrix columns while subsampling them to iteratively learn the matrix factors. At each iteration, the row dimension of a new sample is reduced by subsampling, resulting in lower time complexity compared to a simple streaming algorithm. Our method comes with convergence guarantees to reach a stationary point of the matrix-factorization problem. We demonstrate its efficiency on massive functional Magnetic Resonance Imaging data (2 TB), and on patches extracted from hyperspectral images (103 GB). For both problems, which involve different penalties on rows and columns, we obtain significant speed-ups compared to state-of-the-art algorithms.},
  file = {Mensch et al. - 2018 - Stochastic Subsampling for Factorizing Huge Matric.pdf},
  journal = {IEEE Transactions on Signal Processing},
  language = {en},
  number = {1}
}

@article{Merel2019,
  title = {Deep Neuroethology of a Virtual Rodent},
  author = {Merel, Josh and Aldarondo, Diego and Marshall, Jesse and Tassa, Yuval and Wayne, Greg and {\"O}lveczky, Bence},
  year = {2019},
  month = nov,
  abstract = {Parallel developments in neuroscience and deep learning have led to mutually productive exchanges, pushing our understanding of real and artificial neural networks in sensory and cognitive systems. However, this interaction between fields is less developed in the study of motor control. In this work, we develop a virtual rodent as a platform for the grounded study of motor activity in artificial models of embodied control. We then use this platform to study motor activity across contexts by training a model to solve four complex tasks. Using methods familiar to neuroscientists, we describe the behavioral representations and algorithms employed by different layers of the network using a neuroethological approach to characterize motor activity relative to the rodent's behavior and goals. We find that the model uses two classes of representations which respectively encode the task-specific behavioral strategies and task-invariant behavioral kinematics. These representations are reflected in the sequential activity and population dynamics of neural subpopulations. Overall, the virtual rodent facilitates grounded collaborations between deep reinforcement learning and motor neuroscience.},
  archiveprefix = {arXiv},
  eprint = {1911.09451},
  eprinttype = {arxiv},
  file = {Merel et al. - 2019 - Deep neuroethology of a virtual rodent.pdf},
  journal = {arXiv:1911.09451 [q-bio]},
  keywords = {Quantitative Biology - Neurons and Cognition},
  language = {en},
  primaryclass = {q-bio}
}

@article{Merker2016,
  title = {Cortical {{Gamma Oscillations}}: {{Details}} of {{Their Genesis Preclude}} a {{Role}} in {{Cognition}}},
  shorttitle = {Cortical {{Gamma Oscillations}}},
  author = {Merker, Bjorn H.},
  year = {2016},
  month = jul,
  volume = {10},
  issn = {1662-5188},
  doi = {10.3389/fncom.2016.00078},
  file = {Merker - 2016 - Cortical Gamma Oscillations Details of Their Gene.pdf},
  journal = {Frontiers in Computational Neuroscience},
  language = {en}
}

@article{Meshulam2018,
  title = {Coarse--Graining, Fixed Points, and Scaling in a Large Population of Neurons},
  author = {Meshulam, Leenoy and Gauthier, Jeffrey L. and Brody, Carlos D. and Tank, David W. and Bialek, William},
  year = {2018},
  month = sep,
  abstract = {We develop a phenomenological coarse--graining procedure for activity in a large network of neurons, and apply this to recordings from a population of 1000+ cells in the hippocampus. Distributions of coarse--grained variables seem to approach a fixed non--Gaussian form, and we see evidence of scaling in both static and dynamic quantities. These results suggest that the collective behavior of the network is described by a non--trivial fixed point.},
  archiveprefix = {arXiv},
  eprint = {1809.08461},
  eprinttype = {arxiv},
  file = {Meshulam et al. - 2018 - Coarse--graining, fixed points, and scaling in a l.pdf},
  journal = {arXiv:1809.08461 [physics, q-bio]},
  keywords = {Physics - Biological Physics,Quantitative Biology - Neurons and Cognition},
  language = {en},
  primaryclass = {physics, q-bio}
}

@article{Mesterton-Gibbons1998,
  title = {Animal {{Contests}} as {{Evolutionary Games}}: {{Paradoxical}} Behavior Can Be Understood in the Context of Evolutionary Stable Strategies. {{The}} Trick Is to Discover Which Game the Animal Is Playing},
  author = {{Mesterton-Gibbons}, Michael and {work(s):}, Eldridge S. Adams Reviewed},
  year = {1998},
  volume = {86},
  pages = {334--341},
  file = {Mesterton-Gibbons and work(s) - 1998 - Animal Contests as Evolutionary Games Paradoxical.pdf},
  journal = {American Scientist},
  language = {en},
  number = {4}
}

@article{Meyer2010,
  title = {Predicting Visual Stimuli on the Basis of Activity in Auditory Cortices},
  author = {Meyer, Kaspar and Kaplan, Jonas T and Essex, Ryan and Webber, Cecelia and Damasio, Hanna and Damasio, Antonio},
  year = {2010},
  month = jun,
  volume = {13},
  pages = {667--668},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.2533},
  file = {2010 - Meyer et al. - Predicting visual stimuli on the basis of activity in auditory cortices.pdf},
  journal = {Nature Neuroscience},
  language = {en},
  number = {6}
}

@article{Meyer2011,
  title = {Seeing {{Touch Is Correlated}} with {{Content}}-{{Specific Activity}} in {{Primary Somatosensory Cortex}}},
  author = {Meyer, Kaspar and Kaplan, Jonas T. and Essex, Ryan and Damasio, Hanna and Damasio, Antonio},
  year = {2011},
  month = sep,
  volume = {21},
  pages = {2113--2121},
  issn = {1460-2199, 1047-3211},
  doi = {10.1093/cercor/bhq289},
  file = {2011 - Meyer et al. - Seeing touch is correlated with content-specific activity in primary somatosensory cortex.pdf},
  journal = {Cerebral Cortex},
  language = {en},
  number = {9}
}

@article{Mhaskar,
  title = {Learning {{Functions}}: {{When Is Deep Better Than Shallow}}},
  author = {Mhaskar, Hrushikesh and Liao, Qianli and Poggio, Tomaso},
  pages = {12},
  file = {Mhaskar et al. - Learning Functions When Is Deep Better Than Shall.pdf},
  language = {en}
}

@article{Miconi2017,
  title = {Biologically Plausible Learning in Recurrent Neural Networks Reproduces Neural Dynamics Observed during Cognitive Tasks},
  author = {Miconi, Thomas},
  year = {2017},
  month = feb,
  doi = {10.1101/057729},
  abstract = {Neural activity during cognitive tasks exhibits complex dynamics that flexibly encode task\-relevant variables. Recurrent neural networks operating in the near\-chaotic regime, which spontaneously generate rich dynamics, have been proposed as a model of cortical computation during cognitive tasks. However, existing methods for training these networks are either biologically implausible, and/or require a continuous, real\-time error signal to guide the learning process. The lack of a biological learning method currently restricts the plausibility of recurrent networks as models of cortical computation. Here we show that a biologically plausible learning rule can train such recurrent networks, guided solely by delayed, phasic rewards at the end of each trial, for nontrivial tasks. We use this method to learn various tasks from the experimental literature, showing that this learning rule can successfully implement flexible associations, memory maintenance, nonlinear mixed selectivities, and coordination among multiple outputs. We show that the resulting networks exhibit complex dynamics previously observed in animal cortex, such as dynamic encoding and maintenance of task features, switching from stimulus\-specific to response\-specific representations, and selective integration of relevant input streams. We conclude that recurrent neural networks offer a plausible model of cortical dynamics during both learning and performance of flexible behavior.},
  file = {Miconi - 2017 - Biologically plausible learning in recurrent neura.pdf},
  journal = {bioRxiv},
  language = {en}
}

@article{Miconi2019,
  title = {{{BACKPROPAMINE}}: {{TRAINING SELF}}-{{MODIFYING NEU}}- {{RAL NETWORKS WITH DIFFERENTIABLE NEUROMODU}}- {{LATED PLASTICITY}}},
  author = {Miconi, Thomas and Rawal, Aditya and Clune, Jeff and Stanley, Kenneth O},
  year = {2019},
  pages = {15},
  abstract = {The impressive lifelong learning in animal brains is primarily enabled by plastic changes in synaptic connectivity. Importantly, these changes are not passive, but are actively controlled by neuromodulation, which is itself under the control of the brain. The resulting self-modifying abilities of the brain play an important role in learning and adaptation, and are a major basis for biological reinforcement learning. Here we show for the first time that artificial neural networks with such neuromodulated plasticity can be trained with gradient descent. Extending previous work on differentiable Hebbian plasticity, we propose a differentiable formulation for the neuromodulation of plasticity. We show that neuromodulated plasticity improves the performance of neural networks on both reinforcement learning and supervised learning tasks. In one task, neuromodulated plastic LSTMs with millions of parameters outperform standard LSTMs on a benchmark language modeling task (controlling for the number of parameters). We conclude that differentiable neuromodulation of plasticity offers a powerful new framework for training neural networks.},
  file = {Miconi et al. - 2019 - BACKPROPAMINE TRAINING SELF-MODIFYING NEU- RAL NE.pdf},
  language = {en}
}

@article{Miller,
  title = {When {{Recurrent Models Don}}'t {{Need To Be Recurrent}}},
  author = {Miller, John and Hardt, Moritz},
  pages = {23},
  abstract = {We prove stable recurrent neural networks are well approximated by feed-forward networks for the purpose of both inference and training by gradient descent. Our result applies to a broad range of non-linear recurrent neural networks under a natural stability condition, which we observe is also necessary. Complementing our theoretical findings, we verify the conclusions of our theory on both real and synthetic tasks. Furthermore, we demonstrate recurrent models satisfying the stability assumption of our theory can have excellent performance on real sequence learning tasks.},
  file = {Miller and Hardt - When Recurrent Models Don’t Need To Be Recurrent.pdf},
  language = {en}
}

@article{Miller1956,
  title = {The {{Magical Number Seven}}, {{Plus}} or {{Minus Two}}: {{Some Limits}} on {{Our Capacity}} for {{Processing Information}}},
  author = {Miller, George},
  year = {1956},
  volume = {63},
  pages = {81--97},
  file = {httpspider.apa.orgftdocsrev1994aprilrev101.pdf},
  journal = {The Psychological Review},
  language = {en}
}

@article{Miller2001,
  title = {Processing in Layer 4 of the Neocortical Circuit: New Insights from Visual and Somatosensory Cortex},
  shorttitle = {Processing in Layer 4 of the Neocortical Circuit},
  author = {Miller, K},
  year = {2001},
  month = aug,
  volume = {11},
  pages = {488--497},
  issn = {09594388},
  doi = {10.1016/S0959-4388(00)00239-7},
  file = {2001 - Miller, Pinto, Simons - Processing in layer 4 of the neocortical circuit New insights from visual and somatosensory cortex.pdf},
  journal = {Current Opinion in Neurobiology},
  language = {en},
  number = {4}
}

@article{Miller2002,
  title = {Neural {{Noise Can Explain Expansive}}, {{Power}}-{{Law Nonlinearities}} in {{Neural Response Functions}}},
  author = {Miller, Kenneth D. and Troyer, Todd W.},
  year = {2002},
  month = feb,
  volume = {87},
  pages = {653--659},
  issn = {0022-3077, 1522-1598},
  doi = {10.1152/jn.00425.2001},
  file = {2002 - Miller, Troyer - Neural noise can explain expansive, power-law nonlinearities in neural response functions.pdf},
  journal = {Journal of Neurophysiology},
  language = {en},
  number = {2}
}

@article{Miller2003,
  title = {Understanding {{Layer}} 4 of the {{Cortical Circuit}}: {{A Model Based}} on {{Cat V1}}},
  shorttitle = {Understanding {{Layer}} 4 of the {{Cortical Circuit}}},
  author = {Miller, K. D.},
  year = {2003},
  month = jan,
  volume = {13},
  pages = {73--82},
  issn = {14602199},
  doi = {10.1093/cercor/13.1.73},
  file = {2003 - Miller - Understanding layer 4 of the cortical circuit a model based on cat V1.pdf},
  journal = {Cerebral Cortex},
  language = {en},
  number = {1}
}

@article{Miller2013,
  title = {Cortical Circuits for the Control of Attention},
  author = {Miller, Earl K and Buschman, Timothy J},
  year = {2013},
  month = apr,
  volume = {23},
  pages = {216--222},
  issn = {09594388},
  doi = {10.1016/j.conb.2012.11.011},
  file = {2013 - Miller, Buschman - Cortical circuits for the control of attention.pdf},
  journal = {Current Opinion in Neurobiology},
  language = {en},
  number = {2}
}

@article{Miocinovic2006,
  title = {Computational {{Analysis}} of {{Subthalamic Nucleus}} and {{Lenticular Fasciculus Activation During Therapeutic Deep Brain Stimulation}}},
  author = {Miocinovic, Svjetlana and Parent, Martin and Butson, Christopher R. and Hahn, Philip J. and Russo, Gary S. and Vitek, Jerrold L. and McIntyre, Cameron C.},
  year = {2006},
  month = sep,
  volume = {96},
  pages = {1569--1580},
  issn = {0022-3077, 1522-1598},
  doi = {10.1152/jn.00305.2006},
  file = {2006 - Miocinovic et al. - Computational analysis of subthalamic nucleus and lenticular fasciculus activation during therapeutic deep br.pdf;2006 - Miocinovic et al. - Computational analysis of subthalamic nucleus and lenticular fasciculus activation during therapeutic deep(2).pdf},
  journal = {Journal of Neurophysiology},
  language = {en},
  number = {3}
}

@article{Mirollo1990,
  title = {Synchronization of {{Pulse}}-{{Coupled Biological Oscillators}}},
  author = {Mirollo, Renato E. and Strogatz, Steven H.},
  year = {1990},
  volume = {50},
  pages = {1645--1662},
  abstract = {A simplemodel forsynchronousfiringof biologicaloscillatorsbased on Peskin'smodel of thecardiacpacemaker[Mathematicaal spectsofheartphysiologyC,ourantInstitutoefMathematicalSciences, New York UniversityN, ew York, 1975,pp. 268-278] is studied.The model consistsof a populationof identicalintegrate-and-fiorsecillators.The couplingbetweenoscillatorsis pulsatile:whena givenoscillator fires,it pulls theothersup by a fixedamount,or bringsthemto thefiringthresholdw, hicheveris less. The main resultis thatforalmostall initialconditions,thepopulationevolvesto a statein whichall the oscillatorsare firingsynchronouslyT. he relationshipbetweenthe model and real communitiesof biologicaloscillatorsis discussed;examplesincludepopulationsofsynchronouslyflashingfirefliesc,rickets thatchirpinunison,electricallysynchronoups acemakercells,and groupsofwomenwhosemenstruaclycles become mutuallysynchronized.},
  file = {Mirollo and Strogatz - 1990 - Synchronization of Pulse-Coupled Biological Oscill.pdf},
  journal = {SIAM Journal on Applied Mathematics},
  language = {en},
  number = {6}
}

@article{Mischler2016,
  title = {On a Kinetic {{FitzHugh}}-{{Nagumo}} Model of Neuronal Network},
  author = {Mischler, St{\'e}phane and Qui{\~n}inao, Crist{\'o}bal and Touboul, Jonathan},
  year = {2016},
  month = mar,
  volume = {342},
  pages = {1001--1042},
  issn = {0010-3616, 1432-0916},
  doi = {10.1007/s00220-015-2556-9},
  abstract = {We investigate existence and uniqueness of solutions of a McKean-Vlasov evolution PDE representing the macroscopic behaviour of interacting Fitzhugh-Nagumo neurons. This equation is hypoelliptic, nonlocal and has unbounded coefficients. We prove existence of a solution to the evolution equation and non trivial stationary solutions. Moreover, we demonstrate uniqueness of the stationary solution in the weakly nonlinear regime. Eventually, using a semigroup factorisation method, we show exponential nonlinear stability in the small connectivity regime.},
  archiveprefix = {arXiv},
  eprint = {1503.00492},
  eprinttype = {arxiv},
  file = {Mischler et al. - 2016 - On a kinetic FitzHugh-Nagumo model of neuronal net.pdf},
  journal = {Communications in Mathematical Physics},
  keywords = {Mathematics - Analysis of PDEs,Mathematics - Functional Analysis,Mathematics - Spectral Theory},
  language = {en},
  number = {3}
}

@article{Mishina2014,
  title = {Exploration of Genetically Encoded Voltage Indicators Based on a Chimeric Voltage Sensing Domain},
  author = {Mishina, Yukiko and Mutoh, Hiroki and Song, Chenchen and Kn{\~A}{\textparagraph}pfel, Thomas},
  year = {2014},
  month = sep,
  volume = {7},
  issn = {1662-5099},
  doi = {10.3389/fnmol.2014.00078},
  abstract = {Deciphering how the brain generates cognitive function from patterns of electrical signals is one of the ultimate challenges in neuroscience. To this end, it would be highly desirable to monitor the activities of very large numbers of neurons while an animal engages in complex behaviors. Optical imaging of electrical activity using genetically encoded voltage indicators (GEVIs) has the potential to meet this challenge. Currently prevalent GEVIs are based on the voltage-sensitive fluorescent protein (VSFP) prototypical design or on the voltage-dependent state transitions of microbial opsins.We recently introduced a newVSFP design in which the voltage-sensing domain (VSD) is sandwiched between a fluorescence resonance energy transfer pair of fluorescent proteins (termed VSFP-Butterflies) and also demonstrated a series of chimeric VSD in which portions of the VSD of Ciona intestinalis voltage-sensitive phosphatase are substituted by homologous portions of a voltage-gated potassium channel subunit.These chimeric VSD had faster sensing kinetics than that of the native Ci-VSD. Here, we describe a new set of VSFPs that combine chimeric VSD with the Butterfly structure. We show that these chimeric VSFP-Butterflies can report membrane voltage oscillations of up to 200 Hz in cultured cells and report sensory evoked cortical population responses in living mice. This class of GEVIs may be suitable for imaging of brain rhythms in behaving mammalians.},
  file = {Mishina et al. - 2014 - Exploration of genetically encoded voltage indicat.pdf},
  journal = {Frontiers in Molecular Neuroscience},
  language = {en}
}

@article{Mishra2006,
  title = {Selective Attention through Phase Relationship of Excitatory and Inhibitory Input Synchrony in a Model Cortical Neuron},
  author = {Mishra, Jyoti and Fellous, Jean-Marc and Sejnowski, Terrence J.},
  year = {2006},
  month = nov,
  volume = {19},
  pages = {1329--1346},
  issn = {08936080},
  doi = {10.1016/j.neunet.2006.08.005},
  abstract = {Neurons in area V 2 and V 4 exhibit stimulus specific tuning to single stimuli, and respond at intermediate firing rates when presented with two differentially preferred stimuli (`pair response'). Selective attention to one of the two stimuli causes the neuron's firing rate to shift from the intermediate pair response towards the response to the attended stimulus as if it were presented alone. Attention to single stimuli reduces the response threshold of the neuron and increases spike synchronization at gamma frequencies. The intrinsic and network mechanisms underlying these phenomena were investigated in a multi-compartmental biophysical model of a reconstructed cat V 4 neuron. Differential stimulus preference was generated through a greater ratio of excitatory to inhibitory synapses projecting from one of two input V 2 populations. Feedforward inhibition and synaptic depression dynamics were critical to generating the intermediate pair response. Neuronal gain effects were simulated using gamma frequency range correlations in the feedforward excitatory and inhibitory inputs to the V 4 neuron. For single preferred stimulus presentations, correlations within the inhibitory population out of phase with correlations within the excitatory input significantly reduced the response threshold of the V 4 neuron. The pair response to simultaneously active preferred and non-preferred V 2 populations could also undergo an increase or decrease in gain via the same mechanism, where correlations in feedforward inhibition are out of phase with gamma band correlations within the excitatory input corresponding to the attended stimulus. The results of this model predict that top-down attention may bias the V 4 neuron's response using an inhibitory correlation phase shift mechanism.},
  file = {2006 - Mishra, Fellous, Sejnowski - Selective attention through phase relationship of excitatory and inhibitory input synchrony in a mod.pdf},
  journal = {Neural Networks},
  language = {en},
  number = {9}
}

@techreport{Miskovic2019,
  title = {Control {{Theory Concepts}} for {{Modeling Uncertainty}} in {{Enzyme Kinetics}} of {{Biochemical Networks}}},
  author = {Miskovic, Ljubisa and Tokic, Milenko and Savoglidis, Georgios and Hatzimanikatis, Vassily},
  year = {2019},
  month = apr,
  institution = {{Systems Biology}},
  doi = {10.1101/618777},
  abstract = {Analysis of the dynamic and steady-state properties of biochemical networks hinge on information about the parameters of enzyme kinetics. The lack of experimental data characterizing enzyme activities and kinetics along with the associated uncertainties impede the development of kinetic models, and researchers commonly use Monte Carlo sampling to explore the parameter space. However, the sampling of parameter spaces is a computationally expensive task for larger biochemical networks. To address this issue, we exploit the fact that reaction rates of biochemical reactions and network responses can be expressed as a function of displacements from thermodynamic equilibrium of elementary reaction steps and concentrations of free enzymes and their intermediary complexes. For a set of kinetic mechanisms ubiquitously found in biochemistry, we express kinetic responses of enzymes to changes in network metabolite concentrations through these quantities both analytically and schematically. The tailor-made sampling of these quantities allows for characterizing the missing kinetic parameters and accelerating the efforts towards building genome-scale kinetic metabolic models.},
  file = {Miskovic et al. - 2019 - Control Theory Concepts for Modeling Uncertainty i.pdf},
  language = {en},
  type = {Preprint}
}

@article{Mitchell,
  title = {Explanation-{{Based Generalization}}: {{A Unifying View}}},
  author = {Mitchell, Tom M},
  pages = {34},
  abstract = {The problem of formulating general concepts from specific training examples has long been a major focus of machine learning research. While most previous research has focused on empirical methods for generalizing from a large number of training examples using no domain-specific knowledge, in the past few years new methods have been developed for applying domain-specific knowledge to formulate valid generalizations from single training examples. The characteristic common to these methods is that their ability to generalize from a single example follows from their ability to explain why the training example is a member of the concept being learned. This paper proposes a general, domain-independent mechanism, called EBG, that unifies previous approaches to explanation-based generalization. The EBG method is illustrated in the context of several example problems, and used to contrast several existing systems for explanation-based generalization. The perspective on explanation-based generalization afforded by this general method is also used to identify open research problems in this area.},
  file = {Mitchell - Explanation-Based Generalization A Unifying View.pdf},
  language = {en}
}

@article{Mitchell1980,
  title = {The {{Need}} for {{Biases}} in {{Learning Generalizations}}},
  author = {Mitchell, Tom M},
  year = {1980},
  pages = {184--191},
  file = {Mitchell - The Need for Biases in Learning Generalizations.pdf},
  journal = {New Jersey: Department of Computer Science, Laboratory for Computer Science Research, Rutgers Univ..},
  language = {en}
}

@article{Mitchell2004,
  title = {Learning to {{Decode Cognitive States}} from {{Brain Images}}},
  author = {Mitchell, Tom M. and Hutchinson, Rebecca and Niculescu, Radu S. and Pereira, Francisco and Wang, Xuerui and Just, Marcel and Newman, Sharlene},
  year = {2004},
  month = oct,
  volume = {57},
  pages = {145--175},
  issn = {0885-6125},
  doi = {10.1023/B:MACH.0000035475.85309.1b},
  abstract = {Over the past decade, functional Magnetic Resonance Imaging (fMRI) has emerged as a powerful new instrument to collect vast quantities of data about activity in the human brain. A typical fMRI experiment can produce a three-dimensional image related to the human subject's brain activity every half second, at a spatial resolution of a few millimeters. As in other modern empirical sciences, this new instrumentation has led to a flood of new data, and a corresponding need for new data analysis methods. We describe recent research applying machine learning methods to the problem of classifying the cognitive state of a human subject based on fRMI data observed over a single time interval. In particular, we present case studies in which we have successfully trained classifiers to distinguish cognitive states such as (1) whether the human subject is looking at a picture or a sentence, (2) whether the subject is reading an ambiguous or non-ambiguous sentence, and (3) whether the word the subject is viewing is a word describing food, people, buildings, etc. This learning problem provides an interesting case study of classifier learning from extremely high dimensional (105 features), extremely sparse (tens of training examples), noisy data. This paper summarizes the results obtained in these three case studies, as well as lessons learned about how to successfully apply machine learning methods to train classifiers in such settings.},
  file = {2004 - Mitchell et al. - Learning to Decode Cognitive States from Brain Images.pdf},
  journal = {Machine Learning},
  language = {en},
  number = {1/2}
}

@article{Mizuseki2013,
  title = {Theta Oscillations Decrease Spike Synchrony in the Hippocampus and Entorhinal Cortex},
  author = {Mizuseki, K. and Buzsaki, G.},
  year = {2013},
  month = dec,
  volume = {369},
  pages = {20120530--20120530},
  issn = {0962-8436, 1471-2970},
  doi = {10.1098/rstb.2012.0530},
  file = {Mizuseki and Buzsaki - 2013 - Theta oscillations decrease spike synchrony in the.pdf},
  journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  language = {en},
  number = {1635}
}

@article{Mnih,
  title = {Asynchronous {{Methods}} for {{Deep Reinforcement Learning}}},
  author = {Mnih, Volodymyr and Badia, Adri{\`a} Puigdom{\`e}nech and Mirza, Mehdi and Graves, Alex and Harley, Tim and Lillicrap, Timothy P and Silver, David and Kavukcuoglu, Koray},
  pages = {19},
  abstract = {We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.},
  file = {Mnih et al. - Asynchronous Methods for Deep Reinforcement Learni.pdf},
  language = {en}
}

@article{Mnih2013,
  title = {Playing {{Atari}} with {{Deep Reinforcement Learning}}},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  year = {2013},
  pages = {9},
  abstract = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
  file = {Mnih et al. - Playing Atari with Deep Reinforcement Learning.pdf},
  journal = {NIPS},
  language = {en}
}

@article{Mnih2015,
  title = {Human-Level Control through Deep Reinforcement Learning},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
  year = {2015},
  month = feb,
  volume = {518},
  pages = {529--533},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature14236},
  file = {2015 - Minh - Human-level control through deep reinforcement learning.pdf;Mnih et al. - 2015 - Human-level control through deep reinforcement lea.pdf},
  journal = {Nature},
  language = {en},
  number = {7540}
}

@article{Moca2014,
  title = {Membrane {{Resonance Enables Stable}} and {{Robust Gamma Oscillations}}},
  author = {Moca, Vasile V. and Nikoli{\'c}, Danko and Singer, Wolf and Mure{\c s}an, Raul C.},
  year = {2014},
  month = jan,
  volume = {24},
  pages = {119--142},
  issn = {1460-2199, 1047-3211},
  doi = {10.1093/cercor/bhs293},
  abstract = {Neuronal mechanisms underlying beta/gamma oscillations (20\textendash 80 Hz) are not completely understood. Here, we show that in vivo beta/gamma oscillations in the cat visual cortex sometimes exhibit remarkably stable frequency even when inputs fluctuate dramatically. Enhanced frequency stability is associated with stronger oscillations measured in individual units and larger power in the local field potential. Simulations of neuronal circuitry demonstrate that membrane properties of inhibitory interneurons strongly determine the characteristics of emergent oscillations. Exploration of networks containing either integrator or resonator inhibitory interneurons revealed that: (i) Resonance, as opposed to integration, promotes robust oscillations with large power and stable frequency via a mechanism called RING (Resonance INduced Gamma); resonance favors synchronization by reducing phase delays between interneurons and imposes bounds on oscillation cycle duration; (ii) Stability of frequency and robustness of the oscillation also depend on the relative timing of excitatory and inhibitory volleys within the oscillation cycle; (iii) RING can reproduce characteristics of both Pyramidal INterneuron Gamma (PING) and INterneuron Gamma (ING), transcending such classifications; (iv) In RING, robust gamma oscillations are promoted by slow but are impaired by fast inputs. Results suggest that interneuronal membrane resonance can be an important ingredient for generation of robust gamma oscillations having stable frequency.},
  file = {Moca et al. - 2014 - Membrane Resonance Enables Stable and Robust Gamma 2.pdf;Moca et al. - 2014 - Membrane Resonance Enables Stable and Robust Gamma.pdf},
  journal = {Cerebral Cortex},
  language = {en},
  number = {1}
}

@article{Mochizuki2016,
  title = {Similarity in {{Neuronal Firing Regimes}} across {{Mammalian Species}}},
  author = {Mochizuki, Y. and Onaga, T. and Shimazaki, H. and Shimokawa, T. and Tsubo, Y. and Kimura, R. and Saiki, A. and Sakai, Y. and Isomura, Y. and Fujisawa, S. and Shibata, K.-i. and Hirai, D. and Furuta, T. and Kaneko, T. and Takahashi, S. and Nakazono, T. and Ishino, S. and Sakurai, Y. and Kitsukawa, T. and Lee, J. W. and Lee, H. and Jung, M. W. and Babul, C. and Maldonado, P. E. and Takahashi, K. and {Arce-McShane}, F. I. and Ross, C. F. and Sessle, B. J. and Hatsopoulos, N. G. and Brochier, T. and Riehle, A. and Chorley, P. and Grun, S. and Nishijo, H. and {Ichihara-Takeda}, S. and Funahashi, S. and Shima, K. and Mushiake, H. and Yamane, Y. and Tamura, H. and Fujita, I. and Inaba, N. and Kawano, K. and Kurkin, S. and Fukushima, K. and Kurata, K. and Taira, M. and Tsutsui, K.-I. and Ogawa, T. and Komatsu, H. and Koida, K. and Toyama, K. and Richmond, B. J. and Shinomoto, S.},
  year = {2016},
  month = may,
  volume = {36},
  pages = {5736--5747},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.0230-16.2016},
  file = {Mochizuki et al. - 2016 - Similarity in Neuronal Firing Regimes across Mamma.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {21}
}

@article{Mochol2015,
  title = {Stochastic Transitions into Silence Cause Noise Correlations in Cortical Circuits},
  author = {Mochol, Gabriela and {Hermoso-Mendizabal}, Ainhoa and Sakata, Shuzo and Harris, Kenneth D. and {de la Rocha}, Jaime},
  year = {2015},
  month = mar,
  volume = {112},
  pages = {3529--3534},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1410509112},
  file = {2015 - Mochol et al. - Stochastic transitions into silence cause noise correlations in cortical circuits.pdf;Mochol et al. - 2015 - Stochastic transitions into silence cause noise co.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {11}
}

@article{Modolo2008,
  title = {Dynamics of the {{Subthalamo}}-Pallidal {{Complex}} in {{Parkinson}}'s {{Disease During Deep Brain Stimulation}}},
  author = {Modolo, J. and Henry, J. and Beuter, A.},
  year = {2008},
  month = aug,
  volume = {34},
  pages = {251--266},
  issn = {0092-0606, 1573-0689},
  doi = {10.1007/s10867-008-9095-y},
  abstract = {The dynamics of the subthalamo-pallidal complex in Parkinson's disease during deep brain stimulation (DBS) were studied using two models, a simple firing-rate model and a population-based model. We extended the simple firing-rate model of the complex formed by the subthalamic nucleus (STN) and the external segment of the Globus Pallidus (GPe) to explore its dynamical regime during DBS. More specifically, the modulation of neuronal activity (i.e., pattern and amplitude) during DBS was studied. A similar approach was used with the population-based model. Simulation results revealed a gradual decrease in bursting activity in STN cells when the DBS frequency increased. In addition, the contribution of the stimulation current type (mono- or biphasic) to the results was also examined. A comparison of the two models indicated that the population-based model was more biologically realistic and more appropriate for exploring DBS mechanisms. Understanding the underlying mechanisms of DBS is a prerequisite for developing new stimulation protocols.},
  file = {2008 - Modolo, Henry, Beuter - Dynamics of the subthalamo-pallidal complex in Parkinson's disease during deep brain stimulation.pdf},
  journal = {Journal of Biological Physics},
  language = {en},
  number = {3-4}
}

@article{Moldakarimov2015,
  title = {Feedback Stabilizes Propagation of Synchronous Spiking in Cortical Neural Networks},
  author = {Moldakarimov, Samat and Bazhenov, Maxim and Sejnowski, Terrence J.},
  year = {2015},
  month = feb,
  volume = {112},
  pages = {2545--2550},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1500643112},
  file = {Moldakarimov et al. - 2015 - Feedback stabilizes propagation of synchronous spi.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {8}
}

@article{Molenaar2008,
  title = {On the Implications of the Classical Ergodic Theorems: {{Analysis}} of Developmental Processes Has to Focus on Intra-Individual Variation},
  shorttitle = {On the Implications of the Classical Ergodic Theorems},
  author = {Molenaar, Peter C.M.},
  year = {2008},
  month = jan,
  volume = {50},
  pages = {60--69},
  issn = {00121630, 10982302},
  doi = {10.1002/dev.20262},
  abstract = {It is argued that general mathematical-statistical theorems imply that standard statistical analysis techniques of inter-individual variation are invalid to investigate developmental processes. Developmental processes have to be analyzed at the level of individual subjects, using time series data characterizing the patterns of intra-individual variation. It is shown that standard statistical techniques based on the analysis of inter-individual variation appear to be insensitive to the presence of arbitrary large degrees of inter-individual heterogeneity in the population. An important class of nonlinear epigenetic models of neural growth is described which can explain the occurrence of such heterogeneity in brain structures and behavior. Links with models of developmental instability are discussed. A simulation study based on a chaotic growth model illustrates the invalidity of standard analysis of inter-individual variation, whereas time series analysis of intra-individual variation is able to recover the true state of affairs. \ss{} 2007 Wiley Periodicals, Inc. Dev Psychobiol 50: 60\textendash 69, 2008.},
  file = {Molenaar - 2008 - On the implications of the classical ergodic theor.pdf},
  journal = {Dev. Psychobiol.},
  language = {en},
  number = {1}
}

@article{Molenaar2009,
  title = {The {{New Person}}-{{Specific Paradigm}} in {{Psychology}}},
  author = {Molenaar, Peter C.M. and Campbell, Cynthia G.},
  year = {2009},
  volume = {18},
  pages = {112--117},
  abstract = {Most research methodofogy in the behavioral from the level of interindividual variation to that of intrain?i sciences employs interindividual analyses, which provide vidual variation in time and place. Is this shift between levels information about the state of affairs of the population. valid? It will be shown that, generally speaking, the answer is no. However, as shown by classical mathematical-statistical This is directly evident when applying general mathematical theorems (the ergodic theorems), such analyses do not statistical theorems?the so-called classical ergodic theorems. provide information for, and cannot be applied at, the level These are the first theorems (hence classical) that in the early of the individual, except on rare occasions when the pro 1930s were derived in ergodic theory, a branch of mathematics cesses of interest meet certain stringent conditions. When originally motivated by problems of statistical physics. psychological processes violate these conditions, the in In what follows, we give a heuristic description of the ergodic terindividual analyses that are now standardly applied conditions under which scientific findings based on interindi have to be replaced by analysis ofintraindividual variation vidual variation can be applied to an individual subject. It will in order to obtain valid results. Two illustrations involving also be shown that these conditions are strict and therefore are analysis of intraindividual variation of personality and rarely met. This reality has wide-ranging consequences for emotional processes are given.},
  file = {Molenaar and Campbell - 2009 - The New Person-Specific Paradigm in Psychology.pdf},
  journal = {Current Directions in Psychological Science},
  language = {en},
  number = {2}
}

@article{Molenaar2010,
  title = {Note on Optimization of Individual Psychotherapeutic Processes},
  author = {Molenaar, Peter C.M.},
  year = {2010},
  month = feb,
  volume = {54},
  pages = {208--213},
  issn = {00222496},
  doi = {10.1016/j.jmp.2009.04.003},
  abstract = {Individual psychotherapy typically involves a sequence of recurrent sessions of client\textendash therapist interaction. Accordingly, the psychotherapeutic process can be conceived of as evolving at least at two different time scales: a fast time scale pertaining to the on-going interaction within each session and a slow time scale associated with sequential regularities occurring between consecutive sessions. It is possible to exploit the sequential regularities between sessions in order to assess and optimize an ongoing individual psychotherapeutic process in real time. In this note a computational paradigm is outlined according to which this can be implemented and applied in flexible ways. Illustrations are given with simulated data. A heuristic summary is provided in the closing section.},
  file = {Molenaar - 2010 - Note on optimization of individual psychotherapeut.pdf},
  journal = {Journal of Mathematical Psychology},
  language = {en},
  number = {1}
}

@article{Molgedey1992,
  title = {Suppressing Chaos in Neural Networks by Noise},
  author = {Molgedey, L. and Schuchhardt, J. and Schuster, H. G.},
  year = {1992},
  month = dec,
  volume = {69},
  pages = {3717--3719},
  issn = {0031-9007},
  doi = {10.1103/PhysRevLett.69.3717},
  file = {1992 - Molgedey, Schuchhardt, Schuster - Suppressing chaos in neural networks by noise.pdf},
  journal = {Physical Review Letters},
  language = {en},
  number = {26}
}

@article{Montgomery2004,
  title = {Discrete Synaptic States Define a Major Mechanism of Synapse Plasticity},
  author = {Montgomery, Johanna M. and Madison, Daniel V.},
  year = {2004},
  month = dec,
  volume = {27},
  pages = {744--750},
  issn = {01662236},
  doi = {10.1016/j.tins.2004.10.006},
  file = {2004 - Montgomery, Madison - Discrete synaptic states define a major mechanism of synapse plasticity.pdf},
  journal = {Trends in Neurosciences},
  language = {en},
  number = {12}
}

@article{Moore2017,
  title = {Dynamics of Cortical Dendritic Membrane Potential and Spikes in Freely Behaving Rats},
  author = {Moore, Jason J. and Ravassard, Pascal M. and Ho, David and Acharya, Lavanya and Kees, Ashley L. and Vuong, Cliff and Mehta, Mayank R.},
  year = {2017},
  month = mar,
  volume = {355},
  pages = {eaaj1497},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aaj1497},
  file = {Moore et al. - 2017 - Dynamics of cortical dendritic membrane potential .pdf},
  journal = {Science},
  language = {en},
  number = {6331}
}

@article{Moran2008,
  title = {Bayesian Estimation of Synaptic Physiology from the Spectral Responses of Neural Masses},
  author = {Moran, R.J. and Stephan, K.E. and Kiebel, S.J. and Rombach, N. and O'Connor, W.T. and Murphy, K.J. and Reilly, R.B. and Friston, K.J.},
  year = {2008},
  month = aug,
  volume = {42},
  pages = {272--284},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2008.01.025},
  file = {2008 - Moran et al. - Bayesian estimation of synaptic physiology from the spectral responses of neural masses.pdf},
  journal = {NeuroImage},
  language = {en},
  number = {1}
}

@article{Moran2008a,
  title = {Subthalamic Nucleus Functional Organization Revealed by Parkinsonian Neuronal Oscillations and Synchrony},
  author = {Moran, A. and Bergman, H. and Israel, Z. and {Bar-Gad}, I.},
  year = {2008},
  month = dec,
  volume = {131},
  pages = {3395--3409},
  issn = {1460-2156, 0006-8950},
  doi = {10.1093/brain/awn270},
  file = {2008 - Moran et al. - Subthalamic nucleus functional organization revealed by parkinsonian neuronal oscillations and synchrony.pdf},
  journal = {Brain},
  language = {en},
  number = {12}
}

@article{Morcos2019,
  title = {One Ticket to Win Them All: Generalizing Lottery Ticket Initializations across Datasets and Optimizers},
  shorttitle = {One Ticket to Win Them All},
  author = {Morcos, Ari S. and Yu, Haonan and Paganini, Michela and Tian, Yuandong},
  year = {2019},
  month = oct,
  abstract = {The success of lottery ticket initializations [7] suggests that small, sparsified networks can be trained so long as the network is initialized appropriately. Unfortunately, finding these ``winning ticket'' initializations is computationally expensive. One potential solution is to reuse the same winning tickets across a variety of datasets and optimizers. However, the generality of winning ticket initializations remains unclear. Here, we attempt to answer this question by generating winning tickets for one training configuration (optimizer and dataset) and evaluating their performance on another configuration. Perhaps surprisingly, we found that, within the natural images domain, winning ticket initializations generalized across a variety of datasets, including Fashion MNIST, SVHN, CIFAR-10/100, ImageNet, and Places365, often achieving performance close to that of winning tickets generated on the same dataset. Moreover, winning tickets generated using larger datasets consistently transferred better than those generated using smaller datasets. We also found that winning ticket initializations generalize across optimizers with high performance. These results suggest that winning ticket initializations generated by sufficiently large datasets contain inductive biases generic to neural networks more broadly which improve training across many settings and provide hope for the development of better initialization methods.},
  archiveprefix = {arXiv},
  eprint = {1906.02773},
  eprinttype = {arxiv},
  file = {Morcos et al. - 2019 - One ticket to win them all generalizing lottery t.pdf},
  journal = {arXiv:1906.02773 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  language = {en},
  primaryclass = {cs, stat}
}

@article{Morcos2019a,
  title = {One Ticket to Win Them All: Generalizing Lottery Ticket Initializations across Datasets and Optimizers},
  shorttitle = {One Ticket to Win Them All},
  author = {Morcos, Ari S. and Yu, Haonan and Paganini, Michela and Tian, Yuandong},
  year = {2019},
  month = oct,
  abstract = {The success of lottery ticket initializations [7] suggests that small, sparsified networks can be trained so long as the network is initialized appropriately. Unfortunately, finding these ``winning ticket'' initializations is computationally expensive. One potential solution is to reuse the same winning tickets across a variety of datasets and optimizers. However, the generality of winning ticket initializations remains unclear. Here, we attempt to answer this question by generating winning tickets for one training configuration (optimizer and dataset) and evaluating their performance on another configuration. Perhaps surprisingly, we found that, within the natural images domain, winning ticket initializations generalized across a variety of datasets, including Fashion MNIST, SVHN, CIFAR-10/100, ImageNet, and Places365, often achieving performance close to that of winning tickets generated on the same dataset. Moreover, winning tickets generated using larger datasets consistently transferred better than those generated using smaller datasets. We also found that winning ticket initializations generalize across optimizers with high performance. These results suggest that winning ticket initializations generated by sufficiently large datasets contain inductive biases generic to neural networks more broadly which improve training across many settings and provide hope for the development of better initialization methods.},
  archiveprefix = {arXiv},
  eprint = {1906.02773},
  eprinttype = {arxiv},
  file = {Morcos et al. - 2019 - One ticket to win them all generalizing lottery t 2.pdf},
  journal = {arXiv:1906.02773 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  language = {en},
  primaryclass = {cs, stat}
}

@article{Moreau2009,
  title = {Chance and Strategy in Search Processes},
  author = {Moreau, M and B{\'e}nichou, O and Loverdo, C and Voituriez, R},
  year = {2009},
  month = dec,
  volume = {2009},
  pages = {P12006},
  issn = {1742-5468},
  doi = {10.1088/1742-5468/2009/12/P12006},
  abstract = {We consider a searcher in quest of a target in two situations: in the presence of an infinite number of identical, Poisson distributed targets, and in the presence of a unique target in a finite territory. The searcher alternates intensive search phases, during which it scans the neighbouring territory but does not move, and displacement phases with no target detection. We study the problem of determining the best strategy of displacement for minimizing the mean search time: either a deterministic or a stochastic trajectory. With a reasonable simplifying hypothesis, we show that for Poisson distributed targets, deterministic, self-avoiding trajectories are more efficient than stochastic ones if the detection process involves no memory skills and can be modelled by a Markov process. In contrast, if the detection process is not Markovian, it can be better for the searcher to follow a stochastic trajectory rather than a self-avoiding trajectory, and we give an explicit example of such a memory law. In the case of a unique target, self-avoiding trajectories are always better if an infinite time is available for the search, whereas stochastic trajectories can be more efficient if the searcher has to find the target before a given deadline. Moreover, we show that the gain due to a deterministic trajectory, compared to a stochastic one, is not significant in the case of a large network containing a unique target.},
  file = {Moreau et al. - 2009 - Chance and strategy in search processes.pdf},
  journal = {J. Stat. Mech.},
  language = {en},
  number = {12}
}

@article{Morimoto2005,
  title = {Robust {{Reinforcement Learning}}},
  author = {Morimoto, Jun and Doya, Kenji},
  year = {2005},
  month = feb,
  volume = {17},
  pages = {335--359},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/0899766053011528},
  abstract = {This paper proposes a new reinforcement learning (RL) paradigm that explicitly takes into account input disturbance as well as modeling errors. The use of environmental models in RL is quite popular for both off-line learning by simulations and for on-line action planning. However, the difference between the model and the real environment can lead to unpredictable, often unwanted results. Based on the theory of H oocontrol, we consider a differential game in which a 'disturbing' agent (disturber) tries to make the worst possible disturbance while a 'control' agent (actor) tries to make the best control input. The problem is formulated as finding a minmax solution of a value function that takes into account the norm of the output deviation and the norm of the disturbance. We derive on-line learning algorithms for estimating the value function and for calculating the worst disturbance and the best control in reference to the value function. We tested the paradigm, which we call "Robust Reinforcement Learning (RRL)," in the task of inverted pendulum. In the linear domain, the policy and the value function learned by the on-line algorithms coincided with those derived analytically by the linear H ootheory. For a fully nonlinear swingup task, the control by RRL achieved robust performance against changes in the pendulum weight and friction while a standard RL control could not deal with such environmental changes.},
  file = {Morimoto and Doya - 2005 - Robust Reinforcement Learning.pdf},
  journal = {Neural Computation},
  language = {en},
  number = {2}
}

@article{Morishita2017,
  title = {Postoperative Lead Migration in Deep Brain Stimulation Surgery: {{Incidence}}, Risk Factors, and Clinical Impact},
  shorttitle = {Postoperative Lead Migration in Deep Brain Stimulation Surgery},
  author = {Morishita, Takashi and Hilliard, Justin D. and Okun, Michael S. and Neal, Dan and Nestor, Kelsey A. and Peace, David and Hozouri, Alden A. and Davidson, Mark R. and Bova, Francis J. and Sporrer, Justin M. and Oyama, Genko and Foote, Kelly D.},
  editor = {Toft, Mathias},
  year = {2017},
  month = sep,
  volume = {12},
  pages = {e0183711},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0183711},
  file = {Morishita et al. - 2017 - Postoperative lead migration in deep brain stimula.pdf},
  journal = {PLOS ONE},
  language = {en},
  number = {9}
}

@article{Morris1981,
  title = {Voltage Oscillations in the Barnacle Giant Muscle Fiber},
  author = {Morris, C. and Lecar, H.},
  year = {1981},
  month = jul,
  volume = {35},
  pages = {193--213},
  issn = {00063495},
  doi = {10.1016/S0006-3495(81)84782-0},
  abstract = {Barnacle muscle fibers subjected to constant current stimulation produce a variety of types of oscillatory behavior when the internal medium contains the Ca"+ chelator EGTA. Oscillations are abolished if Ca"+ is removed from the external medium, or if the K+ conductance is blocked. Available voltage-clamp data indicate that the cell's active conductance systems are exceptionally simple. Given the complexity of barnacle fiber voltage behavior, this seems paradoxical. This paper presents an analysis of the possible modes of behavior available to a system of two noninactivating conductance mechanisms, and indicates a good correspondence to the types of behavior exhibited by barnacle fiber. The differential equations of a simple equivalent circuit for the fiber are dealt with by means of some of the mathematical techniques of nonlinear mechanics. General features of the system are (a) a propensity to produce damped or sustained oscillations over a rather broad parameter range, and (b) considerable latitude in the shape of the oscillatory potentials. It is concluded that for cells subject to changeable parameters (either from cell to cell or with time during cellular activity), a system dominated by two noninactivating conductances can exhibit varied oscillatory and bistable behavior.},
  file = {1981 - Morris, Lecar - Voltage oscillations in the barnacle giant muscle fiber.pdf;Morris and Lecar - 1981 - Voltage oscillations in the barnacle giant muscle .pdf},
  journal = {Biophysical Journal},
  language = {en},
  number = {1}
}

@article{Morrison,
  title = {Diversity of Emergent Dynamics in Competitive Threshold-Linear Networks: A Preliminary Report},
  author = {Morrison, Katherine and Degeratu, Anda and Itskov, Vladimir and Curto, Carina},
  pages = {12},
  file = {Morrison et al. - Diversity of emergent dynamics in competitive thre.pdf},
  language = {en}
}

@article{Mosqueiro2017,
  title = {Task Allocation and Site Fidelity Jointly Influence Foraging Regulation in Honeybee Colonies},
  author = {Mosqueiro, Thiago and Cook, Chelsea and Huerta, Ramon and Gadau, J{\"u}rgen and Smith, Brian and {Pinter-Wollman}, Noa},
  year = {2017},
  month = aug,
  volume = {4},
  pages = {170344},
  issn = {2054-5703},
  doi = {10.1098/rsos.170344},
  abstract = {Variation in behavior among group members often impacts collective outcomes. Individuals may vary both in the task that they perform and in the persistence with which they perform each task. Although both the distribution of individuals among tasks and differences among individuals in behavioral persistence can each impact collective behavior, we do not know if and how they jointly affect collective outcomes. Here we use a detailed computational model to examine the joint impact of colony-level distribution among tasks and behavioral persistence of individuals, specifically their fidelity to particular resource sites, on the collective tradeoff between exploring for new resources and exploiting familiar ones. We developed an agent-based model of foraging honey bees, parameterized by data from 5 colonies, in which we simulated scouts, who search the environment for new resources, and individuals who are recruited by the scouts to the newly found resources, i.e., recruits. We varied the persistence to return to a particular food source of both scouts and recruits and found that for each value of persistence there is a different optimal ratio of scouts to recruits that maximizes resource collection by the colony. Furthermore, changes to the persistence of scouts induced opposite effects from changes to the persistence of recruits on the collective foraging of the colony. The proportion of scouts that resulted in the most resources collected by the colony decreased as the persistence of recruits increased. However, this optimal proportion of scouts increased as the persistence of scouts increased. Thus, behavioral persistence and task participation can interact to impact a colony's collective behavior in orthogonal directions. Our work provides new insights and generates new hypotheses into how variation in behavior at both the individual and colony levels jointly impact the trade-off between exploring for new resources and exploiting familiar ones.},
  file = {Mosqueiro et al. - 2017 - Task allocation and site fidelity jointly influenc.pdf},
  journal = {Royal Society Open Science},
  language = {en},
  number = {8}
}

@article{Mostafa2015,
  title = {Rhythmic Inhibition Allows Neural Networks to Search for Maximally Consistent States},
  author = {Mostafa, Hesham and Muller, Lorenz K. and Indiveri, Giacomo},
  year = {2015},
  month = dec,
  volume = {27},
  pages = {2510--2547},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/NECO_a_00785},
  abstract = {Gamma-band rhythmic inhibition is a ubiquitous phenomenon in neural circuits yet its computational role still remains elusive. We show that a model of Gamma-band rhythmic inhibition allows networks of coupled cortical circuit motifs to search for network configurations that best reconcile external inputs with an internal consistency model encoded in the network connectivity. We show that Hebbian plasticity allows the networks to learn the consistency model by example. The search dynamics driven by rhythmic inhibition enable the described networks to solve di cult constraint satisfaction problems without making assumptions about the form of stochastic fluctuations in the network. We show that the search dynamics are well approximated by a stochastic sampling process. We use the described networks to reproduce perceptual multi-stability phenomena with switching times that are a good match to experimental data and show that they provide a general neural framework which can be used to model other 'perceptual inference' phenomena.},
  archiveprefix = {arXiv},
  eprint = {1503.02777},
  eprinttype = {arxiv},
  file = {2015 - Mostafa, Müller, Indiveri - Rhythmic Inhibition Allows Neural Networks to Search for Maximally Consistent States(2).pdf;Mostafa et al. - 2015 - Rhythmic inhibition allows neural networks to sear 2.pdf;Mostafa et al. - 2015 - Rhythmic inhibition allows neural networks to sear 3.pdf;Mostafa et al. - 2015 - Rhythmic inhibition allows neural networks to sear.pdf},
  journal = {Neural Computation},
  keywords = {Quantitative Biology - Neurons and Cognition},
  language = {en},
  number = {12}
}

@article{Mourao-Miranda2007,
  title = {Dynamic Discrimination Analysis: {{A}} Spatial\textendash Temporal {{SVM}}},
  shorttitle = {Dynamic Discrimination Analysis},
  author = {{Mour{\~a}o-Miranda}, Janaina and Friston, Karl J. and Brammer, Michael},
  year = {2007},
  month = may,
  volume = {36},
  pages = {88--99},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2007.02.020},
  file = {2007 - Mourão-Miranda, Friston, Brammer - Dynamic discrimination analysis a spatial-temporal SVM.pdf},
  journal = {NeuroImage},
  language = {en},
  number = {1}
}

@incollection{Mouret2011,
  title = {Novelty-{{Based Multiobjectivization}}},
  booktitle = {New {{Horizons}} in {{Evolutionary Robotics}}},
  author = {Mouret, Jean-Baptiste},
  editor = {Kacprzyk, Janusz and Doncieux, St{\'e}phane and Bred{\`e}che, Nicolas and Mouret, Jean-Baptiste},
  year = {2011},
  volume = {341},
  pages = {139--154},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-18272-3_10},
  abstract = {Novelty search is a recent and promising approach to evolve neuro-controllers, especially to drive robots. The main idea is to maximize the novelty of behaviors instead of the efficiency. However, abandoning the efficiency objective(s) may be too radical in many contexts. In this paper, a Pareto-based multi-objective evolutionary algorithm is employed to reconcile novelty search with objective-based optimization by following a multiobjectivization process. Several multiobjectivizations based on behavioral novelty and on behavioral diversity are compared on a maze navigation task. Results show that the multiobjectivizations is better at fine-tuning behaviors than basic novelty search while keeping a comparable number of iterations to converge.},
  file = {Mouret - 2011 - Novelty-Based Multiobjectivization.pdf},
  isbn = {978-3-642-18271-6 978-3-642-18272-3},
  language = {en}
}

@incollection{Mouret2011b,
  title = {Novelty-{{Based Multiobjectivization}}},
  booktitle = {New {{Horizons}} in {{Evolutionary Robotics}}},
  author = {Mouret, Jean-Baptiste},
  editor = {Kacprzyk, Janusz and Doncieux, St{\'e}phane and Bred{\`e}che, Nicolas and Mouret, Jean-Baptiste},
  year = {2011},
  volume = {341},
  pages = {139--154},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-18272-3_10},
  abstract = {Novelty search is a recent and promising approach to evolve neurocontrollers, especially to drive robots. The main idea is to maximize the novelty of behaviors instead of the efficiency. However, abandoning the efficiency objective(s) may be too radical in many contexts. In this paper, a Pareto-based multi-objective evolutionary algorithm is employed to reconcile novelty search with objective-based optimization by following a multiobjectivization process. Several multiobjectivizations based on behavioral novelty and on behavioral diversity are compared on a maze navigation task. Results show that the bi-objective variant ``Novelty + Fitness'' is better at fine-tuning behaviors than basic novelty search, while keeping a comparable number of iterations to converge.},
  file = {Mouret - 2011 - Novelty-Based Multiobjectivization 3.pdf},
  isbn = {978-3-642-18271-6 978-3-642-18272-3},
  language = {en}
}

@article{Mouret2015,
  title = {Illuminating Search Spaces by Mapping Elites},
  author = {Mouret, Jean-Baptiste and Clune, Jeff},
  year = {2015},
  month = apr,
  abstract = {Many fields use search algorithms, which automatically explore a search space to find high-performing solutions: chemists search through the space of molecules to discover new drugs; engineers search for stronger, cheaper, safer designs, scientists search for models that best explain data, etc. The goal of search algorithms has traditionally been to return the single highest-performing solution in a search space. Here we describe a new, fundamentally different type of algorithm that is more useful because it provides a holistic view of how high-performing solutions are distributed throughout a search space. It creates a map of high-performing solutions at each point in a space defined by dimensions of variation that a user gets to choose. This Multi-dimensional Archive of Phenotypic Elites (MAP-Elites) algorithm illuminates search spaces, allowing researchers to understand how interesting attributes of solutions combine to affect performance, either positively or, equally of interest, negatively. For example, a drug company may wish to understand how performance changes as the size of molecules and their cost-to-produce vary. MAP-Elites produces a large diversity of high-performing, yet qualitatively different solutions, which can be more helpful than a single, high-performing solution. Interestingly, because MAP-Elites explores more of the search space, it also tends to find a better overall solution than state-of-the-art search algorithms. We demonstrate the benefits of this new algorithm in three different problem domains ranging from producing modular neural networks to designing simulated and real soft robots. Because MAP- Elites (1) illuminates the relationship between performance and dimensions of interest in solutions, (2) returns a set of high-performing, yet diverse solutions, and (3) improves finding a single, best solution, it will advance science and engineering.},
  archiveprefix = {arXiv},
  eprint = {1504.04909},
  eprinttype = {arxiv},
  file = {Mouret and Clune - 2015 - Illuminating search spaces by mapping elites.pdf},
  journal = {arXiv:1504.04909 [cs, q-bio]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Neural and Evolutionary Computing,Computer Science - Robotics,Quantitative Biology - Populations and Evolution},
  language = {en},
  primaryclass = {cs, q-bio}
}

@article{Mu2019a,
  title = {Glia {{Accumulate Evidence}} That {{Actions Are Futile}} and {{Suppress Unsuccessful Behavior}}},
  author = {Mu, Yu and Bennett, Davis V. and Rubinov, Mikail and Narayan, Sujatha and Yang, Chao-Tsung and Tanimoto, Masashi and Mensh, Brett D. and Looger, Loren L. and Ahrens, Misha B.},
  year = {2019},
  month = jun,
  volume = {178},
  pages = {27-43.e19},
  issn = {00928674},
  doi = {10.1016/j.cell.2019.05.050},
  abstract = {When a behavior repeatedly fails to achieve its goal, animals often give up and become passive, which can be strategic for preserving energy or regrouping between attempts. It is unknown how the brain identifies behavioral failures and mediates this behavioral-state switch. In larval zebrafish swimming in virtual reality, visual feedback can be withheld so that swim attempts fail to trigger expected visual flow. After tens of seconds of such motor futility, animals became passive for similar durations. Whole-brain calcium imaging revealed noradrenergic neurons that responded specifically to failed swim attempts and radial astrocytes whose calcium levels accumulated with increasing numbers of failed attempts. Using cell ablation and optogenetic or chemogenetic activation, we found that noradrenergic neurons progressively activated brainstem radial astrocytes, which then suppressed swimming. Thus, radial astrocytes perform a computation critical for behavior: they accumulate evidence that current actions are ineffective and consequently drive changes in behavioral states.},
  file = {Mu et al. - 2019 - Glia Accumulate Evidence that Actions Are Futile a 2.pdf},
  journal = {Cell},
  language = {en},
  number = {1}
}

@article{Mugan2020,
  title = {Spatial Planning with Long Visual Range Benefits Escape from Visual Predators in Complex Naturalistic Environments},
  author = {Mugan, Ugurcan and MacIver, Malcolm A.},
  year = {2020},
  month = dec,
  volume = {11},
  pages = {3057},
  issn = {2041-1723},
  doi = {10.1038/s41467-020-16102-1},
  file = {Mugan and MacIver - 2020 - Spatial planning with long visual range benefits e.pdf},
  journal = {Nat Commun},
  language = {en},
  number = {1}
}

@article{Muller2011,
  title = {Spike-{{Timing Dependent Plasticity}} and {{Feed}}-{{Forward Input Oscillations Produce Precise}} and {{Invariant Spike Phase}}-{{Locking}}},
  author = {Muller, Lyle and Brette, Romain and Gutkin, Boris},
  year = {2011},
  volume = {5},
  issn = {1662-5188},
  doi = {10.3389/fncom.2011.00045},
  file = {2011 - Muller, Brette, Gutkin - Spike-Timing Dependent Plasticity and Feed-Forward Input Oscillations Produce Precise and Invariant Spik.pdf},
  journal = {Frontiers in Computational Neuroscience},
  language = {en}
}

@article{Mumford2014,
  title = {The Impact of Study Design on Pattern Estimation for Single-Trial Multivariate Pattern Analysis},
  author = {Mumford, Jeanette A. and Davis, Tyler and Poldrack, Russell A.},
  year = {2014},
  month = dec,
  volume = {103},
  pages = {130--138},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2014.09.026},
  abstract = {A prerequisite for a pattern analysis using functional magnetic resonance imaging (fMRI) data is estimating the patterns from time series data, which then are input into the pattern analysis. Here we focus on how the combination of study design (order and spacing of trials) with pattern estimator impacts the Type I error rate of the subsequent pattern analysis. When Type I errors are inflated, the results are no longer valid, so this work serves as a guide for designing and analyzing MVPA studies with controlled false positive rates. The MVPA strategies examined are pattern classification and similarity, utilizing single trial activation patterns from the same functional run. Primarily focusing on the Least Squares Single and Least Square All pattern estimators, we show that collinearities in the models, along with temporal autocorrelation, can cause false positive correlations between activation pattern estimates that adversely impact the false positive rates of pattern similarity and classification analyses. It may seem intuitive that increasing the interstimulus interval (ISI) would alleviate this issue, but remaining weak correlations between activation patterns persist and have a strong influence in pattern similarity analyses. Pattern similarity analyses using only activation patterns estimated from the same functional run of data are susceptible to inflated false positives unless trials are randomly ordered, with a different randomization for each subject. In other cases, where there is any structure to trial order, valid pattern similarity analysis results can only be obtained if similarity computations are restricted to pairs of activation patterns from independent runs. Likewise, for pattern classification, false positives are minimized when the testing and training sets in cross validation do not contain patterns estimated from the same run.},
  file = {2014 - Mumford, Davis, Poldrack - The impact of study design on pattern estimation for single-trial multivariate pattern analysis.pdf;Mumford et al. - 2014 - The impact of study design on pattern estimation f.pdf},
  journal = {NeuroImage},
  language = {en}
}

@article{Munch2020,
  title = {Nutrient Homeostasis \textemdash{} Translating Internal States to Behavior},
  author = {M{\"u}nch, Daniel and {Ezra-Nevo}, Gili and Francisco, Ana Patr{\'i}cia and Tastekin, Ibrahim and Ribeiro, Carlos},
  year = {2020},
  month = feb,
  volume = {60},
  pages = {67--75},
  issn = {09594388},
  doi = {10.1016/j.conb.2019.10.004},
  file = {Münch et al. - 2020 - Nutrient homeostasis — translating internal states.pdf},
  journal = {Current Opinion in Neurobiology},
  language = {en}
}

@article{Munoz-Moreno2013,
  title = {A {{Magnetic Resonance Image Based Atlas}} of the {{Rabbit Brain}} for {{Automatic Parcellation}}},
  author = {{Mu{\~n}oz-Moreno}, Emma and {Arbat-Plana}, Ariadna and Batalle, Dafnis and Soria, Guadalupe and Illa, Miriam and {Prats-Galino}, Alberto and Eixarch, Elisenda and Gratacos, Eduard},
  editor = {Malmierca, Manuel S.},
  year = {2013},
  month = jul,
  volume = {8},
  pages = {e67418},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0067418},
  abstract = {Rabbit brain has been used in several works for the analysis of neurodevelopment. However, there are not specific digital rabbit brain atlases that allow an automatic identification of brain regions, which is a crucial step for various neuroimage analyses, and, instead, manual delineation of areas of interest must be performed in order to evaluate a specific structure. For this reason, we propose an atlas of the rabbit brain based on magnetic resonance imaging, including both structural and diffusion weighted, that can be used for the automatic parcellation of the rabbit brain. Ten individual atlases, as well as an average template and probabilistic maps of the anatomical regions were built. In addition, an example of automatic segmentation based on this atlas is described.},
  file = {2013 - Muñoz-Moreno et al. - A Magnetic Resonance Image Based Atlas of the Rabbit Brain for Automatic Parcellation.pdf},
  journal = {PLoS ONE},
  language = {en},
  number = {7}
}

@article{Mur2009,
  title = {Revealing Representational Content with Pattern-Information {{fMRI}}\textemdash an Introductory Guide},
  author = {Mur, Marieke and Bandettini, Peter A. and Kriegeskorte, Nikolaus},
  year = {2009},
  month = mar,
  volume = {4},
  pages = {101--109},
  issn = {1749-5024, 1749-5016},
  doi = {10.1093/scan/nsn044},
  file = {2009 - Mur, Bandettini, Kriegeskorte - Revealing representational content with pattern-information fMRI--an introductory guide.pdf},
  journal = {Social Cognitive and Affective Neuroscience},
  language = {en},
  number = {1}
}

@article{Murakami2015,
  title = {Inherent Noise Appears as a {{L\'evy}} Walk in Fish Schools},
  author = {Murakami, Hisashi and Niizato, Takayuki and Tomaru, Takenori and Nishiyama, Yuta and Gunji, Yukio-Pegio},
  year = {2015},
  month = sep,
  volume = {5},
  pages = {10605},
  issn = {2045-2322},
  doi = {10.1038/srep10605},
  file = {Murakami et al. - 2015 - Inherent noise appears as a Lévy walk in fish scho.pdf},
  journal = {Sci Rep},
  language = {en},
  number = {1}
}

@article{Murakami2015a,
  title = {Inherent Noise Appears as a {{L\'evy}} Walk in Fish Schools},
  author = {Murakami, Hisashi and Niizato, Takayuki and Tomaru, Takenori and Nishiyama, Yuta and Gunji, Yukio-Pegio},
  year = {2015},
  month = sep,
  volume = {5},
  pages = {10605},
  issn = {2045-2322},
  doi = {10.1038/srep10605},
  file = {Murakami et al. - 2015 - Inherent noise appears as a Lévy walk in fish scho 2.pdf},
  journal = {Sci Rep},
  language = {en},
  number = {1}
}

@article{Murdock2017,
  title = {Exploration and Exploitation of {{Victorian}} Science in {{Darwin}}'s Reading Notebooks},
  author = {Murdock, Jaimie and Allen, Colin and DeDeo, Simon},
  year = {2017},
  month = feb,
  volume = {159},
  pages = {117--126},
  issn = {00100277},
  doi = {10.1016/j.cognition.2016.11.012},
  abstract = {Search in an environment with an uncertain distribution of resources involves a trade-off between exploitation of past discoveries and further exploration. This extends to information foraging, where a knowledge-seeker shifts between reading in depth and studying new domains. To study this decisionmaking process, we examine the reading choices made by one of the most celebrated scientists of the modern era: Charles Darwin. From the full-text of books listed in his chronologically-organized reading journals, we generate topic models to quantify his local (text-to-text) and global (text-to-past) reading decisions using Kullback-Liebler Divergence, a cognitively-validated, information-theoretic measure of relative surprise. Rather than a pattern of surprise-minimization, corresponding to a pure exploitation strategy, Darwin's behavior shifts from early exploitation to later exploration, seeking unusually high levels of cognitive surprise relative to previous eras. These shifts, detected by an unsupervised Bayesian model, correlate with major intellectual epochs of his career as identified both by qualitative scholarship and Darwin's own self-commentary. Our methods allow us to compare his consumption of texts with their publication order. We find Darwin's consumption more exploratory than the culture's production, suggesting that underneath gradual societal changes are the explorations of individual synthesis and discovery. Our quantitative methods advance the study of cognitive search through a framework for testing interactions between individual and collective behavior and between short- and longterm consumption choices. This novel application of topic modeling to characterize individual reading complements widespread studies of collective scientific behavior.},
  file = {Murdock et al. - 2017 - Exploration and exploitation of Victorian science .pdf},
  journal = {Cognition},
  language = {en}
}

@article{Murdock2017a,
  title = {Exploration and {{Exploitation}} of {{Victorian Science}} in {{Darwin}}'s {{Reading Notebooks}}},
  author = {Murdock, Jaimie and Allen, Colin and DeDeo, Simon},
  year = {2017},
  month = feb,
  volume = {159},
  pages = {117--126},
  issn = {00100277},
  doi = {10.1016/j.cognition.2016.11.012},
  abstract = {Search in an environment with an uncertain distribution of resources involves a trade-off between exploitation of past discoveries and further exploration. This extends to information foraging, where a knowledge-seeker shifts between reading in depth and studying new domains. To study this decision-making process, we examine the reading choices made by one of the most celebrated scientists of the modern era: Charles Darwin. From the full-text of books listed in his chronologically-organized reading journals, we generate topic models to quantify his local (text-to-text) and global (text-to-past) reading decisions using Kullback-Liebler Divergence, a cognitively-validated, information-theoretic measure of relative surprise. Rather than a pattern of surprise-minimization, corresponding to a pure exploitation strategy, Darwin's behavior shifts from early exploitation to later exploration, seeking unusually high levels of cognitive surprise relative to previous eras. These shifts, detected by an unsupervised Bayesian model, correlate with major intellectual epochs of his career as identified both by qualitative scholarship and Darwin's own self-commentary. Our methods allow us to compare his consumption of texts with their publication order. We find Darwin's consumption more exploratory than the culture's production, suggesting that underneath gradual societal changes are the explorations of individual synthesis and discovery. Our quantitative methods advance the study of cognitive search through a framework for testing interactions between individual and collective behavior and between short- and long-term consumption choices. This novel application of topic modeling to characterize individual reading complements widespread studies of collective scientific behavior.},
  archiveprefix = {arXiv},
  eprint = {1509.07175},
  eprinttype = {arxiv},
  file = {Murdock et al. - 2017 - Exploration and Exploitation of Victorian Science  2.pdf},
  journal = {Cognition},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computers and Society,Computer Science - Digital Libraries,Physics - Physics and Society},
  language = {en}
}

@article{Murdock2017b,
  title = {Exploration and Exploitation of {{Victorian}} Science in {{Darwin}}'s Reading Notebooks},
  author = {Murdock, Jaimie and Allen, Colin and DeDeo, Simon},
  year = {2017},
  month = feb,
  volume = {159},
  pages = {117--126},
  issn = {00100277},
  doi = {10.1016/j.cognition.2016.11.012},
  abstract = {Search in an environment with an uncertain distribution of resources involves a trade-off between exploitation of past discoveries and further exploration. This extends to information foraging, where a knowledge-seeker shifts between reading in depth and studying new domains. To study this decisionmaking process, we examine the reading choices made by one of the most celebrated scientists of the modern era: Charles Darwin. From the full-text of books listed in his chronologically-organized reading journals, we generate topic models to quantify his local (text-to-text) and global (text-to-past) reading decisions using Kullback-Liebler Divergence, a cognitively-validated, information-theoretic measure of relative surprise. Rather than a pattern of surprise-minimization, corresponding to a pure exploitation strategy, Darwin's behavior shifts from early exploitation to later exploration, seeking unusually high levels of cognitive surprise relative to previous eras. These shifts, detected by an unsupervised Bayesian model, correlate with major intellectual epochs of his career as identified both by qualitative scholarship and Darwin's own self-commentary. Our methods allow us to compare his consumption of texts with their publication order. We find Darwin's consumption more exploratory than the culture's production, suggesting that underneath gradual societal changes are the explorations of individual synthesis and discovery. Our quantitative methods advance the study of cognitive search through a framework for testing interactions between individual and collective behavior and between short- and longterm consumption choices. This novel application of topic modeling to characterize individual reading complements widespread studies of collective scientific behavior.},
  file = {Murdock et al. - 2017 - Exploration and exploitation of Victorian science  3.pdf},
  journal = {Cognition},
  language = {en}
}

@article{Murphy2003,
  title = {Multiplicative {{Gain Changes Are Induced}} by {{Excitation}} or {{Inhibition Alone}}},
  author = {Murphy, Brendan K. and Miller, Kenneth D.},
  year = {2003},
  month = nov,
  volume = {23},
  pages = {10040--10051},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.23-31-10040.2003},
  file = {2003 - Murphy, Miller - Multiplicative gain changes are induced by excitation or inhibition alone.pdf},
  journal = {The Journal of Neuroscience},
  language = {en},
  number = {31}
}

@article{Murray2017,
  title = {Stable Population Coding for Working Memory Coexists with Heterogeneous Neural Dynamics in Prefrontal Cortex},
  author = {Murray, John D. and Bernacchia, Alberto and Roy, Nicholas A. and Constantinidis, Christos and Romo, Ranulfo and Wang, Xiao-Jing},
  year = {2017},
  month = jan,
  volume = {114},
  pages = {394--399},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1619449114},
  abstract = {Working memory (WM) is a cognitive function for temporary maintenance and manipulation of information, which requires conversion of stimulus-driven signals into internal representations that are maintained across seconds-long mnemonic delays. Within primate prefrontal cortex (PFC), a critical node of the brain's WM network, neurons show stimulus-selective persistent activity during WM, but many of them exhibit strong temporal dynamics and heterogeneity, raising the questions of whether, and how, neuronal populations in PFC maintain stable mnemonic representations of stimuli during WM. Here we show that despite complex and heterogeneous temporal dynamics in single-neuron activity, PFC activity is endowed with a population-level coding of the mnemonic stimulus that is stable and robust throughout WM maintenance. We applied population-level analyses to hundreds of recorded single neurons from lateral PFC of monkeys performing two seminal tasks that demand parametric WM: oculomotor delayed response and vibrotactile delayed discrimination. We found that the high-dimensional state space of PFC population activity contains a low-dimensional subspace in which stimulus representations are stable across time during the cue and delay epochs, enabling robust and generalizable decoding compared with time-optimized subspaces. To explore potential mechanisms, we applied these same population-level analyses to theoretical neural circuit models of WM activity. Three previously proposed models failed to capture the key population-level features observed empirically. We propose network connectivity properties, implemented in a linear network model, which can underlie these features. This work uncovers stable population-level WM representations in PFC, despite strong temporal neural dynamics, thereby providing insights into neural circuit mechanisms supporting WM.},
  file = {Murray et al. - 2017 - Stable population coding for working memory coexis.pdf},
  journal = {Proc Natl Acad Sci USA},
  language = {en},
  number = {2}
}

@article{Musall2019,
  title = {Single-Trial Neural Dynamics Are Dominated by Richly Varied Movements},
  author = {Musall, Simon and Kaufman, Matthew T. and Juavinett, Ashley L. and Gluf, Steven and Churchland, Anne K.},
  year = {2019},
  month = oct,
  volume = {22},
  pages = {1677--1686},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/s41593-019-0502-4},
  file = {Musall et al. - 2019 - Single-trial neural dynamics are dominated by rich.pdf},
  journal = {Nat Neurosci},
  language = {en},
  number = {10}
}

@article{Muscinelli2017,
  title = {Exponentially {{Long Orbits}} in {{Hopfield Neural Networks}}},
  author = {Muscinelli, Samuel P. and Gerstner, Wulfram and Brea, Johanni},
  year = {2017},
  month = feb,
  volume = {29},
  pages = {458--484},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/NECO_a_00919},
  file = {Muscinelli et al. - 2017 - Exponentially Long Orbits in Hopfield Neural Netwo.pdf},
  journal = {Neural Computation},
  language = {en},
  number = {2}
}

@article{Muto2013,
  title = {Real-{{Time Visualization}} of {{Neuronal Activity}} during {{Perception}}},
  author = {Muto, Akira and Ohkura, Masamichi and Abe, Gembu and Nakai, Junichi and Kawakami, Koichi},
  year = {2013},
  month = feb,
  volume = {23},
  pages = {307--311},
  issn = {09609822},
  doi = {10.1016/j.cub.2012.12.040},
  abstract = {To understand how the brain perceives the external world, it is desirable to observe neuronal activity in the brain in real time during perception. The zebrafish is a suitable model animal for fluorescence imaging studies to visualize neuronal activity because its body is transparent through the embryonic and larval stages. Imaging studies have been carried out to monitor neuronal activity in the larval spinal cord and brain using Ca2+ indicator dyes [1\textendash 3] and DNA-encoded Ca2+ indicators, such as Cameleon [4], GFPaequorin [5], and GCaMPs [6\textendash 12]. However, temporal and spatial resolution and sensitivity of these tools are still limited, and imaging of brain activity during perception of a natural object has not yet been demonstrated. Here we demonstrate visualization of neuronal activity in the optic tectum of larval zebrafish by genetically expressing the new version of GCaMP. First, we demonstrate Ca2+ transients in the tectum evoked by a moving spot on a display and identify direction-selective neurons. Second, we show tectal activity during perception of a natural object, a swimming paramecium, revealing a functional visuotopic map. Finally, we image the tectal responses of a free-swimming larval fish to a paramecium and thereby correlate neuronal activity in the brain with prey capture behavior.},
  file = {2013 - Muto et al. - Real-Time Visualization of Neuronal Activity during Perception.pdf},
  journal = {Current Biology},
  language = {en},
  number = {4}
}

@article{Muyesser,
  title = {Learning Model-Based Strategies in Simple Environments with Hierarchical q-Networks},
  author = {Muyesser, Necati Alp and Dunovan, Kyle and Verstynen, Timothy},
  pages = {29},
  abstract = {Recent advances in deep learning have allowed artificial agents to rival human-level performance on a wide range of complex tasks; however, the ability of these networks to learn generalizable strategies remains a pressing challenge. This critical limitation is due in part to two factors: the opaque information representation in deep neural networks and the complexity of the task environments in which they are typically deployed. Here we propose a novel Hierarchical Q-Network (HQN), motivated by theories of the hierarchical organization of the human prefrontal cortex, that attempts to identify lower dimensional patterns in the value landscape that can be exploited to construct an internal model of rules in simple environments. We draw on combinatorial games, where there exists a single optimal strategy for winning that generalizes across other features of the game, to probe the strategy generalization of the HQN and other reinforcement learning (RL) agents using variations of Wythoff's game. Traditional RL approaches failed to reach satisfactory performance on variants of Wythoff's Game; however, the HQN learned heuristic-like strategies that generalized across changes in board configuration. More importantly, the HQN allowed for transparent inspection of the agent's internal model of the game following training. Our results show how a biologically inspired hierarchical learner can facilitate learning abstract rules to promote robust and flexible action policies in simplified training environments with clearly delineated optimal strategies.},
  file = {Muyesser et al. - Learning model-based strategies in simple environm.pdf},
  language = {en}
}

@article{Myerson2003,
  title = {Effects of {{Age}}, {{Domain}}, and {{Processing Demands}} on {{Memory Span}}: {{Evidence}} for {{Differential Decline}}},
  shorttitle = {Effects of {{Age}}, {{Domain}}, and {{Processing Demands}} on {{Memory Span}}},
  author = {Myerson, Joel and Emery, Lisa and White, Desir{\'e}e A. and Hale, Sandra},
  year = {2003},
  month = mar,
  volume = {10},
  pages = {20--27},
  issn = {1382-5585, 1744-4128},
  doi = {10.1076/anec.10.1.20.13454},
  abstract = {Analysis of cross-sectional data from the normative sample of the Wechsler Memory Scale \textendash{} Third Edition (WMS-III) revealed different patterns of age-related differences in memory span measures depending on the type of memory item, processing demands, and the age of the older adult group. Regression of memory span on age revealed that the slope for Spatial Span raw scores was significantly more negative than the slope for Digit Span raw scores. There was no significant difference, however, either between the slopes for forward and backward Digit Span or between the slopes for forward and backward Spatial Span. Regression of Letter-Number Sequencing raw scores on age showed a distinctive, curvilinear pattern. Taken together, the present findings suggest that at least two mechanisms are involved in age-related differences in memory span. One mechanism, associated with a relatively linear decrease in memory span as a function of age, may differentially affect the storage of different types of information (e.g., sequences of digits vs. spatial locations). The other mechanism, evidenced by the curvilinear trend in Letter-Number Sequencing scores, may be tentatively attributed to a decline in executive aspects of working memory that becomes increasingly pronounced with age.},
  file = {2003 - Myerson et al. - Effects of age, domain, and processing demands on memory span Evidence for differential decline.pdf},
  journal = {Aging, Neuropsychology, and Cognition},
  language = {en},
  number = {1}
}

@article{Nadim2014,
  title = {Neuromodulation of Neurons and Synapses},
  author = {Nadim, Farzan and Bucher, Dirk},
  year = {2014},
  month = dec,
  volume = {29},
  pages = {48--56},
  issn = {09594388},
  doi = {10.1016/j.conb.2014.05.003},
  file = {2014 - Nadim, Bucher - Neuromodulation of neurons and synapses.pdf;Nadim and Bucher - 2014 - Neuromodulation of neurons and synapses 2.pdf;Nadim and Bucher - 2014 - Neuromodulation of neurons and synapses.pdf},
  journal = {Current Opinion in Neurobiology},
  language = {en}
}

@article{Nagabandi2019,
  title = {Deep {{Online Learning}} via {{Meta}}-{{Learning}}: {{Continual Adaptation}} for {{Model}}-{{Based RL}}},
  shorttitle = {Deep {{Online Learning}} via {{Meta}}-{{Learning}}},
  author = {Nagabandi, Anusha and Finn, Chelsea and Levine, Sergey},
  year = {2019},
  month = jan,
  abstract = {Humans and animals can learn complex predictive models that allow them to accurately and reliably reason about real-world phenomena, and they can adapt such models extremely quickly in the face of unexpected changes. Deep neural network models allow us to represent very complex functions, but lack this capacity for rapid online adaptation. The goal in this paper is to develop a method for continual online learning from an incoming stream of data, using deep neural network models. We formulate an online learning procedure that uses stochastic gradient descent to update model parameters, and an expectation maximization algorithm with a Chinese restaurant process prior to develop and maintain a mixture of models to handle non-stationary task distributions. This allows for all models to be adapted as necessary, with new models instantiated for task changes and old models recalled when previously seen tasks are encountered again. Furthermore, we observe that meta-learning can be used to meta-train a model such that this direct online adaptation with SGD is effective, which is otherwise not the case for large function approximators. In this work, we apply our meta-learning for online learning (MOLe) approach to model-based reinforcement learning, where adapting the predictive model is critical for control; we demonstrate that MOLe outperforms alternative prior methods, and enables effective continuous adaptation in non-stationary task distributions such as varying terrains, motor failures, and unexpected disturbances.},
  archiveprefix = {arXiv},
  eprint = {1812.07671},
  eprinttype = {arxiv},
  file = {Nagabandi et al. - 2019 - Deep Online Learning via Meta-Learning Continual .pdf},
  journal = {arXiv:1812.07671 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},
  language = {en},
  primaryclass = {cs, stat}
}

@article{Nagy2020,
  title = {Synergistic {{Benefits}} of {{Group Search}} in {{Rats}}},
  author = {Nagy, M{\'a}t{\'e} and Horics{\'a}nyi, Attila and Kubinyi, Enik{\H o} and Couzin, Iain D. and V{\'a}s{\'a}rhelyi, G{\'a}bor and Flack, Andrea and Vicsek, Tam{\'a}s},
  year = {2020},
  month = sep,
  pages = {S0960982220312690},
  issn = {09609822},
  doi = {10.1016/j.cub.2020.08.079},
  abstract = {Locating unpredictable but essential resources is a task that all mobile animals have to perform in order to survive and reproduce. Research on search strategies has focused largely on independent individuals [1\textendash 3], but many organisms display collective behaviors, including during group search and foraging [4\textendash 6]. One classical experimental search task, informing studies of navigation, memory, and learning, is the location of a reward in a confined, complex maze setting [7, 8]. Rats (Rattus norvegicus) have been paradigmatic in psychological and biological studies [9, 10], but despite rats being highly social [11, 12], their group search behavior has not been investigated. Here, we explore the decision making of rats searching individually, or in groups, for a reward in a complex maze environment. Using automated video tracking, we find that rats exhibit\textemdash even when alone\textemdash a partially systematic search, leading to a continuous increase in their chance of finding the reward because of increased attraction to unexplored regions. When searching together, however, synergistic group advantages arise through integration of individual exploratory and social behavior. The superior search performances result from a strategy that represents a hierarchy of influential preferences in response to social and asocial cues. Furthermore, we present a computational model to compare the essential factors that influence how collective search operates and to validate that the collective search strategy increases the search efficiency of individuals in groups. This strategy can serve as direct inspiration for designing computational search algorithms and systems, such as autonomous robot groups, to explore areas inaccessible to humans.},
  file = {Nagy et al. - 2020 - Synergistic Benefits of Group Search in Rats.pdf},
  journal = {Current Biology},
  language = {en}
}

@article{Naik2019,
  title = {Discounted {{Reinforcement Learning Is Not}} an {{Optimization Problem}}},
  author = {Naik, Abhishek and Shariff, Roshan and Yasui, Niko and Yao, Hengshuai and Sutton, Richard S.},
  year = {2019},
  month = nov,
  abstract = {Discounted reinforcement learning is fundamentally incompatible with function approximation for control in continuing tasks. It is not an optimization problem in its usual formulation, so when using function approximation there is no optimal policy. We substantiate these claims, then go on to address some misconceptions about discounting and its connection to the average reward formulation. We encourage researchers to adopt rigorous optimization approaches, such as maximizing average reward, for reinforcement learning in continuing tasks.},
  archiveprefix = {arXiv},
  eprint = {1910.02140},
  eprinttype = {arxiv},
  file = {Naik et al. - 2019 - Discounted Reinforcement Learning Is Not an Optimi.pdf},
  journal = {arXiv:1910.02140 [cs]},
  keywords = {Computer Science - Artificial Intelligence},
  language = {en},
  primaryclass = {cs}
}

@article{Naik2019a,
  title = {Discounted {{Reinforcement Learning Is Not}} an {{Optimization Problem}}},
  author = {Naik, Abhishek and Shariff, Roshan and Yasui, Niko and Yao, Hengshuai and Sutton, Richard S.},
  year = {2019},
  month = nov,
  abstract = {Discounted reinforcement learning is fundamentally incompatible with function approximation for control in continuing tasks. It is not an optimization problem in its usual formulation, so when using function approximation there is no optimal policy. We substantiate these claims, then go on to address some misconceptions about discounting and its connection to the average reward formulation. We encourage researchers to adopt rigorous optimization approaches, such as maximizing average reward, for reinforcement learning in continuing tasks.},
  archiveprefix = {arXiv},
  eprint = {1910.02140},
  eprinttype = {arxiv},
  file = {Naik et al. - 2019 - Discounted Reinforcement Learning Is Not an Optimi 2.pdf},
  journal = {arXiv:1910.02140 [cs]},
  keywords = {Computer Science - Artificial Intelligence},
  language = {en},
  primaryclass = {cs}
}

@article{Naim2020,
  title = {Fundamental {{Law}} of {{Memory Recall}}},
  author = {Naim, Michelangelo and Katkov, Mikhail and Romani, Sandro and Tsodyks, Misha},
  year = {2020},
  month = jan,
  volume = {124},
  pages = {018101},
  issn = {0031-9007, 1079-7114},
  doi = {10.1103/PhysRevLett.124.018101},
  abstract = {Human memory appears to be fragile and unpredictable. Free recall of random lists of words is a standard paradigm used to probe episodic memory. We proposed an associative search process that can be reduced to a deterministic walk on random graphs defined by the structure of memory representations. The corresponding graph model can be solved analytically, resulting in a novel parameter-free prediction for the average number of memory items recalled (R) out of M items in memory: R = 3{$\pi$}M/2. This prediction was verified with a specially designed experimental protocol combining large-scale crowd-sourced free recall and recognition experiments with randomly assembled lists of words or common facts. Our results show that human memory can be described by universal laws derived from first principles.},
  archiveprefix = {arXiv},
  eprint = {1905.02403},
  eprinttype = {arxiv},
  file = {Naim et al. - 2020 - Fundamental Law of Memory Recall.pdf},
  journal = {Phys. Rev. Lett.},
  keywords = {Quantitative Biology - Neurons and Cognition},
  language = {en},
  number = {1}
}

@article{Naim2020a,
  title = {Fundamental {{Law}} of {{Memory Recall}}},
  author = {Naim, Michelangelo and Katkov, Mikhail and Romani, Sandro and Tsodyks, Misha},
  year = {2020},
  month = jan,
  volume = {124},
  pages = {018101},
  issn = {0031-9007, 1079-7114},
  doi = {10.1103/PhysRevLett.124.018101},
  file = {Naim et al. - 2020 - Fundamental Law of Memory Recall 2.pdf},
  journal = {Phys. Rev. Lett.},
  language = {en},
  number = {1}
}

@article{Namboodiri2016,
  title = {Rationalizing Spatial Exploration Patterns of Wild Animals and Humans through a Temporal Discounting Framework},
  author = {Namboodiri, Vijay Mohan K. and Levy, Joshua M. and Mihalas, Stefan and Sims, David W. and Hussain Shuler, Marshall G.},
  year = {2016},
  month = aug,
  volume = {113},
  pages = {8747--8752},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1601664113},
  abstract = {Understanding the exploration patterns of foragers in the wild provides fundamental insight into animal behavior. Recent experimental evidence has demonstrated that path lengths (distances between consecutive turns) taken by foragers are well fitted by a power law distribution. Numerous theoretical contributions have posited that ``L\'evy random walks''\textemdash which can produce power law path length distributions\textemdash are optimal for memoryless agents searching a sparse reward landscape. It is unclear, however, whether such a strategy is efficient for cognitively complex agents, from wild animals to humans. Here, we developed a model to explain the emergence of apparent power law path length distributions in animals that can learn about their environments. In our model, the agent's goal during search is to build an internal model of the distribution of rewards in space that takes into account the cost of time to reach distant locations (i.e., temporally discounting rewards). For an agent with such a goal, we find that an optimal model of exploration in fact produces hyperbolic path lengths, which are well approximated by power laws. We then provide support for our model by showing that humans in a laboratory spatial exploration task search space systematically and modify their search patterns under a cost of time. In addition, we find that path length distributions in a large dataset obtained from free-ranging marine vertebrates are well described by our hyperbolic model. Thus, we provide a general theoretical framework for understanding spatial exploration patterns of cognitively complex foragers.},
  file = {Namboodiri et al. - 2016 - Rationalizing spatial exploration patterns of wild.pdf},
  journal = {Proc Natl Acad Sci USA},
  language = {en},
  number = {31}
}

@article{Nandy2019,
  title = {Optogenetically Induced Low-Frequency Correlations Impair Perception},
  author = {Nandy, Anirvan and Nassi, Jonathan J and Jadi, Monika P and Reynolds, John},
  year = {2019},
  volume = {8:e35123},
  pages = {18},
  file = {Nandy et al. - Optogenetically induced low-frequency correlations.pdf},
  journal = {eLife},
  language = {en}
}

@article{Naselaris2011,
  title = {Encoding and Decoding in {{fMRI}}},
  author = {Naselaris, Thomas and Kay, Kendrick N. and Nishimoto, Shinji and Gallant, Jack L.},
  year = {2011},
  month = may,
  volume = {56},
  pages = {400--410},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2010.07.073},
  abstract = {Over the past decade fMRI researchers have developed increasingly sensitive techniques for analyzing the information represented in BOLD activity. The most popular of these techniques is linear classification, a simple technique for decoding information about experimental stimuli or tasks from patterns of activity across an array of voxels. A more recent development is the voxel-based encoding model, which describes the information about the stimulus or task that is represented in the activity of single voxels. Encoding and decoding are complementary operations: encoding uses stimuli to predict activity while decoding uses activity to predict information about the stimuli. However, in practice these two operations are often confused, and their respective strengths and weaknesses have not been made clear. Here we use the concept of a linearizing feature space to clarify the relationship between encoding and decoding. We show that encoding and decoding operations can both be used to investigate some of the most common questions about how information is represented in the brain. However, focusing on encoding models offers two important advantages over decoding. First, an encoding model can in principle provide a complete functional description of a region of interest, while a decoding model can provide only a partial description. Second, while it is straightforward to derive an optimal decoding model from an encoding model it is much more difficult to derive an encoding model from a decoding model. We propose a systematic modeling approach that begins by estimating an encoding model for every voxel in a scan and ends by using the estimated encoding models to perform decoding.},
  file = {2011 - Naselaris et al. - Encoding and decoding in fMRI.pdf},
  journal = {NeuroImage},
  language = {en},
  number = {2}
}

@article{Naud2008,
  title = {Firing Patterns in the Adaptive Exponential Integrate-and-Fire Model},
  author = {Naud, Richard and Marcille, Nicolas and Clopath, Claudia and Gerstner, Wulfram},
  year = {2008},
  month = nov,
  volume = {99},
  pages = {335--347},
  issn = {0340-1200, 1432-0770},
  doi = {10.1007/s00422-008-0264-7},
  abstract = {For simulations of large spiking neuron networks, an accurate, simple and versatile single-neuron modeling framework is required. Here we explore the versatility of a simple two-equation model: the adaptive exponential integrate-and-fire neuron. We show that this model generates multiple firing patterns depending on the choice of parameter values, and present a phase diagram describing the transition from one firing type to another. We give an analytical criterion to distinguish between continuous adaption, initial bursting, regular bursting and two types of tonic spiking. Also, we report that the deterministic model is capable of producing irregular spiking when stimulated with constant current, indicating low-dimensional chaos. Lastly, the simple model is fitted to real experiments of cortical neurons under step current stimulation. The results provide support for the suitability of simple models such as the adaptive exponential integrate-and-fire neuron for large network simulations.},
  file = {2008 - Naud et al. - Firing patterns in the adaptive exponential integrate-and-fire model.pdf},
  journal = {Biological Cybernetics},
  language = {en},
  number = {4-5}
}

@article{Naumann2016,
  title = {From {{Whole}}-{{Brain Data}} to {{Functional Circuit Models}}: {{The Zebrafish Optomotor Response}}},
  shorttitle = {From {{Whole}}-{{Brain Data}} to {{Functional Circuit Models}}},
  author = {Naumann, Eva A. and Fitzgerald, James E. and Dunn, Timothy W. and Rihel, Jason and Sompolinsky, Haim and Engert, Florian},
  year = {2016},
  month = nov,
  volume = {167},
  pages = {947-960.e20},
  issn = {00928674},
  doi = {10.1016/j.cell.2016.10.019},
  abstract = {Detailed descriptions of brain-scale sensorimotor circuits underlying vertebrate behavior remain elusive. Recent advances in zebrafish neuroscience offer new opportunities to dissect such circuits via whole-brain imaging, behavioral analysis, functional perturbations, and network modeling. Here, we harness these tools to generate a brain-scale circuit model of the optomotor response, an orienting behavior evoked by visual motion. We show that such motion is processed by diverse neural response types distributed across multiple brain regions. To transform sensory input into action, these regions sequentially integrate eye- and direction-specific sensory streams, refine representations via interhemispheric inhibition, and demix locomotor instructions to independently drive turning and forward swimming. While experiments revealed many neural response types throughout the brain, modeling identified the dimensions of functional connectivity most critical for the behavior. We thus reveal how distributed neurons collaborate to generate behavior and illustrate a paradigm for distilling functional circuit models from whole-brain data.},
  file = {Naumann et al. - 2016 - From Whole-Brain Data to Functional Circuit Models.pdf},
  journal = {Cell},
  language = {en},
  number = {4}
}

@article{Nauta1978,
  title = {Efferent Projections of the Subthalamic Nucleus: {{An}} Autoradiographic Study in Monkey and Cat},
  shorttitle = {Efferent Projections of the Subthalamic Nucleus},
  author = {Nauta, Haring J. W. and Cole, Monroe},
  year = {1978},
  month = jul,
  volume = {180},
  pages = {1--16},
  issn = {0021-9967, 1096-9861},
  doi = {10.1002/cne.901800102},
  abstract = {The efferent projections of the subthalamic nucleus were studied with the autoradiographic tracing technique in Rhesus monkey and cat. From the data i t appears that the major efferent projections of the nucleus are to the pallidal complex and the substantia nigra. In both monkey and cat, the projection to the pallidal complex is truly massive and is directed a t both pallidal segments. The projection field includes an infracommissural part of the pallidal complex bordering on the substantia innominata. In the monkey the termination in the pallidal complex is organized in several characteristic bands oriented parallel to the medullary laminae. The subthalamo-pallidal projection in monkey further appears to be topographically organized. The projections to the substantia nigra is prominent in both cat and monkey though not as massive as that to the pallidal complex. The distribution of termination in the substantia nigra favors the more ventral strata near the cerebral peduncle. In the monkey the terminal distribution appears to avoid regions of the substantia nigra containing pigmented neurons and it is suggested that the subthalamonigral pathway may prefer non-dopaminergic neurons. In addition to the above major projections, sparse projections were noted to the thalamic nuclei ventralis lateralis and ventralis anterior, to the putamen, and to the mesencephalic nucleus tegmenti pedunculopontinus, pars compacta. The findings are discussed.},
  file = {1978 - Nauta, Cole - Efferent projections of the subthalamic nucleus An autoradiographic study in monkey and cat.pdf;Nauta and Cole - 1978 - Efferent projections of the subthalamic nucleus A.pdf},
  journal = {The Journal of Comparative Neurology},
  language = {en},
  number = {1}
}

@article{Navarro2016,
  title = {Learning and Choosing in an Uncertain World: {{An}} Investigation of the Explore\textendash Exploit Dilemma in Static and Dynamic Environments},
  shorttitle = {Learning and Choosing in an Uncertain World},
  author = {Navarro, Daniel J. and Newell, Ben R. and Schulze, Christin},
  year = {2016},
  month = mar,
  volume = {85},
  pages = {43--77},
  issn = {00100285},
  doi = {10.1016/j.cogpsych.2016.01.001},
  abstract = {How do people solve the explore\textendash exploit trade-off in a changing environment? In this paper we present experimental evidence from an ``observe or bet'' task, in which people have to determine when to engage in information-seeking behavior and when to switch to reward-taking actions. In particular we focus on the comparison between people's behavior in a changing environment and their behavior in an unchanging one. Our experimental work is motivated by rational analysis of the problem that makes strong predictions about information search and reward seeking in static and changeable environments. Our results show a striking agreement between human behavior and the optimal policy, but also highlight a number of systematic differences. In particular, we find that while people often employ suboptimal strategies the first time they encounter the learning problem, most people are able to approximate the correct strategy after minimal experience. In order to describe both the manner in which people's choices are similar to but slightly different from an optimal standard, we introduce four process models for the observe or bet task and evaluate them as potential theories of human behavior.},
  file = {Navarro et al. - 2016 - Learning and choosing in an uncertain world An in.pdf},
  journal = {Cognitive Psychology},
  language = {en}
}

@article{Neftci,
  title = {Surrogate {{Gradient Learning}} in {{Spiking Neural Networks}}},
  author = {Neftci, Emre O and Mostafa, Hesham and Zenke, Friedemann},
  pages = {21},
  abstract = {A growing number of neuromorphic spiking neural network processors that emulate biological neural networks create an imminent need for methods and tools to enable them to solve real-world signal processing problems. Like conventional neural networks, spiking neural networks are particularly efficient when trained on real, domain specific data. However, their training requires overcoming a number of challenges linked to their binary and dynamical nature. This tutorial elucidates step-by-step the problems typically encountered when training spiking neural networks, and guides the reader through the key concepts of synaptic plasticity and datadriven learning in the spiking setting. To that end, it gives an overview of existing approaches and provides an introduction to surrogate gradient methods, specifically, as a particularly flexible and efficient method to overcome the aforementioned challenges.},
  file = {Neftci et al. - Surrogate Gradient Learning in Spiking Neural Netw.pdf},
  language = {en}
}

@article{Neftci2019,
  title = {Reinforcement Learning in Artificial and Biological Systems},
  author = {Neftci, Emre O. and Averbeck, Bruno B.},
  year = {2019},
  month = mar,
  volume = {1},
  pages = {133--143},
  issn = {2522-5839},
  doi = {10.1038/s42256-019-0025-4},
  file = {Neftci and Averbeck - 2019 - Reinforcement learning in artificial and biologica.pdf},
  journal = {Nat Mach Intell},
  language = {en},
  number = {3}
}

@article{Nelson2015,
  title = {Excitatory/{{Inhibitory Balance}} and {{Circuit Homeostasis}} in {{Autism Spectrum Disorders}}},
  author = {Nelson, Sacha B. and Valakh, Vera},
  year = {2015},
  month = aug,
  volume = {87},
  pages = {684--698},
  issn = {08966273},
  doi = {10.1016/j.neuron.2015.07.033},
  file = {Nelson and Valakh - 2015 - ExcitatoryInhibitory Balance and Circuit Homeosta.pdf},
  journal = {Neuron},
  language = {en},
  number = {4}
}

@article{Netoff2005,
  title = {Synchronization in {{Hybrid Neuronal Networks}} of the {{Hippocampal Formation}}},
  author = {Netoff, Theoden I. and Banks, Matthew I. and Dorval, Alan D. and Acker, Corey D. and Haas, Julie S. and Kopell, Nancy and White, John A.},
  year = {2005},
  month = mar,
  volume = {93},
  pages = {1197--1208},
  issn = {0022-3077, 1522-1598},
  doi = {10.1152/jn.00982.2004},
  file = {2005 - Netoff et al. - Synchronization in hybrid neuronal networks of the hippocampal formation(2).pdf},
  journal = {Journal of Neurophysiology},
  language = {en},
  number = {3}
}

@article{Nevin2010,
  title = {Focusing on Optic Tectum Circuitry through the Lens of Genetics},
  author = {Nevin, Linda M and Robles, Estuardo and Baier, Herwig and Scott, Ethan K},
  year = {2010},
  volume = {8},
  pages = {126},
  issn = {1741-7007},
  doi = {10.1186/1741-7007-8-126},
  abstract = {The visual pathway is tasked with processing incoming signals from the retina and converting this information into adaptive behavior. Recent studies of the larval zebrafish tectum have begun to clarify how the `microcircuitry' of this highly organized midbrain structure filters visual input, which arrives in the superficial layers and directs motor output through efferent projections from its deep layers. The new emphasis has been on the specific function of neuronal cell types, which can now be reproducibly labeled, imaged and manipulated using genetic and optical techniques. Here, we discuss recent advances and emerging experimental approaches for studying tectal circuits as models for visual processing and sensorimotor transformation by the vertebrate brain.},
  file = {2010 - Nevin et al. - Focusing on optic tectum circuitry through the lens of genetics.pdf},
  journal = {BMC Biology},
  language = {en},
  number = {1}
}

@incollection{Newell1973,
  title = {{{YOU CAN}}'{{T PLAY}} 20 {{QUESTIONS WITH NATURE AND WIN}}: {{PROJECTIVE COMMENTS ON THE PAPERS OF THIS SYMPOSIUM}}},
  shorttitle = {{{YOU CAN}}'{{T PLAY}} 20 {{QUESTIONS WITH NATURE AND WIN}}},
  booktitle = {Visual {{Information Processing}}},
  author = {Newell, Allen},
  year = {1973},
  pages = {283--308},
  publisher = {{Elsevier}},
  doi = {10.1016/B978-0-12-170150-5.50012-3},
  file = {1973 - Newell - You can't play 20 questions with nature and win Projective comments on the papers of this symposium.pdf;Newell - 1973 - YOU CAN'T PLAY 20 QUESTIONS WITH NATURE AND WIN P.pdf},
  isbn = {978-0-12-170150-5},
  language = {en}
}

@article{Newhall2015,
  title = {Synchrony in Stochastically Driven Neuronal Networks with Complex Topologies},
  author = {Newhall, Katherine A. and Shkarayev, Maxim S. and Kramer, Peter R. and Kova{\v c}i{\v c}, Gregor and Cai, David},
  year = {2015},
  month = may,
  volume = {91},
  issn = {1539-3755, 1550-2376},
  doi = {10.1103/PhysRevE.91.052806},
  file = {2015 - Newhall et al. - Synchrony in stochastically driven neuronal networks with complex topologies.pdf;Newhall et al. - 2015 - Synchrony in stochastically driven neuronal networ.pdf},
  journal = {Physical Review E},
  language = {en},
  number = {5}
}

@article{Newman,
  title = {Limitations of the Asymptotic Approach to Dynamics},
  author = {Newman, Julian and Lucas, Maxime and Stefanovska, Aneta},
  pages = {13},
  file = {Newman et al. - Limitations of the asymptotic approach to dynamics.pdf},
  language = {en}
}

@article{Newman1997,
  title = {Calcium {{Waves}} in {{Retinal Glial Cells}}},
  author = {Newman, E. A.},
  year = {1997},
  month = feb,
  volume = {275},
  pages = {844--847},
  issn = {00368075, 10959203},
  doi = {10.1126/science.275.5301.844},
  file = {Newman - 1997 - Calcium Waves in Retinal Glial Cells.pdf},
  journal = {Science},
  language = {en},
  number = {5301}
}

@article{Newman2002,
  title = {Random Graph Models of Social Networks},
  author = {Newman, M. E. J. and Watts, D. J. and Strogatz, S. H.},
  year = {2002},
  month = feb,
  volume = {99},
  pages = {2566--2572},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.012582999},
  file = {2002 - Newman, Watts, Strogatz - Random graph models of social networks.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {Supplement 1}
}

@article{Neyman1976,
  title = {Tests of Statistical Hypotheses and Their Use in Studies of Natural Phenomena},
  author = {Neyman, Jerzy},
  year = {1976},
  month = jan,
  volume = {5},
  pages = {737--751},
  issn = {0361-0926, 1532-415X},
  doi = {10.1080/03610927608827392},
  abstract = {Contrary t o ideas suggested by the t i t l e o f the conference a t which t h e present paper was presented, t h e a u t h o r i s n o t aware o f a conceptual difference between a " t e s t o f a s t a t i s t i c a l hypothesis" and a " t e s t o f s i g n i f i c a n c e " and uses these terms i n t e r changeably. A study o f any serious substantive problem involves a sequence o f i n c i d e n t s a t which one i s forced t o pause and cons i d e r what t o do next. I n an e f f o r t t o reduce the frequency o f misdirected a c t i v i t i e s one uses s t a t i s t i c a l tests. The procedure i s i l l u s t r a t e d on two examples: (i)Le Cam's (and a s s o c i a t e s ' ) study o f imnunotherapy o f cancer and (ii)a socio-economic experiment r e l a t i n g t o low-income homeownership problems.},
  file = {1976 - Neyman - Tests of statistical hypotheses and their use in studies of natural phenomena.pdf;Neyman - 1976 - Tests of statistical hypotheses and their use in s.pdf},
  journal = {Communications in Statistics - Theory and Methods},
  language = {en},
  number = {8}
}

@article{Neymotin2011,
  title = {Synaptic Information Transfer in Computer Models of Neocortical Columns},
  author = {Neymotin, Samuel A. and Jacobs, Kimberle M. and Fenton, Andr{\'e} A. and Lytton, William W.},
  year = {2011},
  month = feb,
  volume = {30},
  pages = {69--84},
  issn = {0929-5313, 1573-6873},
  doi = {10.1007/s10827-010-0253-4},
  file = {2011 - Neymotin et al. - Synaptic information transfer in computer models of neocortical columns(2).pdf},
  journal = {Journal of Computational Neuroscience},
  language = {en},
  number = {1}
}

@article{Neymotin2013,
  title = {Ih {{Tunes Theta}}/{{Gamma Oscillations}} and {{Cross}}-{{Frequency Coupling In}} an {{In Silico CA3 Model}}},
  author = {Neymotin, Samuel A. and Hilscher, Markus M. and Moulin, Thiago C. and Skolnick, Yosef and Lazarewicz, Maciej T. and Lytton, William W.},
  editor = {Cymbalyuk, Gennady},
  year = {2013},
  month = oct,
  volume = {8},
  pages = {e76285},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0076285},
  abstract = {Ih channels are uniquely positioned to act as neuromodulatory control points for tuning hippocampal theta (4\textendash 12 Hz) and gamma (w25 Hz) oscillations, oscillations which are thought to have importance for organization of information flow. Ih contributes to neuronal membrane resonance and resting membrane potential, and is modulated by second messengers. We investigated Ih oscillatory control using a multiscale computer model of hippocampal CA3, where each cell class (pyramidal, basket, and oriens-lacunosum moleculare cells), contained type-appropriate isoforms of Ih. Our model demonstrated that modulation of pyramidal and basket Ih allows tuning theta and gamma oscillation frequency and amplitude. Pyramidal Ih also controlled cross-frequency coupling (CFC) and allowed shifting gamma generation towards particular phases of the theta cycle, effected via Ih 's ability to set pyramidal excitability. Our model predicts that in vivo neuromodulatory control of Ih allows flexibly controlling CFC and the timing of gamma discharges at particular theta phases.},
  file = {2013 - Neymotin et al. - Ih Tunes ThetaGamma Oscillations and Cross-Frequency Coupling In an In Silico CA3 Model.pdf},
  journal = {PLoS ONE},
  language = {en},
  number = {10}
}

@article{Ng1999,
  title = {Policy Invariance under Reward Transformations: {{Theory}} and Application to Reward Shaping.},
  author = {Ng, Andrew and Harada, Daishi and Russell, Stuart},
  year = {1999},
  pages = {278--287},
  file = {Ng et al. - 1999 - Policy invariance under reward transformations Th.pdf},
  journal = {In Proceedings of the Sixteenth International Conference on Machine Learning}
}

@article{Ngamga2007,
  title = {Recurrence Analysis of Strange Nonchaotic Dynamics},
  author = {Ngamga, E. J. and Nandi, A. and Ramaswamy, R. and Romano, M. C. and Thiel, M. and Kurths, J.},
  year = {2007},
  month = mar,
  volume = {75},
  issn = {1539-3755, 1550-2376},
  doi = {10.1103/PhysRevE.75.036222},
  file = {2008 - Ngamga et al. - Recurrence analysis of strange nonchaotic dynamics in driven excitable systems.pdf},
  journal = {Physical Review E},
  language = {en},
  number = {3}
}

@article{Nicholas2003,
  title = {Why {{Schumpeter Was Right}}: {{Innovation}}, {{Market Power}}, and {{Creative Destruction}} in 1920s {{America}}},
  author = {Nicholas, Tom},
  year = {2003},
  volume = {63},
  pages = {1023--1058},
  file = {Nicholas - 2003 - Why Schumpeter Was Right Innovation, Market Power.pdf},
  journal = {The Journal of Economic History},
  language = {en},
  number = {4}
}

@article{Nichols2002,
  title = {Nonparametric Permutation Tests for Functional Neuroimaging: {{A}} Primer with Examples},
  shorttitle = {Nonparametric Permutation Tests for Functional Neuroimaging},
  author = {Nichols, Thomas E. and Holmes, Andrew P.},
  year = {2002},
  month = jan,
  volume = {15},
  pages = {1--25},
  issn = {1065-9471, 1097-0193},
  doi = {10.1002/hbm.1058},
  abstract = {Requiring only minimal assumptions for validity, nonparametric permutation testing provides a flexible and intuitive methodology for the statistical analysis of data from functional neuroimaging experiments, at some computational expense. Introduced into the functional neuroimaging literature by Holmes et al. ([1996]: J Cereb Blood Flow Metab 16:7\textendash 22), the permutation approach readily accounts for the multiple comparisons problem implicit in the standard voxel-by-voxel hypothesis testing framework. When the appropriate assumptions hold, the nonparametric permutation approach gives results similar to those obtained from a comparable Statistical Parametric Mapping approach using a general linear model with multiple comparisons corrections derived from random field theory. For analyses with low degrees of freedom, such as single subject PET/SPECT experiments or multi-subject PET/SPECT or fMRI designs assessed for population effects, the nonparametric approach employing a locally pooled (smoothed) variance estimate can outperform the comparable Statistical Parametric Mapping approach. Thus, these nonparametric techniques can be used to verify the validity of less computationally expensive parametric approaches. Although the theory and relative advantages of permutation approaches have been discussed by various authors, there has been no accessible explication of the method, and no freely distributed software implementing it. Consequently, there have been few practical applications of the technique. This article, and the accompanying MATLAB software, attempts to address these issues. The standard nonparametric randomization and permutation testing ideas are developed at an accessible level, using practical examples from functional neuroimaging, and the extensions for multiple comparisons described. Three worked examples from PET and fMRI are presented, with discussion, and comparisons with standard parametric approaches made where appropriate. Practical considerations are given throughout, and relevant statistical concepts are expounded in appendices. Hum. Brain Mapping 15:1\textendash 25, 2001. \textcopyright{} 2001 Wiley-Liss, Inc.},
  file = {2002 - Nichols, Holmes - Nonparametric permutation tests for functional neuroimaging a primer with examples.pdf},
  journal = {Human Brain Mapping},
  language = {en},
  number = {1}
}

@article{Nicholson2019,
  title = {Is the Cell Really a Machine?},
  author = {Nicholson, Daniel J.},
  year = {2019},
  month = sep,
  volume = {477},
  pages = {108--126},
  issn = {00225193},
  doi = {10.1016/j.jtbi.2019.06.002},
  abstract = {It has become customary to conceptualize the living cell as an intricate piece of machinery, different to a man-made machine only in terms of its superior complexity. This familiar understanding grounds the conviction that a cell's organization can be explained reductionistically, as well as the idea that its molecular pathways can be construed as deterministic circuits. The machine conception of the cell owes a great deal of its success to the methods traditionally used in molecular biology. However, the recent introduction of novel experimental techniques capable of tracking individual molecules within cells in real time is leading to the rapid accumulation of data that are inconsistent with an engineering view of the cell. This paper examines four major domains of current research in which the challenges to the machine conception of the cell are particularly pronounced: cellular architecture, protein complexes, intracellular transport, and cellular behaviour. It argues that a new theoretical understanding of the cell is emerging from the study of these phenomena which emphasizes the dynamic, self-organizing nature of its constitution, the fluidity and plasticity of its components, and the stochasticity and non-linearity of its underlying processes.},
  file = {Nicholson - 2019 - Is the cell really a machine.pdf},
  journal = {Journal of Theoretical Biology},
  language = {en}
}

@article{Niebuhr2015,
  title = {Survival in Patchy Landscapes: The Interplay between Dispersal, Habitat Loss and Fragmentation},
  shorttitle = {Survival in Patchy Landscapes},
  author = {Niebuhr, Bernardo B. S. and Wosniack, Marina E. and Santos, Marcos C. and Raposo, Ernesto P. and Viswanathan, Gandhimohan M. and {da Luz}, Marcos G. E. and Pie, Marcio R.},
  year = {2015},
  month = dec,
  volume = {5},
  pages = {11898},
  issn = {2045-2322},
  doi = {10.1038/srep11898},
  file = {Niebuhr et al. - 2015 - Survival in patchy landscapes the interplay betwe.pdf},
  journal = {Sci Rep},
  language = {en},
  number = {1}
}

@article{Nir2008,
  title = {{{BOLD}} and Spiking Activity},
  author = {Nir, Yuval and Dinstein, Ilan and Malach, Rafael and Heeger, David J},
  year = {2008},
  month = may,
  volume = {11},
  pages = {523--524},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn0508-523},
  file = {2008 - Malach, Nir, Heeger - Co r r e s p on d e n c e.pdf},
  journal = {Nature Neuroscience},
  language = {en},
  number = {5}
}

@article{Nishimoto2011,
  title = {Reconstructing {{Visual Experiences}} from {{Brain Activity Evoked}} by {{Natural Movies}}},
  author = {Nishimoto, Shinji and Vu, An T. and Naselaris, Thomas and Benjamini, Yuval and Yu, Bin and Gallant, Jack L.},
  year = {2011},
  month = oct,
  volume = {21},
  pages = {1641--1646},
  issn = {09609822},
  doi = {10.1016/j.cub.2011.08.031},
  abstract = {Quantitative modeling of human brain activity can provide crucial insights about cortical representations [1, 2] and can form the basis for brain decoding devices [3\textendash 5]. Recent functional magnetic resonance imaging (fMRI) studies have modeled brain activity elicited by static visual patterns and have reconstructed these patterns from brain activity [6\textendash 8]. However, blood oxygen level-dependent (BOLD) signals measured via fMRI are very slow [9], so it has been difficult to model brain activity elicited by dynamic stimuli such as natural movies. Here we present a new motion-energy [10, 11] encoding model that largely overcomes this limitation. The model describes fast visual information and slow hemodynamics by separate components. We recorded BOLD signals in occipitotemporal visual cortex of human subjects who watched natural movies and fit the model separately to individual voxels. Visualization of the fit models reveals how early visual areas represent the information in movies. To demonstrate the power of our approach, we also constructed a Bayesian decoder [8] by combining estimated encoding models with a sampled natural movie prior. The decoder provides remarkable reconstructions of the viewed movies. These results demonstrate that dynamic brain activity measured under naturalistic conditions can be decoded using current fMRI technology.},
  file = {2011 - Nishimoto et al. - Reconstructing visual experiences from brain activity evoked by natural movies.pdf},
  journal = {Current Biology},
  language = {en},
  number = {19}
}

@article{Niu2011,
  title = {{{HOGWILD}}!: {{A Lock}}-{{Free Approach}} to {{Parallelizing Stochastic Gradient Descent}}},
  shorttitle = {{{HOGWILD}}!},
  author = {Niu, Feng and Recht, Benjamin and Re, Christopher and Wright, Stephen J.},
  year = {2011},
  month = jun,
  abstract = {Stochastic Gradient Descent (SGD) is a popular algorithm that can achieve state-of-the-art performance on a variety of machine learning tasks. Several researchers have recently proposed schemes to parallelize SGD, but all require performance-destroying memory locking and synchronization. This work aims to show using novel theoretical analysis, algorithms, and implementation that SGD can be implemented without any locking. We present an update scheme called Hogwild! which allows processors access to shared memory with the possibility of overwriting each other's work. We show that when the associated optimization problem is sparse, meaning most gradient updates only modify small parts of the decision variable, then Hogwild! achieves a nearly optimal rate of convergence. We demonstrate experimentally that Hogwild! outperforms alternative schemes that use locking by an order of magnitude.},
  archiveprefix = {arXiv},
  eprint = {1106.5730},
  eprinttype = {arxiv},
  file = {2011 - Niu et al. - HOGWILD! A Lock-Free Approach to Parallelizing Stochastic Gradient Descent.pdf},
  journal = {arXiv:1106.5730 [cs, math]},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control},
  language = {en},
  primaryclass = {cs, math}
}

@article{Niv2005,
  title = {How Fast to Work: {{Response}} Vigor, Motivation and Tonic Dopamine},
  author = {Niv, Yael and Daw, Nathaniel D and Dayan, Peter},
  year = {2005},
  volume = {18},
  pages = {8},
  abstract = {Reinforcement learning models have long promised to unify computational, psychological and neural accounts of appetitively conditioned behavior. However, the bulk of data on animal conditioning comes from free-operant experiments measuring how fast animals will work for reinforcement. Existing reinforcement learning (RL) models are silent about these tasks, because they lack any notion of vigor. They thus fail to address the simple observation that hungrier animals will work harder for food, as well as stranger facts such as their sometimes greater productivity even when working for irrelevant outcomes such as water. Here, we develop an RL framework for free-operant behavior, suggesting that subjects choose how vigorously to perform selected actions by optimally balancing the costs and benefits of quick responding. Motivational states such as hunger shift these factors, skewing the tradeoff. This accounts normatively for the effects of motivation on response rates, as well as many other classic findings. Finally, we suggest that tonic levels of dopamine may be involved in the computation linking motivational state to optimal responding, thereby explaining the complex vigor-related effects of pharmacological manipulation of dopamine.},
  file = {Niv et al. - How fast to work Response vigor, motivation and t.pdf},
  language = {en}
}

@article{Nogaret2016,
  title = {Automatic {{Construction}} of {{Predictive Neuron Models}} through {{Large Scale Assimilation}} of {{Electrophysiological Data}}},
  author = {Nogaret, Alain and Meliza, C. Daniel and Margoliash, Daniel and Abarbanel, Henry D. I.},
  year = {2016},
  month = dec,
  volume = {6},
  issn = {2045-2322},
  doi = {10.1038/srep32749},
  file = {Nogaret et al. - 2016 - Automatic Construction of Predictive Neuron Models 2.pdf;Nogaret et al. - 2016 - Automatic Construction of Predictive Neuron Models 3.pdf;Nogaret et al. - 2016 - Automatic Construction of Predictive Neuron Models.pdf},
  journal = {Scientific Reports},
  language = {en},
  number = {1}
}

@article{Nolte2018,
  title = {Cortical Reliability amid Noise and Chaos},
  author = {Nolte, Max and Reimann, Michael W and King, James G and Markram, Henry and Muller, Eilif B},
  year = {2018},
  month = jun,
  doi = {10.1101/304121},
  abstract = {Typical responses of cortical neurons to identical sensory stimuli are highly variable. It has thus been proposed that the cortex primarily uses a rate code. However, other studies have argued for spike-time coding under certain conditions. The potential role of spike-time coding is constrained by the intrinsic variability of cortical circuits, which remains largely unexplored. Here, we quantified this intrinsic variability using a biophysical model of rat neocortical microcircuitry with biologically realistic noise sources. We found that stochastic neurotransmitter release is a critical component of this variability, which, amplified by recurrent connectivity, causes rapid chaotic divergence with a time constant on the order of 10-20 milliseconds. Surprisingly, weak thalamocortical stimuli can transiently overcome the chaos, and induce reliable spike times with millisecond precision. We show that this effect relies on recurrent cortical connectivity, and is not a simple effect of feed-forward thalamocortical input. We conclude that recurrent cortical architecture supports millisecond spike-time reliability amid noise and chaotic network dynamics, resolving a long-standing debate.},
  file = {Nolte et al. - 2018 - Cortical reliability amid noise and chaos.pdf},
  journal = {bioRxiv},
  language = {en}
}

@article{Noonan2016,
  title = {Distinct {{Mechanisms}} for {{Distractor Suppression}} and {{Target Facilitation}}},
  author = {Noonan, M. P. and Adamian, N. and Pike, A. and Printzlau, F. and Crittenden, B. M. and Stokes, M. G.},
  year = {2016},
  month = feb,
  volume = {36},
  pages = {1797--1807},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.2133-15.2016},
  file = {Noonan et al. - 2016 - Distinct Mechanisms for Distractor Suppression and.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {6}
}

@article{Norman2006,
  title = {Beyond Mind-Reading: Multi-Voxel Pattern Analysis of {{fMRI}} Data},
  shorttitle = {Beyond Mind-Reading},
  author = {Norman, Kenneth A. and Polyn, Sean M. and Detre, Greg J. and Haxby, James V.},
  year = {2006},
  month = sep,
  volume = {10},
  pages = {424--430},
  issn = {13646613},
  doi = {10.1016/j.tics.2006.07.005},
  file = {2006 - Norman et al. - Beyond mind-reading multi-voxel pattern analysis of fMRI data.pdf},
  journal = {Trends in Cognitive Sciences},
  language = {en},
  number = {9}
}

@article{Norton2003,
  title = {Can Ultrasound Be Used to Stimulate Nerve Tissue?},
  author = {Norton, Stephen J},
  year = {2003},
  pages = {9},
  abstract = {Background: The stimulation of nerve or cortical tissue by magnetic induction is a relatively new tool for the non-invasive study of the brain and nervous system. Transcranial magnetic stimulation (TMS), for example, has been used for the functional mapping of the motor cortex and may have potential for treating a variety of brain disorders. Methods and Results: A new method of stimulating active tissue is proposed by propagating ultrasound in the presence of a magnetic field. Since tissue is conductive, particle motion created by an ultrasonic wave will induce an electric current density generated by Lorentz forces. An analytical derivation is given for the electric field distribution induced by a collimated ultrasonic beam. An example shows that peak electric fields of up to 8 V/m appear to be achievable at the upper range of diagnostic intensities. This field strength is about an order of magnitude lower than fields typically associated with TMS; however, the electric field gradients induced by ultrasound can be quite high (about 60 kV/m2 at 4 MHz), which theoretically play a more important role in activation than the field magnitude. The latter value is comparable to TMS-induced gradients. Conclusion: The proposed method could be used to locally stimulate active tissue by inducing an electric field in regions where the ultrasound is focused. Potential advantages of this method compared to TMS is that stimulation of cortical tissue could be highly localized as well as achieved at greater depths in the brain than is currently possible with TMS.},
  file = {2003 - Norton - Can ultrasound be used to stimulate nerve tissue.pdf},
  journal = {BioMedical Engineering OnLine},
  language = {en}
}

@article{Nosofsky1985,
  title = {Overall Similarity and the Identification of Separable-Dimension Stimuli: {{A}} Choice Model Analysis},
  shorttitle = {Overall Similarity and the Identification of Separable-Dimension Stimuli},
  author = {Nosofsky, Robert},
  year = {1985},
  month = sep,
  volume = {38},
  pages = {415--432},
  issn = {0031-5117, 1532-5962},
  doi = {10.3758/BF03207172},
  file = {1985 - Nosofsky - Overall similarity and the identification of separable-dimension stimuli a choice model analysis.pdf},
  journal = {Perception \& Psychophysics},
  language = {en},
  number = {5}
}

@article{Nowak1993,
  title = {A Strategy of Win-Stay Lose-Shift That Outperforms Tit-for-Tat in the {{Prisoner}}'s {{Dilemma}} Game},
  author = {Nowak, Martin and Sigmund, Karl},
  year = {1993},
  volume = {364},
  pages = {56--58},
  file = {nowak1993.pdf},
  journal = {Nature},
  number = {1}
}

@article{Nowak1993a,
  title = {A Strategy of Win-Stay, Lose-Shift That Outperforms Tit-for-Tat in the {{Prisoner}}'s {{Dilemma}} Game},
  author = {Nowak, Martin and Sigmund, Karl},
  year = {1993},
  month = jul,
  volume = {364},
  pages = {56--58},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/364056a0},
  file = {Nowak and Sigmund - 1993 - A strategy of win-stay, lose-shift that outperform.pdf},
  journal = {Nature},
  language = {en},
  number = {6432}
}

@article{Nowak2004,
  title = {Evolutionary {{Dynamics}} of {{Biological Games}}},
  author = {Nowak, M. A.},
  year = {2004},
  month = feb,
  volume = {303},
  pages = {793--799},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1093411},
  file = {Nowak - 2004 - Evolutionary Dynamics of Biological Games.pdf},
  journal = {Science},
  language = {en},
  number = {5659}
}

@article{Nunes2016,
  title = {Multi-Alternative Decision-Making with Non-Stationary Inputs},
  author = {Nunes, Luana F. and Gurney, Kevin},
  year = {2016},
  month = aug,
  volume = {3},
  pages = {160376},
  issn = {2054-5703},
  doi = {10.1098/rsos.160376},
  file = {Nunes and Gurney - 2016 - Multi-alternative decision-making with non-station.pdf},
  journal = {Royal Society Open Science},
  language = {en},
  number = {8}
}

@article{Nutt2001,
  title = {Interactions between Deep Brain Stimulation and Levodopa in {{Parkinson}}'s Disease},
  author = {Nutt, J.G. and Rufener, S.L. and Carter, J.H. and Anderson, V.C. and Pahwa, R. and Hammerstad, J.P. and Burchiel, K.J.},
  year = {2001},
  month = nov,
  volume = {57},
  pages = {1835--1842},
  issn = {0028-3878, 1526-632X},
  doi = {10.1212/WNL.57.10.1835},
  file = {2001 - Nutt et al. - Interactions between deep brain stimulation and levodopa in Parkinson ’ s disease.pdf},
  journal = {Neurology},
  language = {en},
  number = {10}
}

@article{Nyhus2010,
  title = {Functional Role of Gamma and Theta Oscillations in Episodic Memory},
  author = {Nyhus, Erika and Curran, Tim},
  year = {2010},
  month = jun,
  volume = {34},
  pages = {1023--1035},
  issn = {01497634},
  doi = {10.1016/j.neubiorev.2009.12.014},
  abstract = {The primary aim of this review is to examine evidence for a functional role of gamma and theta oscillations in human episodic memory. It is proposed here that gamma and theta oscillations allow for the transient interaction between cortical structures and the hippocampus for the encoding and retrieval of episodic memories as described by the hippocampal memory indexing theory (Teyler, T. J., \& DiScenna, P. (1986). The hippocampal memory indexing theory. Behavioral Neuroscience, 100, 147\textendash 154). Gamma rhythms can act in the cortex to bind perceptual features and in the hippocampus to bind the rich perceptual and contextual information from diverse brain regions into episodic representations. Theta oscillations act to temporally order these individual episodic memory representations. Through feedback projections from the hippocampus to the cortex these gamma and theta patterns could cause the reinstatement of the entire episodic memory representation in the cortex. In addition, theta oscillations could allow for top-down control from the frontal cortex to the hippocampus modulating the encoding and retrieval of episodic memories.},
  file = {Nyhus and Curran - 2010 - Functional role of gamma and theta oscillations in.pdf},
  journal = {Neuroscience \& Biobehavioral Reviews},
  language = {en},
  number = {7}
}

@article{Oakden-Rayner2019,
  title = {Hidden {{Stratification Causes Clinically Meaningful Failures}} in {{Machine Learning}} for {{Medical Imaging}}},
  author = {{Oakden-Rayner}, Luke and Dunnmon, Jared and Carneiro, Gustavo and R{\'e}, Christopher},
  year = {2019},
  month = sep,
  abstract = {Machine learning models for medical image analysis often suffer from poor performance on important subsets of a population that are not identified during training or testing. For example, overall performance of a cancer detection model may be high, but the model still consistently misses a rare but aggressive cancer subtype. We refer to this problem as hidden stratification, and observe that it results from incompletely describing the meaningful variation in a dataset. While hidden stratification can substantially reduce the clinical efficacy of machine learning models, its effects remain difficult to measure. In this work, we assess the utility of several possible techniques for measuring and describing hidden stratification effects, and characterize these effects both on multiple medical imaging datasets and via synthetic experiments on the well-characterised CIFAR-100 benchmark dataset. We find evidence that hidden stratification can occur in unidentified imaging subsets with low prevalence, low label quality, subtle distinguishing features, or spurious correlates, and that it can result in relative performance differences of over 20\% on clinically important subsets. Finally, we explore the clinical implications of our findings, and suggest that evaluation of hidden stratification should be a critical component of any machine learning deployment in medical imaging.},
  archiveprefix = {arXiv},
  eprint = {1909.12475},
  eprinttype = {arxiv},
  file = {Oakden-Rayner et al. - 2019 - Hidden Stratification Causes Clinically Meaningful.pdf},
  journal = {arXiv:1909.12475 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryclass = {cs, stat}
}

@article{Oberheim2006,
  title = {Astrocytic Complexity Distinguishes the Human Brain},
  author = {Oberheim, Nancy Ann and Wang, Xiaohai and Goldman, Steven and Nedergaard, Maiken},
  year = {2006},
  month = oct,
  volume = {29},
  pages = {547--553},
  issn = {01662236},
  doi = {10.1016/j.tins.2006.08.004},
  file = {Oberheim et al. - 2006 - Astrocytic complexity distinguishes the human brai.pdf},
  journal = {Trends in Neurosciences},
  language = {en},
  number = {10}
}

@article{Oberheim2009,
  title = {Uniquely {{Hominid Features}} of {{Adult Human Astrocytes}}},
  author = {Oberheim, N. A. and Takano, T. and Han, X. and He, W. and Lin, J. H. C. and Wang, F. and Xu, Q. and Wyatt, J. D. and Pilcher, W. and Ojemann, J. G. and Ransom, B. R. and Goldman, S. A. and Nedergaard, M.},
  year = {2009},
  month = mar,
  volume = {29},
  pages = {3276--3287},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.4707-08.2009},
  file = {Oberheim et al. - 2009 - Uniquely Hominid Features of Adult Human Astrocyte.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {10}
}

@article{Ocker2014,
  title = {Kv7 Channels Regulate Pairwise Spiking Covariability in Health and Disease},
  author = {Ocker, Gabriel Koch and Doiron, Brent},
  year = {2014},
  month = jul,
  volume = {112},
  pages = {340--352},
  issn = {0022-3077, 1522-1598},
  doi = {10.1152/jn.00084.2014},
  file = {Ocker and Doiron - 2014 - Kv7 channels regulate pairwise spiking covariabili.pdf},
  journal = {Journal of Neurophysiology},
  language = {en},
  number = {2}
}

@article{Ocker2017,
  title = {Linking Structure and Activity in Nonlinear Spiking Networks},
  author = {Ocker, Gabriel Koch and Josi{\'c}, Kre{\v s}imir and {Shea-Brown}, Eric and Buice, Michael A.},
  year = {2017},
  month = mar,
  doi = {10.1101/080705},
  abstract = {Recent experimental advances are producing an avalanche of data on both neural connectivity and neural activity. To take full advantage of these two emerging datasets we need a framework that links them, revealing how collective neural activity arises from the structure of neural connectivity and intrinsic neural dynamics. This problem of structure-driven activity has drawn major interest in computational neuroscience. Existing methods for relating activity and architecture in spiking networks rely on linearizing activity around a central operating point and thus fail to capture the nonlinear responses of individual neurons that are the hallmark of neural information processing. Here, we overcome this limitation and present a new relationship between connectivity and activity in networks of nonlinear spiking neurons by developing a diagrammatic fluctuation expansion based on statistical field theory. We explicitly show how recurrent network structure produces pairwise and higher-order correlated activity, and how nonlinearities impact the networks' spiking activity. Our findings open new avenues to investigating how single-neuron nonlinearities\textemdash including those of different cell types\textemdash combine with connectivity to shape population activity and function.},
  file = {Ocker et al. - 2017 - Linking structure and activity in nonlinear spikin 2.pdf;Ocker et al. - 2017 - Linking structure and activity in nonlinear spikin.pdf},
  journal = {bioRxiv},
  language = {en}
}

@article{OConnor2016,
  title = {Deep {{Spiking Networks}}},
  author = {O'Connor, Peter and Welling, Max},
  year = {2016},
  month = feb,
  abstract = {We introduce an algorithm to do backpropagation on a spiking network. Our network is "spiking" in the sense that our neurons accumulate their activation into a potential over time, and only send out a signal (a ``spike'') when this potential crosses a threshold and the neuron is reset. Neurons only update their states when receiving signals from other neurons. Total computation of the network thus scales with the number of spikes caused by an input rather than network size. We show that the spiking Multi-Layer Perceptron behaves identically, during both prediction and training, to a conventional deep network of rectified-linear units, in the limiting case where we run the spiking network for a long time. We apply this architecture to a conventional classification problem (MNIST) and achieve performance very close to that of a conventional Multi-Layer Perceptron with the same architecture. Our network is a natural architecture for learning based on streaming event-based data, and is a stepping stone towards using spiking neural networks to learn efficiently on streaming data.},
  archiveprefix = {arXiv},
  eprint = {1602.08323},
  eprinttype = {arxiv},
  file = {O'Connor and Welling - 2016 - Deep Spiking Networks.pdf},
  journal = {arXiv:1602.08323 [cs]},
  keywords = {68T01,Computer Science - Neural and Evolutionary Computing,F.1.1},
  language = {en},
  primaryclass = {cs}
}

@techreport{Ogasawara2021,
  title = {Neuronal Mechanisms of Novelty Seeking},
  author = {Ogasawara, Takaya and Sogukpinar, Fatih and Zhang, Kaining and Feng, Yang-Yang and Pai, Julia and Jezzini, Ahmad and Monosov, Ilya E.},
  year = {2021},
  month = mar,
  institution = {{Neuroscience}},
  doi = {10.1101/2021.03.12.435019},
  abstract = {Humans and other primates interact with the world by observing and exploring visual objects. In particular, they often seek out the opportunities to view novel objects that they have never seen before, even when they have no extrinsic primary reward value. However, despite the importance of novel visual objects in our daily life, we currently lack an understanding of how primate brain circuits control the motivation to seek out novelty. We found that novelty-seeking is regulated by a small understudied subcortical region, the zona incerta (ZI). In a task in which monkeys made eye movements to familiar objects to obtain the opportunity to view novel objects, many ZI neurons were preferentially activated by predictions of future novel objects and displayed burst excitations before gaze shifts to gain access to novel objects. Low intensity electrical stimulation of ZI facilitated gaze shifts, while inactivations of ZI reduced novelty-seeking. Surprisingly, additional experiments showed that this ZI-dependent novelty seeking behavior is not regulated by canonical neural circuitry for reward seeking. The habenuladopamine pathway, known to reflect reward predictions that control reward seeking, was relatively inactive during novelty-seeking behavior in which novelty had no extrinsic reward value. Instead, high channel-count electrophysiological experiments and anatomical tracing identified a prominent source of control signals for novelty seeking in the anterior ventral medial temporal cortex (AVMTC), a brain region known to be crucially involved in visual processing and object memory. In addition to their wellknown function in signaling the novelty or familiarity of objects in the current environment, AVMTC neurons reflected the predictions of future novel objects, akin to the way neurons in reward-circuitry predict future rewards in order to control reward-seeking. Our data uncover a network of primate brain areas that regulate novelty-seeking. The behavioral and neural distinctions between novelty-seeking and reward-processing highlight how the brain can accomplish behavioral flexibility, providing a mechanism to explore novel objects.},
  file = {Ogasawara et al. - 2021 - Neuronal mechanisms of novelty seeking.pdf},
  language = {en},
  type = {Preprint}
}

@article{Ogawa2011,
  title = {Neural Representation of Observed Actions in the Parietal and Premotor Cortex},
  author = {Ogawa, Kenji and Inui, Toshio},
  year = {2011},
  month = may,
  volume = {56},
  pages = {728--735},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2010.10.043},
  abstract = {We investigated the neural representation of observed actions in the human parietal and premotor cortex, which comprise the action observation network or the mirror neuron system for action recognition. Participants observed object-directed hand actions, in which action as well as other properties were independently manipulated: action (grasp or touch), object (cup or bottle), perspective (1st or 3rd person), hand (right or left), and image size (large or small). We then used multi-voxel pattern analysis to determine whether each feature could be correctly decoded from regional activities. The early visual area showed significant above-chance classification accuracy, particularly high in perspective, hand, and size, consistent with pixel-wise dissimilarity of stimuli. In contrast, the highest decoding accuracy for action was observed in the anterior intraparietal sulcus (aIPS) and the ventral premotor cortex (PMv). Moreover, the decoder for action could be correctly generalized for images with high dissimilarity in the parietal and premotor region, but not in the visual area. Our study indicates that the parietal and premotor regions encode observed actions independent of retinal variations, which may subserve our capacity for invariant action recognition of others. \textcopyright{} 2010 Elsevier Inc. All rights reserved.},
  file = {2011 - Ogawa, Inui - Neural representation of observed actions in the parietal and premotor cortex.pdf},
  journal = {NeuroImage},
  language = {en},
  number = {2}
}

@article{Ogunmolu,
  title = {Nonlinear {{Systems Identification Using Deep Dynamic Neural Networks}}},
  author = {Ogunmolu, Olalekan and Gu, Xuejun and Jiang, Steve and Gans, Nicholas},
  pages = {8},
  abstract = {Neural networks are known to be effective function approximators. Recently, deep neural networks have proven to be very effective in pattern recognition, classification tasks and human-level control to model highly nonlinear realworld systems. This paper investigates the effectiveness of deep neural networks in the modeling of dynamical systems with complex behavior. Three deep neural network structures are trained on sequential data, and we investigate the effectiveness of these networks in modeling associated characteristics of the underlying dynamical systems. We carry out similar evaluations on select publicly available system identification datasets. We demonstrate that deep neural networks are effective model estimators from input-output data.},
  file = {Ogunmolu et al. - Nonlinear Systems Identiﬁcation Using Deep Dynamic.pdf},
  language = {en}
}

@article{OHare2017,
  title = {Striatal Fast-Spiking Interneurons Selectively Modulate Circuit Output and Are Required for Habitual Behavior},
  author = {O'Hare, Justin K and Li, Haofang and Kim, Namsoo and Gaidis, Erin and Ade, Kristen and Beck, Jeff and Yin, Henry and Calakos, Nicole},
  year = {2017},
  month = sep,
  volume = {6},
  issn = {2050-084X},
  doi = {10.7554/eLife.26231},
  file = {O'Hare et al. - 2017 - Striatal fast-spiking interneurons selectively mod.pdf},
  journal = {eLife},
  language = {en}
}

@article{Ohyama2015,
  title = {A Multilevel Multimodal Circuit Enhances Action Selection in {{Drosophila}}},
  author = {Ohyama, Tomoko and {Schneider-Mizell}, Casey M. and Fetter, Richard D. and Aleman, Javier Valdes and Franconville, Romain and {Rivera-Alba}, Marta and Mensh, Brett D. and Branson, Kristin M. and Simpson, Julie H. and Truman, James W. and Cardona, Albert and Zlatic, Marta},
  year = {2015},
  month = apr,
  volume = {520},
  pages = {633--639},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature14297},
  file = {Ohyama et al. - 2015 - A multilevel multimodal circuit enhances action se.pdf},
  journal = {Nature},
  language = {en},
  number = {7549}
}

@article{OKeeffe2015,
  title = {Synchronization as {{Aggregation}}: {{Cluster Kinetics}} of {{Pulse}}-{{Coupled Oscillators}}},
  shorttitle = {Synchronization as {{Aggregation}}},
  author = {O'Keeffe, Kevin P. and Krapivsky, P. L. and Strogatz, Steven H.},
  year = {2015},
  month = aug,
  volume = {115},
  issn = {0031-9007, 1079-7114},
  doi = {10.1103/PhysRevLett.115.064101},
  file = {2015 - O'Keeffe, Krapivsky, Strogatz - Synchronization as Aggregation Cluster Kinetics of Pulse-Coupled Oscillators.pdf;O’Keeffe et al. - 2015 - Synchronization as Aggregation Cluster Kinetics o.pdf},
  journal = {Physical Review Letters},
  language = {en},
  number = {6}
}

@article{OKeeffe2016,
  title = {Dynamics of a Population of Oscillatory and Excitable Elements},
  author = {O'Keeffe, Kevin P. and Strogatz, Steven H.},
  year = {2016},
  month = jun,
  volume = {93},
  issn = {2470-0045, 2470-0053},
  doi = {10.1103/PhysRevE.93.062203},
  file = {O'Keeffe and Strogatz - 2016 - Dynamics of a population of oscillatory and excita.pdf},
  journal = {Physical Review E},
  language = {en},
  number = {6}
}

@article{OKeeffe2017,
  title = {Oscillators That Sync and Swarm},
  author = {O'Keeffe, Kevin P. and Hong, Hyunsuk and Strogatz, Steven H.},
  year = {2017},
  month = dec,
  volume = {8},
  issn = {2041-1723},
  doi = {10.1038/s41467-017-01190-3},
  file = {O’Keeffe et al. - 2017 - Oscillators that sync and swarm.pdf},
  journal = {Nature Communications},
  language = {en},
  number = {1}
}

@article{Okun2015,
  title = {Diverse Coupling of Neurons to Populations in Sensory Cortex},
  author = {Okun, Michael and Steinmetz, Nicholas A. and Cossell, Lee and Iacaruso, M. Florencia and Ko, Ho and Barth{\'o}, P{\'e}ter and Moore, Tirin and Hofer, Sonja B. and {Mrsic-Flogel}, Thomas D. and Carandini, Matteo and Harris, Kenneth D.},
  year = {2015},
  month = may,
  volume = {521},
  pages = {511--515},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature14273},
  file = {Okun et al. - 2015 - Diverse coupling of neurons to populations in sens.pdf},
  journal = {Nature},
  language = {en},
  number = {7553}
}

@article{OLeary2011,
  title = {Neuronal Homeostasis: Time for a Change?: {{Neuronal}} Homeostasis: Time for a Change?},
  shorttitle = {Neuronal Homeostasis},
  author = {O'Leary, Timothy and Wyllie, David J. A.},
  year = {2011},
  month = oct,
  volume = {589},
  pages = {4811--4826},
  issn = {00223751},
  doi = {10.1113/jphysiol.2011.210179},
  abstract = {Homeostatic processes that regulate electrical activity in neurones are now an established aspect of physiology and rest on a large body of experimental evidence that points to roles in development, learning and memory, and disease. However, the concepts underlying homeostasis are too often summarized in ways that restrict their explanatory power and obviate important subtleties. Here, we present a review of the underlying theory of homeostasis \textendash{} control theory \textendash{} in an attempt to reconcile some existing conceptual problems in the context of neuronal physiology. In addition to clarifying the underlying theory, this review highlights the remaining challenges posed when analysing homeostatic phenomena that underlie the regulation of neuronal excitability. Moreover, we suggest approaches for future experimental and computational work that will further our understanding of neuronal homeostasis and the fundamental neurophysiological functions it serves.},
  file = {2011 - O'Leary, Wyllie - Neuronal homeostasis Time for a change.pdf},
  journal = {The Journal of Physiology},
  language = {en},
  number = {20}
}

@article{OLeary2013,
  title = {Correlations in Ion Channel Expression Emerge from Homeostatic Tuning Rules},
  author = {O'Leary, T. and Williams, A. H. and Caplan, J. S. and Marder, E.},
  year = {2013},
  month = jul,
  volume = {110},
  pages = {E2645-E2654},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1309966110},
  file = {2013 - O'Leary et al. - Correlations in ion channel expression emerge from homeostatic tuning rules.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {28}
}

@article{OLeary2014,
  title = {Cell {{Types}}, {{Network Homeostasis}}, and {{Pathological Compensation}} from a {{Biologically Plausible Ion Channel Expression Model}}},
  author = {O'Leary, Timothy and Williams, Alex H. and Franci, Alessio and Marder, Eve},
  year = {2014},
  month = may,
  volume = {82},
  pages = {809--821},
  issn = {08966273},
  doi = {10.1016/j.neuron.2014.04.002},
  abstract = {How do neurons develop, control, and maintain their electrical signaling properties in spite of ongoing protein turnover and perturbations to activity? From generic assumptions about the molecular biology underlying channel expression, we derive a simple model and show how it encodes an ``activity set point'' in single neurons. The model generates diverse self-regulating cell types and relates correlations in conductance expression observed in vivo to underlying channel expression rates. Synaptic as well as intrinsic conductances can be regulated to make a self-assembling central pattern generator network; thus, network-level homeostasis can emerge from cell-autonomous regulation rules. Finally, we demonstrate that the outcome of homeostatic regulation depends on the complement of ion channels expressed in cells: in some cases, loss of specific ion channels can be compensated; in others, the homeostatic mechanism itself causes pathological loss of function.},
  file = {2014 - O'Leary et al. - Cell Types, Network Homeostasis, and Pathological Compensation from a Biologically Plausible Ion Channel Express.pdf;O’Leary et al. - 2014 - Cell Types, Network Homeostasis, and Pathological .pdf},
  journal = {Neuron},
  language = {en},
  number = {4}
}

@article{OLeary2014a,
  title = {Temperature-Robust Neural Activity Using Feedback Control of Ion Channel Expression},
  author = {O'Leary, Timothy and Marder, Eve},
  year = {2014},
  month = jul,
  volume = {15},
  issn = {1471-2202},
  doi = {10.1186/1471-2202-15-S1-P103},
  file = {2014 - O’Leary, Marder - Temperature-robust neural activity using feedback control of ion channel expression.pdf;O’Leary and Marder - 2014 - Temperature-robust neural activity using feedback .pdf},
  journal = {BMC Neuroscience},
  language = {en},
  number = {S1}
}

@article{Oliveira2010,
  title = {Transcranial Magnetic Stimulation of Posterior Parietal Cortex Affects Decisions of Hand Choice},
  author = {Oliveira, Flavio T. P. and Diedrichsen, J{\"o}rn and Verstynen, Timothy and Duque, Julie and Ivry, Richard B.},
  year = {2010},
  month = oct,
  volume = {107},
  pages = {17751--17756},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1006223107},
  file = {2010 - Oliveira et al. - Transcranial magnetic stimulation of posterior parietal cortex affects decisions of hand choice.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {41}
}

@article{Olkowicz2016,
  title = {Birds Have Primate-like Numbers of Neurons in the Forebrain},
  author = {Olkowicz, Seweryn and Kocourek, Martin and Lu{\v c}an, Radek K. and Porte{\v s}, Michal and Fitch, W. Tecumseh and {Herculano-Houzel}, Suzana and N{\v e}mec, Pavel},
  year = {2016},
  month = jun,
  volume = {113},
  pages = {7255--7260},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1517131113},
  abstract = {Some birds achieve primate-like levels of cognition, even though their brains tend to be much smaller in absolute size. This poses a fundamental problem in comparative and computational neuroscience, because small brains are expected to have a lower information-processing capacity. Using the isotropic fractionator to determine numbers of neurons in specific brain regions, here we show that the brains of parrots and songbirds contain on average twice as many neurons as primate brains of the same mass, indicating that avian brains have higher neuron packing densities than mammalian brains. Additionally, corvids and parrots have much higher proportions of brain neurons located in the pallial telencephalon compared with primates or other mammals and birds. Thus, large-brained parrots and corvids have forebrain neuron counts equal to or greater than primates with much larger brains. We suggest that the large numbers of neurons concentrated in high densities in the telencephalon substantially contribute to the neural basis of avian intelligence.},
  file = {Olkowicz et al. - 2016 - Birds have primate-like numbers of neurons in the .pdf},
  journal = {Proc Natl Acad Sci USA},
  language = {en},
  number = {26}
}

@article{Olsen2012,
  title = {Gain Control by Layer Six in Cortical Circuits of Vision},
  author = {Olsen, Shawn R. and Bortone, Dante S. and Adesnik, Hillel and Scanziani, Massimo},
  year = {2012},
  month = mar,
  volume = {483},
  pages = {47--52},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature10835},
  file = {2012 - Olsen et al. - Gain control by layer six in cortical circuits of vision.pdf},
  journal = {Nature},
  language = {en},
  number = {7387}
}

@article{Onslow2014,
  title = {A {{Canonical Circuit}} for {{Generating Phase}}-{{Amplitude Coupling}}},
  author = {Onslow, Angela C. E. and Jones, Matthew W. and Bogacz, Rafal},
  editor = {Tort, Adriano B. L.},
  year = {2014},
  month = aug,
  volume = {9},
  pages = {e102591},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0102591},
  abstract = {Phase amplitude coupling' (PAC) in oscillatory neural activity describes a phenomenon whereby the amplitude of higher frequency activity is modulated by the phase of lower frequency activity. Such coupled oscillatory activity \textendash{} also referred to as `cross-frequency coupling' or `nested rhythms' \textendash{} has been shown to occur in a number of brain regions and at behaviorally relevant time points during cognitive tasks; this suggests functional relevance, but the circuit mechanisms of PAC generation remain unclear. In this paper we present a model of a canonical circuit for generating PAC activity, showing how interconnected excitatory and inhibitory neural populations can be periodically shifted in to and out of oscillatory firing patterns by afferent drive, hence generating higher frequency oscillations phase-locked to a lower frequency, oscillating input signal. Since many brain regions contain mutually connected excitatory-inhibitory populations receiving oscillatory input, the simplicity of the mechanism generating PAC in such networks may explain the ubiquity of PAC across diverse neural systems and behaviors. Analytic treatment of this circuit as a nonlinear dynamical system demonstrates how connection strengths and inputs to the populations can be varied in order to change the extent and nature of PAC activity, importantly which phase of the lower frequency rhythm the higher frequency activity is locked to. Consequently, this model can inform attempts to associate distinct types of PAC with different network topologies and physiologies in real data.},
  file = {Onslow et al. - 2014 - A Canonical Circuit for Generating Phase-Amplitude.pdf},
  journal = {PLoS ONE},
  language = {en},
  number = {8}
}

@article{Oosterhof2011,
  title = {A Comparison of Volume-Based and Surface-Based Multi-Voxel Pattern Analysis},
  author = {Oosterhof, Nikolaas N. and Wiestler, Tobias and Downing, Paul E. and Diedrichsen, J{\"o}rn},
  year = {2011},
  month = may,
  volume = {56},
  pages = {593--600},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2010.04.270},
  abstract = {For functional magnetic resonance imaging (fMRI), multi-voxel pattern analysis (MVPA) has been shown to be a sensitive method to detect areas that encode certain stimulus dimensions. By moving a searchlight through the volume of the brain, one can continuously map the information content about the experimental conditions of interest to the brain.},
  file = {2011 - Oosterhof et al. - A comparison of volume-based and surface-based multi-voxel pattern analysis.pdf},
  journal = {NeuroImage},
  language = {en},
  number = {2}
}

@article{OpdeBeeck2010,
  title = {Probing the Mysterious Underpinnings of Multi-Voxel {{fMRI}} Analyses},
  author = {{Op de Beeck}, Hans P.},
  year = {2010},
  month = apr,
  volume = {50},
  pages = {567--571},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2009.12.072},
  abstract = {Various arguments have been proposed for or against sub-voxel sensitivity or hyperacuity in functional magnetic resonance imaging (fMRI) at standard resolution. Sub-voxel sensitivity might exist, but nevertheless the performance of multi-voxel fMRI analyses is very likely to be dominated by a largerscale organization, even if this organization is very weak. Up to now, most arguments are indirect in nature: they do not in themselves proof or contradict sub-voxel sensitivity, but they are suggestive, seem consistent or not with sub-voxel sensitivity, or show that the principle might or might not work. Here the previously proposed smoothing argument against hyperacuity is extended with simulations that include more realistic signal, noise, and analysis properties than any of the simulations presented before. These simulations confirm the relevance of the smoothing approach to find out the scale of the functional maps that underlie the outcome of multi-voxel analyses, at least in relative terms (differences in the scale of different maps). However, image smoothing, like most other arguments in the literature, is an indirect argument, and at the end of the day such arguments are not sufficient to decide the issue on whether and how much sub-voxel maps contribute. A few suggestions are made about the type of evidence that is needed to help us understand the as yet mysterious underpinnings of multi-voxel fMRI analyses.},
  file = {2010 - Op de Beeck - Probing the mysterious underpinnings of multi-voxel fMRI analyses.pdf},
  journal = {NeuroImage},
  language = {en},
  number = {2}
}

@article{Ortiz-Jimenez2021,
  title = {Optimism in the {{Face}} of {{Adversity}}: {{Understanding}} and {{Improving Deep Learning}} through {{Adversarial Robustness}}},
  shorttitle = {Optimism in the {{Face}} of {{Adversity}}},
  author = {{Ortiz-Jimenez}, Guillermo and Modas, Apostolos and {Moosavi-Dezfooli}, Seyed-Mohsen and Frossard, Pascal},
  year = {2021},
  month = jan,
  abstract = {Driven by massive amounts of data and important advances in computational resources, new deep learning systems have achieved outstanding results in a large spectrum of applications. Nevertheless, our current theoretical understanding on the mathematical foundations of deep learning lags far behind its empirical success. However, the field of adversarial robustness has recently become one of the main sources of explanations of our deep models. In this article, we provide an in-depth review of the field, and give a self-contained introduction to its main notions. But, in contrast to the mainstream pessimistic perspective of adversarial robustness, we focus on the main positive aspects that it entails. We highlight the intuitive connection between adversarial examples and the geometry of deep neural networks, and eventually explore how the geometric study of adversarial examples can serve as a powerful tool to understand deep learning. Furthermore, we demonstrate the broad applicability of adversarial robustness, providing an overview of the main emerging applications of adversarial robustness beyond security. The goal of this article is to provide readers with a set of new perspectives to understand deep learning, and to supply them with intuitive tools and insights on how to use adversarial robustness to improve it.},
  archiveprefix = {arXiv},
  eprint = {2010.09624},
  eprinttype = {arxiv},
  file = {Ortiz-Jimenez et al. - 2021 - Optimism in the Face of Adversity Understanding a.pdf},
  journal = {arXiv:2010.09624 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  language = {en},
  primaryclass = {cs}
}

@article{Osband2019,
  title = {Behaviour {{Suite}} for {{Reinforcement Learning}}},
  author = {Osband, Ian and Doron, Yotam and Hessel, Matteo and Aslanides, John and Sezener, Eren and Saraiva, Andre and McKinney, Katrina and Lattimore, Tor and Szepezvari, Csaba and Singh, Satinder and Van Roy, Benjamin and Sutton, Richard and Silver, David and Van Hasselt, Hado},
  year = {2019},
  month = aug,
  abstract = {This paper introduces the Behaviour Suite for Reinforcement Learning, or bsuite for short. bsuite is a collection of carefully-designed experiments that investigate core capabilities of reinforcement learning (RL) agents with two objectives. First, to collect clear, informative and scalable problems that capture key issues in the design of general and efficient learning algorithms. Second, to study agent behaviour through their performance on these shared benchmarks. To complement this effort, we open source github.com/deepmind/bsuite, which automates evaluation and analysis of any agent on bsuite. This library facilitates reproducible and accessible research on the core issues in RL, and ultimately the design of superior learning algorithms. Our code is Python, and easy to use within existing projects. We include examples with OpenAI Baselines, Dopamine as well as new reference implementations. Going forward, we hope to incorporate more excellent experiments from the research community, and commit to a periodic review of bsuite from a committee of prominent researchers.},
  archiveprefix = {arXiv},
  eprint = {1908.03568},
  eprinttype = {arxiv},
  file = {Osband et al. - 2019 - Behaviour Suite for Reinforcement Learning.pdf},
  journal = {arXiv:1908.03568 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryclass = {cs, stat}
}

@article{Osborne1977,
  title = {The Free Food (Contrafreeloading) Phenomenon: {{A}} Review and Analysis},
  shorttitle = {The Free Food (Contrafreeloading) Phenomenon},
  author = {Osborne, Steve R.},
  year = {1977},
  month = sep,
  volume = {5},
  pages = {221--235},
  issn = {0090-4996, 1532-5830},
  doi = {10.3758/BF03209232},
  file = {Osborne - 1977 - The free food (contrafreeloading) phenomenon A re.pdf},
  journal = {Animal Learning \& Behavior},
  language = {en},
  number = {3}
}

@article{Osborne1977a,
  title = {The Free Food (Contrafreeloading) Phenomenon: {{A}} Review and Analysis},
  shorttitle = {The Free Food (Contrafreeloading) Phenomenon},
  author = {Osborne, Steve R.},
  year = {1977},
  month = sep,
  volume = {5},
  pages = {221--235},
  issn = {0090-4996, 1532-5830},
  doi = {10.3758/BF03209232},
  file = {Osborne - 1977 - The free food (contrafreeloading) phenomenon A re 2.pdf},
  journal = {Animal Learning \& Behavior},
  language = {en},
  number = {3}
}

@article{Oshanin2009,
  title = {Survival of an Evasive Prey},
  author = {Oshanin, G. and Vasilyev, O. and Krapivsky, P. L. and Klafter, J.},
  year = {2009},
  month = aug,
  volume = {106},
  pages = {13696--13701},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.0904354106},
  file = {Oshanin et al. - 2009 - Survival of an evasive prey.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {33}
}

@article{Ostojic2011,
  title = {From {{Spiking Neuron Models}} to {{Linear}}-{{Nonlinear Models}}},
  author = {Ostojic, Srdjan and Brunel, Nicolas},
  editor = {Latham, Peter E.},
  year = {2011},
  month = jan,
  volume = {7},
  pages = {e1001056},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1001056},
  abstract = {Neurons transform time-varying inputs into action potentials emitted stochastically at a time dependent rate. The mapping from current input to output firing rate is often represented with the help of phenomenological models such as the linearnonlinear (LN) cascade, in which the output firing rate is estimated by applying to the input successively a linear temporal filter and a static non-linear transformation. These simplified models leave out the biophysical details of action potential generation. It is not a priori clear to which extent the input-output mapping of biophysically more realistic, spiking neuron models can be reduced to a simple linear-nonlinear cascade. Here we investigate this question for the leaky integrate-andfire (LIF), exponential integrate-and-fire (EIF) and conductance-based Wang-Buzsa\textasciiacute ki models in presence of background synaptic activity. We exploit available analytic results for these models to determine the corresponding linear filter and static non-linearity in a parameter-free form. We show that the obtained functions are identical to the linear filter and static nonlinearity determined using standard reverse correlation analysis. We then quantitatively compare the output of the corresponding linear-nonlinear cascade with numerical simulations of spiking neurons, systematically varying the parameters of input signal and background noise. We find that the LN cascade provides accurate estimates of the firing rates of spiking neurons in most of parameter space. For the EIF and Wang-Buzsa\textasciiacute ki models, we show that the LN cascade can be reduced to a firing rate model, the timescale of which we determine analytically. Finally we introduce an adaptive timescale rate model in which the timescale of the linear filter depends on the instantaneous firing rate. This model leads to highly accurate estimates of instantaneous firing rates.},
  file = {2011 - Ostojic, Brunel - From Spiking Neuron Models to Linear-Nonlinear Models.pdf},
  journal = {PLoS Computational Biology},
  language = {en},
  number = {1}
}

@article{Ostojic2014,
  title = {Two Types of Asynchronous Activity in Networks of Excitatory and Inhibitory Spiking Neurons},
  author = {Ostojic, Srdjan},
  year = {2014},
  month = apr,
  volume = {17},
  pages = {594--600},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.3658},
  file = {Ostojic - 2014 - Two types of asynchronous activity in networks of .pdf},
  journal = {Nature Neuroscience},
  language = {en},
  number = {4}
}

@article{OToole2005,
  title = {Partially {{Distributed Representations}} of {{Objects}} and {{Faces}} in {{Ventral Temporal Cortex}}},
  author = {O'Toole, Alice J. and Jiang, Fang and Abdi, Herv{\'e} and Haxby, James V.},
  year = {2005},
  month = apr,
  volume = {17},
  pages = {580--590},
  issn = {0898-929X, 1530-8898},
  doi = {10.1162/0898929053467550},
  file = {2005 - O'Toole et al. - Partially distributed representations of objects and faces in ventral temporal cortex.pdf},
  journal = {Journal of Cognitive Neuroscience},
  language = {en},
  number = {4}
}

@article{OToole2007,
  title = {Theoretical, {{Statistical}}, and {{Practical Perspectives}} on {{Pattern}}-Based {{Classification Approaches}} to the {{Analysis}} of {{Functional Neuroimaging Data}}},
  author = {O'Toole, Alice J. and Jiang, Fang and Abdi, Herv{\'e} and P{\'e}nard, Nils and Dunlop, Joseph P. and Parent, Marc A.},
  year = {2007},
  month = nov,
  volume = {19},
  pages = {1735--1752},
  issn = {0898-929X, 1530-8898},
  doi = {10.1162/jocn.2007.19.11.1735},
  file = {2007 - O'Toole et al. - Theoretical, statistical, and practical perspectives on pattern-based classification approaches to the analysis.pdf},
  journal = {Journal of Cognitive Neuroscience},
  language = {en},
  number = {11}
}

@article{Ottino-Loffler,
  title = {Evolutionary Dynamics of Incubation Periods},
  author = {{Ottino-Loffler}, Bertrand and Scott, Jacob G and Strogatz, Steven H},
  pages = {28},
  file = {Ottino-Loffler et al. - Evolutionary dynamics of incubation periods.pdf},
  language = {en}
}

@article{Ottino-Loffler2016,
  title = {Comparing the Locking Threshold for Rings and Chains of Oscillators},
  author = {{Ottino-L{\"o}ffler}, Bertrand and Strogatz, Steven H.},
  year = {2016},
  month = dec,
  volume = {94},
  issn = {2470-0045, 2470-0053},
  doi = {10.1103/PhysRevE.94.062203},
  file = {Ottino-Löffler and Strogatz - 2016 - Comparing the locking threshold for rings and chai.pdf},
  journal = {Physical Review E},
  language = {en},
  number = {6}
}

@article{Ottino-Loffler2016a,
  title = {Frequency Spirals},
  author = {{Ottino-L{\"o}ffler}, Bertrand and Strogatz, Steven H.},
  year = {2016},
  month = sep,
  volume = {26},
  pages = {094804},
  issn = {1054-1500, 1089-7682},
  doi = {10.1063/1.4954038},
  file = {Ottino-Löffler and Strogatz - 2016 - Frequency spirals.pdf},
  journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
  language = {en},
  number = {9}
}

@article{Ottino-Loffler2016b,
  title = {Kuramoto Model with Uniformly Spaced Frequencies: {{Finite}}- {{N}} Asymptotics of the Locking Threshold},
  shorttitle = {Kuramoto Model with Uniformly Spaced Frequencies},
  author = {{Ottino-L{\"o}ffler}, Bertrand and Strogatz, Steven H.},
  year = {2016},
  month = jun,
  volume = {93},
  issn = {2470-0045, 2470-0053},
  doi = {10.1103/PhysRevE.93.062220},
  file = {Ottino-Löffler and Strogatz - 2016 - Kuramoto model with uniformly spaced frequencies .pdf},
  journal = {Physical Review E},
  language = {en},
  number = {6}
}

@article{Oudeyer,
  title = {Curiosity and {{Intrinsic Motivation}} for {{Autonomous Machine Learning}}},
  author = {Oudeyer, Pierre-Yves and Lopes, Manuel and Kidd, Celeste and Gottlieb, Jacqueline},
  pages = {3},
  file = {Oudeyer et al. - Curiosity and Intrinsic Motivation for Autonomous .pdf},
  language = {en}
}

@article{Oudeyer2007,
  title = {What Is Intrinsic Motivation? {{A}} Typology of Computational Approaches},
  shorttitle = {What Is Intrinsic Motivation?},
  author = {Oudeyer, Pierre-Yves},
  year = {2007},
  volume = {1},
  issn = {1662-5218},
  doi = {10.3389/neuro.12.006.2007},
  abstract = {Intrinsic motivation, centrally involved in spontaneous exploration and curiosity, is a crucial concept in developmental psychology. It has been argued to be a crucial mechanism for open-ended cognitive development in humans, and as such has gathered a growing interest from developmental roboticists in the recent years. The goal of this paper is threefold. First, it provides a synthesis of the different approaches of intrinsic motivation in psychology. Second, by interpreting these approaches in a computational reinforcement learning framework, we argue that they are not operational and even sometimes inconsistent. Third, we set the ground for a systematic operational study of intrinsic motivation by presenting a formal typology of possible computational approaches. This typology is partly based on existing computational models, but also presents new ways of conceptualizing intrinsic motivation. We argue that this kind of computational typology might be useful for opening new avenues for research both in psychology and developmental robotics.},
  file = {Oudeyer - 2007 - What is intrinsic motivation A typology of comput.pdf},
  journal = {Front. Neurorobot.},
  language = {en}
}

@article{Oudeyer2007a,
  title = {Intrinsic {{Motivation Systems}} for {{Autonomous Mental Development}}},
  author = {Oudeyer, Pierre-Yves and Kaplan, Frdric and Hafner, Verena V.},
  year = {2007},
  month = apr,
  volume = {11},
  pages = {265--286},
  issn = {1089-778X},
  doi = {10.1109/TEVC.2006.890271},
  abstract = {Exploratory activities seem to be intrinsically rewarding for children and crucial for their cognitive development. Can a machine be endowed with such an intrinsic motivation system? This is the question we study in this paper, presenting a number of computational systems that try to capture this drive towards novel or curious situations. After discussing related research coming from developmental psychology, neuroscience, developmental robotics, and active learning, this paper presents the mechanism of Intelligent Adaptive Curiosity, an intrinsic motivation system which pushes a robot towards situations in which it maximizes its learning progress. This drive makes the robot focus on situations which are neither too predictable nor too unpredictable, thus permitting autonomous mental development. The complexity of the robot's activities autonomously increases and complex developmental sequences self-organize without being constructed in a supervised manner. Two experiments are presented illustrating the stage-like organization emerging with this mechanism. In one of them, a physical robot is placed on a baby play mat with objects that it can learn to manipulate. Experimental results show that the robot first spends time in situations which are easy to learn, then shifts its attention progressively to situations of increasing difficulty, avoiding situations in which nothing can be learned. Finally, these various results are discussed in relation to more complex forms of behavioral organization and data coming from developmental psychology.},
  file = {Oudeyer et al. - 2007 - Intrinsic Motivation Systems for Autonomous Mental.pdf},
  journal = {IEEE Trans. Evol. Computat.},
  language = {en},
  number = {2}
}

@article{Oudeyer2016,
  title = {How {{Evolution May Work Through Curiosity}}-{{Driven Developmental Process}}},
  author = {Oudeyer, Pierre-Yves and Smith, Linda B.},
  year = {2016},
  month = apr,
  volume = {8},
  pages = {492--502},
  issn = {17568757},
  doi = {10.1111/tops.12196},
  file = {Oudeyer and Smith - 2016 - How Evolution May Work Through Curiosity-Driven De.pdf},
  journal = {Top Cogn Sci},
  language = {en},
  number = {2}
}

@article{Oudeyer2016a,
  title = {How {{Evolution May Work Through Curiosity}}-{{Driven Developmental Process}}},
  author = {Oudeyer, Pierre-Yves and Smith, Linda B.},
  year = {2016},
  month = apr,
  volume = {8},
  pages = {492--502},
  issn = {17568757},
  doi = {10.1111/tops.12196},
  file = {Oudeyer and Smith - 2016 - How Evolution May Work Through Curiosity-Driven De 2.pdf},
  journal = {Top Cogn Sci},
  language = {en},
  number = {2}
}

@article{Oudeyer2018,
  title = {Computational {{Theories}} of {{Curiosity}}-{{Driven Learning}}},
  author = {Oudeyer, Pierre-Yves},
  year = {2018},
  month = jun,
  abstract = {What are the functions of curiosity? What are the mechanisms of curiosity-driven learning? We approach these questions about the living using concepts and tools from machine learning and developmental robotics. We argue that curiosity-driven learning enables organisms to make discoveries to solve complex problems with rare or deceptive rewards. By fostering exploration and discovery of a diversity of behavioural skills, and ignoring these rewards, curiosity can be efficient to bootstrap learning when there is no information, or deceptive information, about local improvement towards these problems. We also explain the key role of curiosity for efficient learning of world models. We review both normative and heuristic computational frameworks used to understand the mechanisms of curiosity in humans, conceptualizing the child as a sense-making organism. These frameworks enable us to discuss the bi-directional causal links between curiosity and learning, and to provide new hypotheses about the fundamental role of curiosity in self-organizing developmental structures through curriculum learning. We present various developmental robotics experiments that study these mechanisms in action, both supporting these hypotheses to understand better curiosity in humans and opening new research avenues in machine learning and artificial intelligence. Finally, we discuss challenges for the design of experimental paradigms for studying curiosity in psychology and cognitive neuroscience.},
  archiveprefix = {arXiv},
  eprint = {1802.10546},
  eprinttype = {arxiv},
  file = {Oudeyer - 2018 - Computational Theories of Curiosity-Driven Learnin.pdf},
  journal = {arXiv:1802.10546 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  language = {en},
  primaryclass = {cs}
}

@article{Oudeyer2018a,
  title = {Computational {{Theories}} of {{Curiosity}}-{{Driven Learning}}},
  author = {Oudeyer, Pierre-Yves},
  year = {2018},
  month = jun,
  abstract = {What are the functions of curiosity? What are the mechanisms of curiosity-driven learning? We approach these questions about the living using concepts and tools from machine learning and developmental robotics. We argue that curiosity-driven learning enables organisms to make discoveries to solve complex problems with rare or deceptive rewards. By fostering exploration and discovery of a diversity of behavioural skills, and ignoring these rewards, curiosity can be efficient to bootstrap learning when there is no information, or deceptive information, about local improvement towards these problems. We also explain the key role of curiosity for efficient learning of world models. We review both normative and heuristic computational frameworks used to understand the mechanisms of curiosity in humans, conceptualizing the child as a sense-making organism. These frameworks enable us to discuss the bi-directional causal links between curiosity and learning, and to provide new hypotheses about the fundamental role of curiosity in self-organizing developmental structures through curriculum learning. We present various developmental robotics experiments that study these mechanisms in action, both supporting these hypotheses to understand better curiosity in humans and opening new research avenues in machine learning and artificial intelligence. Finally, we discuss challenges for the design of experimental paradigms for studying curiosity in psychology and cognitive neuroscience.},
  archiveprefix = {arXiv},
  eprint = {1802.10546},
  eprinttype = {arxiv},
  file = {Oudeyer - 2018 - Computational Theories of Curiosity-Driven Learnin 2.pdf},
  journal = {arXiv:1802.10546 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  language = {en},
  primaryclass = {cs}
}

@article{Pajevic2014,
  title = {Role of Myelin Plasticity in Oscillations and Synchrony of Neuronal Activity},
  author = {Pajevic, S. and Basser, P.J. and Fields, R.D.},
  year = {2014},
  month = sep,
  volume = {276},
  pages = {135--147},
  issn = {03064522},
  doi = {10.1016/j.neuroscience.2013.11.007},
  abstract = {Conduction time is typically ignored in computational models of neural network function. Here we consider the effects of conduction delays on the synchrony of neuronal activity and neural oscillators, and evaluate the consequences of allowing conduction velocity (CV) to be regulated adaptively. We propose that CV variation, mediated by myelin, could provide an important mechanism of activity-dependent nervous system plasticity. Even small changes in CV, resulting from small changes in myelin thickness or nodal structure, could have profound effects on neuronal network function in terms of spike-time arrival, oscillation frequency, oscillator coupling, and propagation of brain waves. For example, a conduction delay of 5 ms could change interactions of two coupled oscillators at the upper end of the gamma frequency range ({$\sim$}100 Hz) from constructive to destructive interference; delays smaller than 1 ms could change the phase by 30\textdegree, significantly affecting signal amplitude. Myelin plasticity, as another form of activitydependent plasticity, is relevant not only to nervous system development but also to complex information processing tasks that involve coupling and synchrony among different brain rhythms. We use coupled oscillator models with time delays to explore the importance of adaptive time delays and adaptive synaptic strengths. The impairment of activity-dependent myelination and the loss of adaptive time delays may contribute to disorders where hyper- and hypo-synchrony of neuronal firing leads to dysfunction (e.g., dyslexia, schizophrenia, epilepsy).},
  file = {Pajevic et al. - 2014 - Role of myelin plasticity in oscillations and sync.pdf},
  journal = {Neuroscience},
  language = {en}
}

@article{Palmieri2015,
  title = {The Transfer Function of Neuron Spike},
  author = {Palmieri, Igor and Monteiro, Luiz H.A. and Miranda, Maria D.},
  year = {2015},
  month = aug,
  volume = {68},
  pages = {89--95},
  issn = {08936080},
  doi = {10.1016/j.neunet.2015.04.003},
  abstract = {The mathematical modeling of neuronal signals is a relevant problem in neuroscience. The complexity of the neuron behavior, however, makes this problem a particularly difficult task. Here, we propose a discrete-time linear time-invariant (LTI) model with a rational function in order to represent the neuronal spike detected by an electrode located in the surroundings of the nerve cell. The model is presented as a cascade association of two subsystems: one that generates an action potential from an input stimulus, and one that represents the medium between the cell and the electrode. The suggested approach employs system identification and signal processing concepts, and is dissociated from any considerations about the biophysical processes of the neuronal cell, providing a low-complexity alternative to model the neuronal spike. The model is validated by using in vivo experimental readings of intracellular and extracellular signals. A computational simulation of the model is presented in order to assess its proximity to the neuronal signal and to observe the variability of the estimated parameters. The implications of the results are discussed in the context of spike sorting.},
  file = {Palmieri et al. - 2015 - The transfer function of neuron spike.pdf},
  journal = {Neural Networks},
  language = {en}
}

@article{Palyulin2014,
  title = {Levy Flights Do Not Always Optimize Random Blind Search for Sparse Targets},
  author = {Palyulin, V. V. and Chechkin, A. V. and Metzler, R.},
  year = {2014},
  month = feb,
  volume = {111},
  pages = {2931--2936},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1320424111},
  file = {Palyulin et al. - 2014 - Levy flights do not always optimize random blind s.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {8}
}

@article{Palyulin2014a,
  title = {Levy Flights Do Not Always Optimize Random Blind Search for Sparse Targets},
  author = {Palyulin, V. V. and Chechkin, A. V. and Metzler, R.},
  year = {2014},
  month = feb,
  volume = {111},
  pages = {2931--2936},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1320424111},
  file = {Palyulin et al. - 2014 - Levy flights do not always optimize random blind s 2.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {8}
}

@article{Panzeri2008,
  title = {On the Use of Information Theory for the Analysis of the Relationship between Neural and Imaging Signals},
  author = {Panzeri, Stefano and Magri, Cesare and Logothetis, Nikos K.},
  year = {2008},
  month = sep,
  volume = {26},
  pages = {1015--1025},
  issn = {0730725X},
  doi = {10.1016/j.mri.2008.02.019},
  abstract = {Functional magnetic resonance imaging (fMRI) is a widely used method for studying the neural basis of cognition and of sensory function. A potential problem in the interpretation of fMRI data is that fMRI measures neural activity only indirectly, as a local change of deoxyhemoglobin concentration due to the metabolic demands of neural function. To build correct sensory and cognitive maps in the human brain, it is thus crucial to understand whether fMRI and neural activity convey the same type of information about external correlates. While a substantial experimental effort has been devoted to the simultaneous recordings of hemodynamic and neural signals, so far, the development of analysis methods that elucidate how neural and hemodynamic signals represent sensory information has received less attention. In this article, we critically review why the analytical framework of information theory, the mathematical theory of communication, is ideally suited to this purpose. We review the principles of information theory and explain how they could be applied to the analysis of fMRI and neural signals. We show that a critical advantage of information theory over more traditional analysis paradigms commonly used in the fMRI literature is that it can elucidate, within a single framework, whether an empirically observed correlation between neural and fMRI signals reflects either a similar stimulus tuning or a common source of variability unrelated to the external stimuli. In addition, information theory determines the extent to which these shared sources of stimulus signal and of variability lead fMRI and neural signals to convey similar information about external correlates. We then illustrate the formalism by applying it to the analysis of the information carried by different bands of the local field potential. We conclude by discussing the current methodological challenges that need to be addressed to make the information-theoretic approach more robustly applicable to the simultaneous recordings of neural and imaging data.},
  file = {2008 - Panzeri, Magri, Logothetis - On the use of information theory for the analysis of the relationship between neural and imaging sig.pdf},
  journal = {Magnetic Resonance Imaging},
  language = {en},
  number = {7}
}

@article{Panzeri2015,
  title = {Neural Population Coding: Combining Insights from Microscopic and Mass Signals},
  shorttitle = {Neural Population Coding},
  author = {Panzeri, Stefano and Macke, Jakob H. and Gross, Joachim and Kayser, Christoph},
  year = {2015},
  month = mar,
  volume = {19},
  pages = {162--172},
  issn = {13646613},
  doi = {10.1016/j.tics.2015.01.002},
  file = {2015 - Panzeri et al. - Neural population coding combining insights from microscopic and mass signals.pdf;Panzeri et al. - 2015 - Neural population coding combining insights from .pdf},
  journal = {Trends in Cognitive Sciences},
  language = {en},
  number = {3}
}

@article{Papadopoulos,
  title = {Embedding of Biological Distribution Networks with Differing Environmental Constraints},
  author = {Papadopoulos, Lia and Blinder, Pablo and Ronellenfitsch, Henrik and Katifori, Eleni and Kleinfeld, David and Bassett, Danielle S},
  pages = {20},
  file = {Papadopoulos et al. - Embedding of biological distribution networks with.pdf},
  language = {en}
}

@article{Papadopoulos2017,
  title = {Development of Structural Correlations and Synchronization from Adaptive Rewiring in Networks of {{Kuramoto}} Oscillators},
  author = {Papadopoulos, Lia and Kim, Jason Z. and Kurths, J{\"u}rgen and Bassett, Danielle S.},
  year = {2017},
  month = jul,
  volume = {27},
  pages = {073115},
  issn = {1054-1500, 1089-7682},
  doi = {10.1063/1.4994819},
  file = {Papadopoulos et al. - 2017 - Development of structural correlations and synchro.pdf},
  journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
  language = {en},
  number = {7}
}

@article{Papasavvas2015,
  title = {Gain Control through Divisive Inhibition Prevents Abrupt Transition to Chaos in a Neural Mass Model},
  author = {Papasavvas, Christoforos A. and Wang, Yujiang and Trevelyan, Andrew J. and Kaiser, Marcus},
  year = {2015},
  month = sep,
  volume = {92},
  issn = {1539-3755, 1550-2376},
  doi = {10.1103/PhysRevE.92.032723},
  file = {2015 - Papasavvas et al. - Gain control through divisive inhibition prevents abrupt transition to chaos in a neural mass model.pdf;Papasavvas et al. - 2015 - Gain control through divisive inhibition prevents .pdf},
  journal = {Physical Review E},
  language = {en},
  number = {3}
}

@article{Papernot2017,
  title = {{{SEMI}}-{{SUPERVISED KNOWLEDGE TRANSFER FOR DEEP LEARNING FROM PRIVATE TRAINING DATA}}},
  author = {Papernot, Nicolas and Abadi, Mart{\i}n},
  year = {2017},
  pages = {14},
  abstract = {Some machine learning applications involve training data that is sensitive, such as the medical histories of patients in a clinical trial. A model may inadvertently and implicitly store some of its training data; careful analysis of the model may therefore reveal sensitive information.},
  file = {Papernot and Abadi - 2017 - SEMI-SUPERVISED KNOWLEDGE TRANSFER FOR DEEP LEARNI.pdf},
  language = {en}
}

@techreport{Parameshwaran2017,
  title = {Modernization, Wealth and the Emergence of Strong Alpha Oscillations in the Human {{EEG}}},
  author = {Parameshwaran, Dhanya and Thiagarajan, Tara C.},
  year = {2017},
  month = apr,
  institution = {{Neuroscience}},
  doi = {10.1101/125898},
  abstract = {Oscillations in the alpha range (8-15 Hz) have been found to appear prominently in the EEG signal when people are awake with their eyes closed, and since their discovery have been considered a fundamental cerebral rhythm. While the mechanism of this oscillation continues to be debated, it has been shown to bear positive relation to memory capacity, attention and a host of other cognitive outcomes. Here we show that this feature is largely undetected in the EEG of adults without post-primary education and access to modern technologies. Furthermore, we show that the spatial extent and energy of the oscillation have wide variation, with energy ranging over a thousand fold across the breath of humanity with no centralizing mean. This represents a divergence in a fundamental functional characteristic of an organ demonstrating both that modernization has had a profound influence on brain dynamics and that a meaningful `average' human brain does not exist in a dynamical sense.},
  file = {Parameshwaran and Thiagarajan - 2017 - Modernization, wealth and the emergence of strong .pdf},
  language = {en},
  type = {Preprint}
}

@article{Parent2007,
  title = {The Microcircuitry of Primate Subthalamic Nucleus},
  author = {Parent, Martin and Parent, Andr{\'e}},
  year = {2007},
  volume = {13},
  pages = {S292-S295},
  issn = {13538020},
  doi = {10.1016/S1353-8020(08)70018-X},
  abstract = {Single-cell labeling experiments in cynomolgus monkeys have revealed that the subthalamic nucleus (STN) harbors several subtypes of projection neurons, each endowed with a highly patterned set of axon collaterals. This organizational feature allows single STN neurons to act directly upon the two major output structures of the basal ganglia \textendash{} the substantia nigra pars reticulata and the internal pallidum \textendash{} and, at the same time, to exert a multifarious effect upon the external pallidum with which the STN is reciprocally connected. These findings have clarified the role of the STN in basal ganglia organization and led to the elaboration of more accurate computational models of deep brain stimulation, a therapeutic approach currently used to alleviate the motor symptoms of Parkinson's Disease.},
  file = {2007 - Parent, Parent - The microcircuitry of primate subthalamic nucleus.pdf},
  journal = {Parkinsonism \& Related Disorders},
  language = {en}
}

@article{Parikh2007,
  title = {Prefrontal {{Acetylcholine Release Controls Cue Detection}} on {{Multiple Timescales}}},
  author = {Parikh, Vinay and Kozak, Rouba and Martinez, Vicente and Sarter, Martin},
  year = {2007},
  month = oct,
  volume = {56},
  pages = {141--154},
  issn = {08966273},
  doi = {10.1016/j.neuron.2007.08.025},
  abstract = {Cholinergic neurons originating from the basal forebrain innervate the entire cortical mantle. Choline-sensitive microelectrodes were used to measure the synaptic release of cortical acetylcholine (ACh) at a sub-second resolution in rats performing a task involving the detection of cues. Cues that were detected, defined behaviorally, evoked transient increases in cholinergic activity (at the scale of seconds) in the medial prefrontal cortex (mPFC), but not in a non-associational control region (motor cortex). In trials involving missed cues, cholinergic transients were not observed. Cholinergic deafferentation of the mPFC, but not motor cortex, impaired cue detection. Furthermore, decreases and increases in pre-cue cholinergic activity predicted subsequent cue detection or misses, respectively. Finally, cue-evoked cholinergic transients were superimposed over slower (at the time scale of minutes) changes in cholinergic activity. Cortical cholinergic neurotransmission is regulated on multiple time scales to mediate the detection of behaviorally significant cues and to support cognitive performance.},
  file = {2007 - Parikh et al. - Prefrontal acetylcholine release controls cue detection on multiple time scales.pdf},
  journal = {Neuron},
  language = {en},
  number = {1}
}

@techreport{Park2017,
  title = {Bayesian {{Efficient Coding}}},
  author = {Park, Il Memming and Pillow, Jonathan W.},
  year = {2017},
  month = aug,
  institution = {{Neuroscience}},
  doi = {10.1101/178418},
  abstract = {The efficient coding hypothesis, which proposes that neurons are optimized to maximize information about the environment, has provided a guiding theoretical framework for sensory and systems neuroscience. More recently, a theory known as the Bayesian Brain hypothesis has focused on the brain's ability to integrate sensory and prior sources of information in order to perform Bayesian inference. However, there is as yet no comprehensive theory connecting these two theoretical frameworks. Here we bridge this gap by formalizing a Bayesian theory of efficient coding. We define Bayesian efficient codes in terms of four basic ingredients: (1) a stimulus prior distribution; (2) an encoding model; (3) a capacity constraint, specifying a neural resource limit; and (4) a loss function, quantifying the desirability or undesirability of various posterior distributions. Classic efficient codes can be seen as a special case in which the loss function is the posterior entropy, leading to a code that maximizes mutual information, but alternate loss functions give solutions that differ dramatically from information-maximizing codes. In particular, we show that decorrelation of sensory inputs, which is optimal under classic efficient codes in low-noise settings, can be disadvantageous for loss functions that penalize large errors. Bayesian efficient coding therefore enlarges the family of normatively optimal codes and provides a more general framework for understanding the design principles of sensory systems. We examine Bayesian efficient codes for linear receptive fields and nonlinear input-output functions, and show that our theory invites reinterpretation of Laughlin's seminal analysis of efficient coding in the blowfly visual system.},
  file = {Park and Pillow - 2017 - Bayesian Efficient Coding.pdf},
  language = {en},
  type = {Preprint}
}

@article{Parkes2004,
  title = {Reduced {{BOLD}} Response to Periodic Visual Stimulation},
  author = {Parkes, Laura M and Fries, Pascal and Kerskens, Christian M and Norris, David G},
  year = {2004},
  month = jan,
  volume = {21},
  pages = {236--243},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2003.08.025},
  file = {2004 - Parkes et al. - Reduced BOLD response to periodic visual stimulation.pdf},
  journal = {NeuroImage},
  language = {en},
  number = {1}
}

@article{Parkes2009,
  title = {Multivoxel {{fMRI}} Analysis of Color Tuning in Human Primary Visual Cortex},
  author = {Parkes, L. M. and Marsman, J. B. C. and Oxley, D. C. and Goulermas, J. Y. and Wuerger, S. M.},
  year = {2009},
  month = jan,
  volume = {9},
  pages = {1--1},
  issn = {1534-7362},
  doi = {10.1167/9.1.1},
  abstract = {We use multivoxel pattern analysis (MVPA) to study the spatial clustering of color-selective neurons in the human brain. Our main objective was to investigate whether MVPA reveals the spatial arrangements of color-selective neurons in human primary visual cortex (V1). We measured the distributed fMRI activation patterns for different color stimuli (Experiment 1: cardinal colors (to which the LGN is known to be tuned), Experiment 2: perceptual hues) in V1. Our two main findings were that (i) cone-opponent cardinal color modulations produce highly reproducible patterns of activity in V1, but these were not unique to each color. This suggests that V1 neurons with tuning characteristics similar to those found in LGN are not spatially clustered. (ii) Unique activation patterns for perceptual hues in V1 support current evidence for a spatially clustered hue map. We believe that our work is the first to show evidence of spatial clustering of neurons with similar color preferences in human V1.},
  file = {2009 - Parkes, Marsman - Multivoxel fMRI analysis of color tuning in human primary visual cortex.pdf},
  journal = {Journal of Vision},
  language = {en},
  number = {1}
}

@article{Parpart2018,
  title = {Heuristics as {{Bayesian}} Inference under Extreme Priors},
  author = {Parpart, Paula and Jones, Matt and Love, Bradley C.},
  year = {2018},
  month = may,
  volume = {102},
  pages = {127--144},
  issn = {00100285},
  doi = {10.1016/j.cogpsych.2017.11.006},
  abstract = {Simple heuristics are often regarded as tractable decision strategies because they ignore a great deal of information in the input data. One puzzle is why heuristics can outperform full-information models, such as linear regression, which make full use of the available information. These ``lessis-more'' effects, in which a relatively simpler model outperforms a more complex model, are prevalent throughout cognitive science, and are frequently argued to demonstrate an inherent advantage of simplifying computation or ignoring information. In contrast, we show at the computational level (where algorithmic restrictions are set aside) that it is never optimal to discard information. Through a formal Bayesian analysis, we prove that popular heuristics, such as tallying and take-the-best, are formally equivalent to Bayesian inference under the limit of infinitely strong priors. Varying the strength of the prior yields a continuum of Bayesian models with the heuristics at one end and ordinary regression at the other. Critically, intermediate models perform better across all our simulations, suggesting that down-weighting information with the appropriate prior is preferable to entirely ignoring it. Rather than because of their simplicity, our analyses suggest heuristics perform well because they implement strong priors that approximate the actual structure of the environment. We end by considering how new heuristics could be derived by infinitely strengthening the priors of other Bayesian models. These formal results have implications for work in psychology, machine learning and economics.},
  file = {Parpart et al. - 2018 - Heuristics as Bayesian inference under extreme pri.pdf;Parpart et al. - Heuristics as Bayesian inference under extreme pri.pdf},
  journal = {Cognitive Psychology},
  language = {en}
}

@article{Pascanu2013,
  title = {Revisiting {{Natural Gradient}} for {{Deep Networks}}},
  author = {Pascanu, Razvan and Bengio, Yoshua},
  year = {2013},
  month = jan,
  abstract = {We evaluate natural gradient descent, an algorithm originally proposed in Amari (1997), for learning deep models. The contributions of this paper are as follows. We show the connection between natural gradient descent and three other recently proposed methods for training deep models: Hessian-Free Optimization (Martens, 2010), Krylov Subspace Descent (Vinyals and Povey, 2012) and TONGA (Le Roux et al., 2008). We describe how one can use unlabeled data to improve the generalization error obtained by natural gradient and empirically evaluate the robustness of the algorithm to the ordering of the training set compared to stochastic gradient descent. Finally we extend natural gradient descent to incorporate second order information alongside the manifold information and provide a benchmark of the new algorithm using a truncated Newton approach for inverting the metric matrix instead of using a diagonal approximation of it.},
  archiveprefix = {arXiv},
  eprint = {1301.3584},
  eprinttype = {arxiv},
  file = {Pascanu and Bengio - 2013 - Revisiting Natural Gradient for Deep Networks.pdf},
  journal = {arXiv:1301.3584 [cs]},
  keywords = {Computer Science - Machine Learning,Mathematics - Numerical Analysis},
  language = {en},
  primaryclass = {cs}
}

@article{Pasti1997,
  title = {Intracellular {{Calcium Oscillations}} in {{Astrocytes}}: {{A Highly Plastic}}, {{Bidirectional Form}} of {{Communication}} between {{Neurons}} and {{Astrocytes}} {{{\emph{In Situ}}}}},
  shorttitle = {Intracellular {{Calcium Oscillations}} in {{Astrocytes}}},
  author = {Pasti, Lucia and Volterra, Andrea and Pozzan, Tullio and Carmignoto, Giorgio},
  year = {1997},
  month = oct,
  volume = {17},
  pages = {7817--7830},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.17-20-07817.1997},
  file = {Pasti et al. - 1997 - Intracellular Calcium Oscillations in Astrocytes .pdf},
  journal = {J. Neurosci.},
  language = {en},
  number = {20}
}

@article{Pathak,
  title = {Self-{{Supervised Exploration}} via {{Disagreement}}},
  author = {Pathak, Deepak and Gandhi, Dhiraj and Gupta, Abhinav},
  pages = {10},
  abstract = {Efficient exploration is a long-standing problem in sensorimotor learning. Major advances have been demonstrated in noise-free, non-stochastic domains such as video games and simulation. However, most of these formulations either get stuck in environments with stochastic dynamics or are too inefficient to be scalable to real robotics setups. In this paper, we propose a formulation for exploration inspired by the work in active learning literature. Specifically, we train an ensemble of dynamics models and incentivize the agent to explore such that the disagreement of those ensembles is maximized. This allows the agent to learn skills by exploring in a self-supervised manner without any external reward. Notably, we further leverage the disagreement objective to optimize the agent's policy in a differentiable manner, without using reinforcement learning, which results in a sample-efficient exploration. We demonstrate the efficacy of this formulation across a variety of benchmark environments including stochastic-Atari, Mujoco and Unity. Finally, we implement our differentiable exploration on a real robot which learns to interact with objects completely from scratch. Project videos and code are at https://pathak22.github. io/exploration-by-disagreement/.},
  file = {Pathak et al. - Self-Supervised Exploration via Disagreement 2.pdf},
  language = {en}
}

@inproceedings{Pathak2017,
  title = {Curiosity-{{Driven Exploration}} by {{Self}}-{{Supervised Prediction}}},
  booktitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}} ({{CVPRW}})},
  author = {Pathak, Deepak and Agrawal, Pulkit and Efros, Alexei A. and Darrell, Trevor},
  year = {2017},
  month = jul,
  pages = {488--489},
  publisher = {{IEEE}},
  address = {{Honolulu, HI, USA}},
  doi = {10.1109/CVPRW.2017.70},
  abstract = {In many real-world scenarios, rewards extrinsic to the agent are extremely sparse, or absent altogether. In such cases, curiosity can serve as an intrinsic reward signal to enable the agent to explore its environment and learn skills that might be useful later in its life. We formulate curiosity as the error in an agent's ability to predict the consequence of its own actions in a visual feature space learned by a self-supervised inverse dynamics model. Our formulation scales to high-dimensional continuous state spaces like images, bypasses the difficulties of directly predicting pixels, and, critically, ignores the aspects of the environment that cannot affect the agent. The proposed approach is evaluated in two environments: VizDoom and Super Mario Bros. Three broad settings are investigated: 1) sparse extrinsic reward, where curiosity allows for far fewer interactions with the environment to reach the goal; 2) exploration with no extrinsic reward, where curiosity pushes the agent to explore more efficiently; and 3) generalization to unseen scenarios (e.g. new levels of the same game) where the knowledge gained from earlier experience helps the agent explore new places much faster than starting from scratch.},
  file = {Pathak et al. - 2017 - Curiosity-Driven Exploration by Self-Supervised Pr.pdf},
  isbn = {978-1-5386-0733-6},
  language = {en}
}

@article{Pathak2018,
  title = {Model-{{Free Prediction}} of {{Large Spatiotemporally Chaotic Systems}} from {{Data}}: {{A Reservoir Computing Approach}}},
  shorttitle = {Model-{{Free Prediction}} of {{Large Spatiotemporally Chaotic Systems}} from {{Data}}},
  author = {Pathak, Jaideep and Hunt, Brian and Girvan, Michelle and Lu, Zhixin and Ott, Edward},
  year = {2018},
  month = jan,
  volume = {120},
  issn = {0031-9007, 1079-7114},
  doi = {10.1103/PhysRevLett.120.024102},
  file = {Pathak et al. - 2018 - Model-Free Prediction of Large Spatiotemporally Ch.pdf},
  journal = {Physical Review Letters},
  language = {en},
  number = {2}
}

@article{Pathak2019,
  title = {Self-{{Supervised Exploration}} via {{Disagreement}}},
  author = {Pathak, Deepak and Gandhi, Dhiraj and Gupta, Abhinav},
  year = {2019},
  pages = {10},
  abstract = {Efficient exploration is a long-standing problem in sensorimotor learning. Major advances have been demonstrated in noise-free, non-stochastic domains such as video games and simulation. However, most of these formulations either get stuck in environments with stochastic dynamics or are too inefficient to be scalable to real robotics setups. In this paper, we propose a formulation for exploration inspired by the work in active learning literature. Specifically, we train an ensemble of dynamics models and incentivize the agent to explore such that the disagreement of those ensembles is maximized. This allows the agent to learn skills by exploring in a self-supervised manner without any external reward. Notably, we further leverage the disagreement objective to optimize the agent's policy in a differentiable manner, without using reinforcement learning, which results in a sample-efficient exploration. We demonstrate the efficacy of this formulation across a variety of benchmark environments including stochastic-Atari, Mujoco and Unity. Finally, we implement our differentiable exploration on a real robot which learns to interact with objects completely from scratch. Project videos and code are at https://pathak22.github. io/exploration-by-disagreement/.},
  file = {Pathak et al. - Self-Supervised Exploration via Disagreement.pdf},
  journal = {Proceedings of the 36th International Conference on Machine Learning},
  language = {en}
}

@article{Pavlides2015,
  title = {Computational {{Models Describing Possible Mechanisms}} for {{Generation}} of {{Excessive Beta Oscillations}} in {{Parkinson}}'s {{Disease}}},
  author = {Pavlides, Alex and Hogan, S. John and Bogacz, Rafal},
  editor = {Graham, Lyle J.},
  year = {2015},
  month = dec,
  volume = {11},
  pages = {e1004609},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1004609},
  file = {2015 - Pavlides, Hogan, Bogacz - Computational Models Describing Possible Mechanisms for Generation of Excessive Beta Oscillations in.pdf;Pavlides et al. - 2015 - Computational Models Describing Possible Mechanism.pdf},
  journal = {PLOS Computational Biology},
  language = {en},
  number = {12}
}

@book{Pavon2012,
  title = {Advances in {{Artificial Intelligence}} \textendash{} {{IBERAMIA}} 2012: 13th {{Ibero}}-{{American Conference}} on {{AI}}, {{Cartagena}} de {{Indias}}, {{Colombia}}, {{November}} 13-16, 2012. {{Proceedings}}},
  shorttitle = {Advances in {{Artificial Intelligence}} \textendash{} {{IBERAMIA}} 2012},
  editor = {Pav{\'o}n, Juan and {Duque-M{\'e}ndez}, N{\'e}stor D. and {Fuentes-Fern{\'a}ndez}, Rub{\'e}n and Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard},
  year = {2012},
  volume = {7637},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-34654-5},
  file = {Pavón et al. - 2012 - Advances in Artificial Intelligence – IBERAMIA 201.pdf},
  isbn = {978-3-642-34653-8 978-3-642-34654-5},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@book{Pavon2012a,
  title = {Advances in {{Artificial Intelligence}} \textendash{} {{IBERAMIA}} 2012: 13th {{Ibero}}-{{American Conference}} on {{AI}}, {{Cartagena}} de {{Indias}}, {{Colombia}}, {{November}} 13-16, 2012. {{Proceedings}}},
  shorttitle = {Advances in {{Artificial Intelligence}} \textendash{} {{IBERAMIA}} 2012},
  editor = {Pav{\'o}n, Juan and {Duque-M{\'e}ndez}, N{\'e}stor D. and {Fuentes-Fern{\'a}ndez}, Rub{\'e}n and Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard},
  year = {2012},
  volume = {7637},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-34654-5},
  file = {Pavón et al. - 2012 - Advances in Artificial Intelligence – IBERAMIA 201 2.pdf},
  isbn = {978-3-642-34653-8 978-3-642-34654-5},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@book{Pavon2012b,
  title = {Advances in {{Artificial Intelligence}} \textendash{} {{IBERAMIA}} 2012: 13th {{Ibero}}-{{American Conference}} on {{AI}}, {{Cartagena}} de {{Indias}}, {{Colombia}}, {{November}} 13-16, 2012. {{Proceedings}}},
  shorttitle = {Advances in {{Artificial Intelligence}} \textendash{} {{IBERAMIA}} 2012},
  editor = {Pav{\'o}n, Juan and {Duque-M{\'e}ndez}, N{\'e}stor D. and {Fuentes-Fern{\'a}ndez}, Rub{\'e}n and Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard},
  year = {2012},
  volume = {7637},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-34654-5},
  file = {Pavón et al. - 2012 - Advances in Artificial Intelligence – IBERAMIA 201 3.pdf},
  isbn = {978-3-642-34653-8 978-3-642-34654-5},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{Pearl,
  title = {Theoretical {{Impediments}} to {{Machine Learning}}},
  author = {Pearl, Judea},
  pages = {5},
  abstract = {Current machine learning systems operate, almost exclusively, in a purely statistical mode, which puts severe theoretical limits on their performance. We consider the feasibility of leveraging counterfactual reasoning in machine learning tasks, and to identify areas where such reasoning could lead to major breakthroughs in machine learning applications.},
  file = {Pearl - Theoretical Impediments to Machine Learning.pdf},
  language = {en}
}

@inproceedings{Pearl2018,
  title = {Theoretical {{Impediments}} to {{Machine Learning With Seven Sparks}} from the {{Causal Revolution}}},
  booktitle = {Proceedings of the {{Eleventh ACM International Conference}} on {{Web Search}} and {{Data Mining}}  - {{WSDM}} '18},
  author = {Pearl, Judea},
  year = {2018},
  pages = {3--3},
  publisher = {{ACM Press}},
  address = {{Marina Del Rey, CA, USA}},
  doi = {10.1145/3159652.3176182},
  abstract = {Current machine learning systems operate, almost exclusively, in a statistical, or modelblind mode, which entails severe theoretical limits on their power and performance. Such systems cannot reason about interventions and retrospection and, therefore, cannot serve as the basis for strong AI. To achieve human level intelligence, learning machines need the guidance of a model of reality, similar to the ones used in causal inference. To demonstrate the essential role of such models, I will present a summary of seven tasks which are beyond reach of current machine learning systems and which have been accomplished using the tools of causal inference.},
  file = {Pearl - 2018 - Theoretical Impediments to Machine Learning With S.pdf},
  isbn = {978-1-4503-5581-0},
  language = {en}
}

@techreport{Pebay2008,
  title = {Formulas for Robust, One-Pass Parallel Computation of Covariances and Arbitrary-Order Statistical Moments.},
  author = {Pebay, Philippe Pierre},
  year = {2008},
  month = sep,
  doi = {10.2172/1028931},
  abstract = {We present a formula for the pairwise update of arbitrary-order centered statistical moments. This formula is of particular interest to compute such moments in parallel for large-scale, distributed data sets. As a corollary, we indicate a specialization of this formula for incremental updates, of particular interest to streaming implementations. Finally, we provide pairwise and incremental update formulas for the covariance.},
  file = {2008 - Pébay - Formulas for Robust, One-Pass Parallel Computation of Covariances and Arbitrary-Order Statistical Moments.pdf},
  language = {en},
  number = {SAND2008-6212, 1028931}
}

@techreport{Peles2019,
  title = {Phase-Specific Microstimulation in Brain-Machine Interface Setting Differentially Modulates Beta Oscillations and Affects Behavior},
  author = {Peles, Oren and {Werner-Reiss}, Uri and Bergman, Hagai and Israel, Zvi and Vaadia, Eilon},
  year = {2019},
  month = apr,
  institution = {{Neuroscience}},
  doi = {10.1101/622787},
  abstract = {It is widely accepted that beta-band oscillations play a role in sensorimotor behavior. To further explore this role, we developed a novel hybrid platform to combine operant conditioning and phasespecific intracortical microstimulation (ICMS). We trained monkeys, implanted with 96 electrodes arrays in motor cortex, to volitionally enhance local field potential (LFP) beta-band (20-30Hz) activity at selected sites using a brain-machine interface (BMI). We demonstrate that beta oscillations of LFP and single-unit spiking activity increased dramatically with BMI training, and that pre-movement Beta-power was anti-correlated with task performance. We also show that phase-specific ICMS modulated the power and phase of oscillations, shifting local networks between oscillatory and non-oscillatory states. Furthermore, ICMS induced phase-dependent effects in animal reaction times and success rates. These findings contribute to unraveling of the functional role of cortical oscillations, and to future development of clinical tools for ameliorating abnormal neuronal activities in brain diseases.},
  file = {Peles et al. - 2019 - Phase-specific microstimulation in brain-machine i.pdf},
  language = {en},
  type = {Preprint}
}

@article{Pelz2020,
  title = {The Elaboration of Exploratory Play},
  author = {Pelz, Maddie and Kidd, Celeste},
  year = {2020},
  month = jul,
  volume = {375},
  pages = {20190503},
  issn = {0962-8436, 1471-2970},
  doi = {10.1098/rstb.2019.0503},
  abstract = {We apply a new quantitative method for investigating how children's exploration changes across age in order to gain insight into how exploration unfolds over the course of a human life from a life-history perspective. In this study, different facets of exploratory play were quantified using a novel touchscreen environment across a large sample and wide age range of children in the USA (               n               = 105, ages = 1 year and 10 months to 12 years and 2 months). In contrast with previous theories that have suggested humans transition from more exploratory to less throughout maturation, we see children transition from less broadly exploratory as toddlers to more efficient and broad as adolescents. Our data cast doubt on the picture of human life history as involving a linear transition from more curious in early childhood to less curious with age. Instead, exploration appears to become more elaborate throughout human childhood.                          This article is part of the theme issue `Life history and learning: how childhood, caregiving and old age shape cognition and culture in humans and other animals'.},
  file = {Pelz and Kidd - 2020 - The elaboration of exploratory play.pdf},
  journal = {Phil. Trans. R. Soc. B},
  language = {en},
  number = {1803}
}

@article{Pena2017,
  title = {Particle Swarm Optimization for Programming Deep Brain Stimulation Arrays},
  author = {Pe{\~n}a, Edgar and Zhang, Simeng and Deyo, Steve and Xiao, YiZi and Johnson, Matthew D},
  year = {2017},
  month = feb,
  volume = {14},
  pages = {016014},
  issn = {1741-2560, 1741-2552},
  doi = {10.1088/1741-2552/aa52d1},
  abstract = {Objective. Deep brain stimulation (DBS) therapy relies on both precise neurosurgical targeting and systematic optimization of stimulation settings to achieve beneficial clinical outcomes. One recent advance to improve targeting is the development of DBS arrays (DBSAs) with electrodes segmented both along and around the DBS lead. However, increasing the number of independent electrodes creates the logistical challenge of optimizing stimulation parameters efficiently. Approach. Solving such complex problems with multiple solutions and objectives is well known to occur in biology, in which complex collective behaviors emerge out of swarms of individual organisms engaged in learning through social interactions. Here, we developed a particle swarm optimization (PSO) algorithm to program DBSAs using a swarm of individual particles representing electrode configurations and stimulation amplitudes. Using a finite element model of motor thalamic DBS, we demonstrate how the PSO algorithm can efficiently optimize a multi-objective function that maximizes predictions of axonal activation in regions of interest (ROI, cerebellar-receiving area of motor thalamus), minimizes predictions of axonal activation in regions of avoidance (ROA, somatosensory thalamus), and minimizes power consumption. Main results. The algorithm solved the multi-objective problem by producing a Pareto front. ROI and ROA activation predictions were consistent across swarms ({$<$}1\% median discrepancy in axon activation). The algorithm was able to accommodate for (1) lead displacement (1 mm) with relatively small ROI ({$\leqslant$}9.2\%) and ROA ({$\leqslant$}1\%) activation changes, irrespective of shift direction; (2) reduction in maximum per-electrode current (by 50\% and 80\%) with ROI activation decreasing by 5.6\% and 16\%, respectively; and (3) disabling electrodes (n = 3 and 12) with ROI activation reduction by 1.8\% and 14\%, respectively. Additionally, comparison between PSO predictions and multicompartment axon model simulations showed discrepancies of {$<$}1\% between approaches. Significance. The PSO algorithm provides a computationally efficient way to program DBS systems especially those with higher electrode counts.},
  file = {Peña et al. - 2017 - Particle swarm optimization for programming deep b.pdf},
  journal = {Journal of Neural Engineering},
  language = {en},
  number = {1}
}

@article{Pereira2009,
  title = {Machine Learning Classifiers and {{fMRI}}: {{A}} Tutorial Overview},
  shorttitle = {Machine Learning Classifiers and {{fMRI}}},
  author = {Pereira, Francisco and Mitchell, Tom and Botvinick, Matthew},
  year = {2009},
  month = mar,
  volume = {45},
  pages = {S199-S209},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2008.11.007},
  abstract = {Interpreting brain image experiments requires analysis of complex, multivariate data. In recent years, one analysis approach that has grown in popularity is the use of machine learning algorithms to train classifiers to decode stimuli, mental states, behaviours and other variables of interest from fMRI data and thereby show the data contain information about them. In this tutorial overview we review some of the key choices faced in using this approach as well as how to derive statistically significant results, illustrating each point from a case study. Furthermore, we show how, in addition to answering the question of `is there information about a variable of interest' (pattern discrimination), classifiers can be used to tackle other classes of question, namely `where is the information' (pattern localization) and `how is that information encoded' (pattern characterization).},
  file = {2009 - Pereira, Mitchell, Botvinick - Machine learning classifiers and fMRI a tutorial overview.pdf},
  journal = {NeuroImage},
  language = {en},
  number = {1}
}

@article{Pereira2010,
  title = {Astrocytes and Human Cognition: {{Modeling}} Information Integration and Modulation of Neuronal Activity},
  shorttitle = {Astrocytes and Human Cognition},
  author = {Pereira, Alfredo and Furlan, F{\'a}bio Augusto},
  year = {2010},
  month = nov,
  volume = {92},
  pages = {405--420},
  issn = {03010082},
  doi = {10.1016/j.pneurobio.2010.07.001},
  abstract = {Recent research focusing on the participation of astrocytes in glutamatergic tripartite synapses has revealed mechanisms that support cognitive functions common to human and other mammalian species, such as learning, perception, conscious integration, memory formation/retrieval and the control of voluntary behavior. Astrocytes can modulate neuronal activity by means of release of glutamate, D-serine, adenosine triphosphate and other signaling molecules, contributing to sustain, reinforce or depress pre- and post-synaptic membranes. We review molecular mechanisms present in tripartite synapses and model the cognitive role of astrocytes. Single protoplasmic astrocytes operate as a ``Local Hub'', integrating information patterns from neuronal and glial populations. Two mechanisms, here modeled as the ``domino'' and ``carousel'' effects, contribute to the formation of intercellular calcium waves. As waves propagate through gap junctions and reach other types of astrocytes (interlaminar, polarized, fibrous and varicose projection), the active astroglial network functions as a ``Master Hub'' that integrates results of distributed processing from several brain areas and supports conscious states. Response of this network would define the effect exerted on neuronal plasticity (membrane potentiation or depression), behavior and psychosomatic processes. Theoretical results of our modeling can contribute to the development of new experimental research programs to test cognitive functions of astrocytes.},
  file = {Pereira and Furlan - 2010 - Astrocytes and human cognition Modeling informati.pdf},
  journal = {Progress in Neurobiology},
  language = {en},
  number = {3}
}

@article{Pereira2010a,
  title = {Astrocytes and Human Cognition: {{Modeling}} Information Integration and Modulation of Neuronal Activity},
  shorttitle = {Astrocytes and Human Cognition},
  author = {Pereira, Alfredo and Furlan, F{\'a}bio Augusto},
  year = {2010},
  month = nov,
  volume = {92},
  pages = {405--420},
  issn = {03010082},
  doi = {10.1016/j.pneurobio.2010.07.001},
  abstract = {Recent research focusing on the participation of astrocytes in glutamatergic tripartite synapses has revealed mechanisms that support cognitive functions common to human and other mammalian species, such as learning, perception, conscious integration, memory formation/retrieval and the control of voluntary behavior. Astrocytes can modulate neuronal activity by means of release of glutamate, D-serine, adenosine triphosphate and other signaling molecules, contributing to sustain, reinforce or depress pre- and post-synaptic membranes. We review molecular mechanisms present in tripartite synapses and model the cognitive role of astrocytes. Single protoplasmic astrocytes operate as a ``Local Hub'', integrating information patterns from neuronal and glial populations. Two mechanisms, here modeled as the ``domino'' and ``carousel'' effects, contribute to the formation of intercellular calcium waves. As waves propagate through gap junctions and reach other types of astrocytes (interlaminar, polarized, fibrous and varicose projection), the active astroglial network functions as a ``Master Hub'' that integrates results of distributed processing from several brain areas and supports conscious states. Response of this network would define the effect exerted on neuronal plasticity (membrane potentiation or depression), behavior and psychosomatic processes. Theoretical results of our modeling can contribute to the development of new experimental research programs to test cognitive functions of astrocytes.},
  file = {Pereira and Furlan - 2010 - Astrocytes and human cognition Modeling informati 2.pdf},
  journal = {Progress in Neurobiology},
  language = {en},
  number = {3}
}

@article{Pereira2010b,
  title = {Astrocytes and Human Cognition: {{Modeling}} Information Integration and Modulation of Neuronal Activity},
  shorttitle = {Astrocytes and Human Cognition},
  author = {Pereira, Alfredo and Furlan, F{\'a}bio Augusto},
  year = {2010},
  month = nov,
  volume = {92},
  pages = {405--420},
  issn = {03010082},
  doi = {10.1016/j.pneurobio.2010.07.001},
  abstract = {Recent research focusing on the participation of astrocytes in glutamatergic tripartite synapses has revealed mechanisms that support cognitive functions common to human and other mammalian species, such as learning, perception, conscious integration, memory formation/retrieval and the control of voluntary behavior. Astrocytes can modulate neuronal activity by means of release of glutamate, D-serine, adenosine triphosphate and other signaling molecules, contributing to sustain, reinforce or depress pre- and post-synaptic membranes. We review molecular mechanisms present in tripartite synapses and model the cognitive role of astrocytes. Single protoplasmic astrocytes operate as a ``Local Hub'', integrating information patterns from neuronal and glial populations. Two mechanisms, here modeled as the ``domino'' and ``carousel'' effects, contribute to the formation of intercellular calcium waves. As waves propagate through gap junctions and reach other types of astrocytes (interlaminar, polarized, fibrous and varicose projection), the active astroglial network functions as a ``Master Hub'' that integrates results of distributed processing from several brain areas and supports conscious states. Response of this network would define the effect exerted on neuronal plasticity (membrane potentiation or depression), behavior and psychosomatic processes. Theoretical results of our modeling can contribute to the development of new experimental research programs to test cognitive functions of astrocytes.},
  file = {../../Zotero/storage/TVSA6HVX/Pereira and Furlan - 2010 - Astrocytes and human cognition Modeling informati.pdf},
  journal = {Progress in Neurobiology},
  language = {en},
  number = {3}
}

@article{Pereira2011,
  title = {Information Mapping with Pattern Classifiers: {{A}} Comparative Study},
  shorttitle = {Information Mapping with Pattern Classifiers},
  author = {Pereira, Francisco and Botvinick, Matthew},
  year = {2011},
  month = may,
  volume = {56},
  pages = {476--496},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2010.05.026},
  abstract = {Information mapping using pattern classifiers has become increasingly popular in recent years, although without a clear consensus on which classifier(s) ought to be used or how results should be tested. This paper addresses each of these questions, both analytically and through comparative analyses on five empirical datasets. We also describe how information maps in multiple class situations can provide information concerning the content of neural representations. Finally, we introduce a publically available software toolbox designed specifically for information mapping.},
  file = {2011 - Pereira, Botvinick - Information mapping with pattern classifiers a comparative study.pdf},
  journal = {NeuroImage},
  language = {en},
  number = {2}
}

@article{Perlstein2001,
  title = {Relation of {{Prefrontal Cortex Dysfunction}} to {{Working Memory}} and {{Symptoms}} in {{Schizophrenia}}},
  author = {Perlstein, William M. and Carter, Cameron S. and Noll, Douglas C. and Cohen, Jonathan D.},
  year = {2001},
  month = jul,
  volume = {158},
  pages = {1105--1113},
  issn = {0002-953X, 1535-7228},
  doi = {10.1176/appi.ajp.158.7.1105},
  file = {2001 - Perlstein et al. - Relation of Prefrontal Cortex Dysfunction to Working Memory and Symptoms in Schizophrenia.pdf},
  journal = {American Journal of Psychiatry},
  language = {en},
  number = {7}
}

@article{Peron2013,
  title = {Subthalamic Nucleus: {{A}} Key Structure for Emotional Component Synchronization in Humans},
  shorttitle = {Subthalamic Nucleus},
  author = {P{\'e}ron, Julie and Fr{\"u}hholz, Sascha and V{\'e}rin, Marc and Grandjean, Didier},
  year = {2013},
  month = mar,
  volume = {37},
  pages = {358--373},
  issn = {01497634},
  doi = {10.1016/j.neubiorev.2013.01.001},
  abstract = {Affective neuroscience is concerned with identifying the neural bases of emotion. For historical and methodological reasons, models describing the brain architecture that supports emotional processes in humans have tended to neglect the basal ganglia, focusing instead on cortical and amygdalar mechanisms. Now, however, deep brain stimulation (DBS) of the subthalamic nucleus (STN), a neurosurgical treatment for Parkinson's disease and obsessive\textendash compulsive disorder, is helping researchers explore the possible functional role of this particular basal ganglion in emotional processes. After reviewing studies that have used DBS in this way, we propose a model in which the STN plays a crucial role in producing temporally organized neural co-activation patterns at the cortical and subcortical levels that are essential for generating emotions and related feelings.},
  file = {2013 - Péron et al. - Subthalamic nucleus A key structure for emotional component synchronization in humans.pdf},
  journal = {Neuroscience \& Biobehavioral Reviews},
  language = {en},
  number = {3}
}

@article{Pessoa2006,
  title = {Decoding {{Near}}-{{Threshold Perception}} of {{Fear}} from {{Distributed Single}}-{{Trial Brain Activation}}},
  author = {Pessoa, L. and Padmala, S.},
  year = {2006},
  month = mar,
  volume = {17},
  pages = {691--701},
  issn = {1047-3211, 1460-2199},
  doi = {10.1093/cercor/bhk020},
  abstract = {Instead of contrasting functional magnetic resonance imaging (fMRI) signals associated with 2 conditions, as customarily done in neuroimaging, we reversed the direction of analysis and probed whether brain signals could be used to ``predict'' perceptual states. We probed the neural correlates of perceptual decisions by ``decoding'' brain states during near-threshold fear detection. Decoding was attempted by using support vector machines and other related techniques. Although previous decoding studies have employed relatively ``blocked'' data, our objective was to probe how the ``moment-to-moment'' fluctuation in fMRI signals across a population of voxels reflected the participant's perceptual decision. Accuracy increased from when 1 region was considered (\textasciitilde 64\%) to when 10 regions were used (\textasciitilde 78\%). When the best classifications per subject were averaged, accuracy levels ranged between 74\% and 86\% correct. An information theoretic analysis revealed that the information carried by pairs of regions reliably exceeded the sum of the information carried by individual regions, suggesting that information was combined ``synergistically'' across regions. Our results indicate that the representation of behavioral choice is ``distributed'' across several brain regions. Such distributed encoding may help prepare the organism to appropriately handle emotional stimuli and regulate the associated emotional response upon the conscious decision that a fearful face is present. In addition, the results show that challenging brain states can be decoded with high accuracy even when ``single-trial'' data are employed and suggest that multivariate analysis strategies have considerable potential in helping to elucidate the neural correlates of visual awareness and the encoding of perceptual decisions.},
  file = {2007 - Pessoa, Padmala - Decoding near-threshold perception of fear from distributed single-trial brain activation.pdf},
  journal = {Cerebral Cortex},
  language = {en},
  number = {3}
}

@article{Peters2011,
  title = {Optimal Leverage from Non-Ergodicity},
  author = {Peters, Ole},
  year = {2011},
  month = nov,
  volume = {11},
  pages = {1593--1602},
  issn = {1469-7688, 1469-7696},
  doi = {10.1080/14697688.2010.513338},
  file = {Peters - 2011 - Optimal leverage from non-ergodicity.pdf},
  journal = {Quantitative Finance},
  language = {en},
  number = {11}
}

@article{Peters2013,
  title = {Ergodicity Breaking in Geometric {{Brownian}} Motion},
  author = {Peters, Ole and Klein, William},
  year = {2013},
  month = mar,
  volume = {110},
  pages = {100603},
  issn = {0031-9007, 1079-7114},
  doi = {10.1103/PhysRevLett.110.100603},
  abstract = {Geometric Brownian motion (GBM) is a model for systems as varied as financial instruments and populations. The statistical properties of GBM are complicated by non-ergodicity, which can lead to ensemble averages exhibiting exponential growth while any individual trajectory collapses according to its time-average. A common tactic for bringing time averages closer to ensemble averages is diversification. In this letter we study the effects of diversification using the concept of ergodicity breaking.},
  archiveprefix = {arXiv},
  eprint = {1209.4517},
  eprinttype = {arxiv},
  file = {Peters and Klein - 2013 - Ergodicity breaking in geometric Brownian motion.pdf},
  journal = {Phys. Rev. Lett.},
  keywords = {Mathematical Physics,Quantitative Finance - Risk Management},
  language = {en},
  number = {10}
}

@article{Peters2016,
  title = {Evaluating Gambles Using Dynamics},
  author = {Peters, O. and {Gell-Mann}, M.},
  year = {2016},
  month = feb,
  volume = {26},
  pages = {023103},
  issn = {1054-1500, 1089-7682},
  doi = {10.1063/1.4940236},
  file = {Peters and Gell-Mann - 2016 - Evaluating gambles using dynamics.pdf},
  journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
  language = {en},
  number = {2}
}

@article{Peters2016a,
  title = {Evaluating Gambles Using Dynamics},
  author = {Peters, O. and {Gell-Mann}, M.},
  year = {2016},
  month = feb,
  volume = {26},
  pages = {023103},
  issn = {1054-1500, 1089-7682},
  doi = {10.1063/1.4940236},
  file = {Peters and Gell-Mann - 2016 - Evaluating gambles using dynamics 3.pdf},
  journal = {Chaos},
  language = {en},
  number = {2}
}

@article{Peters2018,
  title = {An Evolutionary Advantage of Cooperation},
  author = {Peters, Ole and Adamou, Alexander},
  year = {2018},
  month = may,
  abstract = {Cooperation is a persistent behavioral pattern of entities pooling and sharing resources. Its ubiquity in nature poses a conundrum. Whenever two entities cooperate, one must willingly relinquish something of value to the other. Why is this apparent altruism favored in evolution? Classical solutions assume a net fitness gain in a cooperative transaction which, through reciprocity or relatedness, finds its way back from recipient to donor. We seek the source of this fitness gain. Our analysis rests on the insight that evolutionary processes are typically multiplicative and noisy. Fluctuations have a net negative effect on the long-time growth rate of resources but no effect on the growth rate of their expectation value. This is an example of nonergodicity. By reducing the amplitude of fluctuations, pooling and sharing increases the long-time growth rate for cooperating entities, meaning that cooperators outgrow similar non-cooperators. We identify this increase in growth rate as the net fitness gain, consistent with the concept of geometric mean fitness in the biological literature. This constitutes a fundamental mechanism for the evolution of cooperation. Its minimal assumptions make it a candidate explanation of cooperation in settings too simple for other fitness gains, such as emergent function and specialization, to be probable. One such example is the transition from single cells to early multicellular life.},
  archiveprefix = {arXiv},
  eprint = {1506.03414},
  eprinttype = {arxiv},
  file = {Peters and Adamou - 2018 - An evolutionary advantage of cooperation.pdf},
  journal = {arXiv:1506.03414 [nlin, q-bio, q-fin]},
  keywords = {Nonlinear Sciences - Adaptation and Self-Organizing Systems,Quantitative Biology - Populations and Evolution,Quantitative Finance - General Finance},
  language = {en},
  primaryclass = {nlin, q-bio, q-fin}
}

@article{Peters2019,
  title = {The Ergodicity Problem in Economics},
  author = {Peters, Ole},
  year = {2019},
  month = dec,
  volume = {15},
  pages = {1216--1221},
  issn = {1745-2473, 1745-2481},
  doi = {10.1038/s41567-019-0732-0},
  file = {Peters - 2019 - The ergodicity problem in economics.pdf},
  journal = {Nat. Phys.},
  language = {en},
  number = {12}
}

@article{Peters2019a,
  title = {Striatal Activity Reflects Cortical Activity Patterns},
  author = {Peters, Andrew J and Steinmetz, Nicholas A and Harris, Kenneth D and Carandini, Matteo},
  year = {2019},
  month = jul,
  doi = {10.1101/703710},
  abstract = {The dorsal striatum is organized into domains that drive characteristic behaviors             1\textendash 7             , and receive inputs from different parts of the cortex             8,9             which modulate similar behaviors             10\textendash 12             . Striatal responses to cortical inputs, however, can be affected by changes in connection strength             13\textendash 15             , local striatal circuitry             16,17             , and thalamic inputs             18,19             . Therefore, it is unclear whether the pattern of activity across striatal domains mirrors that across the cortex             20\textendash 23             or differs from it             24\textendash 28             . Here we use simultaneous large-scale recordings in the cortex and the striatum to show that striatal activity can be accurately predicted by spatiotemporal activity patterns in the cortex. The relationship between activity in the cortex and the striatum was spatially consistent with corticostriatal anatomy, and temporally consistent with a feedforward drive. Each striatal domain exhibited specific sensorimotor responses that predictably followed activity in the associated cortical regions, and the corticostriatal relationship remained unvaried during passive states or performance of a task probing visually guided behavior. However, the task's visual stimuli and corresponding behavioral responses evoked relatively more activity in the striatum than in associated cortical regions. This increased striatal activity involved an additive offset in firing rate, which was independent of task engagement but only present in animals that had learned the task. Thus, striatal activity largely reflects patterns of cortical activity, deviating from them in a simple additive fashion for learned stimuli or actions.},
  file = {Peters et al. - 2019 - Striatal activity reflects cortical activity patte.pdf},
  journal = {bioRxiv},
  language = {en}
}

@article{Peters2021,
  title = {Striatal Activity Topographically Reflects Cortical Activity},
  author = {Peters, Andrew J. and Fabre, Julie M. J. and Steinmetz, Nicholas A. and Harris, Kenneth D. and Carandini, Matteo},
  year = {2021},
  month = jan,
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/s41586-020-03166-8},
  file = {Peters et al. - 2021 - Striatal activity topographically reflects cortica.pdf},
  journal = {Nature},
  language = {en}
}

@article{Peterson,
  title = {Random-Search-Wired-into-Animals-May-Help-Them-Hunt-20200611},
  author = {Peterson, Erik},
  pages = {16},
  file = {Peterson - random-search-wired-into-animals-may-help-them-hun.pdf},
  language = {en}
}

@article{Peterson2015,
  title = {Balanced {{Oscillatory Coupling Improves Information Flow}}},
  author = {Peterson, Erik J and Voytek, Bradley},
  year = {2015},
  month = oct,
  doi = {10.1101/030304},
  abstract = {All animals are able to rapidly change their behavior. The neural basis of such flexibility requires that groups of distant neural ensembles rapidly alter communications with selectivity and fidelity. Low frequency oscillations are a strong candidate for how neurons coordinate communication via the dynamic instantiation of functional networks. These dynamic networks are argued to rapidly guide the flow of information, with the presumption that stronger oscillations more strongly influence information flow. Surprisingly, there is scant evidence or theoretical support for how oscillatory activity might enhance information flow. Here we introduce a novel computational model for oscillatory neural communication and show that, rather than the strength of the oscillation, it is the balance between excitatory and inhibitory neuronal activity that has the largest effect on information flow. When coupling between an oscillation and spiking has balanced excitatory-inhibitory inputs, information flow is enhanced via improved discriminability between signal and noise. In contrast, when coupling is unbalanced, driven either by excessive excitation or inhibition, information flow is obstructed, regardless of the strength of the oscillation. A multitude of neuropathologies, including Parkinson's disease, schizophrenia, and autism, are associated with oscillatory disruptions and excitation-inhibition imbalances. Our results show that understanding the distinction between balanced and unbalanced oscillatory coupling offers a unifying mechanistic framework for understanding effective neural communication and its disruption in neuropathology.},
  file = {2015 - Peterson, Voytek - Balanced Oscillatory Coupling Improves Information Flow.pdf;Peterson and Voytek - 2015 - Balanced Oscillatory Coupling Improves Information.pdf},
  journal = {bioRxiv},
  language = {en}
}

@techreport{Peterson2015a,
  title = {Balanced {{Oscillatory Coupling Improves Information Flow}}},
  author = {Peterson, Erik J and Voytek, Bradley},
  year = {2015},
  month = oct,
  institution = {{Neuroscience}},
  doi = {10.1101/030304},
  abstract = {All animals are able to rapidly change their behavior. The neural basis of such flexibility requires that groups of distant neural ensembles rapidly alter communications with selectivity and fidelity. Low frequency oscillations are a strong candidate for how neurons coordinate communication via the dynamic instantiation of functional networks. These dynamic networks are argued to rapidly guide the flow of information, with the presumption that stronger oscillations more strongly influence information flow. Surprisingly, there is scant evidence or theoretical support for how oscillatory activity might enhance information flow. Here we introduce a novel computational model for oscillatory neural communication and show that, rather than the strength of the oscillation, it is the balance between excitatory and inhibitory neuronal activity that has the largest effect on information flow. When coupling between an oscillation and spiking has balanced excitatory-inhibitory inputs, information flow is enhanced via improved discriminability between signal and noise. In contrast, when coupling is unbalanced, driven either by excessive excitation or inhibition, information flow is obstructed, regardless of the strength of the oscillation. A multitude of neuropathologies, including Parkinson's disease, schizophrenia, and autism, are associated with oscillatory disruptions and excitation-inhibition imbalances. Our results show that understanding the distinction between balanced and unbalanced oscillatory coupling offers a unifying mechanistic framework for understanding effective neural communication and its disruption in neuropathology.},
  file = {Peterson and Voytek - 2015 - Balanced Oscillatory Coupling Improves Information 2.pdf},
  language = {en},
  type = {Preprint}
}

@article{Peterson2017,
  title = {Alpha Oscillations Control Cortical Gain by Modulating Excitatory-Inhibitory Background Activity.},
  author = {Peterson, Erik J. and Voytek, Bradley},
  year = {2017},
  month = sep,
  volume = {185074},
  doi = {10.1101/185074},
  abstract = {The first recordings of human brain activity in 1929 revealed a striking 8-12 Hz oscillation in the visual cortex. During the intervening 90 years, these alpha oscillations have been linked to numerous physiological and cognitive processes. However, because of the vast and seemingly contradictory cognitive and physiological processes to which it has been related, the physiological function of alpha remains unclear. We identify a novel neural circuit mechanism\textemdash the modulation of both excitatory and inhibitory neurons in a balanced configuration\textemdash by which alpha can modulate gain. We find that this model naturally unifies the prior, highly diverse reports on alpha dynamics, while making the novel prediction that alpha rhythms have two functional roles: a sustained high-power mode that suppresses cortical gain and a weak, bursting mode that enhances gain.},
  file = {Peterson and Voytek - 2017 - Alpha oscillations control cortical gain by modula.pdf},
  journal = {bioRxiv},
  language = {en}
}

@techreport{Peterson2017a,
  title = {1/ {\emph{f}} Neural Noise Is a Better Predictor of Schizophrenia than Neural Oscillations},
  author = {Peterson, Erik J. and Rosen, Burke Q. and Campbell, Alana M. and Belger, Aysenil and Voytek, Bradley},
  year = {2017},
  month = mar,
  institution = {{Neuroscience}},
  doi = {10.1101/113449},
  abstract = {Schizophrenia has been associated with separate irregularities in several neural oscillatory frequency bands, including theta, alpha, and gamma. Our multivariate classification of human EEG suggests that instead of irregularities in many frequency bands, schizophrenia-related electrophysiological differences may better be explained by an overall shift in neural noise, reflected by a change in the 1/f slope of the power spectrum.},
  file = {Peterson et al. - 2017 - 1 ifi neural noise is a better predictor of .pdf},
  language = {en},
  type = {Preprint}
}

@article{Peterson2018,
  title = {Keep It Stupid Simple},
  author = {Peterson, Erik J. and M{\"u}yesser, Necati Alp and Verstynen, Timothy and Dunovan, Kyle},
  year = {2018},
  month = sep,
  abstract = {Deep reinforcement learning can match and exceed human performance, but if even minor changes are introduced to the environment artificial networks often can't adapt. Humans meanwhile are quite adaptable. We hypothesize that this is partly because of how humans use heuristics, and partly because humans can imagine new and more challenging environments to learn from. We've developed a model of hierarchical reinforcement learning that combines both these elements into a stumbler-strategist network. We test transfer performance of this network using Wythoff's game, a gridworld environment with a known optimal strategy. We show that combining imagined play with a heuristic\textendash labeling each position as ``good'' or ``bad''\textendash both accelerates learning and promotes transfer to novel games, while also improving model interpretability.},
  archiveprefix = {arXiv},
  eprint = {1809.03406},
  eprinttype = {arxiv},
  file = {Peterson et al. - 2018 - Keep it stupid simple.pdf},
  journal = {arXiv:1809.03406 [cs]},
  keywords = {Computer Science - Artificial Intelligence},
  language = {en},
  primaryclass = {cs}
}

@techreport{Peterson2018a,
  title = {Healthy Oscillatory Coordination Is Bounded by Single-Unit Computation.},
  author = {Peterson, Erik J and Voytek, Bradley},
  year = {2018},
  month = apr,
  institution = {{Neuroscience}},
  doi = {10.1101/309427},
  abstract = {Oscillations can improve neural coding by grouping action potentials into synchronous windows of activity, but this same effect can harm coding when action potentials become over-synchronized. Diseases ranging from Parkinson's to epilepsy suggest that over-synchronization leads to pathology, but the precise boundary separating healthy from pathological synchrony remains an open theoretical problem. Here we study a simple model that shows how error in individual cells' computations is traded for population-level synchronization. To put the in biological terms accessible to the cell we conceive of a ''voltage budget'' where instantaneous moments of membrane voltage can be partitioned into oscillatory and computational terms. By comparing these budget terms we derive a new set of biologically measurable inequalities that separate healthy from pathological synchrony. Finally, we derive an optimal non-biological algorithm for exchanging computational error with population synchrony.},
  file = {Peterson and Voytek - 2018 - Healthy oscillatory coordination is bounded by sin.pdf},
  language = {en},
  type = {Preprint}
}

@techreport{Peterson2018b,
  title = {Healthy Oscillatory Coordination Is Bounded by Single-Unit Computation},
  author = {Peterson, Erik J and Voytek, Bradley},
  year = {2018},
  month = apr,
  institution = {{Neuroscience}},
  doi = {10.1101/309427},
  abstract = {Oscillations can improve neural coding by grouping action potentials into synchronous windows of activity, but this same effect can harm coding when action potentials become over-synchronized. Diseases ranging from Parkinson's to epilepsy suggest that oversynchronization leads to pathology, but the precise boundary separating healthy from pathological synchrony remains an open theoretical problem. Here we study a simple model that shows how error in individual cells' computations is traded for population-level synchronization. To put the in biological terms accessible to the cell we conceive of a ``voltage budget'' where instantaneous moments of membrane voltage can be partitioned into oscillatory and computational terms. By comparing these budget terms we derive a new set of biologically measurable inequalities that bound healthy from pathological synchrony. Finally, we derive an optimal non-biological algorithm for exchanging computational error with population synchrony.},
  file = {Peterson and Voytek - 2018 - Healthy oscillatory coordination is bounded by sin 2.pdf},
  language = {en},
  type = {Preprint}
}

@techreport{Peterson2019b,
  title = {Homeostatic Mechanisms May Shape the Type and Duration of Oscillatory Modulation},
  author = {Peterson, Erik J. and Voytek, Bradley},
  year = {2019},
  month = apr,
  institution = {{Neuroscience}},
  doi = {10.1101/615450},
  abstract = {Neural oscillations are observed ubiquitously in the mammalian brain. However the stability of oscillations is highly variable. Some oscillations are tonic, lasting for seconds or even minutes; others are unstable, appearing only as a single-cycle burst. In a model of hippocampal neurons, we use numerical simulations to show how these different forms of rhythm stability can interact with activity-dependent homeostasis to profoundly alter the modulatory effect of neural oscillations. Under homeostasis, tonic oscillations that are synaptically excitatory have a paradoxical effect; they decrease excitability and desynchronizing firing. Tonic oscillations that are synaptically inhibitory\textendash like those in a real hippocampus\textendash fail to generate new action potentials and so provoke no homeostatic response. This may explain why the theta rhythm in hippocampus is synaptically inhibitory: inhibitory oscillations don't raise the firing threshold, as excitatory oscillations do, and so can preserve each cell's dynamic range. Based on these simulations, we also speculate that homeostasis may explain why excitatory intra-cortical and intra-layer oscillations often appear as bursts. In our model bursts minimally interact with the slow homeostasis time constant and so retain typical excitatory effects.},
  file = {Peterson and Voytek - 2019 - Homeostatic mechanisms may shape the type and dura.pdf},
  language = {en},
  type = {Preprint}
}

@techreport{Peterson2019c,
  title = {A Way around the Exploration-Exploitation Dilemma},
  author = {Peterson, Erik J and Verstynen, Timothy D},
  year = {2019},
  month = jun,
  institution = {{Animal Behavior and Cognition}},
  doi = {10.1101/671362},
  abstract = {For all animals the decision to explore comes with a risk of getting less. For example, a foraging bee might find less nectar, or hunting hawk less prey. This loss is often formalized as regret. It's been mathematically proven that exploring an uncertain world with a specific goal always has some regret. This is why exploration-exploitation can be a dilemma. Given this proof we wondered if the common advice to ``focus on learning and not the goal'' might have mathematical merit. So we re-imagined exploration in the dilemma as an open ended search for any new information. We then developed a new minimal description of information value, which generalizes existing ideas like curiosity, novelty and information gain. We use this description to model the dilemma as a competition between strategies that maximize reward and information independently. Here we prove this competition has a no regret solution. When we study this solution in simulation \textendash{} using classic bandit tasks \textendash{} it outperforms standard approaches, especially when rewards are sparse.},
  file = {Peterson and Verstynen - 2019 - A way around the exploration-exploitation dilemma 2.pdf},
  language = {en},
  type = {Preprint}
}

@article{Peterson2020,
  title = {Homeostatic Mechanisms May Shape the Type and Duration of Oscillatory Modulation},
  author = {Peterson, Erik J and Voytek, Bradley},
  year = {2020},
  volume = {124},
  pages = {168--177},
  file = {Peterson and Voytek - Homeostatic mechanisms may shape the type and dura.pdf},
  journal = {Journal of Neurophysiology},
  language = {en},
  number = {1}
}

@article{Petrovskii2009,
  title = {Dispersal in a {{Statistically Structured Population}}: {{Fat Tails Revisited}}},
  shorttitle = {Dispersal in a {{Statistically Structured Population}}},
  author = {Petrovskii, Sergei and Morozov, Andrew},
  year = {2009},
  month = feb,
  volume = {173},
  pages = {278--289},
  issn = {0003-0147, 1537-5323},
  doi = {10.1086/595755},
  abstract = {Dispersal has long been recognized as a crucial factor affecting population dynamics. Several studies on long-distance dispersal revealed a peculiarity now widely known as a problem of ``fat tail'': instead of the rate of decay in the population density over large distances being described by a normal distribution, which is apparently predicted by the standard diffusion approach, field data often show much lower rates such as exponential or power law. The question as to what are the processes and mechanisms resulting in the fat tail is still largely open. In this note, by introducing the concept of a statistically structured population, we show that a fat-tailed longdistance dispersal is a consequence of the fundamental observation that individuals of the same species are not identical. Fat-tailed dispersal thus appears to be an inherent property of any real population. We show that our theoretical predictions are in good agreement with available data.},
  file = {Petrovskii and Morozov - 2009 - Dispersal in a Statistically Structured Population.pdf},
  journal = {The American Naturalist},
  language = {en},
  number = {2}
}

@article{Petrovskii2011,
  title = {Variation in Individual Walking Behavior Creates the Impression of a {{Levy}} Flight},
  author = {Petrovskii, S. and Mashanova, A. and Jansen, V. A. A.},
  year = {2011},
  month = may,
  volume = {108},
  pages = {8704--8707},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1015208108},
  file = {Petrovskii et al. - 2011 - Variation in individual walking behavior creates t.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {21}
}

@article{Peyre2020,
  title = {Computational {{Optimal Transport}}},
  author = {Peyr{\'e}, Gabriel and Cuturi, Marco},
  year = {2020},
  month = mar,
  abstract = {Optimal transport (OT) theory can be informally described using the words of the French mathematician Gaspard Monge (1746-1818): A worker with a shovel in hand has to move a large pile of sand lying on a construction site. The goal of the worker is to erect with all that sand a target pile with a prescribed shape (for example, that of a giant sand castle). Naturally, the worker wishes to minimize her total effort, quantified for instance as the total distance or time spent carrying shovelfuls of sand. Mathematicians interested in OT cast that problem as that of comparing two probability distributions, two different piles of sand of the same volume. They consider all of the many possible ways to morph, transport or reshape the first pile into the second, and associate a "global" cost to every such transport, using the "local" consideration of how much it costs to move a grain of sand from one place to another. Recent years have witnessed the spread of OT in several fields, thanks to the emergence of approximate solvers that can scale to sizes and dimensions that are relevant to data sciences. Thanks to this newfound scalability, OT is being increasingly used to unlock various problems in imaging sciences (such as color or texture processing), computer vision and graphics (for shape manipulation) or machine learning (for regression, classification and density fitting). This short book reviews OT with a bias toward numerical methods and their applications in data sciences, and sheds lights on the theoretical properties of OT that make it particularly useful for some of these applications.},
  archiveprefix = {arXiv},
  eprint = {1803.00567},
  eprinttype = {arxiv},
  file = {Peyré and Cuturi - 2020 - Computational Optimal Transport.pdf},
  journal = {arXiv:1803.00567 [stat]},
  keywords = {Statistics - Machine Learning},
  language = {en},
  primaryclass = {stat}
}

@article{Pfurtscheller1996,
  title = {Event-Related Synchronization ({{ERS}}) in the Alpha Band \textemdash{} an Electrophysiological Correlate of Cortical Idling: {{A}} Review},
  shorttitle = {Event-Related Synchronization ({{ERS}}) in the Alpha Band \textemdash{} an Electrophysiological Correlate of Cortical Idling},
  author = {Pfurtscheller, G. and Stanc{\'a}k, A. and Neuper, Ch.},
  year = {1996},
  month = nov,
  volume = {24},
  pages = {39--46},
  issn = {01678760},
  doi = {10.1016/S0167-8760(96)00066-9},
  abstract = {EEG desynchronization is a reliable correlate of excited neural structures or activated cortical areas. EEG synchronization within the alpha band may be an electrophysiological correlate of deactivated cortical areas. Such areas are not processing sensory information or motor output and can be considered to be in an idling state. One example of such an idling cortical area is the enhancement of mu rhythms in the primary hand area during visual processing or during foot movement. In both circumstances, the neurons in the hand area are not needed for visual processing or preparation for foot movement. As a result of this, an enhanced hand area mu rhythm can be observed.},
  file = {1996 - Pfurtscheller, Stancák, Neuper - Event-related synchronization (ERS) in the alpha band - An electrophysiological correlate of co.pdf},
  journal = {International Journal of Psychophysiology},
  language = {en},
  number = {1-2}
}

@article{Phillips2003,
  title = {Convergence of Biological and Psychological Perspectives on Cognitive Coordination in Schizophrenia},
  author = {Phillips, William A. and Silverstein, Steven M.},
  year = {2003},
  month = feb,
  volume = {26},
  issn = {0140-525X, 1469-1825},
  doi = {10.1017/S0140525X03000025},
  abstract = {The concept of locally specialized functions dominates research on higher brain function and its disorders. Locally specialized functions must be complemented by processes that coordinate those functions, however, and impairment of coordinating processes may be central to some psychotic conditions. Evidence for processes that coordinate activity is provided by neurobiological and psychological studies of contextual disambiguation and dynamic grouping. Mechanisms by which this important class of cognitive functions could be achieved include those long-range connections within and between cortical regions that activate synaptic channels via NMDAreceptors, and which control gain through their voltage-dependent mode of operation. An impairment of these mechanisms is central to PCP-psychosis, and the cognitive capabilities that they could provide are impaired in some forms of schizophrenia. We conclude that impaired cognitive coordination due to reduced ion flow through NMDA-channels is involved in schizophrenia, and we suggest that it may also be involved in other disorders. This perspective suggests several ways in which further research could enhance our understanding of cognitive coordination, its neural basis, and its relevance to psychopathology.},
  file = {2003 - Phillips, Silverstein - Convergence of biological and psychological perspectives on cognitive coordination in schizophrenia.pdf},
  journal = {Behavioral and Brain Sciences},
  language = {en},
  number = {01}
}

@article{Phillips2018,
  title = {Face Recognition Accuracy of Forensic Examiners, Superrecognizers, and Face Recognition Algorithms},
  author = {Phillips, P. Jonathon and Yates, Amy N. and Hu, Ying and Hahn, Carina A. and Noyes, Eilidh and Jackson, Kelsey and Cavazos, Jacqueline G. and Jeckeln, G{\'e}raldine and Ranjan, Rajeev and Sankaranarayanan, Swami and Chen, Jun-Cheng and Castillo, Carlos D. and Chellappa, Rama and White, David and O'Toole, Alice J.},
  year = {2018},
  month = jun,
  volume = {115},
  pages = {6171--6176},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1721355115},
  file = {Phillips et al. - 2018 - Face recognition accuracy of forensic examiners, s.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {24}
}

@article{Piantadosi2012,
  title = {The Communicative Function of Ambiguity in Language},
  author = {Piantadosi, Steven T. and Tily, Harry and Gibson, Edward},
  year = {2012},
  month = mar,
  volume = {122},
  pages = {280--291},
  issn = {00100277},
  doi = {10.1016/j.cognition.2011.10.004},
  abstract = {We present a general information-theoretic argument that all efficient communication systems will be ambiguous, assuming that context is informative about meaning. We also argue that ambiguity allows for greater ease of processing by permitting efficient linguistic units to be re-used. We test predictions of this theory in English, German, and Dutch. Our results and theoretical analysis suggest that ambiguity is a functional property of language that allows for greater communicative efficiency. This provides theoretical and empirical arguments against recent suggestions that core features of linguistic systems are not designed for communication.},
  file = {Piantadosi et al. - 2012 - The communicative function of ambiguity in languag.pdf},
  journal = {Cognition},
  language = {en},
  number = {3}
}

@book{Pierce2002,
  title = {Types and Programming Languages},
  author = {Pierce, Benjamin C.},
  year = {2002},
  publisher = {{MIT Press}},
  address = {{Cambridge, Mass}},
  file = {2000 - Ferreira - IUuI for Programming Languages.pdf},
  isbn = {978-0-262-16209-8},
  keywords = {Programming languages (Electronic computers)},
  language = {en},
  lccn = {QA76.7 .P54 2002}
}

@article{Pinotsis2011,
  title = {Neural Fields, Spectral Responses and Lateral Connections},
  author = {Pinotsis, D.A. and Friston, K.J.},
  year = {2011},
  month = mar,
  volume = {55},
  pages = {39--48},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2010.11.081},
  abstract = {This paper describes a neural field model for local (mesoscopic) dynamics on the cortical surface. Our focus is on sparse intrinsic connections that are characteristic of real cortical microcircuits. This sparsity is modelled with radial connectivity functions or kernels with non-central peaks. The ensuing analysis allows one to generate or predict spectral responses to known exogenous input or random fluctuations. Here, we characterise the effect of different connectivity architectures (the range, dispersion and propagation speed of intrinsic or lateral connections) and synaptic gains on spatiotemporal dynamics. Specifically, we look at spectral responses to random fluctuations and examine the ability of synaptic gain and connectivity parameters to induce Turing instabilities. We find that although the spatial deployment and speed of lateral connections can have a profound affect on the behaviour of spatial modes over different scales, only synaptic gain is capable of producing phase-transitions. We discuss the implications of these findings for the use of neural fields as generative models in dynamic causal modeling (DCM).},
  file = {2011 - Pinotsis, Friston - Neural fields, spectral responses and lateral connections.pdf},
  journal = {NeuroImage},
  language = {en},
  number = {1}
}

@article{Pinotsis2013,
  title = {On Conductance-Based Neural Field Models},
  author = {Pinotsis, Dimitris A. and Leite, Marco and Friston, Karl J.},
  year = {2013},
  volume = {7},
  issn = {1662-5188},
  doi = {10.3389/fncom.2013.00158},
  file = {2013 - Pinotsis, Leite, Friston - On conductance-based neural field models.pdf},
  journal = {Frontiers in Computational Neuroscience},
  language = {en}
}

@article{Pinotsis2014,
  title = {Neural Masses and Fields: Modeling the Dynamics of Brain Activity},
  shorttitle = {Neural Masses and Fields},
  author = {Pinotsis, Dimitris and Robinson, Peter and {beim Graben}, Peter and Friston, Karl},
  year = {2014},
  month = nov,
  volume = {8},
  issn = {1662-5188},
  doi = {10.3389/fncom.2014.00149},
  file = {2014 - Pinotsis et al. - Neural masses and fields modeling the dynamics of brain activity.pdf;Pinotsis et al. - 2014 - Neural masses and fields modeling the dynamics of.pdf},
  journal = {Frontiers in Computational Neuroscience},
  language = {en}
}

@article{Pirolli1999,
  title = {Information Foraging.},
  author = {Pirolli, Peter and Card, Stuart},
  year = {1999},
  volume = {106},
  pages = {643--675},
  issn = {0033-295X},
  doi = {10.1037/0033-295X.106.4.643},
  file = {pirolli1999.pdf},
  journal = {Psychological Review},
  language = {en},
  number = {4}
}

@book{Pirolli2007,
  title = {Information Foraging Theory: Adaptive Interaction with Information},
  shorttitle = {Information Foraging Theory},
  author = {Pirolli, Peter},
  year = {2007},
  publisher = {{Oxford University Press}},
  address = {{Oxford ; New York}},
  annotation = {OCLC: ocm70334982},
  file = {../Books/ML/Pirolli - 2007 - Information foraging theory adaptive interaction .pdf},
  isbn = {978-0-19-517332-1},
  keywords = {Human-computer interaction,Information behavior},
  language = {en},
  lccn = {ZA3075 .P57 2007},
  series = {Oxford Series in Human-Technology Interaction}
}

@book{Pisula2009,
  title = {Curiosity and Information Seeking in Animal and Human Behavior},
  author = {Pisula, Wojciech},
  year = {2009},
  publisher = {{Brown Walker Press}},
  address = {{Boca Raton}},
  file = {Pisula - 2009 - Curiosity and information seeking in animal and hu.pdf},
  isbn = {978-1-59942-498-9},
  keywords = {Curiosity,Information behavior,Psychology; Comparative},
  language = {en},
  lccn = {BF323.C8 P57 2009}
}

@techreport{Pisupati2019,
  title = {Lapses in Perceptual Judgments Reflect Exploration},
  author = {Pisupati, Sashank and {Chartarifsky-Lynn}, Lital and Khanal, Anup and Churchland, Anne K.},
  year = {2019},
  month = apr,
  institution = {{Neuroscience}},
  doi = {10.1101/613828},
  abstract = {ABSTRACT           During perceptual decision making, subjects often display a constant rate of errors independent of evidence strength, referred to as ``lapses''. Their proper treatment is crucial for accurate estimation of perceptual parameters, however they are often treated as a nuisance arising from motor errors or inattention. Here, we propose that lapses can instead reflect a dynamic form of exploration. We demonstrate that perceptual uncertainty modulates the probability of lapses both across and within modalities on a multisensory discrimination task in rats. These effects cannot be accounted for by inattention or motor error, however they are concisely explained by uncertainty-guided exploration. We confirm the predictions of the exploration model by showing that changing the magnitude or probability of reward associated with one of the decisions selectively affects the lapses associated with that decision in uncertain conditions, while leaving ``sure-bet'' decisions unchanged, as predicted by the model. Finally, we demonstrate that muscimol inactivations of secondary motor cortex and posterior striatum affect lapses asymmetrically across modalities. The inactivations can be captured by a devaluation of actions corresponding to the inactivated side, and do not affect ``sure-bet'' decisions. Together, our results suggest that far from being a nuisance, lapses are informative about subjects' action values and deficits thereof during perceptual decisions.},
  file = {Pisupati et al. - 2019 - Lapses in perceptual judgments reflect exploration.pdf},
  language = {en},
  type = {Preprint}
}

@article{Plank2008,
  title = {Optimal Foraging: {{L\'evy}} Pattern or Process?},
  shorttitle = {Optimal Foraging},
  author = {Plank, M.J and James, A},
  year = {2008},
  month = sep,
  volume = {5},
  pages = {1077--1086},
  issn = {1742-5689, 1742-5662},
  doi = {10.1098/rsif.2008.0006},
  abstract = {Many different species have been suggested to forage according to a L\'evy walk in which the distribution of step lengths is heavy-tailed. Theoretical research has shown that a L\'evy exponent of approximately 2 can provide a higher foraging efficiency than other exponents. In this paper, a composite search model is presented for non-destructive foraging behaviour based on Brownian (i.e. non-heavy-tailed) motion. The model consists of an intensive search phase, followed by an extensive phase, if no food is found in the intensive phase. Quantities commonly observed in the field, such as the distance travelled before finding food and the net displacement in a fixed time interval, are examined and compared with the results of a L\'evy walk model. It is shown that it may be very difficult, in practice, to distinguish between the Brownian and the L\'evy models on the basis of observed data. A mathematical expression for the optimal time to switch from intensive to extensive search mode is derived, and it is shown that the composite search model provides higher foraging efficiency than the L\'evy model.},
  file = {Plank and James - 2008 - Optimal foraging Lévy pattern or process.pdf},
  journal = {J. R. Soc. Interface.},
  language = {en},
  number = {26}
}

@article{Plank2009,
  title = {Sampling Rate and Misidentification of {{L\'evy}} and Non-{{L\'evy}} Movement Paths},
  author = {Plank, Michael J. and Codling, Edward A.},
  year = {2009},
  month = dec,
  volume = {90},
  pages = {3546--3553},
  issn = {0012-9658},
  doi = {10.1890/09-0079.1},
  abstract = {A large number of empirical studies have attributed Le\textasciiacute{} vy search patterns to the foraging movements of animals. Typically, this is done by fitting a power-law distribution with an exponent of 1 , l 3 to the observed step lengths. Most studies record the animal's location at equally spaced time intervals, which are sometimes significantly longer than the natural time scale of the animal's movements. The collected data thus represent a subsample of the animal's movement. In this paper, the effect of subsampling on the observed properties of both Le\textasciiacute{} vy and non-Le\textasciiacute{} vy simulated movement paths is investigated. We find that the apparent properties of the observed movement path can be sensitive to the sampling rate even though Le\textasciiacute{} vy search patterns are supposedly scale-independent. We demonstrate that, in certain contexts and dependent on the sampling rate used in observation, it is possible to misidentify a non-Le\textasciiacute{} vy movement path as being a Le\textasciiacute{} vy path. We also demonstrate that a Le\textasciiacute{} vy movement path can be misidentified as a non-Le\textasciiacute{} vy path, but this is dependent on the value of l of the original simulated path, with the greatest uncertainty for l {$\frac{1}{4}$} 2. We discuss the implications of these results in the context of studies of animal movements and foraging behavior.},
  file = {Plank and Codling - 2009 - Sampling rate and misidentification of Lévy and no.pdf},
  journal = {Ecology},
  language = {en},
  number = {12}
}

@article{Plenz1999,
  title = {A Basal Ganglia Pacemaker Formed by the Subthalamic Nucleus and External Globus Pallidus},
  author = {Plenz, Dietmar and Kital, Stephen T.},
  year = {1999},
  month = aug,
  volume = {400},
  pages = {677--682},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/23281},
  file = {1999 - Plenz, Kital - A basal ganglia pacemaker formed by the subthalamic nucleus and external globus pallidus.pdf},
  journal = {Nature},
  language = {en},
  number = {6745}
}

@article{Pluta2015,
  title = {A Direct Translaminar Inhibitory Circuit Tunes Cortical Output},
  author = {Pluta, Scott and Naka, Alexander and Veit, Julia and Telian, Gregory and Yao, Lucille and Hakim, Richard and Taylor, David and Adesnik, Hillel},
  year = {2015},
  month = nov,
  volume = {18},
  pages = {1631--1640},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.4123},
  file = {2015 - Pluta et al. - A direct translaminar inhibitory circuit tunes cortical output.pdf;Pluta et al. - 2015 - A direct translaminar inhibitory circuit tunes cor.pdf},
  journal = {Nature Neuroscience},
  language = {en},
  number = {11}
}

@article{Podlaski2017,
  title = {Mapping the Function of Neuronal Ion Channels in Model and Experiment},
  author = {Podlaski, William F and Seeholzer, Alexander and Groschner, Lukas N and Miesenb{\"o}ck, Gero and Ranjan, Rajnish and Vogels, Tim P},
  year = {2017},
  month = mar,
  volume = {6},
  issn = {2050-084X},
  doi = {10.7554/eLife.22152},
  file = {Podlaski et al. - 2017 - Mapping the function of neuronal ion channels in m.pdf},
  journal = {eLife},
  language = {en}
}

@article{Podvalny2015,
  title = {A Unifying Principle Underlying the Extracellular Field Potential Spectral Responses in the Human Cortex},
  author = {Podvalny, Ella and Noy, Niv and Harel, Michal and Bickel, Stephan and Chechik, Gal and Schroeder, Charles E. and Mehta, Ashesh D. and Tsodyks, Misha and Malach, Rafael},
  year = {2015},
  month = jul,
  volume = {114},
  pages = {505--519},
  issn = {0022-3077, 1522-1598},
  doi = {10.1152/jn.00943.2014},
  file = {Podvalny et al. - 2015 - A unifying principle underlying the extracellular  2.pdf;Podvalny et al. - 2015 - A unifying principle underlying the extracellular .pdf},
  journal = {Journal of Neurophysiology},
  language = {en},
  number = {1}
}

@article{Poggio2017,
  title = {Why and When Can Deep-but Not Shallow-Networks Avoid the Curse of Dimensionality: {{A}} Review},
  shorttitle = {Why and When Can Deep-but Not Shallow-Networks Avoid the Curse of Dimensionality},
  author = {Poggio, Tomaso and Mhaskar, Hrushikesh and Rosasco, Lorenzo and Miranda, Brando and Liao, Qianli},
  year = {2017},
  month = oct,
  volume = {14},
  pages = {503--519},
  issn = {1476-8186, 1751-8520},
  doi = {10.1007/s11633-017-1054-2},
  abstract = {The paper characterizes classes of functions for which deep learning can be exponentially better than shallow learning. Deep convolutional networks are a special case of these conditions, though weight sharing is not the main reason for their exponential advantage.},
  file = {Poggio et al. - 2017 - Why and when can deep-but not shallow-networks avo.pdf},
  journal = {International Journal of Automation and Computing},
  language = {en},
  number = {5}
}

@article{Pogosyan2009,
  title = {Boosting {{Cortical Activity}} at {{Beta}}-{{Band Frequencies Slows Movement}} in {{Humans}}},
  author = {Pogosyan, Alek and Gaynor, Louise Doyle and Eusebio, Alexandre and Brown, Peter},
  year = {2009},
  month = oct,
  volume = {19},
  pages = {1637--1641},
  issn = {09609822},
  doi = {10.1016/j.cub.2009.07.074},
  abstract = {Neurons have a striking tendency to engage in oscillatory activities. One important type of oscillatory activity prevalent in the motor system occurs in the beta frequency band, at about 20 Hz. It is manifest during the maintenance of tonic contractions and is suppressed prior to and during voluntary movement [1\textendash 7]. This and other correlative evidence suggests that beta activity might promote tonic contraction, while impairing motor processing related to new movements [3, 8, 9]. Hence, bursts of beta activity in the cortex are associated with a strengthening of the motor effects of sensory feedback during tonic contraction and with reductions in the velocity of voluntary movements [9\textendash 11]. Moreover, beta activity is increased when movement has to be resisted or voluntarily suppressed [7, 12, 13]. Here we use imperceptible transcranial alternating-current stimulation to entrain cortical activity at 20 Hz in healthy subjects and show that this slows voluntary movement. The present findings are the first direct evidence of causality between any physiological oscillatory brain activity and concurrent motor behavior in the healthy human and help explain how the exaggerated beta activity found in Parkinson's disease can lead to motor slowing in this illness [14].},
  file = {2009 - Pogosyan et al. - Boosting Cortical Activity at Beta-Band Frequencies Slows Movement in Humans.pdf},
  journal = {Current Biology},
  language = {en},
  number = {19}
}

@article{Poirazi2003,
  title = {Pyramidal {{Neuron}} as {{Two}}-{{Layer Neural Network}}},
  author = {Poirazi, Panayiota and Brannon, Terrence and Mel, Bartlett W.},
  year = {2003},
  month = mar,
  volume = {37},
  pages = {989--999},
  issn = {08966273},
  doi = {10.1016/S0896-6273(03)00149-1},
  abstract = {The pyramidal neuron is the principal cell type in the mammalian forebrain, but its function remains poorly understood. Using a detailed compartmental model of a hippocampal CA1 pyramidal cell, we recorded responses to complex stimuli consisting of dozens of high-frequency activated synapses distributed throughout the apical dendrites. We found the cell's firing rate could be predicted by a simple formula that maps the physical components of the cell onto those of an abstract two-layer ``neural network.'' In the first layer, synaptic inputs drive independent sigmoidal subunits corresponding to the cell's several dozen long, thin terminal dendrites. The subunit outputs are then summed within the main trunk and cell body prior to final thresholding. We conclude that insofar as the neural code is mediated by average firing rate, a twolayer neural network may provide a useful abstraction for the computing function of the individual pyramidal neuron.},
  file = {2003 - Poirazi, Brannon, Mel - Pyramidal Neuron as Two-Layered Neural Network.pdf},
  journal = {Neuron},
  language = {en},
  number = {6}
}

@incollection{Polani2001,
  title = {An {{Information}}-{{Theoretic Approach}} for the {{Quantification}} of {{Relevance}}},
  booktitle = {Advances in {{Artificial Life}}},
  author = {Polani, Daniel and Martinetz, Thomas and Kim, Jan},
  editor = {Goos, G. and Hartmanis, J. and {van Leeuwen}, J. and Kelemen, Jozef and Sos{\'i}k, Petr},
  year = {2001},
  volume = {2159},
  pages = {704--713},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/3-540-44811-X_82},
  file = {1999 - Monmarche, Venturini - Advances in Artificial Life.pdf},
  isbn = {978-3-540-42567-0 978-3-540-44811-2}
}

@article{Pollard2016,
  title = {A {{Second Law}} for {{Open Markov Processes}}},
  author = {Pollard, Blake S.},
  year = {2016},
  month = mar,
  volume = {23},
  pages = {1650006},
  issn = {1230-1612, 1793-7191},
  doi = {10.1142/S1230161216500062},
  abstract = {In this paper we define the notion of an open Markov process. An open Markov process is a generalization of an ordinary Markov process in which populations are allowed to flow in and out of the system at certain boundary states. We show that the rate of change of relative entropy in an open Markov process is less than or equal to the flow of relative entropy through its boundary states. This can be viewed as a generalization of the Second Law for open Markov processes. In the case of a Markov process whose equilibrium obeys detailed balance, this inequality puts an upper bound on the rate of change of the free energy for any non-equilibrium distribution.},
  archiveprefix = {arXiv},
  eprint = {1410.6531},
  eprinttype = {arxiv},
  file = {Pollard - 2016 - A Second Law for Open Markov Processes.pdf},
  journal = {Open Systems \& Information Dynamics},
  keywords = {Condensed Matter - Statistical Mechanics},
  language = {en},
  number = {01}
}

@article{Polsky2004,
  title = {Computational Subunits in Thin Dendrites of Pyramidal Cells},
  author = {Polsky, Alon and Mel, Bartlett W and Schiller, Jackie},
  year = {2004},
  month = jun,
  volume = {7},
  pages = {621--627},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn1253},
  file = {2004 - Polsky, Mel, Schiller - Computational subunits in thin dendrites of pyramidal cells.pdf},
  journal = {Nature Neuroscience},
  language = {en},
  number = {6}
}

@article{Poo2009,
  title = {Odor {{Representations}} in {{Olfactory Cortex}}: ``{{Sparse}}'' {{Coding}}, {{Global Inhibition}}, and {{Oscillations}}},
  shorttitle = {Odor {{Representations}} in {{Olfactory Cortex}}},
  author = {Poo, Cindy and Isaacson, Jeffry S.},
  year = {2009},
  month = jun,
  volume = {62},
  pages = {850--861},
  issn = {08966273},
  doi = {10.1016/j.neuron.2009.05.022},
  abstract = {The properties of cortical circuits underlying central representations of sensory stimuli are poorly understood. Here we use in vivo cell-attached and wholecell voltage-clamp recordings to reveal how excitatory and inhibitory synaptic input govern odor representations in rat primary olfactory (piriform) cortex. We show that odors evoke spiking activity that is sparse across the cortical population. We find that unbalanced synaptic excitation and inhibition underlie sparse activity: inhibition is widespread and broadly tuned, while excitation is less common and odor-specific. ``Global'' inhibition can be explained by local interneurons that receive ubiquitous and nonselective odor-evoked excitation. In the temporal domain, while respiration imposes a slow rhythm to olfactory cortical responses, odors evoke fast (15-30 Hz) oscillations in synaptic activity. Oscillatory excitation precedes inhibition, generating brief time windows for precise and temporally sparse spike output. Together, our results reveal that global inhibition and oscillations are major synaptic mechanisms shaping odor representations in olfactory cortex.},
  file = {2009 - Poo, Isaacson - Odor Representations in Olfactory Cortex Sparse Coding, Global Inhibition, and Oscillations.pdf},
  journal = {Neuron},
  language = {en},
  number = {6}
}

@article{Popov,
  title = {Alpha Oscillations Govern Interhemispheric Spike Timing Coordination in the Honey Bee Brain},
  author = {Popov, Tzvetan and Szyszka, Paul},
  pages = {10},
  abstract = {In 1929 Hans Berger discovered the alpha oscillations: a prominent, ongoing 10 Hz rhythm in the human EEG. These alpha oscillations are amongst the most widely studied cerebral signals, related to cognitive phenomena such as attention, memory and consciousness. However, the mechanisms by which alpha oscillations affect cognition await demonstration. Here we provide a novel model system from an adequately described complex neural circuit of the honey bee (Apis mellifera), that exhibits properties of the alpha oscillations. We found a prominent alpha wavelike ongoing neural activity (\textasciitilde{} 18 Hz) that is reduced in amplitude upon stimulus presentation. The phase of this alpha activity biased both neuronal spikes and amplitude of high frequency gamma activity ({$>$} 30 Hz). These results suggest a common role of oscillatory neuronal activity across phyla and provide an unprecedented new venue for causal studies on the relationship between neuronal spikes, brain oscillations and cognition.},
  file = {Popov and Szyszka - Alpha oscillations govern interhemispheric spike t.pdf},
  language = {en}
}

@article{Popova,
  title = {Alpha Oscillations Govern Interhemispheric Spike Timing Coordination in the Honey Bee Brain},
  author = {Popov, Tzvetan and Szyszka, Paul},
  pages = {7},
  file = {Popov and Szyszka - Alpha oscillations govern interhemispheric spike t 2.pdf},
  language = {en}
}

@article{Porter2014,
  title = {Dynamical {{Systems}} on {{Networks}}: {{A Tutorial}}},
  shorttitle = {Dynamical {{Systems}} on {{Networks}}},
  author = {Porter, Mason A. and Gleeson, James P.},
  year = {2014},
  month = mar,
  abstract = {We give a tutorial for the study of dynamical systems on networks. We focus especially on "simple" situations that are tractable analytically, because they can be very insightful and provide useful springboards for the study of more complicated scenarios. We briefly motivate why examining dynamical systems on networks is interesting and important, and we then give several fascinating examples and discuss some theoretical results. We also briefly discuss dynamical systems on dynamical (i.e., time-dependent) networks, overview software implementations, and give an outlook on the field.},
  archiveprefix = {arXiv},
  eprint = {1403.7663},
  eprinttype = {arxiv},
  file = {Porter and Gleeson - 2014 - Dynamical Systems on Networks A Tutorial.pdf},
  journal = {arXiv:1403.7663 [cond-mat, physics:nlin, physics:physics]},
  keywords = {Computer Science - Social and Information Networks,Condensed Matter - Disordered Systems and Neural Networks,Condensed Matter - Statistical Mechanics,Nonlinear Sciences - Adaptation and Self-Organizing Systems,Physics - Physics and Society},
  language = {en},
  primaryclass = {cond-mat, physics:nlin, physics:physics}
}

@article{Porto-Pazos2011,
  title = {Artificial {{Astrocytes Improve Neural Network Performance}}},
  author = {{Porto-Pazos}, Ana B. and Veiguela, Noha and Mesejo, Pablo and Navarrete, Marta and Alvarellos, Alberto and Ib{\'a}{\~n}ez, Oscar and Pazos, Alejandro and Araque, Alfonso},
  editor = {Am{\'e}d{\'e}e, Thierry},
  year = {2011},
  month = apr,
  volume = {6},
  pages = {e19109},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0019109},
  abstract = {Compelling evidence indicates the existence of bidirectional communication between astrocytes and neurons. Astrocytes, a type of glial cells classically considered to be passive supportive cells, have been recently demonstrated to be actively involved in the processing and regulation of synaptic information, suggesting that brain function arises from the activity of neuron-glia networks. However, the actual impact of astrocytes in neural network function is largely unknown and its application in artificial intelligence remains untested. We have investigated the consequences of including artificial astrocytes, which present the biologically defined properties involved in astrocyte-neuron communication, on artificial neural network performance. Using connectionist systems and evolutionary algorithms, we have compared the performance of artificial neural networks (NN) and artificial neuron-glia networks (NGN) to solve classification problems. We show that the degree of success of NGN is superior to NN. Analysis of performances of NN with different number of neurons or different architectures indicate that the effects of NGN cannot be accounted for an increased number of network elements, but rather they are specifically due to astrocytes. Furthermore, the relative efficacy of NGN vs. NN increases as the complexity of the network increases. These results indicate that artificial astrocytes improve neural network performance, and established the concept of Artificial Neuron-Glia Networks, which represents a novel concept in Artificial Intelligence with implications in computational science as well as in the understanding of brain function.},
  file = {Porto-Pazos et al. - 2011 - Artificial Astrocytes Improve Neural Network Perfo.PDF},
  journal = {PLoS ONE},
  language = {en},
  number = {4}
}

@article{Porto-Pazos2011a,
  title = {Artificial {{Astrocytes Improve Neural Network Performance}}},
  author = {{Porto-Pazos}, Ana B. and Veiguela, Noha and Mesejo, Pablo and Navarrete, Marta and Alvarellos, Alberto and Ib{\'a}{\~n}ez, Oscar and Pazos, Alejandro and Araque, Alfonso},
  editor = {Am{\'e}d{\'e}e, Thierry},
  year = {2011},
  month = apr,
  volume = {6},
  pages = {e19109},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0019109},
  abstract = {Compelling evidence indicates the existence of bidirectional communication between astrocytes and neurons. Astrocytes, a type of glial cells classically considered to be passive supportive cells, have been recently demonstrated to be actively involved in the processing and regulation of synaptic information, suggesting that brain function arises from the activity of neuron-glia networks. However, the actual impact of astrocytes in neural network function is largely unknown and its application in artificial intelligence remains untested. We have investigated the consequences of including artificial astrocytes, which present the biologically defined properties involved in astrocyte-neuron communication, on artificial neural network performance. Using connectionist systems and evolutionary algorithms, we have compared the performance of artificial neural networks (NN) and artificial neuron-glia networks (NGN) to solve classification problems. We show that the degree of success of NGN is superior to NN. Analysis of performances of NN with different number of neurons or different architectures indicate that the effects of NGN cannot be accounted for an increased number of network elements, but rather they are specifically due to astrocytes. Furthermore, the relative efficacy of NGN vs. NN increases as the complexity of the network increases. These results indicate that artificial astrocytes improve neural network performance, and established the concept of Artificial Neuron-Glia Networks, which represents a novel concept in Artificial Intelligence with implications in computational science as well as in the understanding of brain function.},
  file = {Porto-Pazos et al. - 2011 - Artificial Astrocytes Improve Neural Network Perfo 2.pdf},
  journal = {PLoS ONE},
  language = {en},
  number = {4}
}

@article{Posner1980,
  title = {Orienting of Attention},
  author = {Posner, Michael I.},
  year = {1980},
  month = feb,
  volume = {32},
  pages = {3--25},
  issn = {0033-555X},
  doi = {10.1080/00335558008248231},
  file = {1980 - Posner - ORIENTING OF ATTENTION.pdf;Posner - 1980 - Orienting of attention.pdf},
  journal = {Quarterly Journal of Experimental Psychology},
  language = {en},
  number = {1}
}

@article{Potjans2014,
  title = {The {{Cell}}-{{Type Specific Cortical Microcircuit}}: {{Relating Structure}} and {{Activity}} in a {{Full}}-{{Scale Spiking Network Model}}},
  shorttitle = {The {{Cell}}-{{Type Specific Cortical Microcircuit}}},
  author = {Potjans, Tobias C. and Diesmann, Markus},
  year = {2014},
  month = mar,
  volume = {24},
  pages = {785--806},
  issn = {1460-2199, 1047-3211},
  doi = {10.1093/cercor/bhs358},
  abstract = {In the past decade, the cell-type specific connectivity and activity of local cortical networks have been characterized experimentally to some detail. In parallel, modeling has been established as a tool to relate network structure to activity dynamics. While available comprehensive connectivity maps (Thomson, West, et al. 2002; Binzegger et al. 2004) have been used in various computational studies, prominent features of the simulated activity such as the spontaneous firing rates do not match the experimental findings. Here, we analyze the properties of these maps to compile an integrated connectivity map, which additionally incorporates insights on the specific selection of target types. Based on this integrated map, we build a full-scale spiking network model of the local cortical microcircuit. The simulated spontaneous activity is asynchronous irregular and cell-type specific firing rates are in agreement with in vivo recordings in awake animals, including the low rate of layer 2/3 excitatory cells. The interplay of excitation and inhibition captures the flow of activity through cortical layers after transient thalamic stimulation. In conclusion, the integration of a large body of the available connectivity data enables us to expose the dynamical consequences of the cortical microcircuitry.},
  file = {2014 - Potjans, Diesmann - The cell-type specific cortical microcircuit Relating structure and activity in a full-scale spiking network.pdf;Potjans and Diesmann - 2014 - The Cell-Type Specific Cortical Microcircuit Rela.pdf},
  journal = {Cerebral Cortex},
  language = {en},
  number = {3}
}

@article{Poucet1993,
  title = {Spatial Cognitive Maps in Animals: New Hypotheses on Their Structure and Neural Mechanisms.},
  author = {Poucet, B},
  year = {1993},
  volume = {100},
  pages = {162--183},
  file = {Poucet1993.pdf},
  journal = {Psycholocial Review},
  number = {1}
}

@article{Pouget2000,
  title = {Information Processing with Population Codes},
  author = {Pouget, Alexandre and Dayan, Peter and Zemel, Richard},
  year = {2000},
  month = nov,
  volume = {1},
  pages = {125--132},
  issn = {1471-003X, 1471-0048},
  doi = {10.1038/35039062},
  file = {Pouget et al. - 2000 - Information processing with population codes.pdf},
  journal = {Nat Rev Neurosci},
  language = {en},
  number = {2}
}

@techreport{Prat-Carrabin2019,
  title = {Human Inference in Changing Environments},
  author = {{Prat-Carrabin}, Arthur and Wilson, Robert C. and Cohen, Jonathan D. and {da Silveira}, Rava Azeredo},
  year = {2019},
  month = jul,
  institution = {{Neuroscience}},
  doi = {10.1101/720516},
  abstract = {In past decades, the Bayesian paradigm has gained traction as a principled account of human behavior in inference tasks. Yet this success is tainted by the ubiquity of behavioral suboptimality and variability. We explore these discrepancies using an online inference task, in which we modulate the temporal statistics of hidden change points. We show that humans adapt their inference process to the implicit temporal statistics of stimuli, thereby behaving in an approximate Bayesian fashion. However, they exhibit biases and variability, and these depend on the history of stimuli. A systematic study of a broad family of optimal and suboptimal models indicates that noise arises `internally'\textemdash in the inference process itself\textemdash rather than at the behavioral output. Specifically, we argue that humans mimic Bayesian inference by approximating the posterior with a modest number of samples. Our results contribute to a growing literature on sample-based cognition and compression by stochastic pruning.},
  file = {Prat-Carrabin et al. - 2019 - Human inference in changing environments.pdf},
  language = {en},
  type = {Preprint}
}

@article{Prescott1997,
  title = {A {{Robot Trace Maker}}: {{Modeling}} the {{Fossil Evidence}} of {{Early Invertebrate Behavior}}},
  shorttitle = {A {{Robot Trace Maker}}},
  author = {Prescott, Tony J. and Ibbotson, Carl},
  year = {1997},
  month = oct,
  volume = {3},
  pages = {289--306},
  issn = {1064-5462, 1530-9185},
  doi = {10.1162/artl.1997.3.4.289},
  abstract = {The study of trace fossils, the fossilized remains of animal behavior, reveals interesting parallels with recent research in behavior-based robotics. This article reports robot simulations of the meandering foraging trails left by early invertebrates which demonstrate that such trails can be generated by mechanisms similar to those used for robot wall-following. We conclude with the suggestion that the capacity for intelligent behavior shown by many behavior-based robots is similar to that of animals of the late Precambrian and early Cambrian periods approximately 530-565 million years ago.},
  file = {Prescott and Ibbotson - 1997 - A Robot Trace Maker Modeling the Fossil Evidence .pdf},
  journal = {Artificial Life},
  language = {en},
  number = {4}
}

@article{Prescott2006,
  title = {A Robot Model of the Basal Ganglia: {{Behavior}} and Intrinsic Processing},
  shorttitle = {A Robot Model of the Basal Ganglia},
  author = {Prescott, Tony J. and Montes Gonz{\'a}lez, Fernando M. and Gurney, Kevin and Humphries, Mark D. and Redgrave, Peter},
  year = {2006},
  month = jan,
  volume = {19},
  pages = {31--61},
  issn = {08936080},
  doi = {10.1016/j.neunet.2005.06.049},
  abstract = {The existence of multiple parallel loops connecting sensorimotor systems to the basal ganglia has given rise to proposals that these nuclei serve as a selection mechanism resolving competitions between the alternative actions available in a given context. A strong test of this hypothesis is to require a computational model of the basal ganglia to generate integrated selection sequences in an autonomous agent, we therefore describe a robot architecture into which such a model is embedded, and require it to control action selection in a robotic task inspired by animal observations. Our results demonstrate effective action selection by the embedded model under a wide range of sensory and motivational conditions. When confronted with multiple, high salience alternatives, the robot also exhibits forms of behavioral disintegration that show similarities to animal behavior in conflict situations. The model is shown to cast light on recent neurobiological findings concerning behavioral switching and sequencing.},
  file = {2006 - Prescott et al. - A robot model of the basal ganglia Behavior and intrinsic processing.pdf},
  journal = {Neural Networks},
  language = {en},
  number = {1}
}

@article{Preston2008,
  title = {Multivoxel {{Pattern Selectivity}} for {{Perceptually Relevant Binocular Disparities}} in the {{Human Brain}}},
  author = {Preston, T. J. and Li, S. and Kourtzi, Z. and Welchman, A. E.},
  year = {2008},
  month = oct,
  volume = {28},
  pages = {11315--11327},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.2728-08.2008},
  file = {2008 - Preston et al. - Multivoxel pattern selectivity for perceptually relevant binocular disparities in the human brain.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {44}
}

@article{Principe2015,
  title = {Representing and Decomposing Neural Potential Signals},
  author = {Principe, Jose C and Brockmeier, Austin J},
  year = {2015},
  month = apr,
  volume = {31},
  pages = {13--17},
  issn = {09594388},
  doi = {10.1016/j.conb.2014.07.023},
  file = {Principe and Brockmeier - 2015 - Representing and decomposing neural potential sign.pdf},
  journal = {Current Opinion in Neurobiology},
  language = {en}
}

@article{Prinz2004,
  title = {The Dynamic Clamp Comes of Age},
  author = {Prinz, Astrid A and Abbott, L.F and Marder, Eve},
  year = {2004},
  month = apr,
  volume = {27},
  pages = {218--224},
  issn = {01662236},
  doi = {10.1016/j.tins.2004.02.004},
  file = {2004 - Prinz, Abbott, Marder - The dynamic clamp comes of age.pdf},
  journal = {Trends in Neurosciences},
  language = {en},
  number = {4}
}

@article{Prinz2007,
  title = {Shaul {{Druckmann}} 1,{${_\ast}$}, {{Yoav Banitt}} 2, {{Albert Gidon}} 2, {{Felix Schu}}\textasciidieresis{} Rmann 3, {{Henry Markram Idan Segev}}},
  author = {Prinz, Astrid},
  year = {2007},
  pages = {12},
  file = {2007 - Druckmann et al. - Idan Segev.pdf},
  journal = {Frontiers in Neuroscience},
  language = {en}
}

@article{Pritchett2015,
  title = {For Things Needing Your Attention: The Role of Neocortical Gamma in Sensory Perception},
  shorttitle = {For Things Needing Your Attention},
  author = {Pritchett, Dominique L and Siegle, Joshua H and Deister, Christopher A and Moore, Christopher I},
  year = {2015},
  month = apr,
  volume = {31},
  pages = {254--263},
  issn = {09594388},
  doi = {10.1016/j.conb.2015.02.004},
  file = {Pritchett et al. - 2015 - For things needing your attention the role of neo.pdf},
  journal = {Current Opinion in Neurobiology},
  language = {en}
}

@article{Pritzel2017,
  title = {Neural {{Episodic Control}}},
  author = {Pritzel, Alexander and Uria, Benigno and Srinivasan, Sriram and Puigdom{\`e}nech, Adri{\`a} and Vinyals, Oriol and Hassabis, Demis and Wierstra, Daan and Blundell, Charles},
  year = {2017},
  month = mar,
  abstract = {Deep reinforcement learning methods attain super-human performance in a wide range of environments. Such methods are grossly inefficient, often taking orders of magnitudes more data than humans to achieve reasonable performance. We propose Neural Episodic Control: a deep reinforcement learning agent that is able to rapidly assimilate new experiences and act upon them. Our agent uses a semi-tabular representation of the value function: a buffer of past experience containing slowly changing state representations and rapidly updated estimates of the value function. We show across a wide range of environments that our agent learns significantly faster than other state-of-the-art, general purpose deep reinforcement learning agents.},
  archiveprefix = {arXiv},
  eprint = {1703.01988},
  eprinttype = {arxiv},
  file = {Pritzel et al. - 2017 - Neural Episodic Control.pdf},
  journal = {arXiv:1703.01988 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryclass = {cs, stat}
}

@inproceedings{Pugh2015,
  title = {Confronting the {{Challenge}} of {{Quality Diversity}}},
  booktitle = {Proceedings of the 2015 on {{Genetic}} and {{Evolutionary Computation Conference}} - {{GECCO}} '15},
  author = {Pugh, Justin K. and Soros, L. B. and Szerlip, Paul A. and Stanley, Kenneth O.},
  year = {2015},
  pages = {967--974},
  publisher = {{ACM Press}},
  address = {{Madrid, Spain}},
  doi = {10.1145/2739480.2754664},
  abstract = {In contrast to the conventional role of evolution in evolutionary computation (EC) as an optimization algorithm, a new class of evolutionary algorithms has emerged in recent years that instead aim to accumulate as diverse a collection of discoveries as possible, yet where each variant in the collection is as fit as it can be. Often applied in both neuroevolution and morphological evolution, these new quality diversity (QD) algorithms are particularly well-suited to evolution's inherent strengths, thereby offering a promising niche for EC within the broader field of machine learning. However, because QD algorithms are so new, until now no comprehensive study has yet attempted to systematically elucidate their relative strengths and weaknesses under different conditions. Taking a first step in this direction, this paper introduces a new benchmark domain designed specifically to compare and contrast QD algorithms. It then shows how the degree of alignment between the measure of quality and the behavior characterization (which is an essential component of all QD algorithms to date) impacts the ultimate performance of different such algorithms. The hope is that this initial study will help to stimulate interest in QD and begin to unify the disparate ideas in the area.},
  file = {Pugh et al. - 2015 - Confronting the Challenge of Quality Diversity.pdf},
  isbn = {978-1-4503-3472-3},
  language = {en}
}

@incollection{Pugh2016,
  title = {Searching for {{Quality Diversity When Diversity}} Is {{Unaligned}} with {{Quality}}},
  booktitle = {Parallel {{Problem Solving}} from {{Nature}} \textendash{} {{PPSN XIV}}},
  author = {Pugh, Justin K. and Soros, L. B. and Stanley, Kenneth O.},
  editor = {Handl, Julia and Hart, Emma and Lewis, Peter R. and {L{\'o}pez-Ib{\'a}{\~n}ez}, Manuel and Ochoa, Gabriela and Paechter, Ben},
  year = {2016},
  volume = {9921},
  pages = {880--889},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-45823-6_82},
  abstract = {Inspired by natural evolution's affinity for discovering a wide variety of successful organisms, a new evolutionary search paradigm has emerged wherein the goal is not to find the single best solution but rather to collect a diversity of unique phenotypes where each variant is as good as it can be. These quality diversity (QD) algorithms therefore must explore multiple promising niches simultaneously. A QD algorithm's diversity component, formalized by specifying a behavior characterization (BC), not only generates diversity but also promotes quality by helping to overcome deception in the fitness landscape. However, some BCs (particularly those that are unaligned with the notion of quality) do not adequately mitigate deception, rendering QD algorithms unable to discover the best-performing solutions on difficult problems. This paper introduces a solution that enables QD algorithms to pursue arbitrary notions of diversity without compromising their ability to solve hard problems: driving search with multiple BCs simultaneously.},
  file = {Pugh et al. - 2016 - Searching for Quality Diversity When Diversity is .pdf},
  isbn = {978-3-319-45822-9 978-3-319-45823-6},
  language = {en}
}

@article{Pugh2016a,
  title = {Quality {{Diversity}}: {{A New Frontier}} for {{Evolutionary Computation}}},
  shorttitle = {Quality {{Diversity}}},
  author = {Pugh, Justin K. and Soros, Lisa B. and Stanley, Kenneth O.},
  year = {2016},
  month = jul,
  volume = {3},
  issn = {2296-9144},
  doi = {10.3389/frobt.2016.00040},
  abstract = {While evolutionary computation and evolutionary robotics take inspiration from nature, they have long focused mainly on problems of performance optimization. Yet, evolution in nature can be interpreted as more nuanced than a process of simple optimization. In particular, natural evolution is a divergent search that optimizes locally within each niche as it simultaneously diversifies. This tendency to discover both quality and diversity at the same time differs from many of the conventional algorithms of machine learning, and also thereby suggests a different foundation for inferring the approach of greatest potential for evolutionary algorithms. In fact, several recent evolutionary algorithms called quality diversity (QD) algorithms (e.g., novelty search with local competition and MAP-Elites) have drawn inspiration from this more nuanced view, aiming to fill a space of possibilities with the best possible example of each type of achievable behavior. The result is a new class of algorithms that return an archive of diverse, high-quality behaviors in a single run. The aim in this paper is to study the application of QD algorithms in challenging environments (in particular complex mazes) to establish their best practices for ambitious domains in the future. In addition to providing insight into cases when QD succeeds and fails, a new approach is investigated that hybridizes multiple views of behaviors (called behavior characterizations) in the same run, which succeeds in overcoming some of the challenges associated with searching for QD with respect to a behavior characterization that is not necessarily sufficient for generating both quality and diversity at the same time.},
  file = {Pugh et al. - 2016 - Quality Diversity A New Frontier for Evolutionary.pdf},
  journal = {Front. Robot. AI},
  language = {en}
}

@article{Putney,
  title = {Precise Timing Is Ubiquitous, Consistent, and Coordinated across a Comprehensive, Spike-Resolved Flight Motor Program},
  author = {Putney, Joy and Conn, Rachel and Sponberg, Simon},
  pages = {10},
  file = {Putney et al. - Precise timing is ubiquitous, consistent, and coor.pdf},
  journal = {COMPUTATIONAL BIOLOGY},
  language = {en}
}

@article{Pyke2015,
  title = {Understanding Movements of Organisms: It's Time to Abandon the {{L\'evy}} Foraging Hypothesis},
  shorttitle = {Understanding Movements of Organisms},
  author = {Pyke, Graham H.},
  editor = {Giuggioli, Luca},
  year = {2015},
  month = jan,
  volume = {6},
  pages = {1--16},
  issn = {2041210X},
  doi = {10.1111/2041-210X.12298},
  file = {Pyke - 2015 - Understanding movements of organisms it's time to.pdf},
  journal = {Methods Ecol Evol},
  language = {en},
  number = {1}
}

@article{Qi2013,
  title = {Firing Patterns in a Conductance-Based Neuron Model: Bifurcation, Phase Diagram, and Chaos},
  shorttitle = {Firing Patterns in a Conductance-Based Neuron Model},
  author = {Qi, Y. and Watts, A. L. and Kim, J. W. and Robinson, P. A.},
  year = {2013},
  month = feb,
  volume = {107},
  pages = {15--24},
  issn = {0340-1200, 1432-0770},
  doi = {10.1007/s00422-012-0520-8},
  abstract = {Responding to various stimuli, some neurons either remain resting or can fire several distinct patterns of action potentials, such as spiking, bursting, subthreshold oscillations, and chaotic firing. In particular, Wilson's conductance-based neocortical neuron model, derived from the Hodgkin\textendash Huxley model, is explored to understand underlying mechanisms of the firing patterns. Phase diagrams describing boundaries between the domains of different firing patterns are obtained via extensive numerical computations. The boundaries are further studied by standard instability analyses, which demonstrates that the chaotic neural firing could develop via period-doubling and/or periodadding cascades. Sequences of the firing patterns often observed in many neural experiments are also discussed in the phase diagram framework developed. Our results lay the groundwork for wider use of the model, especially for incorporating it into neural field modeling of the brain.},
  file = {2013 - Qi et al. - Firing patterns in a conductance-based neuron model bifurcation, phase diagram, and chaos.pdf},
  journal = {Biological Cybernetics},
  language = {en},
  number = {1}
}

@article{Qi2020,
  title = {Using Machine Learning to Predict Extreme Events in Complex Systems},
  author = {Qi, Di and Majda, Andrew J.},
  year = {2020},
  month = jan,
  volume = {117},
  pages = {52--59},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1917285117},
  abstract = {Extreme events and the related anomalous statistics are ubiquitously observed in many natural systems, and the development of efficient methods to understand and accurately predict such representative features remains a grand challenge. Here, we investigate the skill of deep learning strategies in the prediction of extreme events in complex turbulent dynamical systems. Deep neural networks have been successfully applied to many imaging processing problems involving big data, and have recently shown potential for the study of dynamical systems. We propose to use a densely connected mixed-scale network model to capture the extreme events appearing in a truncated Korteweg\textendash de Vries (tKdV) statistical framework, which creates anomalous skewed distributions consistent with recent laboratory experiments for shallow water waves across an abrupt depth change, where a remarkable statistical phase transition is generated by varying the inverse temperature parameter in the corresponding Gibbs invariant measures. The neural network is trained using data without knowing the explicit model dynamics, and the training data are only drawn from the near-Gaussian regime of the tKdV model solutions without the occurrence of large extreme values. A relative entropy loss function, together with empirical partition functions, is proposed for measuring the accuracy of the network output where the dominant structures in the turbulent field are emphasized. The optimized network is shown to gain uniformly high skill in accurately predicting the solutions in a wide variety of statistical regimes, including highly skewed extreme events. The technique is promising to be further applied to other complicated high-dimensional systems.},
  file = {Qi and Majda - 2020 - Using machine learning to predict extreme events i.pdf},
  journal = {Proc Natl Acad Sci USA},
  language = {en},
  number = {1}
}

@article{Quinn,
  title = {Within-Cycle Instantaneous Frequency Profiles Report Oscillatory Waveform Dynamics.},
  author = {Quinn, Andrew J and {Lopes-dos-Santos}, V{\'i}tor and Huang, Norden and Liang, Wei-Kuang and Juan, Chi-Hung and Yeh, Jia-Rong and Nobre, Anna C and Dupret, David and Woolrich, Mark W},
  pages = {40},
  abstract = {Non-sinusoidal waveform is emerging as an important feature of neuronal oscillations. However, the role of single cycle shape dynamics in rapidly unfolding brain activity remains unclear. Here, we develop an analytical framework that isolates oscillatory signals from time-series using masked Empirical Mode Decomposition to quantify dynamical changes in the shape of individual cycles (along with amplitude, frequency and phase) using instantaneous frequency. We show how phase-alignment, a process of projecting cycles into a regularly sampled phase-grid space, makes it possible to compare cycles of different durations and shapes. `Normalised shapes' can then be constructed with high temporal detail whilst accounting for differences in both duration and amplitude. We find that the instantaneous frequency tracks non-sinusoidal shapes in both simulated and real data. Notably, in local field potential recordings of mouse hippocampal CA1, we find that theta oscillations have a stereotyped slow-descending slope in the cycle-wise average, yet exhibiting high variability on a cycle-by-cycle basis. We show how Principal Components Analysis allows identification of motifs of theta cycle waveform that have distinct associations to cycle amplitude, cycle duration and animal movement speed. By allowing investigation into oscillation shape at high temporal resolution, this analytical framework will open new lines of enquiry into how neuronal oscillations support moment-by-moment information processing and integration in brain networks.},
  file = {Quinn et al. - Within-cycle instantaneous frequency profiles repo.pdf},
  language = {en}
}

@article{Quinn2011,
  title = {Data Assimilation Using a {{GPU}} Accelerated Path Integral {{Monte Carlo}} Approach},
  author = {Quinn, John C. and Abarbanel, Henry D.I.},
  year = {2011},
  month = sep,
  volume = {230},
  pages = {8168--8178},
  issn = {00219991},
  doi = {10.1016/j.jcp.2011.07.015},
  abstract = {The answers to data assimilation questions can be expressed as path integrals over all possible state and parameter histories. We show how these path integrals can be evaluated numerically using a Markov Chain Monte Carlo method designed to run in parallel on a graphics processing unit (GPU). We demonstrate the application of the method to an example with a transmembrane voltage time series of a simulated neuron as an input, and using a Hodgkin\textendash Huxley neuron model. By taking advantage of GPU computing, we gain a parallel speedup factor of up to about 300, compared to an equivalent serial computation on a CPU, with performance increasing as the length of the observation time used for data assimilation increases.},
  file = {2011 - Quinn, Abarbanel - Data assimilation using a GPU accelerated path integral Monte Carlo approach.pdf},
  journal = {Journal of Computational Physics},
  language = {en},
  number = {22}
}

@article{Rabinovich2001,
  title = {Dynamical {{Encoding}} by {{Networks}} of {{Competing Neuron Groups}}: {{Winnerless Competition}}},
  shorttitle = {Dynamical {{Encoding}} by {{Networks}} of {{Competing Neuron Groups}}},
  author = {Rabinovich, M. and Volkovskii, A. and Lecanda, P. and Huerta, R. and Abarbanel, H. D. I. and Laurent, G.},
  year = {2001},
  month = jul,
  volume = {87},
  pages = {068102},
  issn = {0031-9007, 1079-7114},
  doi = {10.1103/PhysRevLett.87.068102},
  file = {Rabinovich et al. - 2001 - Dynamical Encoding by Networks of Competing Neuron.pdf},
  journal = {Phys. Rev. Lett.},
  language = {en},
  number = {6}
}

@article{Radulescu2019,
  title = {Holistic {{Reinforcement Learning}}: {{The Role}} of {{Structure}} and {{Attention}}},
  shorttitle = {Holistic {{Reinforcement Learning}}},
  author = {Radulescu, Angela and Niv, Yael and Ballard, Ian},
  year = {2019},
  month = apr,
  volume = {23},
  pages = {278--292},
  issn = {13646613},
  doi = {10.1016/j.tics.2019.01.010},
  file = {Radulescu et al. - 2019 - Holistic Reinforcement Learning The Role of Struc.pdf},
  journal = {Trends in Cognitive Sciences},
  language = {en},
  number = {4}
}

@article{Rafeh2020,
  title = {Information-{{Limiting Correlations}} in {{Neural Populations}}: {{The Devil Is}} in the {{Details}}},
  shorttitle = {Information-{{Limiting Correlations}} in {{Neural Populations}}},
  author = {Rafeh, Reebal and Gupta, Geetika},
  year = {2020},
  month = oct,
  volume = {40},
  pages = {7782--7784},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.0917-20.2020},
  file = {Rafeh and Gupta - 2020 - Information-Limiting Correlations in Neural Popula.pdf},
  journal = {J. Neurosci.},
  language = {en},
  number = {41}
}

@article{Rafols1976,
  title = {The Neurons in the Primate Subthalamic Nucleus: {{A Golgi}} and Electron Microscopic Study},
  shorttitle = {The Neurons in the Primate Subthalamic Nucleus},
  author = {Rafols, Jos{\dbend} A. and Fox, Clement A.},
  year = {1976},
  month = jul,
  volume = {168},
  pages = {75--111},
  issn = {0021-9967, 1096-9861},
  doi = {10.1002/cne.901680105},
  abstract = {In Golgi preparations of the adult monkey (Macaca mulatta) local interneurons and two varieties of principal neurons, radiating and elongated fusiform, are found in the subthalamic nucleus. The cell bodies of the radiating neurons have a few delicate, somatic spines some of which are occasionally bilobed and trilobed. Five to eight dendritic trunks give rise to branching, tapering dendrites, which may extend for over 400 microns. These dendrites are much thinner than the dendrites in the globus pallidus and the substantia nigra. Some neurons have many and some neurons have few dendritic spines. When numerous the dendritic spines are concentrated on the dendritic trunks and proximal dendrites. The relatively few elongated fusiform neurons are found not only in the capsule but also i n the center of the nucleus. Most dendrites emerge from the opposite poles of their smooth surfaced cell bodies. They have a few dendritic spines. Some of these dendrites extend for more than 750 microns. In 1-micron thick plastic sections lipofuscin granules are present i n some but not all principal neuron cell bodies of the monkey (Macaca mulatta); but these granules are present i n all principal neuron cell bodies of the pig-tail monkey (Macaca nemestrina) and of the squirrel monkey ( S a i m i r i sciureus).},
  file = {1976 - Rafols, Fox - The neurons in the primate subthalamic nucleus a Golgi and electron microscopic study.pdf;Rafols and Fox - 1976 - The neurons in the primate subthalamic nucleus A .pdf},
  journal = {The Journal of Comparative Neurology},
  language = {en},
  number = {1}
}

@article{Raghavan,
  title = {Neural Networks Grown and Self-Organized by Noise},
  author = {Raghavan, Guruprasad and Thomson, Matt},
  pages = {11},
  abstract = {Living neural networks emerge through a process of growth and self-organization that begins with a single cell and results in a brain, an organized and functional computational device. Artificial neural networks, however, rely on human-designed, hand-programmed architectures for their remarkable performance. Can we develop artificial computational devices that can grow and self-organize without human intervention? In this paper, we propose a biologically inspired developmental algorithm that can `grow' a functional, layered neural network from a single initial cell. The algorithm organizes inter-layer connections to construct retinotopic pooling layers. Our approach is inspired by the mechanisms employed by the early visual system to wire the retina to the lateral geniculate nucleus (LGN), days before animals open their eyes. The key ingredients for robust self-organization are an emergent spontaneous spatiotemporal activity wave in the first layer and a local learning rule in the second layer that `learns' the underlying activity pattern in the first layer. The algorithm is adaptable to a wide-range of input-layer geometries, robust to malfunctioning units in the first layer, and so can be used to successfully grow and self-organize pooling architectures of different pool-sizes and shapes. The algorithm provides a primitive procedure for constructing layered neural networks through growth and self-organization. We also demonstrate that networks grown from a single unit perform as well as hand-crafted networks on MNIST. Broadly, our work shows that biologically inspired developmental algorithms can be applied to autonomously grow functional `brains' in-silico.},
  file = {Raghavan and Thomson - Neural networks grown and self-organized by noise.pdf},
  language = {en}
}

@article{Raghu,
  title = {Can {{Deep Reinforcement Learning Solve Erdos}}-{{Selfridge}}-{{Spencer Games}}?},
  author = {Raghu, Maithra and Irpan, Alex and Andreas, Jacob and Kleinberg, Robert and Le, Quoc and Kleinberg, Jon},
  pages = {13},
  abstract = {Deep reinforcement learning has achieved many recent successes, but our understanding of its strengths and limitations is hampered by the lack of rich environments in which we can fully characterize optimal behavior, and correspondingly diagnose individual actions against such a characterization. Here we consider a family of combinatorial games, arising from work of Erdos, Selfridge, and Spencer, and we propose their use as environments for evaluating and comparing different approaches to reinforcement learning. These games have a number of appealing features: they are challenging for current learning approaches, but they form (i) a low-dimensional, simply parametrized environment where (ii) there is a linear closed form solution for optimal behavior from any state, and (iii) the difficulty of the game can be tuned by changing environment parameters in an interpretable way. We use these Erdos-Selfridge-Spencer games not only to compare different algorithms, but test for generalization, make comparisons to supervised learning, analyze multiagent play, and even develop a self play algorithm.},
  file = {Raghu et al. - Can Deep Reinforcement Learning Solve Erdos-Selfri.pdf},
  language = {en}
}

@article{Raghu2020,
  title = {Rapid {{Learning}} or {{Feature Reuse}}? {{Towards Understanding}} the {{Effectiveness}} of {{MAML}}},
  shorttitle = {Rapid {{Learning}} or {{Feature Reuse}}?},
  author = {Raghu, Aniruddh and Raghu, Maithra and Bengio, Samy and Vinyals, Oriol},
  year = {2020},
  month = feb,
  abstract = {An important research direction in machine learning has centered around developing meta-learning algorithms to tackle few-shot learning. An especially successful algorithm has been Model Agnostic Meta-Learning (MAML), a method that consists of two optimization loops, with the outer loop finding a meta-initialization, from which the inner loop can efficiently learn new tasks. Despite MAML's popularity, a fundamental open question remains \textendash{} is the effectiveness of MAML due to the meta-initialization being primed for rapid learning (large, efficient changes in the representations) or due to feature reuse, with the meta-initialization already containing high quality features? We investigate this question, via ablation studies and analysis of the latent representations, finding that feature reuse is the dominant factor. This leads to the ANIL (Almost No Inner Loop) algorithm, a simplification of MAML where we remove the inner loop for all but the (task-specific) head of the underlying neural network. ANIL matches MAML's performance on benchmark few-shot image classification and RL and offers computational improvements over MAML. We further study the precise contributions of the head and body of the network, showing that performance on the test tasks is entirely determined by the quality of the learned features, and we can remove even the head of the network (the NIL algorithm). We conclude with a discussion of the rapid learning vs feature reuse question for meta-learning algorithms more broadly.},
  archiveprefix = {arXiv},
  eprint = {1909.09157},
  eprinttype = {arxiv},
  file = {Raghu et al. - 2020 - Rapid Learning or Feature Reuse Towards Understan.pdf},
  journal = {arXiv:1909.09157 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryclass = {cs, stat}
}

@article{Raghu2020a,
  title = {Rapid {{Learning}} or {{Feature Reuse}}? {{Towards Understanding}} the {{Effectiveness}} of {{MAML}}},
  shorttitle = {Rapid {{Learning}} or {{Feature Reuse}}?},
  author = {Raghu, Aniruddh and Raghu, Maithra and Bengio, Samy and Vinyals, Oriol},
  year = {2020},
  month = feb,
  abstract = {An important research direction in machine learning has centered around developing meta-learning algorithms to tackle few-shot learning. An especially successful algorithm has been Model Agnostic Meta-Learning (MAML), a method that consists of two optimization loops, with the outer loop finding a meta-initialization, from which the inner loop can efficiently learn new tasks. Despite MAML's popularity, a fundamental open question remains \textendash{} is the effectiveness of MAML due to the meta-initialization being primed for rapid learning (large, efficient changes in the representations) or due to feature reuse, with the meta-initialization already containing high quality features? We investigate this question, via ablation studies and analysis of the latent representations, finding that feature reuse is the dominant factor. This leads to the ANIL (Almost No Inner Loop) algorithm, a simplification of MAML where we remove the inner loop for all but the (task-specific) head of the underlying neural network. ANIL matches MAML's performance on benchmark few-shot image classification and RL and offers computational improvements over MAML. We further study the precise contributions of the head and body of the network, showing that performance on the test tasks is entirely determined by the quality of the learned features, and we can remove even the head of the network (the NIL algorithm). We conclude with a discussion of the rapid learning vs feature reuse question for meta-learning algorithms more broadly.},
  archiveprefix = {arXiv},
  eprint = {1909.09157},
  eprinttype = {arxiv},
  file = {Raghu et al. - 2020 - Rapid Learning or Feature Reuse Towards Understan 2.pdf},
  journal = {arXiv:1909.09157 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryclass = {cs, stat}
}

@article{Rahnev2011,
  title = {Prior {{Expectation Modulates}} the {{Interaction}} between {{Sensory}} and {{Prefrontal Regions}} in the {{Human Brain}}},
  author = {Rahnev, D. and Lau, H. and {de Lange}, F. P.},
  year = {2011},
  month = jul,
  volume = {31},
  pages = {10741--10748},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.1478-11.2011},
  file = {2011 - Rahnev, Lau, de Lange - Prior expectation modulates the interaction between sensory and prefrontal regions in the human brain.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {29}
}

@techreport{Rahnev2016,
  title = {Suboptimality in {{Perceptual Decision Making}}},
  author = {Rahnev, Dobromir and Denison, Rachel N},
  year = {2016},
  month = jun,
  institution = {{Neuroscience}},
  doi = {10.1101/060194},
  abstract = {Human perceptual decisions are often described as optimal. Critics of this view have argued that claims of optimality are overly flexible and lack explanatory power. Meanwhile, advocates for optimality have countered that such criticisms single out a few selected papers. To elucidate the issue of optimality in perceptual decision making, we review the extensive literature on suboptimal performance in perceptual tasks. We discuss eight different classes of suboptimal perceptual decisions, including improper placement, maintenance, and adjustment of perceptual criteria, inadequate tradeoff between speed and accuracy, inappropriate confidence ratings, misweightings in cue combination, and findings related to various perceptual illusions and biases. In addition, we discuss conceptual shortcomings of a focus on optimality, such as definitional difficulties and the limited value of optimality claims in and of themselves. We therefore advocate that the field drop its emphasis on whether observed behavior is optimal and instead concentrate on building and testing detailed observer models that explain behavior across a wide range of tasks. To facilitate this transition, we compile the proposed hypotheses regarding the origins of suboptimal perceptual decisions reviewed here. We argue that verifying, rejecting, and expanding these explanations for suboptimal behavior - rather than assessing optimality per se - should be among the major goals of the science of perceptual decision making.},
  file = {Rahnev and Denison - 2016 - Suboptimality in Perceptual Decision Making.pdf},
  language = {en},
  type = {Preprint}
}

@article{Rahnev2018,
  title = {Suboptimality in Perceptual Decision Making},
  author = {Rahnev, Dobromir and Denison, Rachel N.},
  year = {2018},
  volume = {41},
  pages = {e223},
  issn = {0140-525X, 1469-1825},
  doi = {10.1017/S0140525X18000936},
  abstract = {To deny that human perception is optimal is not to claim that it is suboptimal. Rahnev \& Denison point out that optimality is often ill-defined. The fundamental issue is framing perception as a statistical inference problem. Outside the lab, the real perceptual challenge is to determine the lawful structure of the world, not variables of a predetermined statistical model.},
  file = {Rahnev and Denison - 2018 - Suboptimality in perceptual decision making.pdf},
  journal = {Behavioral and Brain Sciences},
  language = {en}
}

@article{Raichlen2014,
  title = {Evidence of {{Levy}} Walk Foraging Patterns in Human Hunter-Gatherers},
  author = {Raichlen, D. A. and Wood, B. M. and Gordon, A. D. and Mabulla, A. Z. P. and Marlowe, F. W. and Pontzer, H.},
  year = {2014},
  month = jan,
  volume = {111},
  pages = {728--733},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1318616111},
  file = {Raichlen et al. - 2014 - Evidence of Levy walk foraging patterns in human h.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {2}
}

@article{Raizada,
  title = {What {{Makes Different People's Representations Alike}}: {{Neural Similarity Space Solves}} the {{Problem}} of {{Across}}-Subject {{fMRI Decoding}}},
  author = {Raizada, Rajeev D S and Connolly, Andrew C},
  volume = {24},
  pages = {10},
  file = {2012 - Raizada, Connolly - What Makes Different Peopleʼs Representations Alike Neural Similarity Space Solves the Problem of Across-su.pdf},
  language = {en},
  number = {4}
}

@article{Raizada2010,
  title = {Linking Brain-Wide Multivoxel Activation Patterns to Behaviour: {{Examples}} from Language and Math},
  shorttitle = {Linking Brain-Wide Multivoxel Activation Patterns to Behaviour},
  author = {Raizada, Rajeev D.S. and Tsao, Feng-Ming and Liu, Huei-Mei and Holloway, Ian D. and Ansari, Daniel and Kuhl, Patricia K.},
  year = {2010},
  month = may,
  volume = {51},
  pages = {462--471},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2010.01.080},
  abstract = {A key goal of cognitive neuroscience is to find simple and direct connections between brain and behaviour. However, fMRI analysis typically involves choices between many possible options, with each choice potentially biasing any brain\textendash behaviour correlations that emerge. Standard methods of fMRI analysis assess each voxel individually, but then face the problem of selection bias when combining those voxels into a region-of-interest, or ROI. Multivariate pattern-based fMRI analysis methods use classifiers to analyse multiple voxels together, but can also introduce selection bias via data-reduction steps as feature selection of voxels, pre-selecting activated regions, or principal components analysis. We show here that strong brain\textendash behaviour links can be revealed without any voxel selection or data reduction, using just plain linear regression as a classifier applied to the whole brain at once, i.e. treating each entire brain volume as a single multi-voxel pattern. The brain\textendash behaviour correlations emerged despite the fact that the classifier was not provided with any information at all about subjects' behaviour, but instead was given only the neural data and its condition-labels. Surprisingly, more powerful classifiers such as a linear SVM and regularised logistic regression produce very similar results. We discuss some possible reasons why the very simple brain-wide linear regression model is able to find correlations with behaviour that are as strong as those obtained on the one hand from a specific ROI and on the other hand from more complex classifiers. In a manner which is unencumbered by arbitrary choices, our approach offers a method for investigating connections between brain and behaviour which is simple, rigorous and direct.},
  file = {2010 - Raizada et al. - Linking brain-wide multivoxel activation patterns to behaviour Examples from language and math.pdf},
  journal = {NeuroImage},
  language = {en},
  number = {1}
}

@article{Rajan2006,
  title = {Eigenvalue {{Spectra}} of {{Random Matrices}} for {{Neural Networks}}},
  author = {Rajan, Kanaka and Abbott, L. F.},
  year = {2006},
  month = nov,
  volume = {97},
  issn = {0031-9007, 1079-7114},
  doi = {10.1103/PhysRevLett.97.188104},
  file = {2006 - Rajan, Abbott - Eigenvalue spectra of random matrices for neural networks.pdf},
  journal = {Physical Review Letters},
  language = {en},
  number = {18}
}

@article{Rajan2016,
  title = {Recurrent {{Network Models}} of {{Sequence Generation}} and {{Memory}}},
  author = {Rajan, Kanaka and Harvey, Christopher D. and Tank, David W.},
  year = {2016},
  month = apr,
  volume = {90},
  pages = {128--142},
  issn = {08966273},
  doi = {10.1016/j.neuron.2016.02.009},
  abstract = {Sequential activation of neurons is a common feature of network activity during a variety of behaviors, including working memory and decision making. Previous network models for sequences and memory emphasized specialized architectures in which a principled mechanism is pre-wired into their connectivity. Here we demonstrate that, starting from random connectivity and modifying a small fraction of connections, a largely disordered recurrent network can produce sequences and implement working memory efficiently. We use this process, called Partial In-Network Training (PINning), to model and match cellular resolution imaging data from the posterior parietal cortex during a virtual memoryguided two-alternative forced-choice task. Analysis of the connectivity reveals that sequences propagate by the cooperation between recurrent synaptic interactions and external inputs, rather than through feedforward or asymmetric connections. Together our results suggest that neural sequences may emerge through learning from largely unstructured network architectures.},
  file = {Rajan et al. - 2016 - Recurrent Network Models of Sequence Generation an.pdf},
  journal = {Neuron},
  language = {en},
  number = {1}
}

@article{Ramanujan2019,
  title = {What's {{Hidden}} in a {{Randomly Weighted Neural Network}}?},
  author = {Ramanujan, Vivek and Wortsman, Mitchell and Kembhavi, Aniruddha and Farhadi, Ali and Rastegari, Mohammad},
  year = {2019},
  month = nov,
  abstract = {Training a neural network is synonymous with learning the values of the weights. In contrast, we demonstrate that randomly weighted neural networks contain subnetworks which achieve impressive performance without ever training the weight values. Hidden in a randomly weighted Wide ResNet-50 [28] we show that there is a subnetwork (with random weights) that is smaller than, but matches the performance of a ResNet-34 [8] trained on ImageNet [3]. Not only do these ``untrained subnetworks'' exist, but we provide an algorithm to effectively find them. We empirically show that as randomly weighted neural networks with fixed weights grow wider and deeper, an ``untrained subnetwork'' approaches a network with learned weights in accuracy.},
  archiveprefix = {arXiv},
  eprint = {1911.13299},
  eprinttype = {arxiv},
  file = {Ramanujan et al. - 2019 - What's Hidden in a Randomly Weighted Neural Networ.pdf},
  journal = {arXiv:1911.13299 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  language = {en},
  primaryclass = {cs}
}

@article{Ramanujan2020,
  title = {What's {{Hidden}} in a {{Randomly Weighted Neural Network}}?},
  author = {Ramanujan, Vivek and Wortsman, Mitchell and Kembhavi, Aniruddha and Farhadi, Ali and Rastegari, Mohammad},
  year = {2020},
  month = mar,
  abstract = {Training a neural network is synonymous with learning the values of the weights. In contrast, we demonstrate that randomly weighted neural networks contain subnetworks which achieve impressive performance without ever training the weight values. Hidden in a randomly weighted Wide ResNet-50 [28] we show that there is a subnetwork (with random weights) that is smaller than, but matches the performance of a ResNet-34 [8] trained on ImageNet [3]. Not only do these ``untrained subnetworks'' exist, but we provide an algorithm to effectively find them. We empirically show that as randomly weighted neural networks with fixed weights grow wider and deeper, an ``untrained subnetwork'' approaches a network with learned weights in accuracy.},
  archiveprefix = {arXiv},
  eprint = {1911.13299},
  eprinttype = {arxiv},
  file = {Ramanujan et al. - 2020 - What's Hidden in a Randomly Weighted Neural Networ.pdf},
  journal = {arXiv:1911.13299 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  language = {en},
  primaryclass = {cs}
}

@article{Ramos-Fernandez,
  title = {4 {{L\'evy}} Walk Patterns in the Foraging Movements of Spider Monkeys ({{Ateles}} Geoffroyi)},
  author = {{Ramos-Fern{\'a}ndez}, Gabriel and Mateos, Jos{\'e} Luis and Miramontes, Octavio},
  pages = {25},
  file = {Ramos-Fernández et al. - 4 Lévy walk patterns in the foraging movements of .pdf},
  language = {en}
}

@article{Ramscar2014,
  title = {The {{Myth}} of {{Cognitive Decline}}: {{Non}}-{{Linear Dynamics}} of {{Lifelong Learning}}},
  shorttitle = {The {{Myth}} of {{Cognitive Decline}}},
  author = {Ramscar, Michael and Hendrix, Peter and Shaoul, Cyrus and Milin, Petar and Baayen, Harald},
  year = {2014},
  month = jan,
  volume = {6},
  pages = {5--42},
  issn = {17568757},
  doi = {10.1111/tops.12078},
  abstract = {As adults age, their performance on many psychometric tests changes systematically, a finding that is widely taken to reveal that cognitive information-processing capacities decline across adulthood. Contrary to this, we suggest that older adults' changing performance reflects memory search demands, which escalate as experience grows. A series of simulations show how the performance patterns observed across adulthood emerge naturally in learning models as they acquire knowledge. The simulations correctly identify greater variation in the cognitive performance of older adults, and successfully predict that older adults will show greater sensitivity to fine-grained differences in the properties of test stimuli than younger adults. Our results indicate that older adults' performance on cognitive tests reflects the predictable consequences of learning on informationprocessing, and not cognitive decline. We consider the implications of this for our scientific and cultural understanding of aging.},
  file = {Ramscar et al. - 2014 - The Myth of Cognitive Decline Non-Linear Dynamics.pdf},
  journal = {Topics in Cognitive Science},
  language = {en},
  number = {1}
}

@article{Rands2003,
  title = {Spontaneous Emergence of Leaders and Followers in Foraging Pairs},
  author = {Rands, Sean A. and Cowlishaw, Guy and Pettifor, Richard A. and Rowcliffe, J. Marcus and Johnstone, Rufus A.},
  year = {2003},
  month = may,
  volume = {423},
  pages = {432--434},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature01630},
  file = {Rands et al. - 2003 - Spontaneous emergence of leaders and followers in .pdf},
  journal = {Nature},
  language = {en},
  number = {6938}
}

@techreport{Rashid2020,
  title = {The Dendritic Spatial Code: Branch-Specific Place Tuning and Its Experience-Dependent Decoupling},
  shorttitle = {The Dendritic Spatial Code},
  author = {Rashid, Shannon K. and Pedrosa, Victor and Dufour, Martial A. and Moore, Jason J. and Chavlis, Spyridon and Delatorre, Rodrigo G. and Poirazi, Panayiota and Clopath, Claudia and Basu, Jayeeta},
  year = {2020},
  month = jan,
  institution = {{Neuroscience}},
  doi = {10.1101/2020.01.24.916643},
  abstract = {Abstract                        Dendrites of pyramidal neurons integrate different sensory inputs, and non-linear dendritic computations drive feature selective tuning and plasticity. Yet little is known about how dendrites themselves represent the environment, the degree to which they are coupled to their soma, and how that coupling is sculpted with experience. In order to answer these questions, we developed a novel preparation in which we image soma and connected dendrites in a single plane across days using             in vivo             two-photon microscopy. Using this preparation, we monitored spatially tuned activity in area CA3 of the hippocampus in head-fixed mice running on a linear track. We identified ``place dendrites'', which can stably and precisely represent both familiar and novel spatial environments. Dendrites could display place tuning independent of their connected soma and even their sister dendritic branches, the first evidence for branch-specific tuning in the hippocampus. In a familiar environment, spatially tuned somata were more decoupled from their dendrites as compared to non-tuned somata. This relationship was absent in a novel environment, suggesting an experience dependent selective gating of dendritic spatial inputs. We then built a data-driven multicompartment computational model that could capture the experimentally observed correlations. Our model predicts that place cells exhibiting branch-specific tuning have more flexible place fields, while neurons with homogenous or co-tuned dendritic branches have higher place field stability. These findings demonstrate that spatial representation is organized in a branch-specific manner within dendrites of hippocampal pyramidal cells. Further, spatial inputs from dendrites to soma are selectively and dynamically gated in an experience-dependent manner, endowing both flexibility and stability to the cognitive map of space.                                   One sentence summary             Hippocampal pyramidal cells show branch-specific tuning for different place fields, and their coupling to their soma changes with experience of an environment.},
  file = {Rashid et al. - 2020 - The dendritic spatial code branch-specific place .pdf},
  language = {en},
  type = {Preprint}
}

@article{Rasmussen,
  title = {A Neural Reinforcement Learning Model for Tasks with Unknown Time Delays},
  author = {Rasmussen, Daniel and Eliasmith, Chris},
  pages = {6},
  abstract = {We present a biologically based neural model capable of performing reinforcement learning in complex tasks. The model is unique in its ability to solve tasks that require the agent to make a sequence of unrewarded actions in order to reach the goal, in an environment where there are unknown and variable time delays between actions, state transitions, and rewards. Specifically, this is the first neural model of reinforcement learning able to function within a Semi-Markov Decision Process (SMDP) framework. We believe that this extension of current modelling efforts lays the groundwork for increasingly sophisticated models of human decision making.},
  file = {1994 - Rasmussen, Eliasmith - A neural reinforcement learning model for tasks with unknown time delays.pdf;1994 - Rasmussen, Eliasmith - A neural reinforcement learning model for tasks with unknown time delays(2).pdf},
  language = {en}
}

@article{Rasmussen2011,
  title = {Visualization of Nonlinear Kernel Models in Neuroimaging by Sensitivity Maps},
  author = {Rasmussen, Peter Mondrup and Madsen, Kristoffer Hougaard and Lund, Torben Ellegaard and Hansen, Lars Kai},
  year = {2011},
  month = apr,
  volume = {55},
  pages = {1120--1131},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2010.12.035},
  abstract = {There is significant current interest in decoding mental states from neuroimages. In this context kernel methods, e.g., support vector machines (SVM) are frequently adopted to learn statistical relations between patterns of brain activation and experimental conditions. In this paper we focus on visualization of such nonlinear kernel models. Specifically, we investigate the sensitivity map as a technique for generation of global summary maps of kernel classification models. We illustrate the performance of the sensitivity map on functional magnetic resonance (fMRI) data based on visual stimuli. We show that the performance of linear models is reduced for certain scan labelings/categorizations in this data set, while the nonlinear models provide more flexibility. We show that the sensitivity map can be used to visualize nonlinear versions of kernel logistic regression, the kernel Fisher discriminant, and the SVM, and conclude that the sensitivity map is a versatile and computationally efficient tool for visualization of nonlinear kernel models in neuroimaging. \textcopyright{} 2010 Elsevier Inc. All rights reserved.},
  file = {2011 - Rasmussen et al. - Visualization of nonlinear kernel models in neuroimaging by sensitivity maps.pdf},
  journal = {NeuroImage},
  language = {en},
  number = {3}
}

@article{Rasmussena,
  title = {The {{Infinite Gaussian Mixture Model}}},
  author = {Rasmussen, Carl Edward},
  pages = {7},
  abstract = {In a Bayesian mixture model it is not necessary a priori to limit the number of components to be finite. In this paper an infinite Gaussian mixture model is presented which neatly sidesteps the difficult problem of finding the ``right'' number of mixture components. Inference in the model is done using an efficient parameter-free Markov Chain that relies entirely on Gibbs sampling.},
  file = {2000 - Rasmussen - The Infinite Gaussian Mixture Model.pdf},
  language = {en}
}

@article{Ratcliff,
  title = {Retrieval {{Processes}} in {{Recognition Memory}}},
  author = {Ratcliff, Roger and Murdock, Bennet B},
  pages = {25},
  file = {1976 - Ratcliff, Murdock - Retrieval processes in recognition memory.pdf},
  language = {en}
}

@article{Rattay1989,
  title = {Analysis of Models for Extracellular Fiber Stimulation},
  author = {Rattay, F.},
  year = {1989},
  month = jul,
  volume = {36},
  pages = {676--682},
  issn = {00189294},
  doi = {10.1109/10.32099},
  abstract = {This paper presents the mathematical basis for analysis as well as for the computer simulation of the stimulus/response characteristics of nerve or muscle fibers. The results follow from the extracellular potential along the fiber as a function of electrode geometry. The theory is of a general nature but special investigations are made on monopolar, bipolar, and ring electrodes. Stimulations with monopolar electrodes show better recruitment characteristics than ring electrodes.},
  file = {1989 - Rattay - Analysis of Models for Extracellular Fiber Stimulation.pdf;Rattay - 1989 - Analysis of models for extracellular fiber stimula.pdf},
  journal = {IEEE Transactions on Biomedical Engineering},
  language = {en},
  number = {7}
}

@article{Ravi2018,
  title = {Homeostatic Feedback Modulates the Development of Two-State Patterned Activity in a Model Serotonin Motor Circuit In},
  author = {Ravi, Bhavya and Garcia, Jessica and Collins, Kevin M.},
  year = {2018},
  month = may,
  doi = {10.1101/202507},
  abstract = {Neuron activity accompanies synapse formation and maintenance, but how early circuit activity contributes to behavior development is not well understood. Here, we use the                          egg-laying motor circuit as a model to understand how coordinated cell and circuit activity develops and drives a robust two-state behavior in adults. Using calcium imaging in behaving animals, we find the serotonergic Hermaphrodite Specific Neurons (HSNs) and vulval muscles show rhythmic calcium transients in L4 larvae before eggs are produced. HSN activity in L4 is tonic and lacks the alternating burst-firing/quiescent pattern seen in egg-laying adults. Vulval muscle activity in L4 is initially uncoordinated but becomes synchronous as the anterior and posterior muscle arms meet at HSN synaptic release sites. However, coordinated muscle activity does not require presynaptic HSN input. Using reversible silencing experiments, we show that neuronal and vulval muscle activity in L4 is not required for the onset of adult behavior. Instead, the accumulation of eggs in the adult uterus renders the muscles sensitive to HSN input. Sterilization or acute electrical silencing of the vulval muscles inhibits presynaptic HSN activity, and reversal of muscle silencing triggers a homeostatic increase in HSN activity and egg release that maintains \textasciitilde 12-15 eggs in the uterus. Feedback of egg accumulation depends upon the vulval muscle postsynaptic terminus, suggesting a retrograde signal sustains HSN synaptic activity and egg release. Our results show that egg-laying behavior in                          is driven by a homeostat that scales serotonin motor neuron activity in response to postsynaptic muscle feedback.},
  file = {Ravi et al. - 2018 - Homeostatic feedback modulates the development of .pdf},
  journal = {bioRxiv},
  language = {en}
}

@article{Ray2015,
  title = {Challenges in the Quantification and Interpretation of Spike-{{LFP}} Relationships},
  author = {Ray, Supratim},
  year = {2015},
  month = apr,
  volume = {31},
  pages = {111--118},
  issn = {09594388},
  doi = {10.1016/j.conb.2014.09.004},
  file = {Ray - 2015 - Challenges in the quantification and interpretatio.pdf},
  journal = {Current Opinion in Neurobiology},
  language = {en}
}

@techreport{Razo-Mejia2019,
  title = {First-Principles Prediction of the Information Processing Capacity of a Simple Genetic Circuit},
  author = {{Razo-Mejia}, Manuel and Phillips, Rob},
  year = {2019},
  month = mar,
  institution = {{Biophysics}},
  doi = {10.1101/594325},
  abstract = {Abstract                        Given the stochastic nature of gene expression, genetically identical cells exposed to the same environmental inputs will produce different outputs. This heterogeneity has consequences for how cells are able to survive in changing environments. Recent work has explored the use of information theory as a framework to understand the accuracy with which cells can ascertain the state of their surroundings. Yet the predictive power of these approaches is limited and has not been rigorously tested using precision measurements. To that end, we generate a minimal model for a simple genetic circuit in which all parameter values for the model come from independently published data sets. We then predict the information processing capacity of the genetic circuit for a suite of biophysical parameters such as protein copy number and protein-DNA affinity. We compare these parameter-free predictions with an experimental determination of the information processing capacity of             E. coli             cells, and find that our minimal model accurately captures the experimental data.},
  file = {Razo-Mejia and Phillips - 2019 - First-principles prediction of the information pro.pdf},
  language = {en},
  type = {Preprint}
}

@article{Reato2015,
  title = {Lasting Modulation of in Vitro Oscillatory Activity with Weak Direct Current Stimulation},
  author = {Reato, Davide and Bikson, Marom and Parra, Lucas C.},
  year = {2015},
  month = mar,
  volume = {113},
  pages = {1334--1341},
  issn = {0022-3077, 1522-1598},
  doi = {10.1152/jn.00208.2014},
  file = {Reato et al. - 2015 - Lasting modulation of in vitro oscillatory activit.pdf},
  journal = {Journal of Neurophysiology},
  language = {en},
  number = {5}
}

@article{Reddy2010,
  title = {Reading the Mind's Eye: {{Decoding}} Category Information during Mental Imagery},
  shorttitle = {Reading the Mind's Eye},
  author = {Reddy, Leila and Tsuchiya, Naotsugu and Serre, Thomas},
  year = {2010},
  month = apr,
  volume = {50},
  pages = {818--825},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2009.11.084},
  abstract = {Category information for visually presented objects can be read out from multi-voxel patterns of fMRI activity in ventral\textendash temporal cortex. What is the nature and reliability of these patterns in the absence of any bottom\textendash up visual input, for example, during visual imagery? Here, we first ask how well category information can be decoded for imagined objects and then compare the representations evoked during imagery and actual viewing. In an fMRI study, four object categories (food, tools, faces, buildings) were either visually presented to subjects, or imagined by them. Using pattern classification techniques, we could reliably decode category information (including for non-special categories, i.e., food and tools) from ventral\textendash temporal cortex in both conditions, but only during actual viewing from retinotopic areas. Interestingly, in temporal cortex when the classifier was trained on the viewed condition and tested on the imagery condition, or vice versa, classification performance was comparable to within the imagery condition. The above results held even when we did not use information in the specialized category-selective areas. Thus, the patterns of representation during imagery and actual viewing are in fact surprisingly similar to each other. Consistent with this observation, the maps of ``diagnostic voxels'' (i.e., the classifier weights) for the perception and imagery classifiers were more similar in ventral\textendash temporal cortex than in retinotopic cortex. These results suggest that in the absence of any bottom\textendash up input, cortical back projections can selectively re-activate specific patterns of neural activity.},
  file = {2010 - Reddy, Tsuchiya, Serre - Reading the mind's eye decoding category information during mental imagery.pdf},
  journal = {NeuroImage},
  language = {en},
  number = {2}
}

@article{Reddy2016,
  title = {Infomax Strategies for an Optimal Balance between Exploration and Exploitation},
  author = {Reddy, Gautam and Celani, Antonio and Vergassola, Massimo},
  year = {2016},
  month = jun,
  volume = {163},
  pages = {1454--1476},
  issn = {0022-4715, 1572-9613},
  doi = {10.1007/s10955-016-1521-0},
  abstract = {Proper balance between exploitation and exploration is what makes good decisions, which achieve high rewards like payoff or evolutionary fitness. The Infomax principle postulates that maximization of information directs the function of diverse systems, from living systems to artificial neural networks. While specific applications are successful, the validity of information as a proxy for reward remains unclear. Here, we consider the multi-armed bandit decision problem, which features arms (slot-machines) of unknown probabilities of success and a player trying to maximize cumulative payoff by choosing the sequence of arms to play. We show that an Infomax strategy (Info-p) which optimally gathers information on the highest mean reward among the arms saturates known optimal bounds and compares favorably to existing policies. The highest mean reward considered by Info-p is not the quantity actually needed for the choice of the arm to play, yet it allows for optimal tradeoffs between exploration and exploitation.},
  archiveprefix = {arXiv},
  eprint = {1601.03073},
  eprinttype = {arxiv},
  file = {Reddy et al. - 2016 - Infomax strategies for an optimal balance between .pdf},
  journal = {Journal of Statistical Physics},
  keywords = {Computer Science - Information Theory,Computer Science - Machine Learning,Physics - Data Analysis; Statistics and Probability,Quantitative Biology - Populations and Evolution,Statistics - Machine Learning},
  language = {en},
  number = {6}
}

@article{Rehan2013,
  title = {Modeling and {{Automatic Feedback Control}} of {{Tremor}}: {{Adaptive Estimation}} of {{Deep Brain Stimulation}}},
  shorttitle = {Modeling and {{Automatic Feedback Control}} of {{Tremor}}},
  author = {Rehan, Muhammad and Hong, Keum-Shik},
  editor = {Androulakis, Ioannis P.},
  year = {2013},
  month = apr,
  volume = {8},
  pages = {e62888},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0062888},
  abstract = {This paper discusses modeling and automatic feedback control of (postural and rest) tremor for adaptive-controlmethodology-based estimation of deep brain stimulation (DBS) parameters. The simplest linear oscillator-based tremor model, between stimulation amplitude and tremor, is investigated by utilizing input-output knowledge. Further, a nonlinear generalization of the oscillator-based tremor model, useful for derivation of a control strategy involving incorporation of parametric-bound knowledge, is provided. Using the Lyapunov method, a robust adaptive output feedback control law, based on measurement of the tremor signal from the fingers of a patient, is formulated to estimate the stimulation amplitude required to control the tremor. By means of the proposed control strategy, an algorithm is developed for estimation of DBS parameters such as amplitude, frequency and pulse width, which provides a framework for development of an automatic clinical device for control of motor symptoms. The DBS parameter estimation results for the proposed control scheme are verified through numerical simulations.},
  file = {2013 - Rehan, Hong - Modeling and Automatic Feedback Control of Tremor Adaptive Estimation of Deep Brain Stimulation.pdf;2013 - Rehan, Hong - Modeling and Automatic Feedback Control of Tremor Adaptive Estimation of Deep Brain Stimulation(2).pdf},
  journal = {PLoS ONE},
  language = {en},
  number = {4}
}

@article{Reimann2017,
  title = {Cliques of {{Neurons Bound}} into {{Cavities Provide}} a {{Missing Link}} between {{Structure}} and {{Function}}},
  author = {Reimann, Michael W. and Nolte, Max and Scolamiero, Martina and Turner, Katharine and Perin, Rodrigo and Chindemi, Giuseppe and D{\l}otko, Pawe{\l} and Levi, Ran and Hess, Kathryn and Markram, Henry},
  year = {2017},
  month = jun,
  volume = {11},
  issn = {1662-5188},
  doi = {10.3389/fncom.2017.00048},
  abstract = {The lack of a formal link between neural network structure and its emergent function has hampered our understanding of how the brain processes information. We have now come closer to describing such a link by taking the direction of synaptic transmission into account, constructing graphs of a network that reflect the direction of information flow, and analyzing these directed graphs using algebraic topology. Applying this approach to a local network of neurons in the neocortex revealed a remarkably intricate and previously unseen topology of synaptic connectivity. The synaptic network contains an abundance of cliques of neurons bound into cavities that guide the emergence of correlated activity. In response to stimuli, correlated activity binds synaptically connected neurons into functional cliques and cavities that evolve in a stereotypical sequence toward peak complexity. We propose that the brain processes stimuli by forming increasingly complex functional cliques and cavities.},
  file = {Reimann et al. - 2017 - Cliques of Neurons Bound into Cavities Provide a M 2.pdf;Reimann et al. - 2017 - Cliques of Neurons Bound into Cavities Provide a M.pdf},
  journal = {Frontiers in Computational Neuroscience},
  language = {en}
}

@article{Renart2014,
  title = {Variability in Neural Activity and Behavior},
  author = {Renart, Alfonso and Machens, Christian K},
  year = {2014},
  month = apr,
  volume = {25},
  pages = {211--220},
  issn = {09594388},
  doi = {10.1016/j.conb.2014.02.013},
  file = {Renart and Machens - 2014 - Variability in neural activity and behavior.pdf},
  journal = {Current Opinion in Neurobiology},
  language = {en}
}

@techreport{Rens2020,
  title = {Evidence for Entropy Maximisation in Human Free Choice Behaviour},
  author = {Rens, Natalie and Schwartenbeck, Philipp and Cunnington, Ross and Pezzulo, Giovanni},
  year = {2020},
  month = dec,
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/3ntku},
  abstract = {The freedom to choose between options is strongly linked to notions of free will. Accordingly, several studies have shown that individuals demonstrate a preference for choice, or the availability of multiple options, over and above utilitarian value. Yet we lack a decision-making framework that integrates preference for choice with traditional utility maximisation in free choice behaviour. Here we test the predictions of an active inference model of decision-making in which an agent actively seeks states yielding entropy (availability of options) in addition to utility (economic reward). We designed a study in which participants freely navigated a virtual environment consisting of two consecutive choices leading to reward locations in separate rooms. Critically, the choice of one room always led to two final doors while, in the second room, only one door was permissible to choose. This design allowed us to separately determine the influence of utility and entropy on participants' choice behaviour and their self-evaluation of free will. We found that choice behaviour was better predicted by an inference-based model than by expected utility alone, and that both the availability of options and the value of the context positively influenced participants' perceived freedom of choice. Moreover, this consideration of options was apparent in the ongoing motion dynamics as individuals navigated the environment. These results show that free choice behaviour is well explained by an inference-based framework in which both utility and entropy are optimised.},
  file = {Rens et al. - 2020 - Evidence for entropy maximisation in human free ch.pdf},
  language = {en},
  type = {Preprint}
}

@article{Rey2014,
  title = {Using Waveform Information in Nonlinear Data Assimilation},
  author = {Rey, Daniel and Eldridge, Michael and Morone, Uriel and Abarbanel, Henry D. I. and Parlitz, Ulrich and {Schumann-Bischoff}, Jan},
  year = {2014},
  month = dec,
  volume = {90},
  issn = {1539-3755, 1550-2376},
  doi = {10.1103/PhysRevE.90.062916},
  file = {2014 - Rey et al. - Using waveform information in nonlinear data assimilation.pdf;Rey et al. - 2014 - Using waveform information in nonlinear data assim.pdf},
  journal = {Physical Review E},
  language = {en},
  number = {6}
}

@article{Reynolds2007,
  title = {Honeybees Perform Optimal Scale-Free Searching Flights When Attempting to Locate a Food Source},
  author = {Reynolds, A. M. and Smith, A. D. and Reynolds, D. R. and Carreck, N. L. and Osborne, J. L.},
  year = {2007},
  month = nov,
  volume = {210},
  pages = {3763--3770},
  issn = {0022-0949, 1477-9145},
  doi = {10.1242/jeb.009563},
  abstract = {Summary The foraging strategies used by animals are key to their success in spatially and temporally heterogeneous environments. We hypothesise that when a food source at a known location ceases to be available, flying insects will exhibit search patterns that optimise the rediscovery of such resources. In order to study these searching patterns, foraging honeybees were trained to an artificial feeder that was then removed, and the subsequent flight patterns of the bees were recorded using harmonic radar. We show that the flight patterns have a scale-free (L\'evy-flight) characteristic that constitutes an optimal searching strategy for the location of the feeder. It is shown that this searching strategy would remain optimal even if the implementation of the L\'evy-flights was imprecise due, for example, to errors in the bees' path integration system or difficulties in responding to variable wind conditions. The implications of these findings for animal foraging in general are discussed.},
  file = {Reynolds et al. - 2007 - Honeybees perform optimal scale-free searching fli.pdf},
  journal = {Journal of Experimental Biology},
  language = {en},
  number = {21}
}

@article{Reynolds2008,
  title = {How {{Many Animals Really Do}} the {{L\'evy Walk}}? {{Comment}}},
  author = {Reynolds, Andy},
  year = {2008},
  volume = {89},
  pages = {2347--2351},
  file = {Reynolds - 2008 - How Many Animals Really Do the Lévy Walk Comment.pdf},
  journal = {Ecology},
  language = {en},
  number = {8}
}

@article{Reynolds2009,
  title = {The {{L\'evy}} Flight Paradigm: Random Search Patterns and Mechanisms},
  shorttitle = {The {{L\'evy}} Flight Paradigm},
  author = {Reynolds, A. M. and Rhodes, C. J.},
  year = {2009},
  month = apr,
  volume = {90},
  pages = {877--887},
  issn = {0012-9658},
  doi = {10.1890/08-0153.1},
  abstract = {Over recent years there has been an accumulation of evidence from a variety of experimental, theoretical, and field studies that many organisms use a movement strategy approximated by Le\textasciiacute{} vy flights when they are searching for resources. Le\textasciiacute{} vy flights are random movements that can maximize the efficiency of resource searches in uncertain environments. This is a highly significant finding because it suggests that Le\textasciiacute{} vy flights provide a rigorous mathematical basis for separating out evolved, innate behaviors from environmental influences. We discuss recent developments in random-search theory, as well as the many different experimental and data collection initiatives that have investigated search strategies. Methods for trajectory construction and robust data analysis procedures are presented. The key to prediction and understanding does, however, lie in the elucidation of mechanisms underlying the observed patterns. We discuss candidate neurological, olfactory, and learning mechanisms for the emergence of Le\textasciiacute{} vy flight patterns in some organisms, and note that convergence of behaviors along such different evolutionary pathways is not surprising given the energetic efficiencies that Le\textasciiacute{} vy flight movement patterns confer.},
  file = {Reynolds and Rhodes - 2009 - The Lévy flight paradigm random search patterns a.pdf},
  journal = {Ecology},
  language = {en},
  number = {4}
}

@article{Reynolds2009a,
  title = {The {{L\'evy Flight Paradigm}}: {{Random Search Patterns}} and {{Mechanisms}}},
  author = {Reynolds, A. M. and Rhodes, C. J.},
  year = {2009},
  volume = {90},
  pages = {877--887},
  file = {Reynolds and Rhodes - 2009 - The Lévy Flight Paradigm Random Search Patterns a 2.pdf},
  journal = {Ecology},
  language = {en},
  number = {4}
}

@article{Reynolds2012,
  title = {Olfactory Search Behaviour in the Wandering Albatross Is Predicted to Give Rise to {{L\'evy}} Flight Movement Patterns},
  author = {Reynolds, A.M.},
  year = {2012},
  month = may,
  volume = {83},
  pages = {1225--1229},
  issn = {00033472},
  doi = {10.1016/j.anbehav.2012.02.014},
  file = {Reynolds - 2012 - Olfactory search behaviour in the wandering albatr.pdf},
  journal = {Animal Behaviour},
  language = {en},
  number = {5}
}

@article{Reynolds2014,
  title = {Does the {{Australian}} Desert Ant {{Melophorus}} Bagoti Approximate a {{L\'evy}} Search by an Intrinsic Bi-Modal Walk?},
  author = {Reynolds, Andy M. and Schultheiss, Patrick and Cheng, Ken},
  year = {2014},
  month = jan,
  volume = {340},
  pages = {17--22},
  issn = {00225193},
  doi = {10.1016/j.jtbi.2013.09.006},
  abstract = {We suggest that the Australian desert ant Melophorus bagoti approximates a L\'evy search pattern by using an intrinsic bi-exponential walk and does so when a L\'evy search pattern is advantageous. When attempting to locate its nest, M. bagoti adopt a stereotypical search pattern. These searches begin at the location where the ant expects to find the nest, and comprise loops that start and end at this location, and are directed in different azimuthal directions. Loop lengths are exponentially distributed when searches are in visually familiar surroundings and are well described by a mixture of two exponentials when searches are in unfamiliar landscapes. The latter approximates a power-law distribution, the hallmark of a L\'evy search. With the aid of a simple analytically tractable theory, we show that an exponential loop-length distribution is advantageous when the distance to the nest can be estimated with some certainty and that a bi-exponential distribution is advantageous when there is considerable uncertainty regarding the nest location. The best bi-exponential search patterns are shown to be those that come closest to approximating advantageous L\'evy looping searches. The bi-exponential search patterns of M. bagoti are found to approximate advantageous L\'evy search patterns.},
  file = {Reynolds et al. - 2014 - Does the Australian desert ant Melophorus bagoti a.pdf},
  journal = {Journal of Theoretical Biology},
  language = {en}
}

@article{Reynolds2014a,
  title = {L\'evy Flight Movement Patterns in Marine Predators May Derive from Turbulence Cues},
  author = {Reynolds, A. M.},
  year = {2014},
  month = nov,
  volume = {470},
  pages = {20140408},
  issn = {1364-5021, 1471-2946},
  doi = {10.1098/rspa.2014.0408},
  abstract = {The L\'evy-flight foraging hypothesis states that because L\'evy flights can optimize search efficiencies, natural selection should have led to adaptations for L\'evy flight foraging. Some of the strongest evidence for this hypothesis has come from telemetry data for sharks, bony fish, sea turtles and penguins. Here, I show that the programming for these L\'evy movement patterns does not need to be very sophisticated or clever on the predator's part, as these movement patterns would arise naturally if the predators change their direction of travel only after encountering patches of relatively strong turbulence (a seemingly natural response to buffeting). This is established with the aid of kinematic simulations of three-dimensional turbulence. L\'evy flights movement patterns are predicted to arise in all but the most quiescent of oceanic waters.},
  file = {Reynolds - 2014 - Lévy flight movement patterns in marine predators .pdf},
  journal = {Proc. R. Soc. A.},
  language = {en},
  number = {2171}
}

@article{Reynolds2015,
  title = {Pelagic Seabird Flight Patterns Are Consistent with a Reliance on Olfactory Maps for Oceanic Navigation},
  author = {Reynolds, Andrew M. and Cecere, Jacopo G. and Paiva, Vitor H. and Ramos, Jaime A. and Focardi, Stefano},
  year = {2015},
  month = jul,
  volume = {282},
  pages = {20150468},
  issn = {0962-8452, 1471-2954},
  doi = {10.1098/rspb.2015.0468},
  abstract = {Homing studies have provided tantalizing evidence that the remarkable ability of shearwaters (               Procellariiformes               ) to pinpoint their breeding colony after crossing vast expanses of featureless open ocean can be attributed to their assembling cognitive maps of wind-borne odours but crucially, it has not been tested whether olfactory cues are actually used as a system for navigation. Obtaining statistically important samples of wild birds for use in experimental approaches is, however, impossible because of invasive sensory manipulation. Using an innovative non-invasive approach, we provide strong evidence that shearwaters rely on olfactory cues for oceanic navigation. We tested for compliance with olfactory-cued navigation in the flight patterns of 210 shearwaters of three species (Cory's shearwaters,               Calonectris borealis               , North Atlantic Ocean, Scopoli's shearwaters,               C. diomedea               Mediterranean Sea, and Cape Verde shearwaters,               C. edwardsii               , Central Atlantic Ocean) tagged with high-resolution GPS loggers during both incubation and chick rearing. We found that most (69\%) birds displayed exponentially truncated scale-free (L\'evy-flight like) displacements, which we show are consistent with olfactory-cued navigation in the presence of atmospheric turbulence. Our analysis provides the strongest evidence yet for cognitive odour map navigation in wild birds. Thus, we may reconcile two highly disputed questions in movement ecology, by mechanistically connecting L\'evy displacements and olfactory navigation. Our approach can be applied to any species which can be tracked at sufficient spatial resolution, using a GPS logger.},
  file = {Reynolds et al. - 2015 - Pelagic seabird flight patterns are consistent wit.pdf},
  journal = {Proc. R. Soc. B.},
  language = {en},
  number = {1811}
}

@article{Reynolds2015a,
  title = {Liberating {{L\'evy}} Walk Research from the Shackles of Optimal Foraging},
  author = {Reynolds, Andy},
  year = {2015},
  month = sep,
  volume = {14},
  pages = {59--83},
  issn = {15710645},
  doi = {10.1016/j.plrev.2015.03.002},
  abstract = {There is now compelling evidence that many organisms have movement patterns that can be described as L\'evy walks, or L\'evy flights. L\'evy movement patterns have been identified in cells, microorganisms, molluscs, insects, reptiles, fish, birds and even human hunter-gatherers. Most research into L\'evy walks as models of organism movement patterns has been shaped by the `L\'evy flight foraging hypothesis'. This states that, since L\'evy walks can optimize search efficiencies, natural selection should lead to adaptations that select for L\'evy walk foraging. However, a growing body of research on generative mechanisms suggests that L\'evy walks can arise freely as by-products of otherwise innocuous behaviours; consequently their advantageous properties are purely coincidental. This suggests that the L\'evy flight foraging hypothesis should be amended, or even replaced, by a simpler and more general hypothesis. This new hypothesis would state that `L\'evy walks emerge spontaneously and naturally from innate behaviours and innocuous responses to the environment but, if advantageous, then there could be selection against losing them'. The new hypothesis has the virtue of making fewer assumptions and being broader than the original hypothesis; it also encompasses the many examples of suboptimal L\'evy patterns that challenge the prevailing paradigm. This does not detract from the L\'evy flight foraging hypothesis, in fact, it adds to the theory by providing a stronger and more compelling case for the occurrence of L\'evy walks. It dispenses with concerns about the theoretical arguments in support of the L\'evy flight foraging hypothesis and so may lead to a wider acceptance of L\'evy walks as models of movement pattern data. Furthermore, organisms can approximate L\'evy walks by adapting intrinsic behaviour in simple ways; this occurs when L\'evy movement patterns are advantageous, but come with an associated cost. These new developments represent a major change in perspective and provide the broadest picture yet of L\'evy movement patterns. However, the process of understanding and identifying L\'evy movement patterns still has a long way to go, and further reinterpretations and shifts in understanding will occur. In conclusion, L\'evy walk research remains exciting precisely because so much remains to be understood, and because, even relatively small studies, are interesting discoveries in their own right. \textcopyright{} 2015 Elsevier B.V. All rights reserved.},
  file = {Reynolds - 2015 - Liberating Lévy walk research from the shackles of.pdf},
  journal = {Physics of Life Reviews},
  language = {en}
}

@article{Reynolds2016,
  title = {L\'evy Patterns in Seabirds Are Multifaceted Describing Both Spatial and Temporal Patterning},
  author = {Reynolds, Andrew M. and Paiva, Vitor H. and Cecere, Jacopo G. and Focardi, Stefano},
  year = {2016},
  month = dec,
  volume = {13},
  pages = {29},
  issn = {1742-9994},
  doi = {10.1186/s12983-016-0160-2},
  abstract = {Background: The flight patterns of albatrosses and shearwaters have become a touchstone for much of L\'evy flight research, spawning an extensive field of enquiry. There is now compelling evidence that the flight patterns of these seabirds would have been appreciated by Paul L\'evy, the mathematician after whom L\'evy flights are named. Here we show that L\'evy patterns (here taken to mean spatial or temporal patterns characterized by distributions with power-law tails) are, in fact, multifaceted in shearwaters being evident in both spatial and temporal patterns of activity. Results: We tested for L\'evy patterns in the at-sea behaviours of two species of shearwater breeding in the North Atlantic Ocean (Calonectris borealis) and the Mediterranean sea (C. diomedea) during their incubating and chick-provisioning periods. We found that distributions of flight durations, on/in water durations and inter-dive time-intervals have power-law tails and so bear the hallmarks of L\'evy patterns. Conclusions: The occurrence of these statistical laws is remarkable given that bird behaviours are strongly shaped by an individual's motivational state and by complex environmental interactions. Our observations could take L\'evy patterns as models of animal behaviour to a new level by going beyond the characterisation of spatial movements to characterise how different behaviours are interwoven throughout daily animal life.},
  file = {Reynolds et al. - 2016 - Lévy patterns in seabirds are multifaceted describ.pdf},
  journal = {Front Zool},
  language = {en},
  number = {1}
}

@article{Reynolds2016a,
  title = {Signatures of Chaos in Animal Search Patterns},
  author = {Reynolds, Andy M and Bartumeus, Frederic and K{\"o}lzsch, Andrea and {van de Koppel}, Johan},
  year = {2016},
  month = mar,
  volume = {6},
  pages = {23492},
  issn = {2045-2322},
  doi = {10.1038/srep23492},
  file = {Reynolds et al. - 2016 - Signatures of chaos in animal search patterns.pdf},
  journal = {Sci Rep},
  language = {en},
  number = {1}
}

@article{Reynolds2016b,
  title = {Swarm Dynamics May Give Rise to {{L\'evy}} Flights},
  author = {Reynolds, Andrew M. and Ouellette, Nicholas T.},
  year = {2016},
  month = sep,
  volume = {6},
  pages = {30515},
  issn = {2045-2322},
  doi = {10.1038/srep30515},
  file = {Reynolds and Ouellette - 2016 - Swarm dynamics may give rise to Lévy flights.pdf},
  journal = {Sci Rep},
  language = {en},
  number = {1}
}

@article{Reynolds2016c,
  title = {Swarm Dynamics May Give Rise to {{L\'evy}} Flights},
  author = {Reynolds, Andrew M. and Ouellette, Nicholas T.},
  year = {2016},
  month = sep,
  volume = {6},
  pages = {30515},
  issn = {2045-2322},
  doi = {10.1038/srep30515},
  file = {Reynolds and Ouellette - 2016 - Swarm dynamics may give rise to Lévy flights 2.pdf},
  journal = {Sci Rep},
  language = {en},
  number = {1}
}

@article{Rhodes2007,
  title = {Human Memory Retrieval as {{L\'evy}} Foraging},
  author = {Rhodes, Theo and Turvey, Michael T.},
  year = {2007},
  month = nov,
  volume = {385},
  pages = {255--260},
  issn = {03784371},
  doi = {10.1016/j.physa.2007.07.001},
  abstract = {When people attempt to recall as many words as possible from a specific category (e.g., animal names) their retrievals occur sporadically over an extended temporal period. Retrievals decline as recall progresses, but short retrieval bursts can occur even after tens of minutes of performing the task. To date, efforts to gain insight into the nature of retrieval from this fundamental phenomenon of semantic memory have focused primarily upon the exponential growth rate of cumulative recall. Here we focus upon the time intervals between retrievals. We expected and found that, for each participant in our experiment, these intervals conformed to a Le\textasciiacute{} vy distribution suggesting that the Le\textasciiacute{} vy flight dynamics that characterize foraging behavior may also characterize retrieval from semantic memory. The closer the exponent on the inverse square power-law distribution of retrieval intervals approximated the optimal foraging value of 2, the more efficient was the retrieval. At an abstract dynamical level, foraging for particular foods in one's niche and searching for particular words in one's memory must be similar processes if particular foods and particular words are randomly and sparsely located in their respective spaces at sites that are not known a priori. We discuss whether Le\textasciiacute{} vy dynamics imply that memory processes, like foraging, are optimized in an ecological way.},
  file = {Rhodes and Turvey - 2007 - Human memory retrieval as Lévy foraging.pdf},
  journal = {Physica A: Statistical Mechanics and its Applications},
  language = {en},
  number = {1}
}

@article{Rhodes2007a,
  title = {Human Memory Retrieval as {{L\'evy}} Foraging},
  author = {Rhodes, Theo and Turvey, Michael T.},
  year = {2007},
  month = nov,
  volume = {385},
  pages = {255--260},
  issn = {03784371},
  doi = {10.1016/j.physa.2007.07.001},
  abstract = {When people attempt to recall as many words as possible from a specific category (e.g., animal names) their retrievals occur sporadically over an extended temporal period. Retrievals decline as recall progresses, but short retrieval bursts can occur even after tens of minutes of performing the task. To date, efforts to gain insight into the nature of retrieval from this fundamental phenomenon of semantic memory have focused primarily upon the exponential growth rate of cumulative recall. Here we focus upon the time intervals between retrievals. We expected and found that, for each participant in our experiment, these intervals conformed to a Le\textasciiacute{} vy distribution suggesting that the Le\textasciiacute{} vy flight dynamics that characterize foraging behavior may also characterize retrieval from semantic memory. The closer the exponent on the inverse square power-law distribution of retrieval intervals approximated the optimal foraging value of 2, the more efficient was the retrieval. At an abstract dynamical level, foraging for particular foods in one's niche and searching for particular words in one's memory must be similar processes if particular foods and particular words are randomly and sparsely located in their respective spaces at sites that are not known a priori. We discuss whether Le\textasciiacute{} vy dynamics imply that memory processes, like foraging, are optimized in an ecological way.},
  file = {Rhodes and Turvey - 2007 - Human memory retrieval as Lévy foraging 2.pdf},
  journal = {Physica A: Statistical Mechanics and its Applications},
  language = {en},
  number = {1}
}

@article{Ribeiro-Neto2012,
  title = {Dissipative {{L\'evy}} Random Searches: {{Universal}} Behavior at Low Target Density},
  shorttitle = {Dissipative {{L\'evy}} Random Searches},
  author = {{Ribeiro-Neto}, P. J. and Raposo, E. P. and Ara{\'u}jo, H. A. and Faustino, C. L. and {da Luz}, M. G. E. and Viswanathan, G. M.},
  year = {2012},
  month = dec,
  volume = {86},
  pages = {061102},
  issn = {1539-3755, 1550-2376},
  doi = {10.1103/PhysRevE.86.061102},
  file = {Ribeiro-Neto et al. - 2012 - Dissipative Lévy random searches Universal behavi.pdf},
  journal = {Phys. Rev. E},
  language = {en},
  number = {6}
}

@inproceedings{Ribeiro2016,
  title = {"{{Why Should I Trust You}}?": {{Explaining}} the {{Predictions}} of {{Any Classifier}}},
  shorttitle = {"{{Why Should I Trust You}}?},
  booktitle = {Proceedings of the 22nd {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}} - {{KDD}} '16},
  author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  year = {2016},
  pages = {1135--1144},
  publisher = {{ACM Press}},
  address = {{San Francisco, California, USA}},
  doi = {10.1145/2939672.2939778},
  abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.},
  file = {Ribeiro et al. - 2016 - Why Should I Trust You Explaining the Predicti.pdf},
  isbn = {978-1-4503-4232-2},
  language = {en}
}

@article{Rich2016,
  title = {Honesty through Repeated Interactions},
  author = {Rich, Patricia and Zollman, Kevin J.S.},
  year = {2016},
  month = apr,
  volume = {395},
  pages = {238--244},
  issn = {00225193},
  doi = {10.1016/j.jtbi.2016.02.002},
  abstract = {In the study of signaling, it is well known that the cost of deception is an essential element for stable honest signaling in nature. In this paper, we show how costs for deception can arise endogenously from repeated interactions between individuals. Utilizing the Sir Philip Sidney game as an illustrative case, we show that repeated interactions can sustain honesty with no observable signal costs, even when deception cannot be directly observed. We provide a number of potential experimental tests for this theory which distinguish it from the available alternatives.},
  file = {Rich and Zollman - 2016 - Honesty through repeated interactions.pdf},
  journal = {Journal of Theoretical Biology},
  language = {en}
}

@article{Rich2016a,
  title = {Axiomatic and Ecological Rationality: Choosing Costs and Benefits},
  shorttitle = {Axiomatic and Ecological Rationality},
  author = {Rich, Patricia},
  year = {2016},
  month = dec,
  volume = {9},
  pages = {90},
  issn = {1876-9098},
  doi = {10.23941/ejpe.v9i2.231},
  abstract = {One important purpose of rationality research is to help individuals improve. There are two main approaches to the task of rendering evaluations of rationality that support guidance: the axiomatic approach evaluates the coherence of behavior according to axiomatic criteria, while ecological rationality evaluates processes according to their expected per- formance. The first part of the paper considers arguments against the axiomatic and ecological approaches and concludes that neither approach is unserviceable; in particular, each has the flexibility to accept important insights from the other. The second part of the paper characterizes each approach according to the profile of costs and benefits that it accepts, and shows that combining the two approaches in a particular way yields a new approach with a superior cost-benefit profile. This 'hybrid approach' uses axiomatic rationality criteria to evaluate processes that agents might use.},
  file = {Rich - 2016 - Axiomatic and ecological rationality choosing cos.pdf},
  journal = {EJPE},
  language = {en},
  number = {2}
}

@article{Rich2019,
  title = {Lessons for Artificial Intelligence from the Study of Natural Stupidity},
  author = {Rich, Alexander S. and Gureckis, Todd M.},
  year = {2019},
  month = apr,
  volume = {1},
  pages = {174--180},
  issn = {2522-5839},
  doi = {10.1038/s42256-019-0038-z},
  file = {Rich and Gureckis - 2019 - Lessons for artificial intelligence from the study.pdf},
  journal = {Nature Machine Intelligence},
  language = {en},
  number = {4}
}

@techreport{Rich2021,
  title = {How Hard Is Cognitive Science?},
  author = {Rich, Patricia and {de Haan}, Ronald and Wareham, Todd and {van Rooij}, Iris},
  year = {2021},
  month = apr,
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/k79nv},
  abstract = {Cognitive science is itself a cognitive activity. Yet, computational cognitive science tools are seldom used to study (limits of) cognitive scientists' thinking. Here, we do so using computational-level modeling and complexity analysis. We present an idealized formal model of a core inference problem faced by cognitive scientists: Given observations of a system's behaviors, infer cognitive processes that could plausibly produce the behavior. We consider variants of this problem at different levels of explanation and prove that at each level, the inference problem is intractable, or even uncomputable. We discuss the implications for cognitive science.},
  file = {Rich et al. - 2021 - How hard is cognitive science.pdf},
  language = {en},
  type = {Preprint}
}

@article{Richards2019,
  title = {A Deep Learning Framework for Neuroscience},
  author = {Richards, Blake A. and Lillicrap, Timothy P. and Beaudoin, Philippe and Bengio, Yoshua and Bogacz, Rafal and Christensen, Amelia and Clopath, Claudia and Costa, Rui Ponte and {de Berker}, Archy and Ganguli, Surya and Gillon, Colleen J. and Hafner, Danijar and Kepecs, Adam and Kriegeskorte, Nikolaus and Latham, Peter and Lindsay, Grace W. and Miller, Kenneth D. and Naud, Richard and Pack, Christopher C. and Poirazi, Panayiota and Roelfsema, Pieter and Sacramento, Jo{\~a}o and Saxe, Andrew and Scellier, Benjamin and Schapiro, Anna C. and Senn, Walter and Wayne, Greg and Yamins, Daniel and Zenke, Friedemann and Zylberberg, Joel and Therien, Denis and Kording, Konrad P.},
  year = {2019},
  month = nov,
  volume = {22},
  pages = {1761--1770},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/s41593-019-0520-2},
  file = {Richards et al. - 2019 - A deep learning framework for neuroscience.pdf},
  journal = {Nat Neurosci},
  language = {en},
  number = {11}
}

@article{Riedl2013,
  title = {Practical Considerations of Permutation Entropy: {{A}} Tutorial Review},
  shorttitle = {Practical Considerations of Permutation Entropy},
  author = {Riedl, M. and M{\"u}ller, A. and Wessel, N.},
  year = {2013},
  month = jun,
  volume = {222},
  pages = {249--262},
  issn = {1951-6355, 1951-6401},
  doi = {10.1140/epjst/e2013-01862-7},
  abstract = {More than ten years ago Bandt and Pompe introduced a new measure to quantify complexity in measured time series. During these ten years, this measure has been modified and extended. In this review we will give a brief introduction to permutation entropy, explore the different fields of utilization where permutation entropy has been applied and provide a guide on how to choose appropriate parameters for different applications of permutation entropy.},
  file = {2013 - Riedl, Mller, Wessel - Practical considerations of permutation entropy A tutorial review.pdf},
  journal = {The European Physical Journal Special Topics},
  language = {en},
  number = {2}
}

@article{Rieskamp2006,
  title = {{{SSL}}: {{A Theory}} of {{How People Learn}} to {{Select Strategies}}.},
  shorttitle = {{{SSL}}},
  author = {Rieskamp, J{\"o}rg and Otto, Philipp E.},
  year = {2006},
  volume = {135},
  pages = {207--236},
  issn = {1939-2222, 0096-3445},
  doi = {10.1037/0096-3445.135.2.207},
  abstract = {The assumption that people possess a repertoire of strategies to solve the inference problems they face has been raised repeatedly. However, a computational model specifying how people select strategies from their repertoire is still lacking. The proposed strategy selection learning (SSL) theory predicts a strategy selection process on the basis of reinforcement learning. The theory assumes that individuals develop subjective expectations for the strategies they have and select strategies proportional to their expectations, which are then updated on the basis of subsequent experience. The learning assumption was supported in 4 experimental studies. Participants substantially improved their inferences through feedback. In all 4 studies, the best-performing strategy from the participants' repertoires most accurately predicted the inferences after sufficient learning opportunities. When testing SSL against 3 models representing extensions of SSL and against an exemplar model assuming a memory-based inference process, the authors found that SSL predicted the inferences most accurately.},
  file = {2006 - Rieskamp, Otto - SSL A theory of how people learn to select strategies.pdf},
  journal = {Journal of Experimental Psychology: General},
  language = {en},
  number = {2}
}

@article{Risi2019,
  title = {Deep {{Neuroevolution}} of {{Recurrent}} and {{Discrete World Models}}},
  author = {Risi, Sebastian and Stanley, Kenneth O},
  year = {2019},
  pages = {7},
  abstract = {Neural architectures inspired by our own human cognitive system, such as the recently introduced world models, have been shown to outperform traditional deep reinforcement learning (RL) methods in a variety of different domains. Instead of the relatively simple architectures employed in most RL experiments, world models rely on multiple different neural components that are responsible for visual information processing, memory, and decision-making. However, so far the components of these models have to be trained separately and through a variety of specialized training methods. This paper demonstrates the surprising finding that models with the same precise parts can be instead efficiently trained end-to-end through a genetic algorithm (GA), reaching a comparable performance to the original world model by solving a challenging car racing task. An analysis of the evolved visual and memory system indicates that they include a similar effective representation to the system trained through gradient descent. Additionally, in contrast to gradient descent methods that struggle with discrete variables, GAs also work directly with such representations, opening up opportunities for classical planning in latent space. This paper adds additional evidence on the effectiveness of deep neuroevolution for tasks that require the intricate orchestration of multiple components in complex heterogeneous architectures.},
  file = {Risi and Stanley - 2019 - Deep Neuroevolution of Recurrent and Discrete Worl.pdf},
  language = {en}
}

@article{Rnkranz,
  title = {Separate-and-{{Conquer Rule Learning}}},
  author = {Rnkranz, Johannes Fu},
  pages = {52},
  abstract = {This paper is a survey of inductive rule learning algorithms that use a separate-andconquer strategy. This strategy can be traced back to the AQ learning system and still enjoys popularity as can be seen from its frequent use in inductive logic programming systems. We will put this wide variety of algorithms into a single framework and analyze them along three different dimensions, namely their search, language and overfitting avoidance biases.},
  file = {Rnkranz - Separate-and-Conquer Rule Learning.pdf},
  language = {en}
}

@article{Rnkranza,
  title = {Separate-and-{{Conquer Rule Learning}}},
  author = {Rnkranz, Johannes Fu},
  pages = {52},
  abstract = {This paper is a survey of inductive rule learning algorithms that use a separate-andconquer strategy. This strategy can be traced back to the AQ learning system and still enjoys popularity as can be seen from its frequent use in inductive logic programming systems. We will put this wide variety of algorithms into a single framework and analyze them along three different dimensions, namely their search, language and overfitting avoidance biases.},
  file = {Rnkranz - Separate-and-Conquer Rule Learning 2.pdf},
  language = {en}
}

@article{Roach2018,
  title = {Resonance with Subthreshold Oscillatory Drive Organizes Activity and Optimizes Learning in Neural Networks},
  author = {Roach, James P. and Pidde, Aleksandra and Katz, Eitan and Wu, Jiaxing and Ognjanovski, Nicolette and Aton, Sara J. and Zochowski, Michal R.},
  year = {2018},
  month = mar,
  volume = {115},
  pages = {E3017-E3025},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1716933115},
  abstract = {Network oscillations across and within brain areas are critical for learning and performance of memory tasks. While a large amount of work has focused on the generation of neural oscillations, their effect on neuronal populations' spiking activity and information encoding is less known. Here, we use computational modeling to demonstrate that a shift in resonance responses can interact with oscillating input to ensure that networks of neurons properly encode new information represented in external inputs to the weights of recurrent synaptic connections. Using a neuronal network model, we find that due to an input current-dependent shift in their resonance response, individual neurons in a network will arrange their phases of firing to represent varying strengths of their respective inputs. As networks encode information, neurons fire more synchronously, and this effect limits the extent to which further ``learning'' (in the form of changes in synaptic strength) can occur. We also demonstrate that sequential patterns of neuronal firing can be accurately stored in the network; these sequences are later reproduced without external input (in the context of subthreshold oscillations) in both the forward and reverse directions (as has been observed following learning in vivo). To test whether a similar mechanism could act in vivo, we show that periodic stimulation of hippocampal neurons coordinates network activity and functional connectivity in a frequency-dependent manner. We conclude that resonance with subthreshold oscillations provides a plausible network-level mechanism to accurately encode and retrieve information without overstrengthening connections between neurons.},
  file = {Roach et al. - 2018 - Resonance with subthreshold oscillatory drive orga.pdf},
  journal = {Proc Natl Acad Sci USA},
  language = {en},
  number = {13}
}

@article{Robbins1952,
  title = {Some Aspects of the Sequential Design of Experiments},
  author = {Robbins, Herbert},
  year = {1952},
  volume = {58},
  pages = {527--535},
  file = {Robbins - SOME ASPECTS OF THE SEQUENTIAL DESIGN OF EXPERIMEN.pdf},
  journal = {Bulletin of the American Mathematical Society},
  language = {en},
  number = {5}
}

@article{Robert1976,
  title = {A {{Unifying Tool}} for {{Linear Multivariate Statistical Methods}}: {{The RV}}- {{Coefficient}}},
  shorttitle = {A {{Unifying Tool}} for {{Linear Multivariate Statistical Methods}}},
  author = {Robert, P. and Escoufier, Y.},
  year = {1976},
  volume = {25},
  pages = {257},
  issn = {00359254},
  doi = {10.2307/2347233},
  abstract = {Considertwodata matriceosn thesamesampleofn individualsX, (p x n), Y(q x n). Fromthesematricesg,eometricarlepresentatioonfsthesampleare obtainedas two configurationosf n points,in RP and \_?q.It is shownthat the RV-coefficient (Escoufier1,970,1973)canbeusedas a measureofsimilaritoyfthetwoconfigurations, takingintoaccountthepossiblydistincmt etrictso be usedon themto measurethe distancesbetweenpoints.The purposeofthispaperis to showthatmostclassical methodsoflinearmultivariatsteatisticaalnalysiscan beinterpreteads thesearchfor optimallineartransformatioonrs, equivalentlyt,hesearchforoptimalmetricsto applyon twodatamatriceosnthesamesample;theoptimalitiys definedintermsof thesimilaritoyfthecorrespondincgonfiguratioonfspoints,whichi,n turn,callsfor themaximizatioonftheassociatedR V-coefficienTth. emethodstudiedareprincipal componentps,rincipaclomponentosfinstrumentvaalriablesm, ultivariatregression, canonicalvariables,discriminanatnalysis;theyare differentiatbeyd the possible relationshipesxistingbetweenthe two data matricesinvolvedand by additional constrainutsnderwhichthemaximumofR Vistobe obtained.It is alsoshownthatthe RV-coefficiecnatn be usedas a measureofgoodnessofa solutionto theproblemof discardinvgariables.},
  file = {2012 - Society, Statistics - Tool for Linear Multivariate A Unifying The RV-Coefficient Methods Statistical.pdf},
  journal = {Applied Statistics},
  language = {en},
  number = {3}
}

@article{Roberts,
  title = {Police Thought That 17-Year-Old {{Marty Tankle}} Seemed Too Calm after Nding His Mother Stabbed to Death and His Father Mortally Bludgeoned in the Family's {{Long Island}} Home. {{Authorities}} Didn't Believe His Claims of},
  author = {Roberts, H Armstrong},
  pages = {12},
  file = {Roberts - Police thought that 17-year-old Marty Tankle seeme.pdf},
  language = {en}
}

@article{Robertson2003,
  title = {Binding, Spatial Attention and Perceptual Awareness},
  author = {Robertson, Lynn C.},
  year = {2003},
  month = feb,
  volume = {4},
  pages = {93--102},
  issn = {1471-003X, 1471-0048},
  doi = {10.1038/nrn1030},
  file = {Robertson - 2003 - Binding, spatial attention and perceptual awarenes.pdf},
  journal = {Nat Rev Neurosci},
  language = {en},
  number = {2}
}

@article{Robinson2012,
  title = {Spike, Rate, Field, and Hybrid Methods for Treating Neuronal Dynamics and Interactions},
  author = {Robinson, P.A. and Kim, J.W.},
  year = {2012},
  month = apr,
  volume = {205},
  pages = {283--294},
  issn = {01650270},
  doi = {10.1016/j.jneumeth.2012.01.018},
  abstract = {Spike-, rate-, and field-based approaches to neural dynamics are adapted and hybridized to provide new methods of analyzing dynamics of single neurons and large neuronal systems, to elucidate the relationships and intermediate forms between these limiting cases, and to enable faster simulations with reduced memory requirements. At the single-neuron level, the new approaches involve reformulation of dynamics in synapses, dendrites, cell bodies, and axons to enable new types of analysis, longer numerical timesteps, and demonstration that rate-based methods can predict spike times. In multineuron systems, hybrids and intermediates between spike-based and field-based coupling between neurons are used to provide stepping stones between descriptions based on pairwise spike-based interactions between neurons and ones based on neural field-based interactions within and between populations, including arbitrary spatial structure and temporal delays in the connections in general. In particular, a new neuronin-cell approach is introduced that is a hybrid between neural field theory and spiking-neuron models in analogy to particle-in-cell methods in plasma physics. This approach enables large speedups in computations while preserving spike shapes and times. Various approaches are illustrated numerically for specific cases.},
  file = {2012 - Robinson, Kim - Spike, rate, field, and hybrid methods for treating neuronal dynamics and interactions.pdf},
  journal = {Journal of Neuroscience Methods},
  language = {en},
  number = {2}
}

@article{Robinson2015,
  title = {Short {{Stimulus}}, {{Long Response}}: {{Sodium}} and {{Calcium Dynamics Explain Persistent Neuronal Firing}}},
  shorttitle = {Short {{Stimulus}}, {{Long Response}}},
  author = {Robinson, Richard},
  year = {2015},
  month = dec,
  volume = {13},
  pages = {e1002320},
  issn = {1545-7885},
  doi = {10.1371/journal.pbio.1002320},
  file = {2015 - Robinson - Short Stimulus, Long Response Sodium and Calcium Dynamics Explain Persistent Neuronal Firing.pdf;Robinson - 2015 - Short Stimulus, Long Response Sodium and Calcium .pdf},
  journal = {PLOS Biology},
  language = {en},
  number = {12}
}

@inproceedings{Rocktaschel2014,
  title = {Low-{{Dimensional Embeddings}} of {{Logic}}},
  booktitle = {Proceedings of the {{ACL}} 2014 {{Workshop}} on {{Semantic Parsing}}},
  author = {Rockt{\"a}schel, Tim and Bo{\v s}njak, Matko and Singh, Sameer and Riedel, Sebastian},
  year = {2014},
  pages = {45--49},
  publisher = {{Association for Computational Linguistics}},
  address = {{Baltimore, MD}},
  doi = {10.3115/v1/W14-2409},
  abstract = {Many machine reading approaches, from shallow information extraction to deep semantic parsing, map natural language to symbolic representations of meaning. Representations such as first-order logic capture the richness of natural language and support complex reasoning, but often fail in practice due to their reliance on logical background knowledge and the difficulty of scaling up inference. In contrast, low-dimensional embeddings (i.e. distributional representations) are efficient and enable generalization, but it is unclear how reasoning with embeddings could support the full power of symbolic representations such as first-order logic. In this proof-ofconcept paper we address this by learning embeddings that simulate the behavior of first-order logic.},
  file = {2014 - Rocktäschel et al. - Low-Dimensional Embeddings of Logic.pdf;Rocktäschel et al. - 2014 - Low-Dimensional Embeddings of Logic.pdf},
  language = {en}
}

@article{Rodrigues2010,
  title = {Mappings between a Macroscopic Neural-Mass Model and a Reduced Conductance-Based Model},
  author = {Rodrigues, Serafim and Chizhov, Anton V. and Marten, Frank and Terry, John R.},
  year = {2010},
  month = may,
  volume = {102},
  pages = {361--371},
  issn = {0340-1200, 1432-0770},
  doi = {10.1007/s00422-010-0372-z},
  abstract = {We present two alternative mappings between macroscopic neuronal models and a reduction of a conductance-based model. These provide possible explanations of the relationship between parameters of these two different approaches to modelling neuronal activity. Obtaining a physical interpretation of neural-mass models is of fundamental importance as they could provide direct and accessible tools for use in diagnosing neurological conditions. Detailed consideration of the assumptions required for the validity of each mapping elucidates strengths and weaknesses of each macroscopic model and suggests improvements for future development.},
  file = {2010 - Rodrigues et al. - Mappings between a macroscopic neural-mass model and a reduced conductance-based model.pdf},
  journal = {Biological Cybernetics},
  language = {en},
  number = {5}
}

@techreport{Rodriguez-Santiago2019,
  title = {Behavioral Traits That Define Social Dominance Are the Same That Reduce Social Influence in a Consensus Task},
  author = {{Rodriguez-Santiago}, Mariana and N{\"u}hrenberg, Paul and Derry, James and Deussen, Oliver and Garrison, Linda K and Garza, Sylvia F and Francisco, Fritz and Hofmann, Hans A and Jordan, Alex},
  year = {2019},
  month = nov,
  institution = {{Animal Behavior and Cognition}},
  doi = {10.1101/845628},
  abstract = {Abstract                        In many species, cultures, and contexts, social dominance reflects the ability to exert influence over others, and the question of what makes an effective leader is pertinent to a range of disciplines and contexts. While dominant individuals are often assumed to be most influential, the behavioral traits that make them dominant may also make them socially aversive and thereby reduce their influence. Here we examine the influence of dominant and subordinate males on group behavior in different social contexts using the cichlid fish             Astatotilapia burtoni             . We find that phenotypically dominant males display behavioral traits that typify leadership across taxonomic systems \textendash{} aggressive, social centrality, and movement leadership, while subordinate males are passive, socially peripheral, and have little influence over movement. However, in a more complex group-consensus task involving visual cue associations, subordinate males become the most effective agents of social change. We find that dominant males are spatially distant and have lower signal-to-noise ratios of informative behavior in the association task, potentially interfering with their ability to generate group-consensus. In contrast, subordinate males are physically close to other group members, have high signal-to-noise behaviors in the association task, and visual connectivity to other group members equal to that of dominant males. The attributes that define effective social influence are therefore highly context-specific, with socially and phenotypically dominant males being influential in routine but not complex social scenarios. These results demonstrate that behavioral traits that are typical of socially dominant individuals may actually reduce their social influence in other contexts.                                   Significance Statement             The attributes that allow individuals to attain positions of social power and dominance are common across many vertebrate social systems \textendash{} aggression, intimidation, coercion. Yet these traits are socially aversive, and can make dominant individuals poor agents of social change. In a vertebrate system (social cichlid fish) we show that dominant males are aggressive, socially central, and lead group movement. Yet dominant males are poor effectors of consensus in an more sophisticated association task compared with passive, socially-peripheral subordinate males. The most effective agents of social influence possess behavioral traits opposite of those typically found in position of social dominance, suggesting the behavioral processes that generate social dominance may simultaneously place the most ineffective leaders in positions of power.},
  file = {Rodriguez-Santiago et al. - 2019 - Behavioral traits that define social dominance are.pdf},
  language = {en},
  type = {Preprint}
}

@article{Rogalsky2011,
  title = {Functional {{Anatomy}} of {{Language}} and {{Music Perception}}: {{Temporal}} and {{Structural Factors Investigated Using Functional Magnetic Resonance Imaging}}},
  shorttitle = {Functional {{Anatomy}} of {{Language}} and {{Music Perception}}},
  author = {Rogalsky, C. and Rong, F. and Saberi, K. and Hickok, G.},
  year = {2011},
  month = mar,
  volume = {31},
  pages = {3843--3852},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.4515-10.2011},
  file = {2011 - Rogalsky et al. - Functional anatomy of language and music perception temporal and structural factors investigated using function.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {10}
}

@incollection{Rojas2018,
  title = {Predator {{Defense}}},
  booktitle = {Encyclopedia of {{Animal Cognition}} and {{Behavior}}},
  author = {Rojas, Bibiana and {Burdfield-Steel}, Emily},
  editor = {Vonk, Jennifer and Shackelford, Todd},
  year = {2018},
  pages = {1--8},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-47829-6_708-1},
  file = {Rojas and Burdfield-Steel - 2018 - Predator Defense.pdf},
  isbn = {978-3-319-47829-6},
  language = {en}
}

@techreport{Roseberry2019,
  title = {Locomotor Suppression by a Monosynaptic Amygdala to Brainstem Circuit},
  author = {Roseberry, Thomas K. and Lalive, Arnaud L. and Margolin, Benjamin D. and Kreitzer, Anatol C.},
  year = {2019},
  month = aug,
  institution = {{Neuroscience}},
  doi = {10.1101/724252},
  abstract = {The control of locomotion is fundamental to vertebrate animal survival. Defensive situations require an animal to rapidly decide whether to run away or suppress locomotor activity to avoid detection. While much of the neural circuitry involved in defensive action selection has been elucidated, top-down modulation of brainstem locomotor circuitry remains unclear. Here we provide evidence for the existence and functionality of a monosynaptic connection from the central amygdala (CeA) to the mesencephalic locomotor region (MLR) that inhibits locomotion in unconditioned and conditioned defensive behavior in mice. We show that locomotion stimulated by airpuff coincides with increased activity of MLR glutamatergic neurons. Using retrograde tracing and ex vivo electrophysiology, we find that the CeA makes a monosynaptic connection with the MLR. In the open field, in vivo stimulation of this projection suppressed spontaneous locomotion, whereas inhibition of this projection had no effect. However, inhibiting CeA terminals within the MLR increased both neural activity and locomotor responses to airpuff. Finally, using a conditioned avoidance paradigm known to activate CeA neurons, we find that inhibition of the CeA projection increased successful escape, whereas activating the projection reduced escape. Together these results provide evidence for a new circuit substrate influencing locomotion and defensive behaviors.},
  file = {Roseberry et al. - 2019 - Locomotor suppression by a monosynaptic amygdala t.pdf},
  language = {en},
  type = {Preprint}
}

@article{Rosenberg2021,
  title = {Mice in a Labyrinth: {{Rapid}} Learning, Sudden Insight, and Efficient Exploration},
  author = {Rosenberg, Matthew and Zhang, Tony and Perona, Pietro and Meister, Markus},
  year = {2021},
  volume = {426746},
  pages = {36},
  file = {Rosenberg et al. - 1 Mice in a labyrinth Rapid learning, 2 sudden in.pdf},
  journal = {bioRxiv},
  language = {en}
}

@article{Rosenblatt,
  title = {A {{COMPARISON OF SEVERAL PERCEPTRON MODELS}} \textbullet},
  author = {Rosenblatt, Frank and Ym, New},
  pages = {23},
  file = {1981 - Rosenblatt - A comparison of several perceptron models.pdf;Rosenblatt and Ym - A COMPARISON OF SEVERAL PERCEPTRON MODELS •.pdf},
  language = {en}
}

@article{Rosenfeld2020,
  title = {The {{Risks}} of {{Invariant Risk Minimization}}},
  author = {Rosenfeld, Elan and Ravikumar, Pradeep and Risteski, Andrej},
  year = {2020},
  month = oct,
  abstract = {Invariant Causal Prediction (Peters et al., 2016) is a technique for out-of-distribution generalization which assumes that some aspects of the data distribution vary across the training set but that the underlying causal mechanisms remain constant. Recently, Arjovsky et al. (2019) proposed Invariant Risk Minimization (IRM), an objective based on this idea for learning deep, invariant features of data which are a complex function of latent variables; many alternatives have subsequently been suggested. However, formal guarantees for all of these works are severely lacking. In this paper, we present the first analysis of classification under the IRM objective\textemdash as well as these recently proposed alternatives\textemdash under a fairly natural and general model. In the linear case, we show simple conditions under which the optimal solution succeeds or, more often, fails to recover the optimal invariant predictor. We furthermore present the very first results in the non-linear regime: we demonstrate that IRM can fail catastrophically unless the test data are sufficiently similar to the training distribution\textemdash this is precisely the issue that it was intended to solve. Thus, in this setting we find that IRM and its alternatives fundamentally do not improve over standard Empirical Risk Minimization.},
  archiveprefix = {arXiv},
  eprint = {2010.05761},
  eprinttype = {arxiv},
  file = {Rosenfeld et al. - 2020 - The Risks of Invariant Risk Minimization.pdf},
  journal = {arXiv:2010.05761 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryclass = {cs, stat}
}

@article{Ross,
  title = {A {{Bayesian Approach}} for {{Learning}} and {{Planning}} in {{Partially Observable Markov Decision Processes}}},
  author = {Ross, Stephane and Pineau, Joelle and {Chaib-draa}, Brahim and Kreitmann, Pierre},
  pages = {45},
  abstract = {Bayesian learning methods have recently been shown to provide an elegant solution to the exploration-exploitation trade-off in reinforcement learning. However most investigations of Bayesian reinforcement learning to date focus on the standard Markov Decision Processes (MDPs). The primary focus of this paper is to extend these ideas to the case of partially observable domains, by introducing the Bayes-Adaptive Partially Observable Markov Decision Processes. This new framework can be used to simultaneously (1) learn a model of the POMDP domain through interaction with the environment, (2) track the state of the system under partial observability, and (3) plan (near-)optimal sequences of actions. An important contribution of this paper is to provide theoretical results showing how the model can be finitely approximated while preserving good learning performance. We present approximate algorithms for belief tracking and planning in this model, as well as empirical results that illustrate how the model estimate and agent's return improve as a function of experience.},
  file = {2011 - Ross, Pineau - A Bayesian approach for learning and planning in partially observable Markov decision processes.pdf},
  language = {en}
}

@article{Rossert,
  title = {Automated Point-Neuron Simplification of Data-Driven Microcircuit Models},
  author = {Rossert, Christian and Pozzorini, Christian and Chindemi, Giuseppe and Eroe, Csaba and King, James and Newton, Taylor H and Nolte, Max and Reimann, Michael W and Gewaltig, Marc-Oliver and Gerstner, Wulfram and Markram, Henry and Segev, Idan and Muller, Eilif},
  pages = {26},
  abstract = {A method is presented for the reduction of morphologically detailed microcircuit models to a point-neuron representation without human intervention. The simplification occurs in a modular workflow, in the neighborhood of a user specified network activity state for the reference model, the ``operating point''. First, synapses are moved to the soma, correcting for dendritic filtering by low-pass filtering the delivered synaptic current. Filter parameters are computed numerically and independently for inhibitory and excitatory input on the basal and apical dendrites, respectively, in a distance dependent and post-synaptic m-type specific manner. Next, point-neuron models for each neuron in the microcircuit are fit to their respective morphologically detailed counterparts. Here, generalized integrate-and-fire point neuron models are used, leveraging a recently published fitting toolbox. The fits are constrained by currents and voltages computed in the morphologically detailed partner neurons with soma corrected synapses at three depolarizations about the user specified operating point. The result is a simplified circuit which is well constrained by the reference circuit, and can be continuously updated as the latter iteratively integrates new data. The modularity of the approach makes it applicable also for other point-neuron and synapse models.},
  file = {Rossert et al. - Automated point-neuron simpliﬁcation of data-drive.pdf},
  language = {en}
}

@article{Rottenstreich2006,
  title = {On Decision Making without Likelihood Judgment},
  author = {Rottenstreich, Yuval and Kivetz, Ran},
  year = {2006},
  month = sep,
  volume = {101},
  pages = {74--88},
  issn = {07495978},
  doi = {10.1016/j.obhdp.2006.06.004},
  abstract = {Subjective expected utility, prospect theory and most other formal models of decision making under uncertainty are probabilistic: they assume that in making choices people judge the likelihood of relevant uncertainties. Clearly, in many situations people do indeed judge likelihood. However, we present studies suggesting that there are also many situations in which people do not judge likelihood and instead base their decisions on intuitively generated, non-probabilistic rules or rationales. Thus, we argue that real-world situations are of two types. In situations eliciting a probabilistic mindset, people rely on judgments of likelihood. In situations eliciting a non-probabilistic mindset, they neglect judgments of likelihood. We suggest three factors that may inXuence the tendency towards either probabilistic or non-probabilistic mindsets. We also outline how extant probabilistic theories may be complemented by non-probabilistic models.},
  file = {2006 - Rottenstreich, Kivetz - On decision making without likelihood judgment.pdf},
  journal = {Organizational Behavior and Human Decision Processes},
  language = {en},
  number = {1}
}

@book{Roughgarden2019,
  title = {Algorithms {{Illuminated}} ({{Part}} 3): {{Greedy Algorithms}} and {{Dynamic Programming}}},
  author = {Roughgarden, Tim},
  year = {2019},
  volume = {1}
}

@techreport{Roussel2019,
  title = {Acoustic Contamination of Electrophysiological Brain Signals during Speech Production and Sound Perception},
  author = {Roussel, Phil{\'e}mon and Bocquelet, Florent and Palma, Marie and Kahane, Philippe and Chabard{\`e}s, St{\'e}phan and Yvert, Blaise},
  year = {2019},
  month = aug,
  institution = {{Neuroscience}},
  doi = {10.1101/722207},
  abstract = {A current challenge of neurotechnologies is the development of speech brain-computer interfaces to restore communication in people unable to speak. To achieve a proof of concept of such system, neural activity can be investigated in patients implanted for clinical reasons while they speak. Using such simultaneously recorded audio and neural data, decoders can be built to predict speech features using features extracted from brain signals. A typical neural feature is the spectral power of field potentials in the high-gamma frequency band (between 70 and 200 Hz), a range that happen to overlap with the fundamental frequency of speech. Here, we analyzed human electrocorticographic (ECoG) and intracortical recordings during speech production and perception as well as rat microelectrocorticographic (\textmu -ECoG) recordings during sound perception. We observed that electrophysiological recordings, obtained with different recording setups, often contain spectrotemporal features of the sound, especially within the high-gamma band. Further analysis and in vitro replication suggest that these correlations are caused by a microphonic effect, transforming sound vibrations into an undesired electrical noise that contaminates the biopotential measurements. This study does not question the existence of relevant physiological neural information underlying speech production or sound perception in the high-gamma frequency band, but alerts on the fact that care should be taken to evaluate and eliminate any possible acoustic contamination of neural signals to investigate cortical dynamics underlying speech production and auditory perception.},
  file = {Roussel et al. - 2019 - Acoustic contamination of electrophysiological bra.pdf},
  language = {en},
  type = {Preprint}
}

@article{Roux2013,
  title = {The {{Phase}} of {{Thalamic Alpha Activity Modulates Cortical Gamma}}-{{Band Activity}}: {{Evidence}} from {{Resting}}-{{State MEG Recordings}}},
  shorttitle = {The {{Phase}} of {{Thalamic Alpha Activity Modulates Cortical Gamma}}-{{Band Activity}}},
  author = {Roux, F. and Wibral, M. and Singer, W. and Aru, J. and Uhlhaas, P. J.},
  year = {2013},
  month = nov,
  volume = {33},
  pages = {17827--17835},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.5778-12.2013},
  file = {2013 - Wibral et al. - The Phase of Thalamic Alpha Activity Modulates Cortical Gamma-Band Activity Evidence from Resting-State MEG Recor.pdf;Roux et al. - 2013 - The Phase of Thalamic Alpha Activity Modulates Cor.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {45}
}

@article{Roux2015,
  title = {Tasks for Inhibitory Interneurons in Intact Brain Circuits},
  author = {Roux, Lisa and Buzs{\'a}ki, Gy{\"o}rgy},
  year = {2015},
  month = jan,
  volume = {88},
  pages = {10--23},
  issn = {00283908},
  doi = {10.1016/j.neuropharm.2014.09.011},
  abstract = {Synaptic inhibition, brought about by a rich variety of interneuron types, counters excitation, modulates the gain, timing, tuning, bursting properties of principal cell firing, and exerts selective filtering of synaptic excitation. At the network level, it allows for coordinating transient interactions among the principal cells to form cooperative assemblies for efficient transmission of information and routing of excitatory activity across networks, typically in the form of brain oscillations. Recent techniques based on targeted expression of neuronal activity modulators, such as optogenetics, allow physiological identification and perturbation of specific interneuron subtypes in the intact brain. Combined with large-scale recordings or imaging techniques, these approaches facilitate our understanding of the multiple roles of inhibitory interneurons in shaping circuit functions.},
  file = {Roux and Buzsáki - 2015 - Tasks for inhibitory interneurons in intact brain .pdf},
  journal = {Neuropharmacology},
  language = {en}
}

@article{Rowan2013,
  title = {Synaptic {{Scaling Balances Learning}} in a {{Spiking Model}} of {{Neocortex}}},
  author = {Rowan, Mark and Neymotin, Samuel},
  year = {2013},
  month = apr,
  abstract = {Learning in the brain requires complementary mechanisms: potentiation and activity-dependent homeostatic scaling. We introduce synaptic scaling to a biologically-realistic spiking model of neocortex which can learn changes in oscillatory rhythms using STDP, and show that scaling is necessary to balance both positive and negative changes in input from potentiation and atrophy. We discuss some of the issues that arise when considering synaptic scaling in such a model, and show that scaling regulates activity whilst allowing learning to remain unaltered.},
  archiveprefix = {arXiv},
  eprint = {1304.2266},
  eprinttype = {arxiv},
  file = {2013 - Rowan, Neymotin - Synaptic Scaling Balances Learning in a Spiking Model of Neocortex.pdf},
  journal = {arXiv:1304.2266 [cs, q-bio]},
  keywords = {Computer Science - Neural and Evolutionary Computing,Quantitative Biology - Neurons and Cognition},
  language = {en},
  primaryclass = {cs, q-bio}
}

@article{Rowe2009,
  title = {Reinterpreting {{No Free Lunch}}},
  author = {Rowe, Jon E. and Vose, M. D. and Wright, Alden H.},
  year = {2009},
  month = mar,
  volume = {17},
  pages = {117--129},
  issn = {1063-6560, 1530-9304},
  doi = {10.1162/evco.2009.17.1.117},
  abstract = {Since its inception, the ``No Free Lunch'' theorem (NFL) has concerned the application of symmetry results rather than the symmetries themselves. In our view, the conflation of result and application obscures the simplicity, generality, and power of the symmetries involved. This paper separates result from application, focusing on and clarifying the nature of underlying symmetries. The result is a general set-theoretic version of NFL which speaks to symmetries when arbitrary domains and co-domains are involved. Although our framework is deterministic, we note situations where our deterministic set-theoretic results speak nevertheless to stochastic algorithms.},
  file = {Rowe et al. - 2009 - Reinterpreting No Free Lunch.pdf},
  journal = {Evolutionary Computation},
  language = {en},
  number = {1}
}

@article{Rowe2010,
  title = {Action Selection: {{A}} Race Model for Selected and Non-Selected Actions Distinguishes the Contribution of Premotor and Prefrontal Areas},
  shorttitle = {Action Selection},
  author = {Rowe, J.B. and Hughes, L. and {Nimmo-Smith}, I.},
  year = {2010},
  month = jun,
  volume = {51},
  pages = {888--896},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2010.02.045},
  abstract = {Race models have been used to explain perceptual, motor and oculomotor decisions. Here we developed a race model to explain how human subjects select actions when there are no overt rewards and no external cues to specify which action to make. Critically, we were able to estimate the cumulative activity of neuronal decision-units for selected and non-selected actions. We used functional magnetic resonance imaging (fMRI) to test for regional brain activity that correlated with the predictions of this race model. Activity in the preSMA, cingulate motor and premotor areas correlated with prospective selection between responses according to the race model. Activity in the lateral prefrontal cortex did not correlate with the race model, even though this area was active during action selection. This activity related to the degree to which individuals switched between alternative actions. Crucially, a follow-up experiment showed that it was not present on the first trial. Taken together, these results suggest that the lateral prefrontal cortex is not the source for the generation of action. It is more likely that it is involved in switching to alternatives or monitoring previous actions. Thus, our experiment shows the power of the race model in distinguishing the contribution of different areas in the selection of action.},
  file = {2010 - Rowe, Hughes, Nimmo-Smith - Action selection a race model for selected and non-selected actions distinguishes the contribution of.pdf},
  journal = {NeuroImage},
  language = {en},
  number = {2}
}

@article{Rubin2004,
  title = {High {{Frequency Stimulation}} of the {{Subthalamic Nucleus Eliminates Pathological Thalamic Rhythmicity}} in a {{Computational Model}}},
  author = {Rubin, Jonathan E. and Terman, David},
  year = {2004},
  month = may,
  volume = {16},
  pages = {211--235},
  issn = {0929-5313},
  doi = {10.1023/B:JCNS.0000025686.47117.67},
  abstract = {Deep brain stimulation (DBS) of the subthalamic nucleus (STN) or the internal segment of the globus pallidus (GPi) has recently been recognized as an important form of intervention for alleviating motor symptoms associated with Parkinson's disease, but the mechanism underlying its effectiveness remains unknown. Using a computational model, this paper considers the hypothesis that DBS works by replacing pathologically rhythmic basal ganglia output with tonic, high frequency firing. In our simulations of parkinsonian conditions, rhythmic inhibition from GPi to the thalamus compromises the ability of thalamocortical relay (TC) cells to respond to depolarizing inputs, such as sensorimotor signals. High frequency stimulation of STN regularizes GPi firing, and this restores TC responsiveness, despite the increased frequency and amplitude of GPi inhibition to thalamus that result. We provide a mathematical phase plane analysis of the mechanisms that determine TC relay capabilities in normal, parkinsonian, and DBS states in a reduced model. This analysis highlights the differences in deinactivation of the low-threshold calcium T -current that we observe in TC cells in these different conditions. Alternative scenarios involving convergence of thalamic signals in the cortex are also discussed, and predictions associated with these results, including the occurrence of rhythmic rebound bursts in certain TC cells in parkinsonian states and their drastic reduction by DBS, are stated. These results demonstrate how DBS could work by increasing firing rates of target cells, rather than shutting them down.},
  file = {2004 - Rubin, Terman - High Frequency Stimulation of the Subthalamic Nucleus Eliminates .pdf},
  journal = {Journal of Computational Neuroscience},
  language = {en},
  number = {3}
}

@article{Rubin2017,
  title = {Decoding Brain Activity Using a Large-Scale Probabilistic Functional-Anatomical Atlas of Human Cognition},
  author = {Rubin, Timothy N. and Koyejo, Oluwasanmi and Gorgolewski, Krzysztof J. and Jones, Michael N. and Poldrack, Russell A. and Yarkoni, Tal},
  editor = {Gershman, Samuel J.},
  year = {2017},
  month = oct,
  volume = {13},
  pages = {e1005649},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1005649},
  abstract = {A central goal of cognitive neuroscience is to decode human brain activity\textemdash that is, to infer mental processes from observed patterns of whole-brain activation. Previous decoding efforts have focused on classifying brain activity into a small set of discrete cognitive states. To attain maximal utility, a decoding framework must be open-ended, systematic, and context-sensitive\textemdash that is, capable of interpreting numerous brain states, presented in arbitrary combinations, in light of prior information. Here we take steps towards this objective by introducing a probabilistic decoding framework based on a novel topic model\textemdash Generalized Correspondence Latent Dirichlet Allocation\textemdash that learns latent topics from a database of over 11,000 published fMRI studies. The model produces highly interpretable, spatially-circumscribed topics that enable flexible decoding of whole-brain images. Importantly, the Bayesian nature of the model allows one to ``seed'' decoder priors with arbitrary images and text\textemdash enabling researchers, for the first time, to generate quantitative, context-sensitive interpretations of whole-brain patterns of brain activity.},
  file = {Rubin et al. - 2017 - Decoding brain activity using a large-scale probab.pdf},
  journal = {PLOS Computational Biology},
  language = {en},
  number = {10}
}

@article{Ruder,
  title = {An Overview of Gradient Descent Optimization Algorithms},
  author = {Ruder, Sebastian},
  pages = {14},
  abstract = {Gradient descent optimization algorithms, while increasingly popular, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by. This article aims to provide the reader with intuitions with regard to the behaviour of different algorithms that will allow her to put them to use. In the course of this overview, we look at different variants of gradient descent, summarize challenges, introduce the most common optimization algorithms, review architectures in a parallel and distributed setting, and investigate additional strategies for optimizing gradient descent.},
  file = {Ruder - An overview of gradient descent optimization algor.pdf},
  language = {en}
}

@article{RuiZhang2012,
  title = {Universal {{Approximation}} of {{Extreme Learning Machine With Adaptive Growth}} of {{Hidden Nodes}}},
  author = {{Rui Zhang} and {Yuan Lan} and {Guang-Bin Huang} and {Zong-Ben Xu}},
  year = {2012},
  month = feb,
  volume = {23},
  pages = {365--371},
  issn = {2162-237X, 2162-2388},
  doi = {10.1109/TNNLS.2011.2178124},
  file = {Rui Zhang et al. - 2012 - Universal Approximation of Extreme Learning Machin.pdf},
  journal = {IEEE Trans. Neural Netw. Learning Syst.},
  language = {en},
  number = {2}
}

@article{Runyan2017,
  title = {Distinct Timescales of Population Coding across Cortex},
  author = {Runyan, Caroline A. and Piasini, Eugenio and Panzeri, Stefano and Harvey, Christopher D.},
  year = {2017},
  month = aug,
  volume = {548},
  pages = {92--96},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature23020},
  file = {Runyan et al. - 2017 - Distinct timescales of population coding across co.pdf},
  journal = {Nature},
  language = {en},
  number = {7665}
}

@article{Russo,
  title = {Neural {{Dynamics}} and the {{Geometry}} of {{Population Activity}}},
  author = {Russo, Abigail A},
  pages = {169},
  file = {Russo - Neural Dynamics and the Geometry of Population Act.pdf},
  language = {en}
}

@article{Rutstrom2009,
  title = {Stated Beliefs versus Inferred Beliefs: {{A}} Methodological Inquiry and Experimental Test},
  shorttitle = {Stated Beliefs versus Inferred Beliefs},
  author = {Rutstr{\"o}m, E. Elisabet and Wilcox, Nathaniel T.},
  year = {2009},
  month = nov,
  volume = {67},
  pages = {616--632},
  issn = {08998256},
  doi = {10.1016/j.geb.2009.04.001},
  abstract = {If asking subjects their beliefs during repeated game play changes the way those subjects play, using those stated beliefs to evaluate and compare theories of strategic behavior is problematic. We experimentally verify that belief elicitation can alter paths of play in a repeated asymmetric matching pennies game. In this setting, belief elicitation improves the goodness of fit of structural models of belief learning, and the prior beliefs implied by such structural models are both stronger and more realistic when beliefs are elicited than when they are not. These effects are, however, confined to the player type who sees a strong asymmetry between payoff possibilities for her two strategies in the game. We also find that ``inferred beliefs'' (beliefs estimated from past observed actions of opponents) can be better predictors of observed actions than the ``stated beliefs'' resulting from belief elicitation.},
  file = {2008 - Rutstrom, Wilcox - Stated Beliefs Versus Inferred Beliefs.pdf},
  journal = {Games and Economic Behavior},
  language = {en},
  number = {2}
}

@article{Ryglewski2014,
  title = {Dendrites Are Dispensable for Basic Motoneuron Function but Essential for Fine Tuning of Behavior},
  author = {Ryglewski, Stefanie and Kadas, Dimitrios and Hutchinson, Katie and Schuetzler, Natalie and Vonhoff, Fernando and Duch, Carsten},
  year = {2014},
  month = dec,
  volume = {111},
  pages = {18049--18054},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1416247111},
  file = {Ryglewski et al. - 2014 - Dendrites are dispensable for basic motoneuron fun.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {50}
}

@article{Saalmann2009,
  title = {Gain Control in the Visual Thalamus during Perception and Cognition},
  author = {Saalmann, Yuri B and Kastner, Sabine},
  year = {2009},
  month = aug,
  volume = {19},
  pages = {408--414},
  issn = {09594388},
  doi = {10.1016/j.conb.2009.05.007},
  file = {2009 - Saalmann, Kastner - Gain control in the visual thalamus during perception and cognition.pdf},
  journal = {Current Opinion in Neurobiology},
  language = {en},
  number = {4}
}

@article{Saalmann2012,
  title = {The {{Pulvinar Regulates Information Transmission Between Cortical Areas Based}} on {{Attention Demands}}},
  author = {Saalmann, Y. B. and Pinsk, M. A. and Wang, L. and Li, X. and Kastner, S.},
  year = {2012},
  month = aug,
  volume = {337},
  pages = {753--756},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1223082},
  file = {2012 - Saalmann et al. - The Pulvinar Regulates Information Transmission Between Cortical Areas Based on Attention Demands.pdf},
  journal = {Science},
  language = {en},
  number = {6095}
}

@article{Saalmann2014,
  title = {Intralaminar and Medial Thalamic Influence on Cortical Synchrony, Information Transmission and Cognition},
  author = {Saalmann, Yuri B.},
  year = {2014},
  month = may,
  volume = {8},
  issn = {1662-5137},
  doi = {10.3389/fnsys.2014.00083},
  abstract = {The intralaminar and medial thalamic nuclei are part of the higher-order thalamus, which receives little sensory input, and instead forms extensive cortico-thalamo-cortical pathways. The large mediodorsal thalamic nucleus predominantly connects with the prefrontal cortex, the adjacent intralaminar nuclei connect with fronto-parietal cortex, and the midline thalamic nuclei connect with medial prefrontal cortex and medial temporal lobe. Taking into account this connectivity pattern, it is not surprising that the intralaminar and medial thalamus has been implicated in a variety of cognitive functions, including memory processing, attention and orienting, as well as reward-based behavior. This review addresses how the intralaminar and medial thalamus may regulate information transmission in cortical circuits. A key neural mechanism may involve intralaminar and medial thalamic neurons modulating the degree of synchrony between different groups of cortical neurons according to behavioral demands. Such a thalamic-mediated synchronization mechanism may give rise to large-scale integration of information across multiple cortical circuits, consequently influencing the level of arousal and consciousness. Overall, the growing evidence supports a general role for the higher-order thalamus in the control of cortical information transmission and cognitive processing.},
  file = {Saalmann - 2014 - Intralaminar and medial thalamic influence on cort.pdf},
  journal = {Frontiers in Systems Neuroscience},
  language = {en}
}

@article{Sabuncu2010,
  title = {Function-Based {{Intersubject Alignment}} of {{Human Cortical Anatomy}}},
  author = {Sabuncu, Mert R. and Singer, Benjamin D. and Conroy, Bryan and Bryan, Ronald E. and Ramadge, Peter J. and Haxby, James V.},
  year = {2010},
  month = jan,
  volume = {20},
  pages = {130--140},
  issn = {1460-2199, 1047-3211},
  doi = {10.1093/cercor/bhp085},
  file = {2010 - Sabuncu et al. - Function-based intersubject alignment of human cortical anatomy.pdf},
  journal = {Cerebral Cortex},
  language = {en},
  number = {1}
}

@article{Sacramento2017,
  title = {Dendritic Error Backpropagation in Deep Cortical Microcircuits},
  author = {Sacramento, Jo{\~a}o and Costa, Rui Ponte and Bengio, Yoshua and Senn, Walter},
  year = {2017},
  month = dec,
  abstract = {Animal behaviour depends on learning to associate sensory stimuli with the desired motor command. Understanding how the brain orchestrates the necessary synaptic modifications across different brain areas has remained a longstanding puzzle. Here, we introduce a multi-area neuronal network model in which synaptic plasticity continuously adapts the network towards a global desired output. In this model synaptic learning is driven by a local dendritic prediction error that arises from a failure to predict the top-down input given the bottom-up activities. Such errors occur at apical dendrites of pyramidal neurons where both long-range excitatory feedback and local inhibitory predictions are integrated. When local inhibition fails to match excitatory feedback an error occurs which triggers plasticity at bottom-up synapses at basal dendrites of the same pyramidal neurons. We demonstrate the learning capabilities of the model in a number of tasks and show that it approximates the classical error backpropagation algorithm. Finally, complementing this cortical circuit with a disinhibitory mechanism enables attention-like stimulus denoising and generation. Our framework makes several experimental predictions on the function of dendritic integration and cortical microcircuits, is consistent with recent observations of cross-area learning, and suggests a biological implementation of deep learning.},
  archiveprefix = {arXiv},
  eprint = {1801.00062},
  eprinttype = {arxiv},
  file = {Sacramento et al. - 2017 - Dendritic error backpropagation in deep cortical m.pdf},
  journal = {arXiv:1801.00062 [cs, q-bio]},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Quantitative Biology - Neurons and Cognition},
  language = {en},
  primaryclass = {cs, q-bio}
}

@article{Sacramento2018,
  title = {Dendritic Cortical Microcircuits Approximate the Backpropagation Algorithm},
  author = {Sacramento, Jo{\~a}o and Costa, Rui Ponte and Bengio, Yoshua and Senn, Walter},
  year = {2018},
  month = oct,
  abstract = {Deep learning has seen remarkable developments over the last years, many of them inspired by neuroscience. However, the main learning mechanism behind these advances \textendash{} error backpropagation \textendash{} appears to be at odds with neurobiology. Here, we introduce a multilayer neuronal network model with simplified dendritic compartments in which error-driven synaptic plasticity adapts the network towards a global desired output. In contrast to previous work our model does not require separate phases and synaptic learning is driven by local dendritic prediction errors continuously in time. Such errors originate at apical dendrites and occur due to a mismatch between predictive input from lateral interneurons and activity from actual top-down feedback. Through the use of simple dendritic compartments and different cell-types our model can represent both error and normal activity within a pyramidal neuron. We demonstrate the learning capabilities of the model in regression and classification tasks, and show analytically that it approximates the error backpropagation algorithm. Moreover, our framework is consistent with recent observations of learning between brain areas and the architecture of cortical microcircuits. Overall, we introduce a novel view of learning on dendritic cortical circuits and on how the brain may solve the long-standing synaptic credit assignment problem.},
  archiveprefix = {arXiv},
  eprint = {1810.11393},
  eprinttype = {arxiv},
  file = {Sacramento et al. - 2018 - Dendritic cortical microcircuits approximate the b.pdf},
  journal = {arXiv:1810.11393 [cs, q-bio]},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Quantitative Biology - Neurons and Cognition},
  language = {en},
  primaryclass = {cs, q-bio}
}

@article{Saeb2017,
  title = {The Need to Approximate the Use-Case in Clinical Machine Learning},
  author = {Saeb, Sohrab and Lonini, Luca and Jayaraman, Arun and Mohr, David C. and Kording, Konrad P.},
  year = {2017},
  month = may,
  volume = {6},
  issn = {2047-217X},
  doi = {10.1093/gigascience/gix019},
  abstract = {Background. The availability of smartphone and wearable sensor technology is leading to a rapid accumulation of human subject data, and machine learning is emerging as a technique to map that data into clinical predictions. As machine learning algorithms are increasingly used to support clinical decision making, it is vital to reliably quantify their prediction accuracy. Cross-validation is the standard approach where the accuracy of such algorithms is evaluated on data the algorithm has not seen during training. However, for this procedure to be meaningful, the relationship between the training and validation set should mimic the relationship between the training set and the dataset expected for the clinical use. Here we compared two popular cross-validation methods: record-wise and subject-wise. The subjectwise procedure mirrors the clinically relevant use-case scenario of diagnosing/identifying patterns in newly recruited subjects. The record-wise strategy has no such interpretation. Results. Using both a publicly available dataset and a simulation, we found that record-wise crossvalidation often massively overestimates the prediction accuracy of the algorithms. We also conducted a systematic review of the relevant literature, and found that this overly optimistic method is used by almost half of the retrieved studies that used accelerometers, wearable sensors, or smartphones to predict clinical outcomes. Conclusions. As we move towards an era of machine learning based diagnosis and treatment, using proper methods to evaluate their accuracy is crucial, as results that are overly optimistic can mislead both clinicians and data scientists.},
  file = {Saeb et al. - 2017 - The need to approximate the use-case in clinical m.pdf},
  journal = {GigaScience},
  language = {en},
  number = {5}
}

@article{Saenger2017,
  title = {Uncovering the Underlying Mechanisms and Whole-Brain Dynamics of Deep Brain Stimulation for {{Parkinson}}'s Disease},
  author = {Saenger, Victor M. and Kahan, Joshua and Foltynie, Tom and Friston, Karl and Aziz, Tipu Z. and Green, Alexander L. and {van Hartevelt}, Tim J. and Cabral, Joana and Stevner, Angus B. A. and Fernandes, Henrique M. and Mancini, Laura and Thornton, John and Yousry, Tarek and Limousin, Patricia and Zrinzo, Ludvic and Hariz, Marwan and Marques, Paulo and Sousa, Nuno and Kringelbach, Morten L. and Deco, Gustavo},
  year = {2017},
  month = dec,
  volume = {7},
  issn = {2045-2322},
  doi = {10.1038/s41598-017-10003-y},
  file = {Saenger et al. - 2017 - Uncovering the underlying mechanisms and whole-bra.pdf},
  journal = {Scientific Reports},
  language = {en},
  number = {1}
}

@article{Said2010,
  title = {Distributed Representations of Dynamic Facial Expressions in the Superior Temporal Sulcus},
  author = {Said, C. P. and Moore, C. D. and Engell, A. D. and Todorov, A. and Haxby, J. V.},
  year = {2010},
  month = may,
  volume = {10},
  pages = {11--11},
  issn = {1534-7362},
  doi = {10.1167/10.5.11},
  abstract = {Previous research on the superior temporal sulcus (STS) has shown that it responds more to facial expressions than to neutral faces. Here, we extend our understanding of the STS in two ways. First, using targeted high-resolution fMRI measurements of the lateral cortex and multivoxel pattern analysis, we show that the response to seven categories of dynamic facial expressions can be decoded in both the posterior STS (pSTS) and anterior STS (aSTS). We were also able to decode patterns corresponding to these expressions in the frontal operculum (FO), a structure that has also been shown to respond to facial expressions. Second, we measured the similarity structure of these representations and found that the similarity structure in the pSTS significantly correlated with the perceptual similarity structure of the expressions. This was the case regardless of whether we used pattern classification or more traditional correlation techniques to extract the neural similarity structure. These results suggest that distributed representations in the pSTS could underlie the perception of facial expressions.},
  file = {2010 - Said, Moore - Distributed representations of dynamic facial expressions in the superior temporal sulcus.pdf},
  journal = {Journal of Vision},
  language = {en},
  number = {5}
}

@article{Saisubramanian2020,
  title = {Avoiding {{Negative Side Effects}} Due to {{Incomplete Knowledge}} of {{AI Systems}}},
  author = {Saisubramanian, Sandhya and Zilberstein, Shlomo and Kamar, Ece},
  year = {2020},
  month = aug,
  abstract = {Autonomous agents acting in the real-world often operate based on models that ignore certain aspects of the environment. The incompleteness of any given model\textemdash handcrafted or machine acquired\textemdash is inevitable due to practical limitations of any modeling technique for complex real-world settings. Due to the limited fidelity of its model, an agent's actions may have unexpected, undesirable consequences during execution. Learning to recognize and avoid such negative side effects of the agent's actions is critical to improving the safety and reliability of autonomous systems. This emerging research topic is attracting increased attention due to the increased deployment of AI systems and their broad societal impacts. This article provides a comprehensive overview of different forms of negative side effects and the recent research efforts to address them. We identify key characteristics of negative side effects, highlight the challenges in avoiding negative side effects, and discuss recently developed approaches, contrasting their benefits and limitations. We conclude with a discussion of open questions and suggestions for future research directions.},
  archiveprefix = {arXiv},
  eprint = {2008.12146},
  eprinttype = {arxiv},
  file = {Saisubramanian et al. - 2020 - Avoiding Negative Side Effects due to Incomplete K.pdf},
  journal = {arXiv:2008.12146 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computers and Society},
  language = {en},
  primaryclass = {cs}
}

@article{Saleem2016,
  title = {Subcortical Source and Modulation of the Narrowband Gamma Oscillation in Mouse Visual Cortex},
  author = {Saleem, Aman B and Lien, Anthony D and Krumin, Michael and Haider, Bilal and Roman Roson, Miroslav and Ayaz, Asli and Reinhold, Kimberley and Busse, Laura and Carandini, Matteo and Harris, Kenneth D},
  year = {2016},
  month = oct,
  doi = {10.1101/050245},
  abstract = {Visual cortex (V1) exhibits two types of gamma oscillation: a well-characterized broadband (30-90Hz) rhythm, and a narrowband oscillation occurring at frequencies close to 60 Hz in mice. We investigated the source of narrowband gamma, the factors modulating its strength, and its relationship to broadband gamma. Narrowband and broadband gamma power were uncorrelated. Increasing visual contrast had opposite effects on the two kinds of gamma activity: it increased broadband power, but suppressed the narrowband oscillation. Narrowband power was strongest in layer 4, and was mediated primarily by excitatory currents entrained by rhythmically firing neuronal ensembles in the lateral geniculate nucleus (LGN). Silencing the cortex optogenetically did not affect narrowband rhythmicity in either LGN spike trains or cortical EPSCs, suggesting that this oscillation reflects unidirectional flow of information from LGN to V1.},
  file = {Saleem et al. - 2016 - Subcortical source and modulation of the narrowban.pdf},
  journal = {bioRxiv},
  language = {en}
}

@article{Salkoff2015,
  title = {Synaptic {{Mechanisms}} of {{Tight Spike Synchrony}} at {{Gamma Frequency}} in {{Cerebral Cortex}}},
  author = {Salkoff, D. B. and Zagha, E. and Yuzgec, O. and McCormick, D. A.},
  year = {2015},
  month = jul,
  volume = {35},
  pages = {10236--10251},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.0828-15.2015},
  file = {Salkoff et al. - 2015 - Synaptic Mechanisms of Tight Spike Synchrony at Ga.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {28}
}

@article{SalthouseandWemaraLichty,
  title = {Testsof {{theNeuralNoiseHypothesiosf Age}}-{{RelatedCognitiveChange}}'},
  author = {SalthouseandWemaraLichty, TimothyA},
  pages = {8},
  file = {1985 - Salthouse, Lichty - of the Neural Noise Hypothesis Cognitive Change ' J T Jrj J '.pdf},
  language = {en}
}

@article{Samaha2015,
  title = {The {{Speed}} of {{Alpha}}-{{Band Oscillations Predicts}} the {{Temporal Resolution}} of {{Visual Perception}}},
  author = {Samaha, Jason and Postle, Bradley R.},
  year = {2015},
  month = nov,
  volume = {25},
  pages = {2985--2990},
  issn = {09609822},
  doi = {10.1016/j.cub.2015.10.007},
  abstract = {Evidence suggests that scalp-recorded occipital alpha-band (8\textendash 13 Hz) oscillations reflect phasic information transfer in thalamocortical neurons projecting from lateral geniculate nucleus to visual cortex [1\textendash 5]. In animals, the phase of ongoing alpha oscillations has been shown to modulate stimulus discrimination and neuronal spiking [6]. Human research has shown that alpha phase predicts visual perception of nearthreshold stimuli [7\textendash 11] and subsequent neural activity [12\textendash 14] and that the frequency of these oscillations predicts reaction times [15], as well as the maximum temporal interval necessary for perceived simultaneity [16]. These phasic effects have led to the hypothesis that conscious perception occurs in discrete temporal windows, clocked by the frequency of alpha oscillations [17\textendash 21]. Under this hypothesis, variation in the frequency of occipital alpha oscillations should predict variation in the temporal resolution of visual perception. Specifically, when two stimuli fall within the same alpha cycle, they may be perceived as a single stimulus, resulting in perception with lower temporal resolution when alpha frequency is lower. We tested this by assessing the relationship between two-flash fusion thresholds (a measure of the temporal resolution of visual perception) and the frequency of eyes-closed and task-related alpha rhythms. We found, both between and within subjects, that faster alpha frequencies predicted more accurate flash discrimination, providing novel evidence linking alpha frequency to the temporal resolution of perception.},
  file = {2015 - Samaha, Postle - The Speed of Alpha-Band Oscillations Predicts the Temporal Resolution of Visual Perception.pdf},
  journal = {Current Biology},
  language = {en},
  number = {22}
}

@article{Samaha2016,
  title = {Decoding and {{Reconstructing}} the {{Focus}} of {{Spatial Attention}} from the {{Topography}} of {{Alpha}}-Band {{Oscillations}}},
  author = {Samaha, Jason and Sprague, Thomas C. and Postle, Bradley R.},
  year = {2016},
  month = aug,
  volume = {28},
  pages = {1090--1097},
  issn = {0898-929X, 1530-8898},
  doi = {10.1162/jocn_a_00955},
  file = {Samaha et al. - 2016 - Decoding and Reconstructing the Focus of Spatial A.pdf},
  journal = {Journal of Cognitive Neuroscience},
  language = {en},
  number = {8}
}

@article{Sanborn2016,
  title = {Bayesian {{Brains}} without {{Probabilities}}},
  author = {Sanborn, Adam N. and Chater, Nick},
  year = {2016},
  month = dec,
  volume = {20},
  pages = {883--893},
  issn = {13646613},
  doi = {10.1016/j.tics.2016.10.003},
  file = {Sanborn and Chater - 2016 - Bayesian Brains without Probabilities.pdf},
  journal = {Trends in Cognitive Sciences},
  language = {en},
  number = {12}
}

@article{Santi2014,
  title = {Quantifying the Benefits of Vehicle Pooling with Shareability Networks},
  author = {Santi, Paolo and Resta, Giovanni and Szell, Michael and Sobolevsky, Stanislav and Strogatz, Steven H. and Ratti, Carlo},
  year = {2014},
  month = sep,
  volume = {111},
  pages = {13290--13294},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1403657111},
  file = {2013 - Santi et al. - Quantifying the benefits of vehicle pooling with shareability networks.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {37}
}

@techreport{Santos-Pata2020,
  title = {Basal Forebrain Rhythmicity Is Modulated by the Exploration Phase of Novel Environments},
  author = {{Santos-Pata}, Diogo and Verschure, Paul FMJ},
  year = {2020},
  month = jan,
  institution = {{Neuroscience}},
  doi = {10.1101/2020.01.11.902742},
  abstract = {Acquaintance to novel environments requires the encoding of spatial memories and the processing of unfamiliar sensory information in the hippocampus. Cholinergic signaling promotes the stabilization of hippocampal long-term potentiation (LTP) and contributes to theta-gamma oscillations balance, which is known to be crucial for learning and memory. However, the oscillatory mechanisms by which cholinergic signals are conveyed to the hippocampus are still poorly defined. We analyzed local field potentials from the basal forebrain (BF), a major source of cholinergic projections to the hippocampus, while rats explored a novel environment, and compared the modulation of BF theta (4-10Hz) and gamma (40-80Hz) frequency bands at distinct stages of spatial exploration. We found that BF theta and gamma display learning stage-related rhythmicity and that theta-gamma coupling is stronger at the later stages of exploration, a phenomenon previously observed in the hippocampus. Overall, our results suggest that the BF-hippocampal cholinergic signaling is conveyed via the stereotypical oscillatory patterns found during mnemonic processes, which questions the origins of the learning-related rhythmic activity found in the hippocampus.                        KEY-POINTS                                                                Basal forebrain theta oscillations decrease their strength in function of exploration time, as observed in the hippocampus.                                                     BF gamma ripples (bursting events) are longer after learning.                                                     BF Theta-gamma coupling increases after initial spatial exploration, suggesting BF cross-frequency coupling relation to the learning stage.},
  file = {Santos-Pata and Verschure - 2020 - Basal forebrain rhythmicity is modulated by the ex.pdf},
  language = {en},
  type = {Preprint}
}

@article{Santos2005,
  title = {Optimization of Random Searches on Regular Lattices},
  author = {Santos, M. C. and Viswanathan, G. M. and Raposo, E. P. and {da Luz}, M. G. E.},
  year = {2005},
  month = oct,
  volume = {72},
  pages = {046143},
  issn = {1539-3755, 1550-2376},
  doi = {10.1103/PhysRevE.72.046143},
  file = {Santos et al. - 2005 - Optimization of random searches on regular lattice.pdf},
  journal = {Phys. Rev. E},
  language = {en},
  number = {4}
}

@article{Santos2005a,
  title = {Optimization of Random Searches on Regular Lattices},
  author = {Santos, M. C. and Viswanathan, G. M. and Raposo, E. P. and {da Luz}, M. G. E.},
  year = {2005},
  month = oct,
  volume = {72},
  pages = {046143},
  issn = {1539-3755, 1550-2376},
  doi = {10.1103/PhysRevE.72.046143},
  file = {Santos et al. - 2005 - Optimization of random searches on regular lattice 2.pdf},
  journal = {Phys. Rev. E},
  language = {en},
  number = {4}
}

@article{Santos2005b,
  title = {Optimization of Random Searches on Regular Lattices},
  author = {Santos, M. C. and Viswanathan, G. M. and Raposo, E. P. and {da Luz}, M. G. E.},
  year = {2005},
  month = oct,
  volume = {72},
  pages = {046143},
  issn = {1539-3755, 1550-2376},
  doi = {10.1103/PhysRevE.72.046143},
  file = {Santos et al. - 2005 - Optimization of random searches on regular lattice 3.pdf},
  journal = {Phys. Rev. E},
  language = {en},
  number = {4}
}

@article{Santos2005c,
  title = {Optimization of Random Searches on Regular Lattices},
  author = {Santos, M. C. and Viswanathan, G. M. and Raposo, E. P. and {da Luz}, M. G. E.},
  year = {2005},
  month = oct,
  volume = {72},
  pages = {046143},
  issn = {1539-3755, 1550-2376},
  doi = {10.1103/PhysRevE.72.046143},
  file = {Santos et al. - 2005 - Optimization of random searches on regular lattice 4.pdf},
  journal = {Phys. Rev. E},
  language = {en},
  number = {4}
}

@article{Sapountzis2010,
  title = {A Comparison of {{fMRI}} Adaptation and Multivariate Pattern Classification Analysis in Visual Cortex},
  author = {Sapountzis, Panagiotis and Schluppeck, Denis and Bowtell, Richard and Peirce, Jonathan W.},
  year = {2010},
  month = jan,
  volume = {49},
  pages = {1632--1640},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2009.09.066},
  abstract = {Functional magnetic resonance imaging (fMRI) has become a ubiquitous tool in cognitive neuroscience. The technique allows noninvasive measurements of cortical responses in the human brain, but only on the millimeter scale. Because a typical voxel contains many thousands of neurons with varied properties, establishing the selectivity of their responses directly is impossible. In recent years, two methods using fMRI aimed at studying the selectivity of neuronal populations on a `subvoxel' scale have been heavily used. The first technique, fMRI adaptation, relies on the observation that the blood oxygen level-dependent (BOLD) response in a given voxel is reduced after prolonged presentation of a stimulus, and that this reduction is selective to the characteristics of the repeated stimuli (adapters). The second technique, multivariate pattern analysis (MVPA), makes use of multivariate statistics to recover small biases in individual voxels in their responses to different stimuli. It is thought that these biases arise due to the uneven distribution of neurons (with different properties) sampled by the many voxels in the imaged volume. These two techniques have not been compared explicitly, however, and little is known about their relative sensitivities. Here, we compared fMRI results from orientation-specific visual adaptation and orientation\textendash classification by MVPA, using optimized experimental designs for each, and found that the multivariate pattern classification approach was more sensitive to small differences in stimulus orientation than the adaptation paradigm. Estimates of orientation selectivity obtained with the two methods were, however, very highly correlated across visual areas.},
  file = {2010 - Sapountzis et al. - A comparison of fMRI adaptation and multivariate pattern classification analysis in visual cortex.pdf},
  journal = {NeuroImage},
  language = {en},
  number = {2}
}

@article{Sarin1999,
  title = {Payoff {{Assessments}} without {{Probabilities}}: {{A Simple Dynamic Model}} of {{Choice}}},
  shorttitle = {Payoff {{Assessments}} without {{Probabilities}}},
  author = {Sarin, Rajiv and Vahid, Farshid},
  year = {1999},
  month = aug,
  volume = {28},
  pages = {294--309},
  issn = {08998256},
  doi = {10.1006/game.1998.0702},
  file = {1999 - Sarin, Vahid - Payoff assessments without probabilities A simple dynamic model of choice.pdf},
  journal = {Games and Economic Behavior},
  language = {en},
  number = {2}
}

@article{Sarin2001,
  title = {Predicting {{How People Play Games}}: {{A Simple Dynamic Model}} of {{Choice}}},
  shorttitle = {Predicting {{How People Play Games}}},
  author = {Sarin, Rajiv and Vahid, Farshid},
  year = {2001},
  month = jan,
  volume = {34},
  pages = {104--122},
  issn = {08998256},
  doi = {10.1006/game.1999.0783},
  file = {1999 - Sarin, Vahid - Predicting How People Play Games Suhglfwlqj Krz Shrsoh Sod Jdphv = D Vlpsoh.pdf},
  journal = {Games and Economic Behavior},
  language = {en},
  number = {1}
}

@article{Sato2000,
  title = {Axonal Branching Pattern of Neurons of the Subthalamic Nucleus in Primates},
  author = {Sato, Fumi and Parent, Martin and Levesque, Martin and Parent, Andre},
  year = {2000},
  month = aug,
  volume = {424},
  pages = {142--152},
  issn = {0021-9967, 1096-9861},
  doi = {10.1002/1096-9861(20000814)424:1<142::AID-CNE10>3.0.CO;2-8},
  abstract = {Axonal projections arising from the subthalamic nucleus (STN) in cynomolgus monkeys (Macaca fascicularis) were traced after labeling small pools (5\textendash 15 cells) of neurons with biotinylated dextran amine. Seventy-five single axons were reconstructed from serial sagittal sections with a camera lucida. Most of the STN labeled cells displayed five to eight long, sparsely spined dendrites that arborized mostly along the main axis of the nucleus. Based on their axonal targets, five distinct types of STN projection neurons have been identified: 1) neurons projecting to the substantia nigra pars reticulata (SNr), the internal (GPi) and external (GPe) segments of the globus pallidus (21.3\%); 2) neurons targeting SNr and GPe (2.7\%); 3) neurons projecting to GPi and GPe (48\%); 4) neurons targeting GPe only (10.7 \%); and 5) neurons with axons that coursed toward the sriatum, but whose terminal arborization could not be visualized in detail (17.3\%). Axons of the first two types bifurcated into rostral subthalamopallidal and caudal pallidonigral branches. However, the majority of STN axons had only a single branch that coursed rostrally toward the pallidum and striatum. These results reveal that, in contrast to current beliefs, the primate STN is not a monolithic entity. This nucleus harbors several subtypes of projection neurons, each endowed with a highly patterned set of collaterals. This organization allows STN neurons to exert a multifarious effect not only on the GPe, with which the STN is reciprocally connected, but also on the two major output structures of the basal ganglia, the SNr and the GPi. J. Comp. Neurol. 424: 142\textendash 152, 2000. \textcopyright{} 2000 Wiley-Liss, Inc.},
  file = {2000 - Sato et al. - Axonal branching pattern of neurons of the subthalamic nucleus in primates.pdf},
  journal = {The Journal of Comparative Neurology},
  language = {en},
  number = {1}
}

@article{Sato2001,
  title = {Search {{Efficiency}} but {{Not Response Interference Affects Visual Selection}} in {{Frontal Eye Field}}},
  author = {Sato, Takashi and Murthy, Aditya and Thompson, Kirk G. and Schall, Jeffrey D.},
  year = {2001},
  month = may,
  volume = {30},
  pages = {583--591},
  issn = {08966273},
  doi = {10.1016/S0896-6273(01)00304-X},
  abstract = {Two manipulations of a visual search task were used to test the hypothesis that the discrimination of a target from distractors by visually responsive neurons in the frontal eye field (FEF) marks the outcome and conclusion of visual processing instead of saccade preparation. First, search efficiency was reduced by increasing the similarity of the distractors to the target. Second, response interference was introduced by infrequently changing the location of the target in the array. Both manipulations increased reaction time, but only the change in search efficiency affected the time needed to select the target by visually responsive neurons. This result indicates that visually responsive neurons in FEF form an explicit representation of the location of the target in the image.},
  file = {2001 - Sato et al. - Search efficiency but not response interference affects visual selection in frontal eye field.pdf},
  journal = {Neuron},
  language = {en},
  number = {2}
}

@article{Saxe2013,
  title = {Exact Solutions to the Nonlinear Dynamics of Learning in Deep Linear Neural Networks},
  author = {Saxe, Andrew M. and McClelland, James L. and Ganguli, Surya},
  year = {2013},
  month = dec,
  abstract = {Despite the widespread practical success of deep learning methods, our theoretical understanding of the dynamics of learning in deep neural networks remains quite sparse. We attempt to bridge the gap between the theory and practice of deep learning by systematically analyzing learning dynamics for the restricted case of deep linear neural networks. Despite the linearity of their input-output map, such networks have nonlinear gradient descent dynamics on weights that change with the addition of each new hidden layer. We show that deep linear networks exhibit nonlinear learning phenomena similar to those seen in simulations of nonlinear networks, including long plateaus followed by rapid transitions to lower error solutions, and faster convergence from greedy unsupervised pretraining initial conditions than from random initial conditions. We provide an analytical description of these phenomena by finding new exact solutions to the nonlinear dynamics of deep learning. Our theoretical analysis also reveals the surprising finding that as the depth of a network approaches infinity, learning speed can nevertheless remain finite: for a special class of initial conditions on the weights, very deep networks incur only a finite, depth independent, delay in learning speed relative to shallow networks. We show that, under certain conditions on the training data, unsupervised pretraining can find this special class of initial conditions, while scaled random Gaussian initializations cannot. We further exhibit a new class of random orthogonal initial conditions on weights that, like unsupervised pre-training, enjoys depth independent learning times. We further show that these initial conditions also lead to faithful propagation of gradients even in deep nonlinear networks, as long as they operate in a special regime known as the edge of chaos.},
  archiveprefix = {arXiv},
  eprint = {1312.6120},
  eprinttype = {arxiv},
  file = {2013 - Saxe, McClelland, Ganguli - Exact solutions to the nonlinear dynamics of learning in deep linear neural networks.pdf},
  journal = {arXiv:1312.6120 [cond-mat, q-bio, stat]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Condensed Matter - Disordered Systems and Neural Networks,Quantitative Biology - Neurons and Cognition,Statistics - Machine Learning},
  language = {en},
  primaryclass = {cond-mat, q-bio, stat}
}

@article{Saxe2021,
  title = {If Deep Learning Is the Answer, What Is the Question?},
  author = {Saxe, Andrew and Nelli, Stephanie and Summerfield, Christopher},
  year = {2021},
  month = jan,
  volume = {22},
  pages = {55--67},
  issn = {1471-003X, 1471-0048},
  doi = {10.1038/s41583-020-00395-8},
  abstract = {Neuroscience research is undergoing a minor revolution. Recent advances in machine learning and artificial intelligence research have opened up new ways of thinking about neural computation. Many researchers are excited by the possibility that deep neural networks may offer theories of perception, cognition and action for biological brains. This approach has the potential to radically reshape our approach to understanding neural systems, because the computations performed by deep networks are learned from experience, and not endowed by the researcher. If so, how can neuroscientists use deep networks to model and understand biological brains? What is the outlook for neuroscientists who seek to characterize computations or neural codes, or who wish to understand perception, attention, memory and executive functions? In this Perspective, our goal is to offer a road map for systems neuroscience research in the age of deep learning. We discuss the conceptual and methodological challenges of comparing behaviour, learning dynamics and neural representations in artificial and biological systems, and we highlight new research questions that have emerged for neuroscience as a direct consequence of recent advances in machine learning.},
  file = {Saxe et al. - 2021 - If deep learning is the answer, what is the questi.pdf},
  journal = {Nat Rev Neurosci},
  language = {en},
  number = {1}
}

@article{Scemes2006,
  title = {Astrocyte Calcium Waves: {{What}} They Are and What They Do},
  shorttitle = {Astrocyte Calcium Waves},
  author = {Scemes, Eliana and Giaume, Christian},
  year = {2006},
  month = nov,
  volume = {54},
  pages = {716--725},
  issn = {0894-1491, 1098-1136},
  doi = {10.1002/glia.20374},
  abstract = {Several lines of evidence indicate that the elaborated calcium signals and the occurrence of calcium waves in astrocytes provide these cells with a specific form of excitability. The identification of the cellular and molecular steps involved in the triggering and transmission of Ca21 waves between astrocytes resulted in the identification of two pathways mediating this form of intercellular communication. One of them involves the direct communication between the cytosols of two adjoining cells through gap junction channels, while the other depends upon the release of ``gliotransmitters'' that activates membrane receptors on neighboring cells. In this review we summarize evidence in favor of these two mechanisms of Ca21 wave transmission and we discuss that they may not be mutually exclusive, but are likely to work in conjunction to coordinate the activity of a group of cells. To address a key question regarding the functional consequences following the passage of a Ca21 wave, we list, in this review, some of the potential intracellular targets of these Ca21 transients in astrocytes, and discuss the functional consequences of the activation of these targets for the interactions that astrocytes maintain with themselves and with other cellular partners, including those at the glial/vasculature interface and at perisynaptic sites where astrocytic processes tightly interact with neurons. VC 2006 Wiley-Liss, Inc.},
  file = {Scemes and Giaume - 2006 - Astrocyte calcium waves What they are and what th.pdf},
  journal = {Glia},
  language = {en},
  number = {7}
}

@article{Schaffer2013,
  title = {A {{Complex}}-{{Valued Firing}}-{{Rate Model That Approximates}} the {{Dynamics}} of {{Spiking Networks}}},
  author = {Schaffer, Evan S. and Ostojic, Srdjan and Abbott, L. F.},
  editor = {Ermentrout, Bard},
  year = {2013},
  month = oct,
  volume = {9},
  pages = {e1003301},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1003301},
  abstract = {Firing-rate models provide an attractive approach for studying large neural networks because they can be simulated rapidly and are amenable to mathematical analysis. Traditional firing-rate models assume a simple form in which the dynamics are governed by a single time constant. These models fail to replicate certain dynamic features of populations of spiking neurons, especially those involving synchronization. We present a complex-valued firing-rate model derived from an eigenfunction expansion of the Fokker-Planck equation and apply it to the linear, quadratic and exponential integrate-andfire models. Despite being almost as simple as a traditional firing-rate description, this model can reproduce firing-rate dynamics due to partial synchronization of the action potentials in a spiking model, and it successfully predicts the transition to spike synchronization in networks of coupled excitatory and inhibitory neurons.},
  file = {2013 - Schaffer, Ostojic, Abbott - A Complex-Valued Firing-Rate Model That Approximates the Dynamics of Spiking Networks.pdf},
  journal = {PLoS Computational Biology},
  language = {en},
  number = {10}
}

@article{Schaworonkow2015,
  title = {Power-Law Dynamics in Neuronal and Behavioral Data Introduce Spurious Correlations: {{Power}}-{{Law Dynamics}} in {{Neuronal}} and {{Behavioral Data}}},
  shorttitle = {Power-Law Dynamics in Neuronal and Behavioral Data Introduce Spurious Correlations},
  author = {Schaworonkow, Natalie and Blythe, Duncan A.J. and Kegeles, Jewgeni and Curio, Gabriel and Nikulin, Vadim V.},
  year = {2015},
  month = aug,
  volume = {36},
  pages = {2901--2914},
  issn = {10659471},
  doi = {10.1002/hbm.22816},
  abstract = {Relating behavioral and neuroimaging measures is essential to understanding human brain function. Often, this is achieved by computing a correlation between behavioral measures, e.g., reaction times, and neurophysiological recordings, e.g., prestimulus EEG alpha-power, on a single-trial-basis. This approach treats individual trials as independent measurements and ignores the fact that data are acquired in a temporal order. It has already been shown that behavioral measures as well as neurophysiological recordings display power-law dynamics, which implies that trials are not in fact independent. Critically, computing the correlation coefficient between two measures exhibiting long-range temporal dependencies may introduce spurious correlations, thus leading to erroneous conclusions about the relationship between brain activity and behavioral measures. Here, we address data-analytic pitfalls which may arise when long-range temporal dependencies in neural as well as behavioral measures are ignored. We quantify the influence of temporal dependencies of neural and behavioral measures on the observed correlations through simulations. Results are further supported in analysis of real EEG data recorded in a simple reaction time task, where the aim is to predict the latency of responses on the basis of prestimulus alpha oscillations. We show that it is possible to "predict" reaction times from one subject on the basis of EEG activity recorded in another subject simply owing to the fact that both measures display power-law dynamics. The same is true when correlating EEG activity obtained from different subjects. A surrogatedata procedure is described which correctly tests for the presence of correlation while controlling for the effect of power-law dynamics. Hum Brain Mapp 00:000\textendash 000, 2015. VC 2015 Wiley Periodicals, Inc.},
  file = {Schaworonkow et al. - 2015 - Power-law dynamics in neuronal and behavioral data.pdf},
  journal = {Human Brain Mapping},
  language = {en},
  number = {8}
}

@article{Schaworonkow2015a,
  title = {Power-Law Dynamics in Neuronal and Behavioral Data Introduce Spurious Correlations: {{Power}}-{{Law Dynamics}} in {{Neuronal}} and {{Behavioral Data}}},
  shorttitle = {Power-Law Dynamics in Neuronal and Behavioral Data Introduce Spurious Correlations},
  author = {Schaworonkow, Natalie and Blythe, Duncan A.J. and Kegeles, Jewgeni and Curio, Gabriel and Nikulin, Vadim V.},
  year = {2015},
  month = aug,
  volume = {36},
  pages = {2901--2914},
  issn = {10659471},
  doi = {10.1002/hbm.22816},
  abstract = {Relating behavioral and neuroimaging measures is essential to understanding human brain function. Often, this is achieved by computing a correlation between behavioral measures, e.g., reaction times, and neurophysiological recordings, e.g., prestimulus EEG alpha-power, on a single-trial-basis. This approach treats individual trials as independent measurements and ignores the fact that data are acquired in a temporal order. It has already been shown that behavioral measures as well as neurophysiological recordings display power-law dynamics, which implies that trials are not in fact independent. Critically, computing the correlation coefficient between two measures exhibiting long-range temporal dependencies may introduce spurious correlations, thus leading to erroneous conclusions about the relationship between brain activity and behavioral measures. Here, we address data-analytic pitfalls which may arise when long-range temporal dependencies in neural as well as behavioral measures are ignored. We quantify the influence of temporal dependencies of neural and behavioral measures on the observed correlations through simulations. Results are further supported in analysis of real EEG data recorded in a simple reaction time task, where the aim is to predict the latency of responses on the basis of prestimulus alpha oscillations. We show that it is possible to "predict" reaction times from one subject on the basis of EEG activity recorded in another subject simply owing to the fact that both measures display power-law dynamics. The same is true when correlating EEG activity obtained from different subjects. A surrogatedata procedure is described which correctly tests for the presence of correlation while controlling for the effect of power-law dynamics. Hum Brain Mapp 36:2901\textendash 2914, 2015. VC 2015 Wiley Periodicals, Inc.},
  file = {Schaworonkow et al. - 2015 - Power-law dynamics in neuronal and behavioral data 2.pdf},
  journal = {Hum. Brain Mapp.},
  language = {en},
  number = {8}
}

@article{Schillebeeckx2013,
  title = {The Missing Piece to Changing the University Culture},
  author = {Schillebeeckx, Maximiliaan and Maricque, Brett and Lewis, Cory},
  year = {2013},
  month = oct,
  volume = {31},
  pages = {938--941},
  issn = {1087-0156, 1546-1696},
  doi = {10.1038/nbt.2706},
  file = {2013 - Schillebeeckx, Maricque, Lewis - The missing piece to changing the university culture.pdf},
  journal = {Nature Biotechnology},
  language = {en},
  number = {10}
}

@article{Schmah2010,
  title = {Comparing {{Classification Methods}} for {{Longitudinal fMRI Studies}}},
  author = {Schmah, Tanya and Yourganov, Grigori and Zemel, Richard S. and Hinton, Geoffrey E. and Small, Steven L. and Strother, Stephen C.},
  year = {2010},
  month = nov,
  volume = {22},
  pages = {2729--2762},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/NECO_a_00024},
  file = {2010 - Schmah et al. - Comparing classification methods for longitudinal fMRI studies.pdf},
  journal = {Neural Computation},
  language = {en},
  number = {11}
}

@article{Schmidhuber,
  title = {A {{Possibility}} for {{Implementing Curiosity}} and {{Boredom}} in {{Model}}-{{Building Neural Controllers}}},
  author = {Schmidhuber, Jurgen},
  pages = {7},
  file = {1991 - Urgen Schmidhuber - A Possibility for Implementing Curiosity and Boredom in Model-Building Neural Controllers.pdf},
  language = {en}
}

@article{Schmidhuber1991,
  title = {A Possibility for Implementing Curiosity and Boredom in Model-Building Neural Controllers},
  author = {Schmidhuber},
  year = {1991},
  pages = {222--227},
  file = {1991 - Schmidhuber - A possibility for implementing curiosity and boredom in model-building neural controllers.pdf},
  journal = {Proc. of the international conference on simulation of adaptive behavior: From animals to animats}
}

@inproceedings{Schmidhuber1991a,
  title = {Curious Model-Building Control Systems},
  booktitle = {[{{Proceedings}}] 1991 {{IEEE International Joint Conference}} on {{Neural Networks}}},
  author = {Schmidhuber, J.},
  year = {1991},
  pages = {1458-1463 vol.2},
  publisher = {{IEEE}},
  address = {{Singapore}},
  doi = {10.1109/IJCNN.1991.170605},
  abstract = {A controller is a device which receives inputs from a (dynamic) environment and produces outputs that manipulate the environmental state. A model-building control system is a controller with an additional module (the `world model') which is trained to predict future inputs from previous input/action pairs. The novel curious model-building control system described in this paper is a model-building control system which actively tries to provoke situations for which it learned to expect to learn something about the environment. Such a system has been implemented as a 4-network system based on Watkins' Q-learning algorithm which can be used to maximize the expectation of the temporal derivative of the adaptive assumed reliability of future predictions. An experiment with an arti cial non-deterministic environment demonstrates that the system can be superior to previous model-building control systems (the latter do not address the problem of modelling the reliability of the world model's predictions in uncertain environments and use ad-hoc methods (like random search) to train the world model).},
  file = {Schmidhuber - 1991 - Curious model-building control systems.pdf},
  isbn = {978-0-7803-0227-3},
  language = {en}
}

@inproceedings{Schmidhuber1991b,
  title = {Curious Model-Building Control Systems},
  booktitle = {[{{Proceedings}}] 1991 {{IEEE International Joint Conference}} on {{Neural Networks}}},
  author = {Schmidhuber, J.},
  year = {1991},
  pages = {1458-1463 vol.2},
  publisher = {{IEEE}},
  address = {{Singapore}},
  doi = {10.1109/IJCNN.1991.170605},
  abstract = {A controller is a device which receives inputs from a (dynamic) environment and produces outputs that manipulate the environmental state. A model-building control system is a controller with an additional module (the `world model') which is trained to predict future inputs from previous input/action pairs. The novel curious model-building control system described in this paper is a model-building control system which actively tries to provoke situations for which it learned to expect to learn something about the environment. Such a system has been implemented as a 4-network system based on Watkins' Q-learning algorithm which can be used to maximize the expectation of the temporal derivative of the adaptive assumed reliability of future predictions. An experiment with an arti cial non-deterministic environment demonstrates that the system can be superior to previous model-building control systems (the latter do not address the problem of modelling the reliability of the world model's predictions in uncertain environments and use ad-hoc methods (like random search) to train the world model).},
  file = {Schmidhuber - 1991 - Curious model-building control systems 2.pdf},
  isbn = {978-0-7803-0227-3},
  language = {en}
}

@article{Schmidhuber2008,
  title = {Driven by {{Compression Progress}}: {{A Simple Principle Explains Essential Aspects}} of {{Subjective Beauty}}, {{Novelty}}, {{Surprise}}, {{Interestingness}}, {{Attention}}, {{Curiosity}}, {{Creativity}}, {{Art}}, {{Science}}, {{Music}}, {{Jokes}}},
  shorttitle = {Driven by {{Compression Progress}}},
  author = {Schmidhuber, Juergen},
  year = {2008},
  month = dec,
  abstract = {I argue that data becomes temporarily interesting by itself to some self-improving, but computationally limited, subjective observer once he learns to predict or compress the data in a better way, thus making it subjectively simpler and more beautiful. Curiosity is the desire to create or discover more non-random, nonarbitrary, regular data that is novel and surprising not in the traditional sense of Boltzmann and Shannon but in the sense that it allows for compression progress because its regularity was not yet known. This drive maximizes interestingness, the first derivative of subjective beauty or compressibility, that is, the steepness of the learning curve. It motivates exploring infants, pure mathematicians, composers, artists, dancers, comedians, yourself, and (since 1990) artificial systems.},
  archiveprefix = {arXiv},
  eprint = {0812.4360},
  eprinttype = {arxiv},
  file = {Schmidhuber - 2008 - Driven by Compression Progress A Simple Principle.pdf},
  journal = {arXiv:0812.4360 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Neural and Evolutionary Computing},
  language = {en},
  primaryclass = {cs}
}

@article{Schmidhuber2010,
  title = {Formal {{Theory}} of {{Creativity}}, {{Fun}}, and {{Intrinsic Motivation}} (1990\textendash 2010)},
  author = {Schmidhuber, J{\"u}rgen},
  year = {2010},
  month = sep,
  volume = {2},
  pages = {230--247},
  issn = {1943-0604, 1943-0612},
  doi = {10.1109/TAMD.2010.2056368},
  abstract = {The simple but general formal theory of fun \& intrinsic motivation \& creativity (1990-) is based on the concept of maximizing intrinsic reward for the active creation or discovery of novel, surprising patterns allowing for improved prediction or data compression. It generalizes the traditional field of active learning, and is related to old but less formal ideas in aesthetics theory and developmental psychology. It has been argued that the theory explains many essential aspects of intelligence including autonomous development, science, art, music, humor. This overview first describes theoretically optimal (but not necessarily practical) ways of implementing the basic computational principles on exploratory, intrinsically motivated agents or robots, encouraging them to provoke event sequences exhibiting previously unknown but learnable algorithmic regularities. Emphasis is put on the importance of limited computational resources for online prediction and compression. Discrete and continuous time formulations are given. Previous practical but non-optimal implementations (1991, 1995, 1997-2002) are reviewed, as well as several recent variants by others (2005-). A simplified typology addresses current confusion concerning the precise nature of intrinsic motivation.},
  file = {Schmidhuber - 2010 - Formal Theory of Creativity, Fun, and Intrinsic Mo.pdf},
  journal = {IEEE Transactions on Autonomous Mental Development},
  language = {en},
  number = {3}
}

@article{Schmidhuber2013,
  title = {{{PowerPlay}}: {{Training}} an {{Increasingly General Problem Solver}} by {{Continually Searching}} for the {{Simplest Still Unsolvable Problem}}},
  shorttitle = {{{PowerPlay}}},
  author = {Schmidhuber, J{\"u}rgen},
  year = {2013},
  volume = {4},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2013.00313},
  abstract = {Most of computer science focuses on automatically solving given computational problems. I focus on automatically inventing or discovering problems in a way inspired by the playful behavior of animals and humans, to train a more and more general problem solver from scratch in an unsupervised fashion. Consider the infinite set of all computable descriptions of tasks with possibly computable solutions. Given a general problem-solving architecture, at any given time, the novel algorithmic framework PowerPlay (Schmidhuber, 2011) searches the space of possible pairs of new tasks and modifications of the current problem solver, until it finds a more powerful problem solver that provably solves all previously learned tasks plus the new one, while the unmodified predecessor does not. Newly invented tasks may require to achieve a wow-effect by making previously learned skills more efficient such that they require less time and space. New skills may (partially) re-use previously learned skills. The greedy search of typical PowerPlay variants uses timeoptimal program search to order candidate pairs of tasks and solver modifications by their conditional computational (time and space) complexity, given the stored experience so far. The new task and its corresponding task-solving skill are those first found and validated. This biases the search toward pairs that can be described compactly and validated quickly. The computational costs of validating new tasks need not grow with task repertoire size. Standard problem solver architectures of personal computers or neural networks tend to generalize by solving numerous tasks outside the self-invented training set; PowerPlay's ongoing search for novelty keeps breaking the generalization abilities of its present solver. This is related to G\"odel's sequence of increasingly powerful formal theories based on adding formerly unprovable statements to the axioms without affecting previously provable theorems.The continually increasing repertoire of problem-solving procedures can be exploited by a parallel search for solutions to additional externally posed tasks. PowerPlay may be viewed as a greedy but practical implementation of basic principles of creativity (Schmidhuber, 2006a, 2010). A first experimental analysis can be found in separate papers (Srivastava et al., 2012a,b, 2013).},
  file = {Schmidhuber - 2013 - PowerPlay Training an Increasingly General Proble.pdf},
  journal = {Front. Psychol.},
  language = {en}
}

@article{Schmidhuber2015,
  title = {On {{Learning}} to {{Think}}: {{Algorithmic Information Theory}} for {{Novel Combinations}} of {{Reinforcement Learning Controllers}} and {{Recurrent Neural World Models}}},
  shorttitle = {On {{Learning}} to {{Think}}},
  author = {Schmidhuber, Juergen},
  year = {2015},
  month = nov,
  abstract = {This paper addresses the general problem of reinforcement learning (RL) in partially observable environments. In 2013, our large RL recurrent neural networks (RNNs) learned from scratch to drive simulated cars from high-dimensional video input. However, real brains are more powerful in many ways. In particular, they learn a predictive model of their initially unknown environment, and somehow use it for abstract (e.g., hierarchical) planning and reasoning. Guided by algorithmic information theory, we describe RNN-based AIs (RNNAIs) designed to do the same. Such an RNNAI can be trained on never-ending sequences of tasks, some of them provided by the user, others invented by the RNNAI itself in a curious, playful fashion, to improve its RNN-based world model. Unlike our previous model-building RNN-based RL machines dating back to 1990, the RNNAI learns to actively query its model for abstract reasoning and planning and decision making, essentially ``learning to think.'' The basic ideas of this report can be applied to many other cases where one RNN-like system exploits the algorithmic information content of another. They are taken from a grant proposal submitted in Fall 2014, and also explain concepts such as ``mirror neurons.'' Experimental results will be described in separate papers.},
  archiveprefix = {arXiv},
  eprint = {1511.09249},
  eprinttype = {arxiv},
  file = {2015 - Schmidhuber - On Learning to Think Algorithmic Information Theory for Novel Combinations of Reinforcement Learning Controllers an.pdf},
  journal = {arXiv:1511.09249 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  language = {en},
  primaryclass = {cs}
}

@article{Schmidhuber2015a,
  title = {On {{Learning}} to {{Think}}: {{Algorithmic Information Theory}} for {{Novel Combinations}} of {{Reinforcement Learning Controllers}} and {{Recurrent Neural World Models}}},
  shorttitle = {On {{Learning}} to {{Think}}},
  author = {Schmidhuber, Juergen},
  year = {2015},
  month = nov,
  abstract = {This paper addresses the general problem of reinforcement learning (RL) in partially observable environments. In 2013, our large RL recurrent neural networks (RNNs) learned from scratch to drive simulated cars from high-dimensional video input. However, real brains are more powerful in many ways. In particular, they learn a predictive model of their initially unknown environment, and somehow use it for abstract (e.g., hierarchical) planning and reasoning. Guided by algorithmic information theory, we describe RNN-based AIs (RNNAIs) designed to do the same. Such an RNNAI can be trained on never-ending sequences of tasks, some of them provided by the user, others invented by the RNNAI itself in a curious, playful fashion, to improve its RNN-based world model. Unlike our previous model-building RNN-based RL machines dating back to 1990, the RNNAI learns to actively query its model for abstract reasoning and planning and decision making, essentially ``learning to think.'' The basic ideas of this report can be applied to many other cases where one RNN-like system exploits the algorithmic information content of another. They are taken from a grant proposal submitted in Fall 2014, and also explain concepts such as ``mirror neurons.'' Experimental results will be described in separate papers.},
  archiveprefix = {arXiv},
  eprint = {1511.09249},
  eprinttype = {arxiv},
  file = {Schmidhuber - 2015 - On Learning to Think Algorithmic Information Theo.pdf},
  journal = {arXiv:1511.09249 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  language = {en},
  primaryclass = {cs}
}

@article{Schmidhuber2019,
  title = {Unsupervised {{Minimax}}: {{Adversarial Curiosity}}, {{Generative Adversarial Networks}}, and {{Predictability Minimization}}},
  shorttitle = {Unsupervised {{Minimax}}},
  author = {Schmidhuber, Juergen},
  year = {2019},
  month = jun,
  abstract = {Generative Adversarial Networks (GANs) learn to model data distributions through two unsupervised neural networks, each minimizing the objective function maximized by the other. We relate this game theoretic strategy to earlier neural networks playing unsupervised minimax games. (i) GANs can be formulated as a special case of Adversarial Curiosity (1990) based on a minimax duel between two networks, one generating data through its probabilistic actions, the other predicting consequences thereof. (ii) We correct a previously published claim that Predictability Minimization (PM, 1990s) is not based on a minimax game. PM models data distributions through a neural encoder that maximizes the objective function minimized by a neural predictor of the code components.},
  archiveprefix = {arXiv},
  eprint = {1906.04493},
  eprinttype = {arxiv},
  file = {Schmidhuber - 2019 - Unsupervised Minimax Adversarial Curiosity, Gener.pdf},
  journal = {arXiv:1906.04493 [cs]},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  language = {en},
  primaryclass = {cs}
}

@article{Schmidhuber2019b,
  title = {Reinforcement {{Learning Upside Down}}: {{Don}}'t {{Predict Rewards}} -- {{Just Map Them}} to {{Actions}}},
  shorttitle = {Reinforcement {{Learning Upside Down}}},
  author = {Schmidhuber, Juergen},
  year = {2019},
  month = dec,
  abstract = {We transform reinforcement learning (RL) into a form of supervised learning (SL) by turning traditional RL on its head, calling this or Upside Down RL (UDRL). Standard RL predicts rewards, while instead uses rewards as task-defining inputs, together with representations of time horizons and other computable functions of historic and desired future data. learns to interpret these input observations as commands, mapping them to actions (or action probabilities) through SL on past (possibly accidental) experience. generalizes to achieve high rewards or other goals, through input commands such as: get lots of reward within at most so much time! A separate paper [61] on first experiments with shows that even a pilot version of can outperform traditional baseline algorithms on certain challenging RL problems.},
  archiveprefix = {arXiv},
  eprint = {1912.02875},
  eprinttype = {arxiv},
  file = {Schmidhuber - 2019 - Reinforcement Learning Upside Down Don't Predict .pdf},
  journal = {arXiv:1912.02875 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  language = {en},
  primaryclass = {cs}
}

@article{Schmidhubera,
  title = {{{EVALUATING BENCHMARK PROBLEMS BY RANDOM GUESSING}}},
  author = {Schmidhuber, Jurgen and Hochreiter, Sepp and Bengio, Yoshua},
  pages = {12},
  file = {Schmidhuber et al. - EVALUATING BENCHMARK PROBLEMS BY RANDOM GUESSING.pdf},
  language = {en}
}

@article{Schmidhuberb,
  title = {Artificial {{Curiosity Based}} on {{Discovering Novel Algorithmic Predictability Through Coevolution}}},
  author = {Schmidhuber, Jurgen},
  pages = {7},
  abstract = {How to explore a spatio-temporal simulations include an example where surprisedomain? By predicting and learning from suc- generation of this kind helps to speed up external cess/failure what's predictable and what's not. reward.},
  file = {Schmidhuber - Arti cial Curiosity Based on Discovering Novel Alg.pdf},
  language = {en}
}

@article{Schmidt2013,
  title = {Patterned {{Brain Stimulation}}, {{What}} a {{Framework}} with {{Rhythmic}} and {{Noisy Components Might Tell Us}} about {{Recovery Maximization}}},
  author = {Schmidt, Sein and Scholz, Michael and Obermayer, Klaus and Brandt, Stephan A.},
  year = {2013},
  volume = {7},
  issn = {1662-5161},
  doi = {10.3389/fnhum.2013.00325},
  abstract = {Brain stimulation is having remarkable impact on clinical neurology. Brain stimulation can modulate neuronal activity in functionally segregated circumscribed regions of the human brain. Polarity, frequency, and noise specific stimulation can induce specific manipulations on neural activity. In contrast to neocortical stimulation, deep-brain stimulation has become a tool that can dramatically improve the impact clinicians can possibly have on movement disorders. In contrast, neocortical brain stimulation is proving to be remarkably susceptible to intrinsic brain-states. Although evidence is accumulating that brain stimulation can facilitate recovery processes in patients with cerebral stroke, the high variability of results impedes successful clinical implementation. Interestingly, recent data in healthy subjects suggests that brain-state dependent patterned stimulation might help resolve some of the intrinsic variability found in previous studies. In parallel, other studies suggest that noisy ``stochastic resonance'' (SR)-like processes are a non-negligible component in non-invasive brain stimulation studies. The hypothesis developed in this manuscript is that stimulation patterning with noisy and oscillatory components will help patients recover from stroke related deficits more reliably. To address this hypothesis we focus on two factors common to both neural computation (intrinsic variables) as well as brain stimulation (extrinsic variables): noise and oscillation. We review diverse theoretical and experimental evidence that demonstrates that subject-function specific brain-states are associated with specific oscillatory activity patterns. These states are transient and can be maintained by noisy processes. The resulting control procedures can resemble homeostatic or SR processes. In this context we try to extend awareness for inter-individual differences and the use of individualized stimulation in the recovery maximization of stroke patients.},
  file = {2013 - Schmidt et al. - Patterned Brain Stimulation, What a Framework with Rhythmic and Noisy Components Might Tell Us about Recovery Ma.pdf},
  journal = {Frontiers in Human Neuroscience},
  language = {en}
}

@article{Schmidt2019a,
  title = {Self-{{Play Learning Without}} a {{Reward Metric}}},
  author = {Schmidt, Dan and Moran, Nick and Rosenfeld, Jonathan S. and Rosenthal, Jonathan and Yedidia, Jonathan},
  year = {2019},
  month = dec,
  abstract = {The AlphaZero algorithm for the learning of strategy games via self-play, which has produced superhuman ability in the games of Go, chess, and shogi, uses a quantitative reward function for game outcomes, requiring the users of the algorithm to explicitly balance different components of the reward against each other, such as the game winner and margin of victory. We present a modification to the AlphaZero algorithm that requires only a total ordering over game outcomes, obviating the need to perform any quantitative balancing of reward components. We demonstrate that this system learns optimal play in a comparable amount of time to AlphaZero on a sample game.},
  archiveprefix = {arXiv},
  eprint = {1912.07557},
  eprinttype = {arxiv},
  file = {Schmidt et al. - 2019 - Self-Play Learning Without a Reward Metric 2.pdf},
  journal = {arXiv:1912.07557 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryclass = {cs, stat}
}

@techreport{Schmidt2020,
  title = {Ephaptic Coupling in White Matter Fibre Bundles Modulates Axonal Transmission Delays},
  author = {Schmidt, Helmut and Hahn, Gerald and Deco, Gustavo and Kn{\"o}sche, Thomas R.},
  year = {2020},
  month = apr,
  institution = {{Neuroscience}},
  doi = {10.1101/2020.04.08.031641},
  abstract = {Axonal connections are widely regarded as faithful transmitters of neuronal signals with fixed delays. The reasoning behind this is that extra-cellular potentials caused by spikes travelling along axons are too small to have an effect on other axons. Here we devise a computational framework that allows us to study the effect of extracellular potentials generated by spike volleys in axonal fibre bundles on axonal transmission delays. We demonstrate that, although the extracellular potentials generated by single spikes are of the order of microvolts, the collective extracellular potential generated by spike volleys can reach several millivolts. As a consequence, the resulting depolarisation of the axonal membranes increases the velocity of spikes, and therefore reduces axonal delays between brain areas. Driving a neural mass model with such spike volleys, we further demonstrate that only ephaptic coupling can explain the reduction of stimulus latencies with increased stimulus intensities, as observed in many psychological experiments.},
  file = {Schmidt et al. - 2020 - Ephaptic coupling in white matter fibre bundles mo.pdf},
  language = {en},
  type = {Preprint}
}

@article{Schmidt2020a,
  title = {Descending through a {{Crowded Valley}} -- {{Benchmarking Deep Learning Optimizers}}},
  author = {Schmidt, Robin M. and Schneider, Frank and Hennig, Philipp},
  year = {2020},
  month = oct,
  abstract = {Choosing the optimizer is considered to be among the most crucial design decisions in deep learning, and it is not an easy one. The growing literature now lists hundreds of optimization methods. In the absence of clear theoretical guidance and conclusive empirical evidence, the decision is often made based on anecdotes. In this work, we aim to replace these anecdotes, if not with a conclusive ranking, then at least with evidence-backed heuristics. To do so, we perform an extensive, standardized benchmark of more than a dozen particularly popular deep learning optimizers while giving a concise overview of the wide range of possible choices. Analyzing almost 35,000 individual runs, we contribute the following three points: (i) Optimizer performance varies greatly across tasks. (ii) We observe that evaluating multiple optimizers with default parameters works approximately as well as tuning the hyperparameters of a single, fixed optimizer. (iii) While we can not discern an optimization method clearly dominating across all tested tasks, we identify a significantly reduced subset of specific algorithms and parameter choices that generally lead to competitive results in our experiments. This subset includes popular favorites and some lesser-known contenders. We have open-sourced all our experimental results, making them directly available as challenging and well-tuned baselines.1 This allows for more meaningful comparisons when evaluating novel optimization methods without requiring any further computational efforts.},
  archiveprefix = {arXiv},
  eprint = {2007.01547},
  eprinttype = {arxiv},
  file = {Schmidt et al. - 2020 - Descending through a Crowded Valley -- Benchmarkin.pdf},
  journal = {arXiv:2007.01547 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryclass = {cs, stat}
}

@article{Schneider2014,
  title = {Linking {{Macroscopic}} with {{Microscopic Neuroanatomy Using Synthetic Neuronal Populations}}},
  author = {Schneider, Calvin J. and Cuntz, Hermann and Soltesz, Ivan},
  editor = {Hilgetag, Claus C.},
  year = {2014},
  month = oct,
  volume = {10},
  pages = {e1003921},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1003921},
  abstract = {Dendritic morphology has been shown to have a dramatic impact on neuronal function. However, population features such as the inherent variability in dendritic morphology between cells belonging to the same neuronal type are often overlooked when studying computation in neural networks. While detailed models for morphology and electrophysiology exist for many types of single neurons, the role of detailed single cell morphology in the population has not been studied quantitatively or computationally. Here we use the structural context of the neural tissue in which dendritic trees exist to drive their generation in silico. We synthesize the entire population of dentate gyrus granule cells, the most numerous cell type in the hippocampus, by growing their dendritic trees within their characteristic dendritic fields bounded by the realistic structural context of (1) the granule cell layer that contains all somata and (2) the molecular layer that contains the dendritic forest. This process enables branching statistics to be linked to larger scale neuroanatomical features. We find large differences in dendritic total length and individual path length measures as a function of location in the dentate gyrus and of somatic depth in the granule cell layer. We also predict the number of unique granule cell dendrites invading a given volume in the molecular layer. This work enables the complete population-level study of morphological properties and provides a framework to develop complex and realistic neural network models.},
  file = {Schneider et al. - 2014 - Linking Macroscopic with Microscopic Neuroanatomy .pdf},
  journal = {PLoS Computational Biology},
  language = {en},
  number = {10}
}

@article{Schoenholz2017,
  title = {{{DEEP INFORMATION PROPAGATION}}},
  author = {Schoenholz, Samuel S and Gilmer, Justin and Ganguli, Surya and {Sohl-Dickstein}, Jascha},
  year = {2017},
  pages = {18},
  abstract = {We study the behavior of untrained neural networks whose weights and biases are randomly distributed using mean field theory. We show the existence of depth scales that naturally limit the maximum depth of signal propagation through these random networks. Our main practical result is to show that random networks may be trained precisely when information can travel through them. Thus, the depth scales that we identify provide bounds on how deep a network may be trained for a specific choice of hyperparameters. As a corollary to this, we argue that in networks at the edge of chaos, one of these depth scales diverges. Thus arbitrarily deep networks may be trained only sufficiently close to criticality. We show that the presence of dropout destroys the order-to-chaos critical point and therefore strongly limits the maximum trainable depth for random networks. Finally, we develop a mean field theory for backpropagation and we show that the ordered and chaotic phases correspond to regions of vanishing and exploding gradient respectively.},
  file = {Schoenholz et al. - 2017 - DEEP INFORMATION PROPAGATION.pdf},
  language = {en}
}

@article{Schoenick2017,
  title = {Moving beyond the {{Turing Test}} with the {{Allen AI Science Challenge}}},
  author = {Schoenick, Carissa and Clark, Peter and Tafjord, Oyvind and Turney, Peter and Etzioni, Oren},
  year = {2017},
  month = aug,
  volume = {60},
  pages = {60--64},
  issn = {00010782},
  doi = {10.1145/3122814},
  file = {Schoenick et al. - 2017 - Moving beyond the Turing Test with the Allen AI Sc.pdf},
  journal = {Communications of the ACM},
  language = {en},
  number = {9}
}

@article{Scholte2017,
  title = {Visual Pathways from the Perspective of Cost Functions and Multi-Task Deep Neural Networks},
  author = {Scholte, H. Steven and Losch, Max M. and Ramakrishnan, Kandan and {de Haan}, Edward and Bohte, Sander},
  year = {2017},
  month = sep,
  doi = {10.1101/146472},
  abstract = {Vision research has been shaped by the seminal insight that we can understand higher-tier visual cortex from the perspective of multiple functional pathways with different goals. In this paper we try to give a computational account of the functional organization of this system by reasoning from the perspective of multi-task deep neural networks. Machine learning has shown that tasks become easier to solve when they are decomposed into subtasks with their own cost function. We hypothesise that the visual system optimizes multiple cost functions of unrelated tasks and this causes the emergence of the ventral pathway, dedicated to vision for perception and dorsal pathway, dedicated to vision for action. To evaluate the functional organization in multi-task deep neural networks we propose a method that measures the contribution of a unit towards each task and apply it to two networks that have been trained on either two related or two unrelated tasks using an identical stimulus set. Results show that the network trained on the unrelated tasks shows a decreasing degree of feature representation sharing towards higher-tier layers while the network trained on related tasks uniformly shows high degree of sharing. We conjecture that the method we propose can be used to reason about the anatomical and functional organization of the visual system and beyond as we predict that the degree to which tasks are related is a good descriptor of the degree to which they can share downstream corticalunits.},
  file = {Scholte et al. - 2017 - Visual pathways from the perspective of cost funct.pdf},
  journal = {bioRxiv},
  language = {en}
}

@article{Schomburg,
  title = {{{BIOPHYSICAL AND NETWORK MECHANISMS OF HIGH FREQUENCY EXTRACELLULAR POTENTIALS IN THE RAT HIPPOCAMPUS}}},
  author = {Schomburg, Erik W},
  pages = {230},
  file = {2014 - Unknown - Thesis by.pdf;Schomburg - BIOPHYSICAL AND NETWORK MECHANISMS OF HIGH FREQUEN.pdf},
  language = {en}
}

@article{Schonberg2020,
  title = {A {{Neural Pathway}} for {{Nonreinforced Preference Change}}},
  author = {Schonberg, Tom and Katz, Leor N.},
  year = {2020},
  month = jul,
  volume = {24},
  pages = {504--514},
  issn = {13646613},
  doi = {10.1016/j.tics.2020.04.002},
  file = {Schonberg and Katz - 2020 - A Neural Pathway for Nonreinforced Preference Chan.pdf},
  journal = {Trends in Cognitive Sciences},
  language = {en},
  number = {7}
}

@article{Schonberg2020a,
  title = {A {{Neural Pathway}} for {{Nonreinforced Preference Change}}},
  author = {Schonberg, Tom and Katz, Leor N.},
  year = {2020},
  month = jul,
  volume = {24},
  pages = {504--514},
  issn = {13646613},
  doi = {10.1016/j.tics.2020.04.002},
  file = {Schonberg and Katz - 2020 - A Neural Pathway for Nonreinforced Preference Chan 2.pdf},
  journal = {Trends in Cognitive Sciences},
  language = {en},
  number = {7}
}

@techreport{Schoonover2020,
  title = {Representational Drift in Primary Olfactory Cortex},
  author = {Schoonover, Carl E. and Ohashi, Sarah N. and Axel, Richard and Fink, Andrew J.P.},
  year = {2020},
  month = sep,
  institution = {{Neuroscience}},
  doi = {10.1101/2020.09.24.312132},
  abstract = {Representations of the external world in sensory cortices may define the identity of a stimulus and should therefore vary little over the life of the organism. In the olfactory system the primary olfactory cortex, piriform, is thought to determine odor identity1\textendash 6. We have performed electrophysiological recordings of single units maintained over weeks to examine the stability of odor representations in the mouse piriform cortex. We observed that odor representations drift over time, such that the performance of a linear classifier trained on the first recording day approaches chance levels after 32 days. Daily exposure to the same odorant slows the rate of drift, but when exposure is halted that rate increases once again. Moreover, behavioral salience does not stabilize odor representations. Continuous drift poses the question of the role of piriform in odor identification. This instability may reflect the unstructured connectivity of piriform7\textendash 15 and may be a property of other unstructured cortices.},
  file = {Schoonover et al. - 2020 - Representational drift in primary olfactory cortex.pdf},
  language = {en},
  type = {Preprint}
}

@article{Schreckenberger2004,
  title = {The Thalamus as the Generator and Modulator of {{EEG}} Alpha Rhythm: A Combined {{PET}}/{{EEG}} Study with Lorazepam Challenge in Humans},
  shorttitle = {The Thalamus as the Generator and Modulator of {{EEG}} Alpha Rhythm},
  author = {Schreckenberger, Mathias and {Lange-Asschenfeld}, Christian and Lochmann, Matthias and Mann, Klaus and Siessmeier, Thomas and Buchholz, Hans-Georg and Bartenstein, Peter and Gr{\"u}nder, Gerhard},
  year = {2004},
  month = jun,
  volume = {22},
  pages = {637--644},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2004.01.047},
  file = {2004 - Schreckenberger et al. - The thalamus as the generator and modulator of EEG alpha rhythm A combined PETEEG study with lorazepam c.pdf},
  journal = {NeuroImage},
  language = {en},
  number = {2}
}

@article{Schulman,
  title = {Proximal {{Policy Optimization Algorithms}}},
  author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  pages = {12},
  abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a ``surrogate'' objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
  file = {Schulman et al. - Proximal Policy Optimization Algorithms.pdf},
  language = {en}
}

@article{Schulman2015,
  title = {High-{{Dimensional Continuous Control Using Generalized Advantage Estimation}}},
  author = {Schulman, John and Moritz, Philipp and Levine, Sergey and Jordan, Michael and Abbeel, Pieter},
  year = {2015},
  month = jun,
  abstract = {Policy gradient methods are an appealing approach in reinforcement learning because they directly optimize the cumulative reward and can straightforwardly be used with nonlinear function approximators such as neural networks. The two main challenges are the large number of samples typically required, and the difficulty of obtaining stable and steady improvement despite the nonstationarity of the incoming data. We address the first challenge by using value functions to substantially reduce the variance of policy gradient estimates at the cost of some bias, with an exponentially-weighted estimator of the advantage function that is analogous to TD({$\lambda$}). We address the second challenge by using trust region optimization procedure for both the policy and the value function, which are represented by neural networks.},
  archiveprefix = {arXiv},
  eprint = {1506.02438},
  eprinttype = {arxiv},
  file = {2015 - Schulman et al. - High-Dimensional Continuous Control Using Generalized Advantage Estimation.pdf},
  journal = {arXiv:1506.02438 [cs]},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics,Computer Science - Systems and Control},
  language = {en},
  primaryclass = {cs}
}

@article{Schulman2015a,
  title = {Trust {{Region Policy Optimization}}},
  author = {Schulman, John and Levine, Sergey and Moritz, Philipp and Jordan, Michael I. and Abbeel, Pieter},
  year = {2015},
  month = feb,
  abstract = {We describe an iterative procedure for optimizing policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified procedure, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is similar to natural policy gradient methods and is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.},
  archiveprefix = {arXiv},
  eprint = {1502.05477},
  eprinttype = {arxiv},
  file = {2015 - Schulman et al. - Trust Region Policy Optimization.pdf},
  journal = {arXiv:1502.05477 [cs]},
  keywords = {Computer Science - Machine Learning},
  language = {en},
  primaryclass = {cs}
}

@article{Schultz1998,
  title = {Predictive {{Reward Signal}} of {{Dopamine Neurons}}},
  author = {Schultz, Wolfram},
  year = {1998},
  month = jul,
  volume = {80},
  pages = {1--27},
  issn = {0022-3077, 1522-1598},
  doi = {10.1152/jn.1998.80.1.1},
  file = {Schultz - 1998 - Predictive Reward Signal of Dopamine Neurons.pdf},
  journal = {Journal of Neurophysiology},
  language = {en},
  number = {1}
}

@article{Schulz,
  title = {Searching for {{Rewards Like}} a {{Child Means Less Generalization}} and {{More Directed Exploration}}},
  author = {Schulz, Eric and Wu, Charley M and Ruggeri, Azzurra and Meder, Bj{\"o}rn},
  pages = {12},
  abstract = {How do children and adults differ in their search for rewards? We considered three different hypotheses that attribute developmental differences to (a) children's increased random sampling, (b) more directed exploration toward uncertain options, or (c) narrower generalization. Using a search task in which noisy rewards were spatially correlated on a grid, we compared the ability of 55 younger children (ages 7 and 8 years), 55 older children (ages 9\textendash 11 years), and 50 adults (ages 19\textendash 55 years) to successfully generalize about unobserved outcomes and balance the exploration\textendash exploitation dilemma. Our results show that children explore more eagerly than adults but obtain lower rewards. We built a predictive model of search to disentangle the unique contributions of the three hypotheses of developmental differences and found robust and recoverable parameter estimates indicating that children generalize less and rely on directed exploration more than adults. We did not, however, find reliable differences in terms of random sampling.},
  file = {Schulz et al. - Searching for Rewards Like a Child Means Less Gene.pdf},
  language = {en}
}

@article{Schulz2006,
  title = {Plasticity and Stability in Neuronal Output via Changes in Intrinsic Excitability: It's What's inside That Counts},
  shorttitle = {Plasticity and Stability in Neuronal Output via Changes in Intrinsic Excitability},
  author = {Schulz, D. J.},
  year = {2006},
  month = dec,
  volume = {209},
  pages = {4821--4827},
  issn = {0022-0949, 1477-9145},
  doi = {10.1242/jeb.02567},
  abstract = {Summary The nervous system faces an extremely difficult task. It must be flexible, both during development and in adult life, so that it can respond to a variety of environmental demands and produce adaptive behavior. At the same time the nervous system must be stable, so that the neural circuits that produce behavior function throughout the lifetime of the animal and that changes produced by learning endure. We are only beginning to understand how neural networks strike a balance between altering individual neurons in the name of plasticity, while maintaining long-term stability in neural system function. The balance of this plasticity and stability in neural networks undoubtedly plays a critical role in the normal functioning of the nervous system. While mechanisms of synaptic plasticity have garnered extensive study over the past three decades, it is only recently that more attention has been turned to plasticity of intrinsic excitability as a key player in neural network function. This review will focus on this emerging area of research that undoubtedly will contribute a great deal to our understanding of the functionality of the nervous system.},
  file = {2006 - Schulz - Plasticity and stability in neuronal output via changes in intrinsic excitability it's what's inside that counts.pdf},
  journal = {Journal of Experimental Biology},
  language = {en},
  number = {24}
}

@article{Schulz2018,
  title = {Finding Structure in Multi-Armed Bandits},
  author = {Schulz, Eric and Franklin, Nicholas T and Gershman, Samuel J},
  year = {2018},
  month = dec,
  doi = {10.1101/432534},
  abstract = {How do humans search for rewards? This question is commonly studied using multi-armed bandit tasks, which require participants to trade off exploration and exploitation. Standard multi-armed bandits assume that each option has an independent reward distribution. However, learning about options independently is unrealistic, since in the real world options often share an underlying structure. We introduce a class of structured bandit tasks, which we use to probe how generalization guides exploration. In a structured multi-armed bandit, options have a correlation structure dictated by a latent function. We focus on bandits in which rewards are linear functions of an option's spatial position. Across 5 experiments, we find evidence that participants utilize functional structure to guide their exploration, and also exhibit a learning-to-learn effect across rounds, becoming progressively faster at identifying the latent function. The experiments rule out several heuristic explanations, and show that the same findings obtain with non-linear functions. Comparing several models of learning and decision making, we find that the best model of human behavior in our tasks combines three computational mechanisms: (1) function learning, (2) clustering of reward distributions across rounds, and (3) uncertainty-guided exploration. Our results suggest that human reinforcement learning can utilize latent structure in sophisticated ways to improve efficiency.},
  file = {Schulz et al. - 2018 - Finding structure in multi-armed bandits.pdf},
  journal = {bioRxiv},
  language = {en}
}

@techreport{Schulz2018a,
  title = {Exploration in the Wild},
  author = {Schulz, Eric and Bhui, Rahul and Love, Bradley C and Brier, Bastien and Todd, Michael T and Gershman, Samuel J},
  year = {2018},
  month = dec,
  institution = {{Animal Behavior and Cognition}},
  doi = {10.1101/492058},
  abstract = {Making good decisions requires people to appropriately explore their available options and generalize what they have learned. While computational models have successfully explained exploratory behavior in constrained laboratory tasks, it is unclear to what extent these models generalize to complex real world choice problems. We investigate the factors guiding exploratory behavior in a data set consisting of 195,333 customers placing 1,613,967 orders from a large online food delivery service. We find important hallmarks of adaptive exploration and generalization, which we analyze using computational models. We find evidence for several theoretical predictions: (1) customers engage in uncertainty-directed exploration, (2) they adjust their level of exploration to the average restaurant quality in a city, and (3) they use feature-based generalization to guide exploration towards promising restaurants. Our results provide new evidence that people use sophisticated strategies to explore complex, real-world environments.},
  file = {Schulz et al. - 2018 - Exploration in the wild.pdf},
  language = {en},
  type = {Preprint}
}

@article{Schulze2018,
  title = {Active {{Reinforcement Learning}} with {{Monte}}-{{Carlo Tree Search}}},
  author = {Schulze, Sebastian and Evans, Owain},
  year = {2018},
  month = mar,
  abstract = {Active Reinforcement Learning (ARL) is a twist on RL where the agent observes reward information only if it pays a cost. This subtle change makes exploration substantially more challenging. Powerful principles in RL like optimism, Thompson sampling, and random exploration do not help with ARL. We relate ARL in tabular environments to BayesAdaptive MDPs. We provide an ARL algorithm using Monte-Carlo Tree Search that is asymptotically Bayes optimal. Experimentally, this algorithm is near-optimal on small Bandit problems and MDPs. On larger MDPs it outperforms a Q-learner augmented with specialised heuristics for ARL. By analysing exploration behaviour in detail, we uncover obstacles to scaling up simulation-based algorithms for ARL.},
  archiveprefix = {arXiv},
  eprint = {1803.04926},
  eprinttype = {arxiv},
  file = {Schulze and Evans - 2018 - Active Reinforcement Learning with Monte-Carlo Tre.pdf},
  journal = {arXiv:1803.04926 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryclass = {cs, stat}
}

@article{Schumacher,
  title = {The {{No Free Lunch}} and {{Problem Description Length}}},
  author = {Schumacher, C and Vose, M D and Whitley, L D},
  pages = {7},
  abstract = {The No Free Lunch theorem is reviewed and cast within a simple framework for blackbox search. A duality result which relates functions being optimized to algorithms optimizing them is obtained and is used to sharpen the No Free Lunch theorem. Observations are made concerning problem description length within the context provided by the results of this paper. It is seen that No Free Lunch results are independent from whether or not the set of functions (over which a No Free Lunch result holds) is compressible.},
  file = {Schumacher et al. - The No Free Lunch and Problem Description Length.pdf},
  language = {en}
}

@article{Schuman2019,
  title = {Four {{Unique Interneuron Populations Reside}} in {{Neocortical Layer}} 1},
  author = {Schuman, Benjamin and Machold, Robert P. and Hashikawa, Yoshiko and Fuzik, J{\'a}nos and Fishell, Gord J. and Rudy, Bernardo},
  year = {2019},
  month = jan,
  volume = {39},
  pages = {125--139},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.1613-18.2018},
  file = {Schuman et al. - 2019 - Four Unique Interneuron Populations Reside in Neoc.pdf},
  journal = {The Journal of Neuroscience},
  language = {en},
  number = {1}
}

@article{Schurger2010,
  title = {Reproducibility {{Distinguishes Conscious}} from {{Nonconscious Neural Representations}}},
  author = {Schurger, A. and Pereira, F. and Treisman, A. and Cohen, J. D.},
  year = {2010},
  month = jan,
  volume = {327},
  pages = {97--99},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1180029},
  file = {2010 - Schurger et al. - Reproducibility distinguishes conscious from nonconscious neural representations.pdf},
  journal = {Science},
  language = {en},
  number = {5961}
}

@article{Schwabe2006,
  title = {The {{Role}} of {{Feedback}} in {{Shaping}} the {{Extra}}-{{Classical Receptive Field}} of {{Cortical Neurons}}: {{A Recurrent Network Model}}},
  shorttitle = {The {{Role}} of {{Feedback}} in {{Shaping}} the {{Extra}}-{{Classical Receptive Field}} of {{Cortical Neurons}}},
  author = {Schwabe, L.},
  year = {2006},
  month = sep,
  volume = {26},
  pages = {9117--9129},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.1253-06.2006},
  file = {2006 - Schwabe et al. - The role of feedback in shaping the extra-classical receptive field of cortical neurons a recurrent network mode.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {36}
}

@article{Schwartenbeck2019,
  title = {Computational Mechanisms of Curiosity and Goal-Directed Exploration},
  author = {Schwartenbeck, Philipp and Passecker, Johannes and Hauser, Tobias U and FitzGerald, Thomas HB and Kronbichler, Martin and Friston, Karl J},
  year = {2019},
  pages = {45},
  file = {Schwartenbeck et al. - Computational mechanisms of curiosity and goal-dir.pdf},
  journal = {eLife},
  language = {en},
  number = {e41703}
}

@article{Schwartz-Ziv,
  title = {Opening the Black Box of {{Deep Neural Networks}} via {{Information}}},
  author = {{Schwartz-Ziv}, Ravid and Tishby, Naftali},
  pages = {19},
  abstract = {Despite their great success, there is still no comprehensive theoretical understanding of learning with Deep Neural Networks (DNNs) or their inner organization. Previous work [Tishby and Zaslavsky (2015)] proposed to analyze DNNs in the Information Plane; i.e., the plane of the Mutual Information values that each layer preserves on the input and output variables. They suggested that the goal of the network is to optimize the Information Bottleneck (IB) tradeoff between compression and prediction, successively, for each layer.},
  file = {Schwartz-Ziv and Tishby - Opening the black box of Deep Neural Networks via .pdf},
  language = {en}
}

@article{Schwemmer2015,
  title = {Constructing {{Precisely Computing Networks}} with {{Biophysical Spiking Neurons}}},
  author = {Schwemmer, M. A. and Fairhall, A. L. and Deneve, S. and {Shea-Brown}, E. T.},
  year = {2015},
  month = jul,
  volume = {35},
  pages = {10112--10134},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.4951-14.2015},
  file = {Schwemmer et al. - 2015 - Constructing Precisely Computing Networks with Bio.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {28}
}

@article{Scott2014,
  title = {Voltage {{Imaging}} of {{Waking Mouse Cortex Reveals Emergence}} of {{Critical Neuronal Dynamics}}},
  author = {Scott, G. and Fagerholm, E. D. and Mutoh, H. and Leech, R. and Sharp, D. J. and Shew, W. L. and Knopfel, T.},
  year = {2014},
  month = dec,
  volume = {34},
  pages = {16611--16620},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.3474-14.2014},
  file = {Scott et al. - 2014 - Voltage Imaging of Waking Mouse Cortex Reveals Eme.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {50}
}

@article{Searcy,
  title = {Bird {{Song}} and the {{Problem}} of {{Honest Communication}}},
  author = {Searcy, William A and Nowicki, Stephen},
  pages = {9},
  file = {Searcy and Nowicki - Bird Song and the Problem of Honest Communication.pdf},
  language = {en}
}

@article{Sebastian-Gonzalez2019,
  title = {The Extent, Frequency and Ecological Functions of Food Wasting by Parrots},
  author = {{Sebasti{\'a}n-Gonz{\'a}lez}, Esther and Hiraldo, Fernando and Blanco, Guillermo and {Hern{\'a}ndez-Brito}, Dailos and {Romero-Vidal}, Pedro and Carrete, Martina and {G{\'o}mez-Llanos}, Eduardo and Pac{\'i}fico, Erica C. and {D{\'i}az-Luque}, Jos{\'e} A. and D{\'e}nes, Francisco V. and Tella, Jos{\'e} L.},
  year = {2019},
  month = dec,
  volume = {9},
  pages = {15280},
  issn = {2045-2322},
  doi = {10.1038/s41598-019-51430-3},
  file = {Sebastián-González et al. - 2019 - The extent, frequency and ecological functions of .pdf},
  journal = {Sci Rep},
  language = {en},
  number = {1}
}

@article{Sedigh-Sarvestani2012,
  title = {Reconstructing {{Mammalian Sleep Dynamics}} with {{Data Assimilation}}},
  author = {{Sedigh-Sarvestani}, Madineh and Schiff, Steven J. and Gluckman, Bruce J.},
  editor = {Gutkin, Boris S.},
  year = {2012},
  month = nov,
  volume = {8},
  pages = {e1002788},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1002788},
  abstract = {Data assimilation is a valuable tool in the study of any complex system, where measurements are incomplete, uncertain, or both. It enables the user to take advantage of all available information including experimental measurements and shortterm model forecasts of a system. Although data assimilation has been used to study other biological systems, the study of the sleep-wake regulatory network has yet to benefit from this toolset. We present a data assimilation framework based on the unscented Kalman filter (UKF) for combining sparse measurements together with a relatively high-dimensional nonlinear computational model to estimate the state of a model of the sleep-wake regulatory system. We demonstrate with simulation studies that a few noisy variables can be used to accurately reconstruct the remaining hidden variables. We introduce a metric for ranking relative partial observability of computational models, within the UKF framework, that allows us to choose the optimal variables for measurement and also provides a methodology for optimizing framework parameters such as UKF covariance inflation. In addition, we demonstrate a parameter estimation method that allows us to track nonstationary model parameters and accommodate slow dynamics not included in the UKF filter model. Finally, we show that we can even use observed discretized sleep-state, which is not one of the model variables, to reconstruct model state and estimate unknown parameters. Sleep is implicated in many neurological disorders from epilepsy to schizophrenia, but simultaneous observation of the many brain components that regulate this behavior is difficult. We anticipate that this data assimilation framework will enable better understanding of the detailed interactions governing sleep and wake behavior and provide for better, more targeted, therapies.},
  file = {2012 - Sedigh-Sarvestani, Schiff, Gluckman - Reconstructing Mammalian Sleep Dynamics with Data Assimilation.pdf},
  journal = {PLoS Computational Biology},
  language = {en},
  number = {11}
}

@article{Seeds2014,
  title = {A Suppression Hierarchy among Competing Motor Programs Drives Sequential Grooming in {{Drosophila}}},
  author = {Seeds, Andrew M and Ravbar, Primoz and Chung, Phuong and Hampel, Stefanie and Midgley, Frank M and Mensh, Brett D and Simpson, Julie H},
  year = {2014},
  month = aug,
  volume = {3},
  pages = {e02951},
  issn = {2050-084X},
  doi = {10.7554/eLife.02951},
  abstract = {Motor sequences are formed through the serial execution of different movements, but how nervous systems implement this process remains largely unknown. We determined the organizational principles governing how dirty fruit flies groom their bodies with sequential movements. Using genetically targeted activation of neural subsets, we drove distinct motor programs that clean individual body parts. This enabled competition experiments revealing that the motor programs are organized into a suppression hierarchy; motor programs that occur first suppress those that occur later. Cleaning one body part reduces the sensory drive to its motor program, which relieves suppression of the next movement, allowing the grooming sequence to progress down the hierarchy. A model featuring independently evoked cleaning movements activated in parallel, but selected serially through hierarchical suppression, was successful in reproducing the grooming sequence. This provides the first example of an innate motor sequence implemented by the prevailing model for generating human action sequences.           ,              Anyone who has ever lived with a cat is familiar with its grooming behavior. This innate behavior follows a particular sequence as the cat methodically cleans its body parts one-by-one. Many animals also have grooming habits, even insects such as fruit flies. The fact that grooming sequences are seen across such different species suggests that this behavior is important for survival. Nevertheless, how the brain organizes grooming sequences, or other behaviors that involve a sequence of tasks, is not well understood.             Fruit flies make a good model for studying grooming behavior for a couple of reasons. First, they are fastidious cleaners. When coated with dust they will faithfully carry out a series of cleaning tasks to clean each body part. Second, there are many genetic tools and techniques that researchers can use to manipulate the fruit flies' behaviors. One technique allows specific brain cells to be targeted and activated to trigger particular behaviors.             Seeds et al. used these sophisticated techniques, computer modeling, and behavioral observations to uncover how the brains of fruit flies orchestrate a grooming sequence. Dust-covered flies follow a predictable sequence of cleaning tasks: beginning by using their front legs to clean their eyes, they then clean their antennae and head. This likely helps to protect their sensory organs. Next, they move on to the abdomen, possibly to ensure that dust doesn't interfere with their ability to breathe. Wings and thorax follow last. Periodically, the flies stop to rub their legs together to remove any accumulated dust before resuming the cleaning sequence.             Seeds et al. activated different sets of brain cells one-by-one to see if they could trigger a particular grooming task and found that individual cleaning tasks could be triggered, in the absence of dust, by stimulating a specific group of brain cells. This suggests each cleaning task is a discrete behavior controlled by a subset of cells. Then Seeds et al. tried to stimulate more than one cleaning behavior at a time; they discovered that wing-cleaning suppressed thorax-cleaning, abdomen-cleaning suppressed both of these, and head-cleaning suppressed all the others. This suggests that a `hierarchy' exists in the brain that exactly matches the sequence that flies normally follow as they clean their body parts.             By learning more about how the brain coordinates grooming sequences, the findings of Seeds et al. may also provide insights into other behaviors that involve a sequence of tasks, such as nest building in animals or typing in humans. Following on from this work, one of the next challenges will be to see if such behaviors also use a `suppression hierarchy' to ensure that individual tasks are carried out in the right order.},
  file = {Seeds et al. - 2014 - A suppression hierarchy among competing motor prog.pdf},
  journal = {eLife},
  language = {en}
}

@article{Sehnke2010,
  title = {Parameter-Exploring Policy Gradients},
  author = {Sehnke, Frank and Osendorfer, Christian and R{\"u}ckstie{\ss}, Thomas and Graves, Alex and Peters, Jan and Schmidhuber, J{\"u}rgen},
  year = {2010},
  month = may,
  volume = {23},
  pages = {551--559},
  issn = {08936080},
  doi = {10.1016/j.neunet.2009.12.004},
  abstract = {We present a model-free reinforcement learning method for partially observable Markov decision problems. Our method estimates a likelihood gradient by sampling directly in parameter space, which leads to lower variance gradient estimates than obtained by regular policy gradient methods. We show that for several complex control tasks, including robust standing with a humanoid robot, this method outperforms well-known algorithms from the fields of standard policy gradients, finite difference methods and population based heuristics. We also show that the improvement is largest when the parameter samples are drawn symmetrically. Lastly we analyse the importance of the individual components of our method by incrementally incorporating them into the other algorithms, and measuring the gain in performance after each step.},
  file = {Sehnke et al. - 2010 - Parameter-exploring policy gradients.pdf},
  journal = {Neural Networks},
  language = {en},
  number = {4}
}

@article{Seiver2013,
  title = {Did {{She Jump Because She Was}} the {{Big Sister}} or {{Because}} the {{Trampoline Was Safe}}? {{Causal Inference}} and the {{Development}} of {{Social Attribution}}},
  shorttitle = {Did {{She Jump Because She Was}} the {{Big Sister}} or {{Because}} the {{Trampoline Was Safe}}?},
  author = {Seiver, Elizabeth and Gopnik, Alison and Goodman, Noah D.},
  year = {2013},
  month = mar,
  volume = {84},
  pages = {443--454},
  issn = {00093920},
  doi = {10.1111/j.1467-8624.2012.01865.x},
  file = {Seiver et al. - 2013 - Did She Jump Because She Was the Big Sister or Bec.pdf},
  journal = {Child Dev},
  language = {en},
  number = {2}
}

@article{Seldin,
  title = {{{PAC}}-{{Bayesian Analysis}} of {{Contextual Bandits}}},
  author = {Seldin, Yevgeny and Auer, Peter and {Shawe-taylor}, John S and Ortner, Ronald and Laviolette, Fran{\c c}ois},
  pages = {9},
  abstract = {We derive an instantaneous (per-round) data-dependent regret bound for stochastic multiarmed bandits with side information (also known as contextual bandits). The scaling of our regret bound with the number of states (contexts) N goes as pN I{$\dashrightarrow$} (S; A), where I{$\dashrightarrow$} (S; A) is the mutual information between states and act t tions (the side information) used by the algorithm at roupnd t. If the algorithm uses all the side information, the regret bound scales as N ln K, where K is the number of actions (arms). However, if the side information I{$\dashrightarrow$} (S; A) is not t fully used, the regret bound is significantly tighter. In the extreme case, when I{$\dashrightarrow$} (S; A) = 0, the dependence on the number of states reduces from linear to t logarithmic. Our analysis allows to provide the algorithm large amount of side information, let the algorithm to decide which side information is relevant for the task, and penalize the algorithm only for the side information that it is using de facto. We also present an algorithm for multiarmed bandits with side information with O(K) computational complexity per game round.},
  file = {Seldin et al. - PAC-Bayesian Analysis of Contextual Bandits.pdf},
  language = {en}
}

@article{Sengupta2011,
  title = {Chemotactic Predator-Prey Dynamics},
  author = {Sengupta, Ankush and Kruppa, Tobias and L{\"o}wen, Hartmut},
  year = {2011},
  month = mar,
  volume = {83},
  pages = {031914},
  issn = {1539-3755, 1550-2376},
  doi = {10.1103/PhysRevE.83.031914},
  abstract = {A discrete chemotactic predator-prey model is proposed in which the prey secrets a diffusing chemical which is sensed by the predator and vice versa. Two dynamical states corresponding to catching and escaping are identified and it is shown that steady hunting is unstable. For the escape process, the predator-prey distance is diffusive for short times but exhibits a transient subdiffusive behavior which scales as a power law \$t\^\{1/3\}\$ with time \$t\$ and ultimately crosses over to diffusion again. This allows to classify the motility and dynamics of various predatory bacteria and phagocytes. In particular, there is a distinct region in the parameter space where they prove to be infallible predators.},
  archiveprefix = {arXiv},
  eprint = {1009.5521},
  eprinttype = {arxiv},
  file = {Sengupta et al. - 2011 - Chemotactic predator-prey dynamics.pdf},
  journal = {Phys. Rev. E},
  keywords = {Physics - Biological Physics,Physics - Medical Physics,Quantitative Biology - Populations and Evolution},
  language = {en},
  number = {3}
}

@article{Serences2004,
  title = {A Comparison of Methods for Characterizing the Event-Related {{BOLD}} Timeseries in Rapid {{fMRI}}},
  author = {Serences, John T.},
  year = {2004},
  month = apr,
  volume = {21},
  pages = {1690--1700},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2003.12.021},
  file = {2004 - Serences - A comparison of methods for characterizing the event-related BOLD timeseries in rapid fMRI.pdf},
  journal = {NeuroImage},
  language = {en},
  number = {4}
}

@article{Serences2009,
  title = {Stimulus-{{Specific Delay Activity}} in {{Human Primary Visual Cortex}}},
  author = {Serences, John T. and Ester, Edward F. and Vogel, Edward K. and Awh, Edward},
  year = {2009},
  month = feb,
  volume = {20},
  pages = {207--214},
  issn = {0956-7976, 1467-9280},
  doi = {10.1111/j.1467-9280.2009.02276.x},
  abstract = {Working memory (WM) involves maintaining information in an on-line state. One emerging view is that information in WM is maintained via sensory recruitment, such that information is stored via sustained activity in the sensory areas that encode the to-be-remembered information. Using functional magnetic resonance imaging, we observed that key sensory regions such as primary visual cortex (V1) showed little evidence of sustained increases in mean activation during a WM delay period, though such amplitude increases have typically been used to determine whether a region is involved in on-line maintenance. However, a multivoxel pattern analysis of delay-period activity revealed a sustained pattern of activation in V1 that represented only the intentionally stored feature of a multifeature object. Moreover, the pattern of delay activity was qualitatively similar to that observed during the discrimination of sensory stimuli, suggesting that WM representations in V1 are reasonable ``copies'' of those evoked during pure sensory processing.},
  file = {2009 - Serences et al. - Stimulus-specific delay activity in human primary visual cortex.pdf},
  journal = {Psychological Science},
  language = {en},
  number = {2}
}

@article{Serences2011,
  title = {Mechanisms of {{Selective Attention}}: {{Response Enhancement}}, {{Noise Reduction}}, and {{Efficient Pooling}} of {{Sensory Responses}}},
  shorttitle = {Mechanisms of {{Selective Attention}}},
  author = {Serences, John T.},
  year = {2011},
  month = dec,
  volume = {72},
  pages = {685--687},
  issn = {08966273},
  doi = {10.1016/j.neuron.2011.11.005},
  file = {2011 - Serences - Mechanisms of selective attention Response enhancement, noise reduction, and efficient pooling of sensory responses.pdf},
  journal = {Neuron},
  language = {en},
  number = {5}
}

@article{Serrano2013,
  title = {Gain {{Control Network Conditions}} in {{Early Sensory Coding}}},
  author = {Serrano, Eduardo and Nowotny, Thomas and Levi, Rafael and Smith, Brian H. and Huerta, Ram{\'o}n},
  editor = {Sporns, Olaf},
  year = {2013},
  month = jul,
  volume = {9},
  pages = {e1003133},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1003133},
  abstract = {Gain control is essential for the proper function of any sensory system. However, the precise mechanisms for achieving effective gain control in the brain are unknown. Based on our understanding of the existence and strength of connections in the insect olfactory system, we analyze the conditions that lead to controlled gain in a randomly connected network of excitatory and inhibitory neurons. We consider two scenarios for the variation of input into the system. In the first case, the intensity of the sensory input controls the input currents to a fixed proportion of neurons of the excitatory and inhibitory populations. In the second case, increasing intensity of the sensory stimulus will both, recruit an increasing number of neurons that receive input and change the input current that they receive. Using a mean field approximation for the network activity we derive relationships between the parameters of the network that ensure that the overall level of activity of the excitatory population remains unchanged for increasing intensity of the external stimulation. We find that, first, the main parameters that regulate network gain are the probabilities of connections from the inhibitory population to the excitatory population and of the connections within the inhibitory population. Second, we show that strict gain control is not achievable in a random network in the second case, when the input recruits an increasing number of neurons. Finally, we confirm that the gain control conditions derived from the mean field approximation are valid in simulations of firing rate models and Hodgkin-Huxley conductance based models.},
  file = {2013 - Serrano et al. - Gain Control Network Conditions in Early Sensory Coding.pdf},
  journal = {PLoS Computational Biology},
  language = {en},
  number = {7}
}

@article{Seung2003,
  title = {Learning in {{Spiking Neural Networks}} by {{Reinforcement}} of {{Stochastic Synaptic Transmission}}},
  author = {Seung, H.Sebastian},
  year = {2003},
  month = dec,
  volume = {40},
  pages = {1063--1073},
  issn = {08966273},
  doi = {10.1016/S0896-6273(03)00761-X},
  abstract = {It is well-known that chemical synaptic transmission is an unreliable process, but the function of such unreliability remains unclear. Here I consider the hypothesis that the randomness of synaptic transmission is harnessed by the brain for learning, in analogy to the way that genetic mutation is utilized by Darwinian evolution. This is possible if synapses are ``hedonistic,'' responding to a global reward signal by increasing their probabilities of vesicle release or failure, depending on which action immediately preceded reward. Hedonistic synapses learn by computing a stochastic approximation to the gradient of the average reward. They are compatible with synaptic dynamics such as short-term facilitation and depression and with the intricacies of dendritic integration and action potential generation. A network of hedonistic synapses can be trained to perform a desired computation by administering reward appropriately, as illustrated here through numerical simulations of integrate-andfire model neurons.},
  file = {2003 - Seung - Learning in Spiking Neural Networks by Reinforcement of Stochastics Transmission.pdf},
  journal = {Neuron},
  language = {en},
  number = {6}
}

@article{Shafto2011,
  title = {A Probabilistic Model of Cross-Categorization},
  author = {Shafto, Patrick and Kemp, Charles and Mansinghka, Vikash and Tenenbaum, Joshua B.},
  year = {2011},
  month = jul,
  volume = {120},
  pages = {1--25},
  issn = {00100277},
  doi = {10.1016/j.cognition.2011.02.010},
  abstract = {Most natural domains can be represented in multiple ways: we can categorize foods in terms of their nutritional content or social role, animals in terms of their taxonomic groupings or their ecological niches, and musical instruments in terms of their taxonomic categories or social uses. Previous approaches to modeling human categorization have largely ignored the problem of cross-categorization, focusing on learning just a single system of categories that explains all of the features. Cross-categorization presents a difficult problem: how can we infer categories without first knowing which features the categories are meant to explain? We present a novel model that suggests that human cross-categorization is a result of joint inference about multiple systems of categories and the features that they explain. We also formalize two commonly proposed alternative explanations for cross-categorization behavior: a features-first and an objects-first approach. The features-first approach suggests that cross-categorization is a consequence of attentional processes, where features are selected by an attentional mechanism first and categories are derived second. The objects-first approach suggests that cross-categorization is a consequence of repeated, sequential attempts to explain features, where categories are derived first, then features that are poorly explained are recategorized. We present two sets of simulations and experiments testing the models' predictions about human categorization. We find that an approach based on joint inference provides the best fit to human categorization behavior, and we suggest that a full account of human category learning will need to incorporate something akin to these capabilities.},
  file = {2011 - Shafto et al. - A probabilistic model of cross-categorization.pdf},
  journal = {Cognition},
  language = {en},
  number = {1}
}

@article{Shannon1948a,
  title = {A {{Mathematical Theory}} of {{Communication}}},
  author = {Shannon, C E},
  year = {1948},
  volume = {27},
  pages = {55},
  file = {Shannon - A Mathematical Theory of Communication 2.pdf},
  journal = {The Bell system technical journal},
  language = {en},
  number = {3}
}

@book{Shannon1964,
  title = {The {{Mathematical Theory}} of {{Communication}}},
  author = {Shannon, Claude and Weaver, Warren},
  year = {1964},
  publisher = {{The university of Illinois Press}},
  file = {Shannon and Weaver - The Mathematical Theory of Communication.pdf},
  language = {en}
}

@techreport{Sharpe2019,
  title = {Dopamine Transients Delivered in Learning Contexts Do Not Act as Model-Free Prediction Errors},
  author = {Sharpe, Melissa J. and Batchelor, Hannah M. and Mueller, Lauren E. and Chang, Chun Yun and Maes, Etienne J.P. and Niv, Yael and Schoenbaum, Geoffrey},
  year = {2019},
  month = mar,
  institution = {{Neuroscience}},
  doi = {10.1101/574541},
  abstract = {Dopamine neurons fire transiently in response to unexpected rewards. These neural correlates are proposed to signal the reward prediction error described in model-free reinforcement learning algorithms. This error term represents the unpredicted or `excess' value of the rewarding event. In model-free reinforcement learning, this value is then stored as part of the learned value of any antecedent cues, contexts or events, making them intrinsically valuable, independent of the specific rewarding event that caused the prediction error. In support of equivalence between dopamine transients and this model-free error term, proponents cite causal optogenetic studies showing that artificially induced dopamine transients cause lasting changes in behavior. Yet none of these studies directly demonstrate the presence of cached value under conditions appropriate for associative learning. To address this gap in our knowledge, we conducted three studies where we optogenetically activated dopamine neurons while rats were learning associative relationships, both with and without reward. In each experiment, the antecedent cues failed to acquired value and instead entered into value-independent associative relationships with the other cues or rewards. These results show that dopamine transients, constrained within appropriate learning situations, support valueless associative learning.},
  file = {Sharpe et al. - 2019 - Dopamine transients delivered in learning contexts.pdf},
  language = {en},
  type = {Preprint}
}

@techreport{Sharpe2019a,
  title = {Dopamine Transients Delivered in Learning Contexts Do Not Act as Model-Free Prediction Errors},
  author = {Sharpe, Melissa J. and Batchelor, Hannah M. and Mueller, Lauren E. and Chang, Chun Yun and Maes, Etienne J.P. and Niv, Yael and Schoenbaum, Geoffrey},
  year = {2019},
  month = mar,
  institution = {{Neuroscience}},
  doi = {10.1101/574541},
  abstract = {Dopamine neurons fire transiently in response to unexpected rewards. These neural correlates are proposed to signal the reward prediction error described in model-free reinforcement learning algorithms. This error term represents the unpredicted or `excess' value of the rewarding event. In model-free reinforcement learning, this value is then stored as part of the learned value of any antecedent cues, contexts or events, making them intrinsically valuable, independent of the specific rewarding event that caused the prediction error. In support of equivalence between dopamine transients and this model-free error term, proponents cite causal optogenetic studies showing that artificially induced dopamine transients cause lasting changes in behavior. Yet none of these studies directly demonstrate the presence of cached value under conditions appropriate for associative learning. To address this gap in our knowledge, we conducted three studies where we optogenetically activated dopamine neurons while rats were learning associative relationships, both with and without reward. In each experiment, the antecedent cues failed to acquired value and instead entered into value-independent associative relationships with the other cues or rewards. These results show that dopamine transients, constrained within appropriate learning situations, support valueless associative learning.},
  file = {Sharpe et al. - 2019 - Dopamine transients delivered in learning contexts 2.pdf},
  language = {en},
  type = {Preprint}
}

@article{Sharpe2020,
  title = {Dopamine Transients Do Not Act as Model-Free Prediction Errors during Associative Learning},
  author = {Sharpe, Melissa J. and Batchelor, Hannah M. and Mueller, Lauren E. and Yun Chang, Chun and Maes, Etienne J. P. and Niv, Yael and Schoenbaum, Geoffrey},
  year = {2020},
  month = dec,
  volume = {11},
  pages = {106},
  issn = {2041-1723},
  doi = {10.1038/s41467-019-13953-1},
  file = {Sharpe et al. - 2020 - Dopamine transients do not act as model-free predi.pdf},
  journal = {Nat Commun},
  language = {en},
  number = {1}
}

@article{Sharpe2020a,
  title = {Dopamine Transients Do Not Act as Model-Free Prediction Errors during Associative Learning},
  author = {Sharpe, Melissa J. and Batchelor, Hannah M. and Mueller, Lauren E. and Yun Chang, Chun and Maes, Etienne J. P. and Niv, Yael and Schoenbaum, Geoffrey},
  year = {2020},
  month = dec,
  volume = {11},
  pages = {106},
  issn = {2041-1723},
  doi = {10.1038/s41467-019-13953-1},
  file = {Sharpe et al. - 2020 - Dopamine transients do not act as model-free predi 2.pdf},
  journal = {Nat Commun},
  language = {en},
  number = {1}
}

@article{Sharpee2014,
  title = {Toward {{Functional Classification}} of {{Neuronal Types}}},
  author = {Sharpee, Tatyana O.},
  year = {2014},
  month = sep,
  volume = {83},
  pages = {1329--1334},
  issn = {08966273},
  doi = {10.1016/j.neuron.2014.08.040},
  file = {Sharpee - 2014 - Toward Functional Classification of Neuronal Types.pdf},
  journal = {Neuron},
  language = {en},
  number = {6}
}

@article{Shattuck2008,
  title = {Construction of a {{3D}} Probabilistic Atlas of Human Cortical Structures},
  author = {Shattuck, David W. and Mirza, Mubeena and Adisetiyo, Vitria and Hojatkashani, Cornelius and Salamon, Georges and Narr, Katherine L. and Poldrack, Russell A. and Bilder, Robert M. and Toga, Arthur W.},
  year = {2008},
  month = feb,
  volume = {39},
  pages = {1064--1080},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2007.09.031},
  file = {2008 - Shattuck et al. - Construction of a 3D probabilistic atlas of human cortical structures.pdf},
  journal = {NeuroImage},
  language = {en},
  number = {3}
}

@article{Shazeer2017,
  title = {{{OUTRAGEOUSLY LARGE NEURAL NETWORKS}}: {{THE SPARSELY}}-{{GATED MIXTURE}}-{{OF}}-{{EXPERTS LAYER}}},
  author = {Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof and Davis, Andy and Le, Quoc and Dean, Jeff},
  year = {2017},
  pages = {19},
  abstract = {The capacity of a neural network to absorb information is limited by its number of parameters. Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation. In practice, however, there are significant algorithmic and performance challenges. In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters. We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example. We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora. We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers. On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.},
  file = {Shazeer et al. - 2017 - OUTRAGEOUSLY LARGE NEURAL NETWORKS THE SPARSELY-G.pdf},
  language = {en}
}

@article{Shenoy2021,
  title = {Measurement, Manipulation and Modeling of Brain-Wide Neural Population Dynamics},
  author = {Shenoy, Krishna V. and Kao, Jonathan C.},
  year = {2021},
  month = dec,
  volume = {12},
  pages = {633},
  issn = {2041-1723},
  doi = {10.1038/s41467-020-20371-1},
  file = {Shenoy and Kao - 2021 - Measurement, manipulation and modeling of brain-wi.pdf},
  journal = {Nat Commun},
  language = {en},
  number = {1}
}

@article{Shepard2011,
  title = {Energy {{Beyond Food}}: {{Foraging Theory Informs Time Spent}} in {{Thermals}} by a {{Large Soaring Bird}}},
  shorttitle = {Energy {{Beyond Food}}},
  author = {Shepard, Emily L. C. and Lambertucci, Sergio A. and Vallmitjana, Diego and Wilson, Rory P.},
  editor = {Fenton, Brock},
  year = {2011},
  month = nov,
  volume = {6},
  pages = {e27375},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0027375},
  abstract = {Current understanding of how animals search for and exploit food resources is based on microeconomic models. Although widely used to examine feeding, such constructs should inform other energy-harvesting situations where theoretical assumptions are met. In fact, some animals extract non-food forms of energy from the environment, such as birds that soar in updraughts. This study examined whether the gains in potential energy (altitude) followed efficiency-maximising predictions in the world's heaviest soaring bird, the Andean condor (Vultur gryphus). Animal-attached technology was used to record condor flight paths in three-dimensions. Tracks showed that time spent in patchy thermals was broadly consistent with a strategy to maximise the rate of potential energy gain. However, the rate of climb just prior to leaving a thermal increased with thermal strength and exit altitude. This suggests higher rates of energetic gain may not be advantageous where the resulting gain in altitude would lead to a reduction in the ability to search the ground for food. Consequently, soaring behaviour appeared to be modulated by the need to reconcile differing potential energy and food energy distributions. We suggest that foraging constructs may provide insight into the exploitation of non-food energy forms, and that non-food energy distributions may be more important in informing patterns of movement and residency over a range of scales than previously considered.},
  file = {Shepard et al. - 2011 - Energy Beyond Food Foraging Theory Informs Time S.pdf},
  journal = {PLoS ONE},
  language = {en},
  number = {11}
}

@article{Shepherd1987,
  title = {Logic Operations Are Properties of Computer-Simulated Interactions between Excitable Dendritic Spines},
  author = {Shepherd, Gordon M. and Brayton, Robert K.},
  year = {1987},
  month = apr,
  volume = {21},
  pages = {151--165},
  issn = {03064522},
  doi = {10.1016/0306-4522(87)90329-0},
  abstract = {Neurons in the central nervous system of mammals and many other species receive most of their synaptic inputs in their dendritic branches and spines, but the precise manner in which this information is processed in the dendrites is not understood. In order to gain insight into these mechanisms, simulations of interactions between distal dendritic spines with an excitable membrane have been carried out, using an electrical circuit analysis program for the compartmental representation of a dendrite and several spines. Interactions between responses to single and paired excitatory and inhibitory synaptic inputs have been analyzed. Basic logic operations, including AND gates, OR gates and ANDNOT gates, arise from these interactions.},
  file = {1987 - Shepherd, Brayton - M. and.pdf;Shepherd and Brayton - 1987 - Logic operations are properties of computer-simula.pdf},
  journal = {Neuroscience},
  language = {en},
  number = {1}
}

@article{Sheremet2016,
  title = {Movement {{Enhances}} the {{Nonlinearity}} of {{Hippocampal Theta}}},
  author = {Sheremet, A. and Burke, S. N. and Maurer, A. P.},
  year = {2016},
  month = apr,
  volume = {36},
  pages = {4218--4230},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.3564-15.2016},
  file = {Sheremet et al. - 2016 - Movement Enhances the Nonlinearity of Hippocampal .pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {15}
}

@article{Sherman1998,
  title = {On the Actions That One Nerve Cell Can Have on Another: {{Distinguishing}} "Drivers" from "Modulators"},
  shorttitle = {On the Actions That One Nerve Cell Can Have on Another},
  author = {Sherman, S. M. and Guillery, R. W.},
  year = {1998},
  month = jun,
  volume = {95},
  pages = {7121--7126},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.95.12.7121},
  abstract = {When one nerve cell acts on another, its postsynaptic effect can vary greatly. In sensory systems, inputs from ``drivers'' can be differentiated from those of ``modulators.'' The driver can be identified as the transmitter of receptive field properties; the modulator can be identified as altering the probability of certain aspects of that transmission. Where receptive fields are not available, the distinction is more difficult and currently is undefined. We use the visual pathways, particularly the thalamic geniculate relay for which much relevant evidence is available, to explore ways in which drivers can be distinguished from modulators. The extent to which the distinction may apply first to other parts of the thalamus and then, possibly, to other parts of the brain is considered. We suggest the following distinctions: Crosscorrelograms from driver inputs have sharper peaks than those from modulators; there are likely to be few drivers but many modulators for any one cell; and drivers are likely to act only through ionotropic receptors having a fast postsynaptic effect whereas modulators also are likely to activate metabotropic receptors having a slow and prolonged postsynaptic effect.},
  file = {1998 - Sherman, Guillery - On the actions that one nerve cell can have on another distinguishing drivers from modulators.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {12}
}

@article{Sherman2016,
  title = {Neural Mechanisms of Transient Neocortical Beta Rhythms: {{Converging}} Evidence from Humans, Computational Modeling, Monkeys, and Mice},
  shorttitle = {Neural Mechanisms of Transient Neocortical Beta Rhythms},
  author = {Sherman, Maxwell A. and Lee, Shane and Law, Robert and Haegens, Saskia and Thorn, Catherine A. and H{\"a}m{\"a}l{\"a}inen, Matti S. and Moore, Christopher I. and Jones, Stephanie R.},
  year = {2016},
  month = aug,
  volume = {113},
  pages = {E4885-E4894},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1604135113},
  file = {Sherman et al. - 2016 - Neural mechanisms of transient neocortical beta rh.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {33}
}

@article{Shimazaki,
  title = {Neurons as an {{Information}}-Theoretic {{Engine}}},
  author = {Shimazaki, Hideaki},
  pages = {16},
  abstract = {We show that dynamical gain modulation of neurons' stimulus response is described as an information-theoretic cycle that generates entropy associated with the stimulus-related activity from entropy produced by the modulation. To articulate this theory, we describe stimulus-evoked activity of a neural population based on the maximum entropy principle with constraints on two types of overlapping activities, one that is controlled by stimulus conditions and the other, termed internal activity, that is regulated internally in an organism. We demonstrate that modulation of the internal activity realises gain control of stimulus response, and controls stimulus information. A cycle of neural dynamics is then introduced to model information processing by the neurons during which the stimulus information is dynamically enhanced by the internal gain-modulation mechanism. Based on the conservation law for entropy production, we demonstrate that the cycle generates entropy ascribed to the stimulus-related activity using entropy supplied by the internal mechanism, analogously to a heat engine that produces work from heat. We provide an efficient cycle that achieves the highest entropic efficiency to retain the stimulus information. The theory allows us to quantify efficiency of the internal computation and its theoretical limit.},
  file = {2015 - Shimazaki - Neurons as an Information-theoretic Engine.pdf;Shimazaki - Neurons as an Information-theoretic Engine.pdf},
  language = {en}
}

@article{Shin2017,
  title = {The Rate of Transient Beta Frequency Events Predicts Behavior across Tasks and Species},
  author = {Shin, Hyeyoung and Law, Robert and Tsutsui, Shawn and Moore, Christopher I and Jones, Stephanie R},
  year = {2017},
  month = nov,
  volume = {6},
  issn = {2050-084X},
  doi = {10.7554/eLife.29086},
  file = {Shin et al. - 2017 - The rate of transient beta frequency events predic.pdf},
  journal = {eLife},
  language = {en}
}

@article{Shinkareva2012,
  title = {Exploring Commonalities across Participants in the Neural Representation of Objects},
  author = {Shinkareva, Svetlana V. and Malave, Vicente L. and Just, Marcel Adam and Mitchell, Tom M.},
  year = {2012},
  month = jun,
  volume = {33},
  pages = {1375--1383},
  issn = {10659471},
  doi = {10.1002/hbm.21296},
  abstract = {The question of whether the neural encodings of objects are similar across different people is one of the key questions in cognitive neuroscience. This article examines the commonalities in the internal representation of objects, as measured with fMRI, across individuals in two complementary ways. First, we examine the commonalities in the internal representation of objects across people at the level of interobject distances, derived from whole brain fMRI data, and second, at the level of spatially localized anatomical brain regions that contain sufficient information for identification of object categories, without making the assumption that their voxel patterns are spatially matched in a common space. We examine the commonalities in internal representation of objects on 3T fMRI data collected while participants viewed line drawings depicting various tools and dwellings. This exploratory study revealed the extent to which the representation of individual concepts, and their mutual similarity, is shared across participants. Hum Brain Mapp 33:1375\textendash 1383, 2012. VC 2011 Wiley Periodicals, Inc.},
  file = {2012 - Shinkareva et al. - Exploring commonalities across participants in the neural representation of objects.pdf},
  journal = {Human Brain Mapping},
  language = {en},
  number = {6}
}

@article{Shmuel2007,
  title = {Spatio-Temporal Point-Spread Function of {{fMRI}} Signal in Human Gray Matter at 7 {{Tesla}}},
  author = {Shmuel, Amir and Yacoub, Essa and Chaimow, Denis and Logothetis, Nikos K. and Ugurbil, Kamil},
  year = {2007},
  month = apr,
  volume = {35},
  pages = {539--552},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2006.12.030},
  file = {2007 - Shmuel et al. - Spatio-temporal point-spread function of fMRI signal in human gray matter at 7 Tesla.pdf},
  journal = {NeuroImage},
  language = {en},
  number = {2}
}

@article{Shoham,
  title = {Multi-{{Agent Reinforcement Learning}}: A Critical Survey},
  author = {Shoham, Yoav and Powers, Rob and Grenager, Trond},
  pages = {13},
  abstract = {We survey the recent work in AI on multi-agent reinforcement learning (that is, learning in stochastic games). We then argue that, while exciting, this work is flawed. The fundamental flaw is unclarity about the problem or problems being addressed. After tracing a representative sample of the recent literature, we identify four well-defined problems in multi-agent reinforcement learning, single out the problem that in our view is most suitable for AI, and make some remarks about how we believe progress is to be made on this problem.},
  file = {2003 - Shoham, Powers, Grenager - Multi-agent reinforcement learning a critical survey.pdf},
  language = {en}
}

@article{Shoham2007,
  title = {If Multi-Agent Learning Is the Answer, What Is the Question?},
  author = {Shoham, Yoav and Powers, Rob and Grenager, Trond},
  year = {2007},
  month = may,
  volume = {171},
  pages = {365--377},
  issn = {00043702},
  doi = {10.1016/j.artint.2006.02.006},
  file = {2007 - Shoham, Powers, Grenager - If multi-agent learning is the answer, what is the question.pdf},
  journal = {Artificial Intelligence},
  language = {en},
  number = {7}
}

@article{Shouval2002,
  title = {Converging Evidence for a Simplified Biophysical Model of Synaptic Plasticity},
  author = {Shouval, Harel Z. and Castellani, Gastone C. and Blais, Brian S. and Yeung, Luk C. and Cooper, Leon N},
  year = {2002},
  month = dec,
  volume = {87},
  pages = {383--391},
  issn = {03401200},
  doi = {10.1007/s00422-002-0362-x},
  abstract = {Different mechanisms that could form the molecular basis for bi-directional synaptic plasticity have been identified experimentally and corresponding biophysical models can be constructed. However, such models are complex and therefore it is hard to deduce their consequences to compare them to existing abstract models of synaptic plasticity. In this paper we examine two such models: a phenomenological one inspired by the phenomena of AMPA receptor insertion, and a more complex biophysical model based on the phenomena of AMPA receptor phosphorylation. We show that under certain approximations both these models can be mapped on to an equivalent, calcium-dependent, differential equation. Intracellular calcium concentration varies locally in each postsynaptic compartment, thus the plasticity rule we extract is a single-synapse rule. We convert this single synapse plasticity equation to a multisynapse rule by incorporating a model of the NMDA receptor. Finally we suggest a mathematical embodiment of metaplasticity, which is consistent with observations on NMDA receptor properties and dependence on cellular activity. These results, in combination with some of our previous results, produce converging evidence for the calcium control hypothesis including a dependence of synaptic plasticity on the level of intercellular calcium as well as on the temporal pattern of calcium transients.},
  file = {2002 - Shouval et al. - Converging evidence for a simplified biophysical model of synaptic plasticity.pdf},
  journal = {Biological Cybernetics},
  language = {en},
  number = {5-6}
}

@article{Shriki2003,
  title = {Rate {{Models}} for {{Conductance}}-{{Based Cortical Neuronal Networks}}},
  author = {Shriki, Oren and Hansel, David and Sompolinsky, Haim},
  year = {2003},
  month = aug,
  volume = {15},
  pages = {1809--1841},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/08997660360675053},
  file = {2003 - Shriki, Hansel, Sompolinsky - Rate models for conductance-based cortical neuronal networks.pdf},
  journal = {Neural Computation},
  language = {en},
  number = {8}
}

@article{Shyam2018,
  title = {Model-{{Based Active Exploration}}},
  author = {Shyam, Pranav and Ja{\'s}kowski, Wojciech and Gomez, Faustino},
  year = {2018},
  month = oct,
  abstract = {Efficient exploration is an unsolved problem in Reinforcement Learning which is usually addressed by reactively rewarding the agent for fortuitously encountering novel situations. This paper introduces an efficient active exploration algorithm, Model-Based Active eXploration (MAX), which uses an ensemble of forward models to plan to observe novel events. This is carried out by optimizing agent behaviour with respect to a measure of novelty derived from the Bayesian perspective of exploration, which is estimated using the disagreement between the futures predicted by the ensemble members. We show empirically that in semi-random discrete environments where directed exploration is critical to make progress, MAX is at least an order of magnitude more efficient than strong baselines. MAX scales to highdimensional continuous environments where it builds task-agnostic models that can be used for any downstream task.},
  archiveprefix = {arXiv},
  eprint = {1810.12162},
  eprinttype = {arxiv},
  file = {Shyam et al. - 2018 - Model-Based Active Exploration.pdf},
  journal = {arXiv:1810.12162 [cs, math, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Information Theory,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  language = {en},
  primaryclass = {cs, math, stat}
}

@article{Siegel1994,
  title = {Activity-Dependent Current Distributions in Model Neurons.},
  author = {Siegel, M. and Marder, E. and Abbott, L. F.},
  year = {1994},
  month = nov,
  volume = {91},
  pages = {11308--11312},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.91.24.11308},
  abstract = {The electrical activity of a neuron can affect its intrinsic physiological characteristics through a wide range of processes. We study a computer-simulated multicompartment model neuron In which channel density depends on local Ca2+ concentrations. This has three interesting consequences for the spatial distribution of conductances and the physiological behavior ofthe neuron: (i) the model neuron spontaneously develops a realistic, nonuniform distribution of conductances that is linked both to the morphology of the neuron and to the pattern of synaptic input that it receives, (i) the response to synaptic Input reveals a form of intrinsic localized plasticity that balances the synaptic contribution from dendritic regions receiving unequal stimulation, and (i) intrinsic plasticity establishes a biophysical gain control that restores the neuron to its optimal firing range after synapses are strengthened by "Hebbian" long-term potentiation.},
  file = {1994 - Siegel, Marder, Abbottt - Activity-dependent current distributions in model neurons.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {24}
}

@article{Siegel2012,
  title = {Spectral Fingerprints of Large-Scale Neuronal Interactions},
  author = {Siegel, Markus and Donner, Tobias H. and Engel, Andreas K.},
  year = {2012},
  month = feb,
  volume = {13},
  pages = {121--134},
  issn = {1471-003X, 1471-0048},
  doi = {10.1038/nrn3137},
  abstract = {Cognition results from interactions among functionally specialized but widely distributed brain regions; however, neuroscience has so far largely focused on characterizing the function of individual brain regions and neurons therein. Here we discuss recent studies that have instead investigated the interactions between brain regions during cognitive processes by assessing correlations between neuronal oscillations in different regions of the primate cerebral cortex. These studies have opened a new window onto the large-scale circuit mechanisms underlying sensorimotor decision-making and top-down attention. We propose that frequency-specific neuronal correlations in large-scale cortical networks may be `fingerprints' of canonical neuronal computations underlying cognitive processes.},
  file = {2012 - Siegel, Donner, Engel - Spectral fingerprints of large-scale neuronal interactions.pdf},
  journal = {Nature Reviews Neuroscience},
  language = {en},
  number = {2}
}

@article{Siegel2015,
  title = {Cortical Information Flow during Flexible Sensorimotor Decisions},
  author = {Siegel, M. and Buschman, T. J. and Miller, E. K.},
  year = {2015},
  month = jun,
  volume = {348},
  pages = {1352--1355},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aab0551},
  abstract = {IT and V4 first extracted task information from the cues along with the encoding of cue identity. After this transient burst, there was a flow of sustained task information from PFC and LIP across the entire sensorimotor hierarchy.},
  file = {Siegel et al. - 2015 - Cortical information flow during flexible sensorim.pdf},
  journal = {Science},
  language = {en},
  number = {6241}
}

@article{Siero2013,
  title = {{{BOLD Consistently Matches Electrophysiology}} in {{Human Sensorimotor Cortex}} at {{Increasing Movement Rates}}: {{A Combined 7T fMRI}} and {{ECoG Study}} on {{Neurovascular Coupling}}},
  shorttitle = {{{BOLD Consistently Matches Electrophysiology}} in {{Human Sensorimotor Cortex}} at {{Increasing Movement Rates}}},
  author = {Siero, Jeroen CW and Hermes, Dora and Hoogduin, Hans and Luijten, Peter R and Petridou, Natalia and Ramsey, Nick F},
  year = {2013},
  month = sep,
  volume = {33},
  pages = {1448--1456},
  issn = {0271-678X, 1559-7016},
  doi = {10.1038/jcbfm.2013.97},
  file = {2013 - Siero et al. - BOLD Consistently Matches Electrophysiology in Human Sensorimotor Cortex at Increasing Movement Rates A Combined 7.pdf;Siero et al. - 2013 - BOLD Consistently Matches Electrophysiology in Hum.pdf},
  journal = {Journal of Cerebral Blood Flow \& Metabolism},
  language = {en},
  number = {9}
}

@article{Sigeti1987,
  title = {High-Frequency Power Spectra for Systems Subject to Noise},
  author = {Sigeti, D. and Horsthemke, W.},
  year = {1987},
  month = mar,
  volume = {35},
  pages = {2276--2282},
  issn = {0556-2791},
  doi = {10.1103/PhysRevA.35.2276},
  file = {1987 - Sigeti, Horsthemke - High-frequency power spectra for systems subject to noise.pdf},
  journal = {Physical Review A},
  language = {en},
  number = {5}
}

@article{Silberberg2004,
  title = {Synaptic Dynamics Control the Timing of Neuronal Excitation in the Activated Neocortical Microcircuit: {{Synaptic}} Dynamics Shape Cross-Correlations},
  shorttitle = {Synaptic Dynamics Control the Timing of Neuronal Excitation in the Activated Neocortical Microcircuit},
  author = {Silberberg, Gilad and Wu, Caizhi and Markram, Henry},
  year = {2004},
  month = apr,
  volume = {556},
  pages = {19--27},
  issn = {00223751},
  doi = {10.1113/jphysiol.2004.060962},
  file = {2004 - Silberberg, Wu, Markram - Synaptic dynamics control the timing of neuronal excitation in the activated neocortical microcircuit.pdf},
  journal = {The Journal of Physiology},
  language = {en},
  number = {1}
}

@article{Silver,
  title = {Lifelong {{Machine Learning Systems}}: {{Beyond Learning Algorithms}}},
  author = {Silver, Daniel L and Yang, Qiang and Li, Lianghao},
  pages = {7},
  abstract = {Lifelong Machine Learning, or LML, considers systems that can learn many tasks from one or more domains over its lifetime. The goal is to sequentially retain learned knowledge and to selectively transfer that knowledge when learning a new task so as to develop more accurate hypotheses or policies. Following a review of prior work on LML, we propose that it is now appropriate for the AI community to move beyond learning algorithms to more seriously consider the nature of systems that are capable of learning over a lifetime. Reasons for our position are presented and potential counter-arguments are discussed. The remainder of the paper contributes by defining LML, presenting a reference framework that considers all forms of machine learning, and listing several key challenges for and benefits from LML research. We conclude with ideas for next steps to advance the field.},
  file = {Silver et al. - Lifelong Machine Learning Systems Beyond Learning.pdf},
  language = {en}
}

@article{Silver2010,
  title = {Neuronal Arithmetic},
  author = {Silver, R. Angus},
  year = {2010},
  month = jul,
  volume = {11},
  pages = {474--489},
  issn = {1471-003X, 1471-0048},
  doi = {10.1038/nrn2864},
  abstract = {The vast computational power of the brain has traditionally been viewed as arising from the complex connectivity of neural networks, in which an individual neuron acts as a simple linear summation and thresholding device. However, recent studies show that individual neurons utilize a wealth of nonlinear mechanisms to transform synaptic input into output firing. These mechanisms can arise from synaptic plasticity, synaptic noise, and somatic and dendritic conductances. This tool kit of nonlinear mechanisms confers considerable computational power on both morphologically simple and more complex neurons, enabling them to perform a range of arithmetic operations on signals encoded in a variety of different ways.},
  file = {Silver - 2010 - Neuronal arithmetic.pdf},
  journal = {Nat Rev Neurosci},
  language = {en},
  number = {7}
}

@article{Silver2016,
  title = {Mastering the Game of {{Go}} with Deep Neural Networks and Tree Search},
  author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and {van den Driessche}, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
  year = {2016},
  month = jan,
  volume = {529},
  pages = {484--489},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature16961},
  file = {Silver et al. - 2016 - Mastering the game of Go with deep neural networks.pdf},
  journal = {Nature},
  language = {en},
  number = {7587}
}

@article{Silver2016a,
  title = {Mastering the Game of {{Go}} with Deep Neural Networks and Tree Search},
  author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and {van den Driessche}, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
  year = {2016},
  month = jan,
  volume = {529},
  pages = {484--489},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature16961},
  file = {Silver et al. - 2016 - Mastering the game of Go with deep neural networks 2.pdf},
  journal = {Nature},
  language = {en},
  number = {7587}
}

@article{Silver2016b,
  title = {Mastering the Game of {{Go}} with Deep Neural Networks and Tree Search},
  author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and {van den Driessche}, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
  year = {2016},
  month = jan,
  volume = {529},
  pages = {484--489},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature16961},
  file = {Silver et al. - 2016 - Mastering the game of Go with deep neural networks 3.pdf},
  journal = {Nature},
  language = {en},
  number = {7587}
}

@article{Silver2018,
  title = {Mastering {{Chess}} and {{Shogi}} by {{Self}}-{{Play}} with a {{General Reinforcement Learning Algorithm}}},
  author = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis},
  year = {2018},
  volume = {362},
  pages = {1140--1144},
  abstract = {The game of chess is the most widely-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. In contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go, by tabula rasa reinforcement learning from games of self-play. In this paper, we generalise this approach into a single AlphaZero algorithm that can achieve, tabula rasa, superhuman performance in many challenging domains. Starting from random play, and given no domain knowledge except the game rules, AlphaZero achieved within 24 hours a superhuman level of play in the games of chess and shogi (Japanese chess) as well as Go, and convincingly defeated a world-champion program in each case.},
  file = {Silver et al. - Mastering Chess and Shogi by Self-Play with a Gene.pdf},
  journal = {Science},
  language = {en},
  number = {6419}
}

@article{Silvera,
  title = {Lecture 9: {{Exploration}} and {{Exploitation}}},
  author = {Silver, David},
  pages = {47},
  file = {Silver - Lecture 9 Exploration and Exploitation.pdf},
  language = {en}
}

@book{Sim2018,
  title = {Applications of {{Evolutionary Computation}}},
  editor = {Sim, Kevin and Kaufmann, Paul},
  year = {2018},
  volume = {10784},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-77538-8},
  file = {Sim and Kaufmann - 2018 - Applications of Evolutionary Computation.pdf},
  isbn = {978-3-319-77537-1 978-3-319-77538-8},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{Sims2008,
  title = {Scaling Laws of Marine Predator Search Behaviour},
  author = {Sims, David W. and Southall, Emily J. and Humphries, Nicolas E. and Hays, Graeme C. and Bradshaw, Corey J. A. and Pitchford, Jonathan W. and James, Alex and Ahmed, Mohammed Z. and Brierley, Andrew S. and Hindell, Mark A. and Morritt, David and Musyl, Michael K. and Righton, David and Shepard, Emily L. C. and Wearmouth, Victoria J. and Wilson, Rory P. and Witt, Matthew J. and Metcalfe, Julian D.},
  year = {2008},
  month = feb,
  volume = {451},
  pages = {1098--1102},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature06518},
  file = {Sims et al. - 2008 - Scaling laws of marine predator search behaviour.pdf},
  journal = {Nature},
  language = {en},
  number = {7182}
}

@article{Sims2014,
  title = {Hierarchical Random Walks in Trace Fossils and the Origin of Optimal Search Behavior},
  author = {Sims, D. W. and Reynolds, A. M. and Humphries, N. E. and Southall, E. J. and Wearmouth, V. J. and Metcalfe, B. and Twitchett, R. J.},
  year = {2014},
  month = jul,
  volume = {111},
  pages = {11073--11078},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1405966111},
  file = {Sims et al. - 2014 - Hierarchical random walks in trace fossils and the.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {30}
}

@article{Sims2016,
  title = {Rate\textendash Distortion Theory and Human Perception},
  author = {Sims, Chris R.},
  year = {2016},
  month = jul,
  volume = {152},
  pages = {181--198},
  issn = {00100277},
  doi = {10.1016/j.cognition.2016.03.020},
  abstract = {The fundamental goal of perception is to aid in the achievement of behavioral objectives. This requires extracting and communicating useful information from noisy and uncertain sensory signals. At the same time, given the complexity of sensory information and the limitations of biological information processing, it is necessary that some information must be lost or discarded in the act of perception. Under these circumstances, what constitutes an `optimal' perceptual system? This paper describes the mathematical framework of rate\textendash distortion theory as the optimal solution to the problem of minimizing the costs of perceptual error subject to strong constraints on the ability to communicate or transmit information. Rate\textendash distortion theory offers a general and principled theoretical framework for developing computational-level models of human perception (Marr, 1982). Models developed in this framework are capable of producing quantitatively precise explanations for human perceptual performance, while yielding new insights regarding the nature and goals of perception. This paper demonstrates the application of rate\textendash distortion theory to two benchmark domains where capacity limits are especially salient in human perception: discrete categorization of stimuli (also known as absolute identification) and visual working memory. A software package written for the R statistical programming language is described that aids in the development of models based on rate\textendash distortion theory.},
  file = {Sims - 2016 - Rate–distortion theory and human perception.pdf},
  journal = {Cognition},
  language = {en}
}

@article{Sims2018,
  title = {Efficient Coding Explains the Universal Law of Generalization in Human Perception},
  author = {Sims, Chris R.},
  year = {2018},
  month = may,
  volume = {360},
  pages = {652--656},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aaq1118},
  file = {Sims - 2018 - Efficient coding explains the universal law of gen.pdf},
  journal = {Science},
  language = {en},
  number = {6389}
}

@article{Sims2019,
  title = {Optimal Searching Behaviour Generated Intrinsically by the Central Pattern Generator for Locomotion},
  author = {Sims, David W and Humphries, Nicolas E and Hu, Nan and Medan, Violeta and Berni, Jimena},
  year = {2019},
  month = nov,
  volume = {8},
  pages = {e50316},
  issn = {2050-084X},
  doi = {10.7554/eLife.50316},
  abstract = {Efficient searching for resources such as food by animals is key to their survival. It has been proposed that diverse animals from insects to sharks and humans adopt searching patterns that resemble a simple Le\textasciiacute{} vy random walk, which is theoretically optimal for `blind foragers' to locate sparse, patchy resources. To test if such patterns are generated intrinsically, or arise via environmental interactions, we tracked free-moving Drosophila larvae with (and without) blocked synaptic activity in the brain, suboesophageal ganglion (SOG) and sensory neurons. In brainblocked larvae, we found that extended substrate exploration emerges as multi-scale movement paths similar to truncated Le\textasciiacute{} vy walks. Strikingly, power-law exponents of brain/SOG/sensoryblocked larvae averaged 1.96, close to a theoretical optimum (m ffi 2.0) for locating sparse resources. Thus, efficient spatial exploration can emerge from autonomous patterns in neural activity. Our results provide the strongest evidence so far for the intrinsic generation of Le\textasciiacute{} vy-like movement patterns.},
  file = {Sims et al. - 2019 - Optimal searching behaviour generated intrinsicall.pdf},
  journal = {eLife},
  language = {en}
}

@article{Sims2019a,
  title = {Optimal Searching Behaviour Generated Intrinsically by the Central Pattern Generator for Locomotion},
  author = {Sims, David W and Humphries, Nicolas E and Hu, Nan and Medan, Violeta and Berni, Jimena},
  year = {2019},
  month = nov,
  volume = {8},
  pages = {e50316},
  issn = {2050-084X},
  doi = {10.7554/eLife.50316},
  abstract = {Efficient searching for resources such as food by animals is key to their survival. It has been proposed that diverse animals from insects to sharks and humans adopt searching patterns that resemble a simple Le\textasciiacute{} vy random walk, which is theoretically optimal for `blind foragers' to locate sparse, patchy resources. To test if such patterns are generated intrinsically, or arise via environmental interactions, we tracked free-moving Drosophila larvae with (and without) blocked synaptic activity in the brain, suboesophageal ganglion (SOG) and sensory neurons. In brainblocked larvae, we found that extended substrate exploration emerges as multi-scale movement paths similar to truncated Le\textasciiacute{} vy walks. Strikingly, power-law exponents of brain/SOG/sensoryblocked larvae averaged 1.96, close to a theoretical optimum (m ffi 2.0) for locating sparse resources. Thus, efficient spatial exploration can emerge from autonomous patterns in neural activity. Our results provide the strongest evidence so far for the intrinsic generation of Le\textasciiacute{} vy-like movement patterns.},
  file = {Sims et al. - 2019 - Optimal searching behaviour generated intrinsicall 2.pdf},
  journal = {eLife},
  language = {en}
}

@inproceedings{Simsek2006,
  title = {An Intrinsic Reward Mechanism for Efficient Exploration},
  booktitle = {Proceedings of the 23rd International Conference on {{Machine}} Learning  - {{ICML}} '06},
  author = {{\c S}im{\c s}ek, {\"O}zg{\"u}r and Barto, Andrew G.},
  year = {2006},
  pages = {833--840},
  publisher = {{ACM Press}},
  address = {{Pittsburgh, Pennsylvania}},
  doi = {10.1145/1143844.1143949},
  abstract = {How should a reinforcement learning agent act if its sole purpose is to efficiently learn an optimal policy for later use? In other words, how should it explore, to be able to exploit later? We formulate this problem as a Markov Decision Process by explicitly modeling the internal state of the agent and propose a principled heuristic for its solution. We present experimental results in a number of domains, also exploring the algorithm's use for learning a policy for a skill given its reward function\textemdash an important but neglected component of skill discovery.},
  file = {Şimşek and Barto - 2006 - An intrinsic reward mechanism for efficient explor.pdf},
  isbn = {978-1-59593-383-6},
  language = {en}
}

@article{Sinervo1996,
  title = {The Rock\textendash Paper\textendash Scissors Game and the Evolution of Alternative Male Strategies},
  author = {Sinervo, B. and Lively, C. M.},
  year = {1996},
  month = mar,
  volume = {380},
  pages = {240--243},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/380240a0},
  file = {Sinervo and Lively - 1996 - The rock–paper–scissors game and the evolution of .pdf},
  journal = {Nature},
  language = {en},
  number = {6571}
}

@article{Singh,
  title = {Where {{Do Rewards Come From}}?},
  author = {Singh, Satinder and Lewis, Richard L and Barto, Andrew G},
  pages = {6},
  abstract = {Reinforcement learning has achieved broad and successful application in cognitive science in part because of its general formulation of the adaptive control problem as the maximization of a scalar reward function. The computational reinforcement learning framework is motivated by correspondences to animal reward processes, but it leaves the source and nature of the rewards unspecified. This paper advances a general computational framework for reward that places it in an evolutionary context, formulating a notion of an optimal reward function given a fitness function and some distribution of environments. Novel results from computational experiments show how traditional notions of extrinsically and intrinsically motivated behaviors may emerge from such optimal reward functions. In the experiments these rewards are discovered through automated search rather than crafted by hand. The precise form of the optimal reward functions need not bear a direct relationship to the fitness function, but may nonetheless confer significant advantages over rewards based only on fitness.},
  file = {Singh et al. - Where Do Rewards Come From.pdf},
  language = {en}
}

@article{Singh1970,
  title = {Preference for Bar Pressing to Obtain Reward over Freeloading in Rats and Children.},
  author = {Singh, Devendra},
  year = {1970},
  volume = {73},
  pages = {320--327},
  issn = {0021-9940},
  doi = {10.1037/h0030222},
  file = {Singh - 1970 - Preference for bar pressing to obtain reward over .pdf},
  journal = {Journal of Comparative and Physiological Psychology},
  language = {en},
  number = {2}
}

@article{Singh1970a,
  title = {Preference for Bar Pressing to Obtain Reward over Freeloading in Rats and Children.},
  author = {Singh, Devendra},
  year = {1970},
  volume = {73},
  pages = {320--327},
  issn = {0021-9940},
  doi = {10.1037/h0030222},
  file = {Singh - 1970 - Preference for bar pressing to obtain reward over  2.pdf},
  journal = {Journal of Comparative and Physiological Psychology},
  language = {en},
  number = {2}
}

@techreport{Singh2005,
  title = {Intrinsically {{Motivated Reinforcement Learning}}:},
  shorttitle = {Intrinsically {{Motivated Reinforcement Learning}}},
  author = {Singh, Satinder and Barto, Andrew G. and Chentanez, Nuttapong},
  year = {2005},
  month = jan,
  address = {{Fort Belvoir, VA}},
  institution = {{Defense Technical Information Center}},
  doi = {10.21236/ADA440280},
  abstract = {Psychologists call behavior intrinsically motivated when it is engaged in for its own sake rather than as a step toward solving a specific problem of clear practical value. But what we learn during intrinsically motivated behavior is essential for our development as competent autonomous entities able to efficiently solve a wide range of practical problems as they arise. In this paper we present initial results from a computational study of intrinsically motivated reinforcement learning aimed at allowing artificial agents to construct and extend hierarchies of reusable skills that are needed for competent autonomy.},
  file = {Singh et al. - 2005 - Intrinsically Motivated Reinforcement Learning.pdf},
  language = {en}
}

@article{Sinha2014,
  title = {Autism as a Disorder of Prediction},
  author = {Sinha, Pawan and Kjelgaard, Margaret M. and Gandhi, Tapan K. and Tsourides, Kleovoulos and Cardinaux, Annie L. and Pantazis, Dimitrios and Diamond, Sidney P. and Held, Richard M.},
  year = {2014},
  month = oct,
  volume = {111},
  pages = {15220--15225},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1416797111},
  file = {Sinha et al. - 2014 - Autism as a disorder of prediction.pdf},
  journal = {Proc Natl Acad Sci USA},
  language = {en},
  number = {42}
}

@inproceedings{Sinha2016,
  title = {Causality Preserving Information Transfer Measure for Control Dynamical System},
  booktitle = {2016 {{IEEE}} 55th {{Conference}} on {{Decision}} and {{Control}} ({{CDC}})},
  author = {Sinha, Subhrajit and Vaidya, Umesh},
  year = {2016},
  month = dec,
  pages = {7329--7334},
  publisher = {{IEEE}},
  address = {{Las Vegas, NV, USA}},
  doi = {10.1109/CDC.2016.7799401},
  abstract = {In this paper, we show through examples, how the existing definitions of information transfer, namely directed information and transfer entropy fail to capture true causal interaction between states in control dynamical system. Furthermore, existing definitions are shown to be too weak to have any implication on two of the most fundamental concepts in system theory, namely controllability and observability. We propose a new definition of information transfer, based on the ideas from dynamical system theory, and show that this new definition can not only capture true causal interaction between states, but also have implication on system controllability and observability properties. In particular, we show that non-zero transfer of information from input-to-state and state-to-output implies structural controllability and observability properties of the control dynamical system respectively. Analytical expression for information transfer between state-to-state, input-to-state, state-to-output, and input-to-output are provided for linear system. There is a natural extension of our proposed definition to define information transfer over n time steps and average information transfer over infinite time step. We show that the average information transfer in feedback control system between plant output and input is equal to the entropy of the open loop dynamics thereby re-deriving the Bode fundamental limitation results using the proposed definition of transfer.},
  file = {Sinha and Vaidya - 2016 - Causality preserving information transfer measure .pdf},
  isbn = {978-1-5090-1837-6},
  language = {en}
}

@techreport{Sipe2020,
  title = {Astrocytic Glutamate Uptake Coordinates Experience-Dependent, Eye-Specific Refinement in Developing Visual Cortex},
  author = {Sipe, Grayson and Petravicz, Jeremy and Rikhye, Rajeev and Garcia, Rodrigo and Mellios, Nikolaos and Sur, Mriganka},
  year = {2020},
  month = may,
  institution = {{Neuroscience}},
  doi = {10.1101/2020.05.25.113613},
  abstract = {The uptake of glutamate by astrocytes actively shapes synaptic transmission, however its role in the development and plasticity of neuronal circuits remains poorly understood. The astrocytic glutamate transporter, GLT1 is the predominant source of glutamate clearance in the adult mouse cortex. Here, we examined the structural and functional development of the visual cortex in GLT1 heterozygous (HET) mice using two-photon microscopy, immunohistochemistry and slice electrophysiology. We find that though eye-specific thalamic axonal segregation is intact, binocular refinement in the primary visual cortex is disrupted. Eye-specific responses to visual stimuli in GLT1 HET mice show altered binocular matching, with abnormally high responses to ipsilateral compared to contralateral eye stimulation and a greater mismatch between preferred orientation selectivity of ipsilateral and contralateral eye responses. Furthermore, the balance of excitation and inhibition in cortical circuits is dysregulated with an increase in somatostatin positive interneurons, decrease in parvalbumin positive interneurons, and increase in dendritic spine density in the basal dendrites of layer 2/3 excitatory neurons. Monocular deprivation induces atypical ocular dominance plasticity in GLT1 HET mice, with an unusual depression of ipsilateral open eye responses; however, this change in ipsilateral responses correlates well with an upregulation of GLT1 protein following monocular deprivation. These results demonstrate that a key function of astrocytic GLT1 function during development is the experience-dependent refinement of ipsilateral eye inputs relative to contralateral eye inputs in visual cortex.},
  file = {Sipe et al. - 2020 - Astrocytic glutamate uptake coordinates experience.pdf},
  language = {en},
  type = {Preprint}
}

@techreport{Sipe2020a,
  title = {Astrocytic Glutamate Uptake Coordinates Experience-Dependent, Eye-Specific Refinement in Developing Visual Cortex},
  author = {Sipe, Grayson and Petravicz, Jeremy and Rikhye, Rajeev and Garcia, Rodrigo and Mellios, Nikolaos and Sur, Mriganka},
  year = {2020},
  month = may,
  institution = {{Neuroscience}},
  doi = {10.1101/2020.05.25.113613},
  abstract = {The uptake of glutamate by astrocytes actively shapes synaptic transmission, however its role in the development and plasticity of neuronal circuits remains poorly understood. The astrocytic glutamate transporter, GLT1 is the predominant source of glutamate clearance in the adult mouse cortex. Here, we examined the structural and functional development of the visual cortex in GLT1 heterozygous (HET) mice using two-photon microscopy, immunohistochemistry and slice electrophysiology. We find that though eye-specific thalamic axonal segregation is intact, binocular refinement in the primary visual cortex is disrupted. Eye-specific responses to visual stimuli in GLT1 HET mice show altered binocular matching, with abnormally high responses to ipsilateral compared to contralateral eye stimulation and a greater mismatch between preferred orientation selectivity of ipsilateral and contralateral eye responses. Furthermore, the balance of excitation and inhibition in cortical circuits is dysregulated with an increase in somatostatin positive interneurons, decrease in parvalbumin positive interneurons, and increase in dendritic spine density in the basal dendrites of layer 2/3 excitatory neurons. Monocular deprivation induces atypical ocular dominance plasticity in GLT1 HET mice, with an unusual depression of ipsilateral open eye responses; however, this change in ipsilateral responses correlates well with an upregulation of GLT1 protein following monocular deprivation. These results demonstrate that a key function of astrocytic GLT1 function during development is the experience-dependent refinement of ipsilateral eye inputs relative to contralateral eye inputs in visual cortex.},
  file = {Sipe et al. - 2020 - Astrocytic glutamate uptake coordinates experience 2.pdf},
  language = {en},
  type = {Preprint}
}

@book{Sipser2006,
  title = {Introduction to the Theory of Computation},
  author = {Sipser, Michael},
  year = {2006},
  edition = {2nd ed},
  publisher = {{Thomson Course Technology}},
  address = {{Boston}},
  file = {Sipser - 2006 - Introduction to the theory of computation.pdf},
  isbn = {978-0-534-95097-2},
  keywords = {Computational complexity,Machine theory},
  language = {en},
  lccn = {QA267 .S56 2006}
}

@article{Skarda1987,
  title = {How Brains Make Chaos in Order to Make Sense of the World},
  author = {Skarda, Christine A. and Freeman, Walter J.},
  year = {1987},
  month = jun,
  volume = {10},
  pages = {161},
  issn = {0140-525X, 1469-1825},
  doi = {10.1017/S0140525X00047336},
  abstract = {Recent "connectionist" models provide a new explanatory alternative to the digital computer as a model for brain function. Evidence from our EEG research on the olfactory bulb suggests that the brain may indeed use computational mechanisms like those found in connectionist models. In the present paper we discuss our data and develop a model to describe the neural dynamics responsible for odor recognition and discrimination. The results indicate the existence of sensory- and motor-specific information in the spatial dimension of EEG activity and call for new physiological metaphors and techniques of analysis. Special emphasis is placed in our model on chaotic neural activity. We hypothesize that chaotic behavior serves as the essential ground state for the neural perceptual apparatus, and we propose a mechanism for acquiring new forms of patterned activity corresponding to new learned odors. Finally, some of the implications of our neural model for behavioral theories are briefly discussed. Our research, in concert with the connectionist work, encourages a reevaluation of explanatory models that are based only on the digital computer metaphor.},
  file = {1987 - Skarda, Freeman - How brains make chaos in order to make sense of the world.pdf;Skarda and Freeman - 1987 - How brains make chaos in order to make sense of th.pdf},
  journal = {Behavioral and Brain Sciences},
  language = {en},
  number = {02}
}

@article{Slezak2019,
  title = {Distinct {{Mechanisms}} for {{Visual}} and {{Motor}}-{{Related Astrocyte Responses}} in {{Mouse Visual Cortex}}},
  author = {Slezak, Michal and Kandler, Steffen and Van Veldhoven, Paul P. and {Van den Haute}, Chris and Bonin, Vincent and Holt, Matthew G.},
  year = {2019},
  month = sep,
  pages = {S0960982219309583},
  issn = {09609822},
  doi = {10.1016/j.cub.2019.07.078},
  file = {Slezak et al. - 2019 - Distinct Mechanisms for Visual and Motor-Related A.pdf},
  journal = {Current Biology},
  language = {en}
}

@article{Smith2011,
  title = {The Confounding Effect of Response Amplitude on {{MVPA}} Performance Measures},
  author = {Smith, A.T. and Kosillo, P. and Williams, A.L.},
  year = {2011},
  month = may,
  volume = {56},
  pages = {525--530},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2010.05.079},
  abstract = {Multi-voxel pattern analysis (MVPA) is proving very powerful in the analysis of fMRI timeseries data, yielding surprising sensitivity, in many different contexts, to the response characteristics of neurons in a given brain region. However, MVPA yields a metric (classification performance) that does not readily lend itself to quantitative comparisons across experimental conditions, brain regions or people. This is because performance is influenced by a number of factors other than the sensitivity of neurons to the experimental manipulation. One such factor that varies widely but has been largely ignored in MVPA studies is the amplitude of the response being decoded. In a noisy system, it is expected that measured classification performance will decline with declining response amplitude, even if the underlying neuronal specificity is constant. We document the relationship between response amplitude and classification performance in the context of orientation decoding in the visual cortex. Flickering sine gratings were presented at each of two orthogonal orientations in a block design (multivariate experiment) or an event-related design (univariate experiment). Response amplitude was manipulated by varying stimulus contrast. Orientation classification performance in retinotopically defined occipital area V1 increased approximately linearly with the logarithm of stimulus contrast. As expected, univariate response amplitude also increased with contrast. Similar results were obtained in V2, V3 and V3A. Plotting classification performance against response amplitude gave a function with a compressive non-linearity that was well fit by a power function. Knowledge of this function potentially allows adjustment of classification performance to take account of the effect of response size, making comparisons across brain areas, categories or people more meaningful.},
  file = {2011 - Smith, Kosillo, Williams - The confounding effect of response amplitude on MVPA performance measures.pdf},
  journal = {NeuroImage},
  language = {en},
  number = {2}
}

@article{Smith2018,
  title = {A Disciplined Approach to Neural Network Hyper-Parameters: {{Part}} 1 -- Learning Rate, Batch Size, Momentum, and Weight Decay},
  shorttitle = {A Disciplined Approach to Neural Network Hyper-Parameters},
  author = {Smith, Leslie N.},
  year = {2018},
  month = apr,
  abstract = {Although deep learning has produced dazzling successes for applications of image, speech, and video processing in the past few years, most trainings are with suboptimal hyper-parameters, requiring unnecessarily long training times. Setting the hyper-parameters remains a black art that requires years of experience to acquire. This report proposes several efficient ways to set the hyper-parameters that significantly reduce training time and improves performance. Specifically, this report shows how to examine the training validation/test loss function for subtle clues of underfitting and overfitting and suggests guidelines for moving toward the optimal balance point. Then it discusses how to increase/decrease the learning rate/momentum to speed up training. Our experiments show that it is crucial to balance every manner of regularization for each dataset and architecture. Weight decay is used as a sample regularizer to show how its optimal value is tightly coupled with the learning rates and momentum. Files to help replicate the results reported here are available at https://github.com/lnsmith54/hyperParam1.},
  archiveprefix = {arXiv},
  eprint = {1803.09820},
  eprinttype = {arxiv},
  file = {Smith - 2018 - A disciplined approach to neural network hyper-par.pdf},
  journal = {arXiv:1803.09820 [cs, stat]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  language = {en},
  primaryclass = {cs, stat}
}

@article{Smith2018a,
  title = {A Disciplined Approach to Neural Network Hyper-Parameters: {{Part}} 1 -- Learning Rate, Batch Size, Momentum, and Weight Decay},
  shorttitle = {A Disciplined Approach to Neural Network Hyper-Parameters},
  author = {Smith, Leslie N.},
  year = {2018},
  month = apr,
  abstract = {Although deep learning has produced dazzling successes for applications of image, speech, and video processing in the past few years, most trainings are with suboptimal hyper-parameters, requiring unnecessarily long training times. Setting the hyper-parameters remains a black art that requires years of experience to acquire. This report proposes several efficient ways to set the hyper-parameters that significantly reduce training time and improves performance. Specifically, this report shows how to examine the training validation/test loss function for subtle clues of underfitting and overfitting and suggests guidelines for moving toward the optimal balance point. Then it discusses how to increase/decrease the learning rate/momentum to speed up training. Our experiments show that it is crucial to balance every manner of regularization for each dataset and architecture. Weight decay is used as a sample regularizer to show how its optimal value is tightly coupled with the learning rates and momentum. Files to help replicate the results reported here are available at https://github.com/lnsmith54/hyperParam1.},
  archiveprefix = {arXiv},
  eprint = {1803.09820},
  eprinttype = {arxiv},
  file = {Smith - 2018 - A disciplined approach to neural network hyper-par 2.pdf},
  journal = {arXiv:1803.09820 [cs, stat]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  language = {en},
  primaryclass = {cs, stat}
}

@article{Smith2021,
  title = {Building {{One}}-{{Shot Semi}}-Supervised ({{BOSS}}) {{Learning}} up to {{Fully Supervised Performance}}},
  author = {Smith, Leslie N. and Conovaloff, Adam},
  year = {2021},
  month = jan,
  abstract = {Reaching the performance of fully supervised learning with unlabeled data and only labeling one sample per class might be ideal for deep learning applications. We demonstrate for the first time the potential for building one-shot semi-supervised (BOSS) learning on Cifar-10 and SVHN up to attain test accuracies that are comparable to fully supervised learning. Our method combines class prototype refining, class balancing, and self-training. A good prototype choice is essential and we propose a technique for obtaining iconic examples. In addition, we demonstrate that class balancing methods substantially improve accuracy results in semi-supervised learning to levels that allow self-training to reach the level of fully supervised learning performance. Rigorous empirical evaluations provide evidence that labeling large datasets is not necessary for training deep neural networks. We made our code available at https://github.com/lnsmith54/BOSS to facilitate replication and for use with future real-world applications.},
  archiveprefix = {arXiv},
  eprint = {2006.09363},
  eprinttype = {arxiv},
  file = {Smith and Conovaloff - 2021 - Building One-Shot Semi-supervised (BOSS) Learning .pdf},
  journal = {arXiv:2006.09363 [cs, eess, stat]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Electrical Engineering and Systems Science - Image and Video Processing,Statistics - Machine Learning},
  language = {en},
  primaryclass = {cs, eess, stat}
}

@article{Smith2021a,
  title = {If {{I}} Fits {{I}} Sits: {{A}} Citizen Science Investigation into Illusory Contour Susceptibility in Domestic Cats ({{Felis}} Silvestris Catus)},
  shorttitle = {If {{I}} Fits {{I}} Sits},
  author = {Smith, Gabriella E. and Chouinard, Philippe A. and Byosiere, Sarah-Elizabeth},
  year = {2021},
  month = jul,
  volume = {240},
  pages = {105338},
  issn = {01681591},
  doi = {10.1016/j.applanim.2021.105338},
  abstract = {A well-known phenomenon to cat owners is the tendency of their cats to sit in enclosed spaces such as boxes, laundry baskets, and even shape outlines taped on the floor. This investigative study asks whether domestic cats (Felis silvestris catus) are also susceptible to sitting in enclosures that are illusory in nature, utilizing cats' attraction to box-like spaces to assess their perception of the Kanizsa square visual illusion. Carried out during the COVID-19 pandemic, this study randomly assigned citizen science participants Booklets of six randomized, counterbalanced daily stimuli to print out, prepare, and place on the floor in pairs. Owners observed and vid\- eorecorded their cats' behavior with the stimuli and reported findings from home over the course of the six daily trials. This study ultimately reached over 500 pet cats and cat owners, and of those, 30 completed all of the study's trials. Of these, nine cat subjects selected at least one stimulus by sitting within the contours (illusory or otherwise) with all limbs for at least three seconds. This study revealed that cats selected the Kanizsa illusion just as often as the square and more often than the control, indicating that domestic cats may treat the subjective Kanizsa contours as they do real contours. Given the drawbacks of citizen science projects such as participant attrition, future research would benefit from replicating this study in controlled settings. To the best of our knowledge, this investigation is the first of its kind in three regards: a citizen science study of cat cognition; a formal examination into cats' attraction to 2D rather than 3D enclosures; and study into cats' susceptibility to illusory contours in an ecologically relevant paradigm. This study demonstrates the potential of more ecologi\- cally valid study of pet cats, and more broadly provides an interesting new perspective into cat visual perception research.},
  file = {Smith et al. - 2021 - If I fits I sits A citizen science investigation .pdf},
  journal = {Applied Animal Behaviour Science},
  language = {en}
}

@article{Smith2021b,
  title = {If {{I}} Fits {{I}} Sits: {{A}} Citizen Science Investigation into Illusory Contour Susceptibility in Domestic Cats ({{Felis}} Silvestris Catus)},
  shorttitle = {If {{I}} Fits {{I}} Sits},
  author = {Smith, Gabriella E. and Chouinard, Philippe A. and Byosiere, Sarah-Elizabeth},
  year = {2021},
  month = jul,
  volume = {240},
  pages = {105338},
  issn = {01681591},
  doi = {10.1016/j.applanim.2021.105338},
  abstract = {A well-known phenomenon to cat owners is the tendency of their cats to sit in enclosed spaces such as boxes, laundry baskets, and even shape outlines taped on the floor. This investigative study asks whether domestic cats (Felis silvestris catus) are also susceptible to sitting in enclosures that are illusory in nature, utilizing cats' attraction to box-like spaces to assess their perception of the Kanizsa square visual illusion. Carried out during the COVID-19 pandemic, this study randomly assigned citizen science participants Booklets of six randomized, counterbalanced daily stimuli to print out, prepare, and place on the floor in pairs. Owners observed and vid\- eorecorded their cats' behavior with the stimuli and reported findings from home over the course of the six daily trials. This study ultimately reached over 500 pet cats and cat owners, and of those, 30 completed all of the study's trials. Of these, nine cat subjects selected at least one stimulus by sitting within the contours (illusory or otherwise) with all limbs for at least three seconds. This study revealed that cats selected the Kanizsa illusion just as often as the square and more often than the control, indicating that domestic cats may treat the subjective Kanizsa contours as they do real contours. Given the drawbacks of citizen science projects such as participant attrition, future research would benefit from replicating this study in controlled settings. To the best of our knowledge, this investigation is the first of its kind in three regards: a citizen science study of cat cognition; a formal examination into cats' attraction to 2D rather than 3D enclosures; and study into cats' susceptibility to illusory contours in an ecologically relevant paradigm. This study demonstrates the potential of more ecologi\- cally valid study of pet cats, and more broadly provides an interesting new perspective into cat visual perception research.},
  file = {Smith et al. - 2021 - If I fits I sits A citizen science investigation .pdf},
  journal = {Applied Animal Behaviour Science},
  language = {en}
}

@article{Snell2017,
  title = {Prototypical {{Networks}} for {{Few}}-Shot {{Learning}}},
  author = {Snell, Jake and Swersky, Kevin and Zemel, Richard S.},
  year = {2017},
  month = jun,
  abstract = {We propose prototypical networks for the problem of few-shot classification, where a classifier must generalize to new classes not seen in the training set, given only a small number of examples of each new class. Prototypical networks learn a metric space in which classification can be performed by computing distances to prototype representations of each class. Compared to recent approaches for few-shot learning, they reflect a simpler inductive bias that is beneficial in this limited-data regime, and achieve excellent results. We provide an analysis showing that some simple design decisions can yield substantial improvements over recent approaches involving complicated architectural choices and meta-learning. We further extend prototypical networks to zero-shot learning and achieve state-of-theart results on the CU-Birds dataset.},
  archiveprefix = {arXiv},
  eprint = {1703.05175},
  eprinttype = {arxiv},
  file = {Snell et al. - 2017 - Prototypical Networks for Few-shot Learning.pdf},
  journal = {arXiv:1703.05175 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryclass = {cs, stat}
}

@article{Snyder2015,
  title = {Global Network Influences on Local Functional Connectivity},
  author = {Snyder, Adam C and Morais, Michael J and Willis, Cory M and Smith, Matthew A},
  year = {2015},
  month = may,
  volume = {18},
  pages = {736--743},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.3979},
  file = {2015 - Snyder et al. - Global network influences on local functional connectivity.pdf;Snyder et al. - 2015 - Global network influences on local functional conn.pdf},
  journal = {Nature Neuroscience},
  language = {en},
  number = {5}
}

@article{So2012,
  title = {Relative Contributions of Local Cell and Passing Fiber Activation and Silencing to Changes in Thalamic Fidelity during Deep Brain Stimulation and Lesioning: A Computational Modeling Study},
  shorttitle = {Relative Contributions of Local Cell and Passing Fiber Activation and Silencing to Changes in Thalamic Fidelity during Deep Brain Stimulation and Lesioning},
  author = {So, Rosa Q. and Kent, Alexander R. and Grill, Warren M.},
  year = {2012},
  month = jun,
  volume = {32},
  pages = {499--519},
  issn = {0929-5313, 1573-6873},
  doi = {10.1007/s10827-011-0366-4},
  abstract = {Deep brain stimulation (DBS) and lesioning are two surgical techniques used in the treatment of advanced Parkinson's disease (PD) in patients whose symptoms are not well controlled by drugs, or who experience dyskinesias as a side effect of medications. Although these treatments have been widely practiced, the mechanisms behind DBS and lesioning are still not well understood. The subthalamic nucleus (STN) and globus pallidus pars interna (GPi) are two common targets for both DBS and lesioning. Previous studies have indicated that DBS not only affects local cells within the target, but also passing axons within neighboring regions. Using a computational model of the basal ganglia-thalamic network, we studied the relative contributions of activation and silencing of local cells (LCs) and fibers of passage (FOPs) to changes in the accuracy of information transmission through the thalamus (thalamic fidelity), which is correlated with the effectiveness of DBS. Activation of both LCs and FOPs during STN and GPi-DBS were beneficial to the outcome of stimulation. During STN and GPi lesioning, effects of silencing LCs and FOPs were different between the two types of lesioning. For STN lesioning, silencing GPi FOPs mainly contributed to its effectiveness, while silencing only STN LCs did not improve thalamic fidelity. In contrast, silencing both GPi LCs and GPe FOPs during GPi lesioning contributed to improvements in thalamic fidelity. Thus, two distinct mechanisms produced comparable improvements in thalamic function: driving the output of the basal ganglia to produce tonic inhibition and silencing the output of the basal ganglia to produce tonic disinhibition. These results show the importance of considering effects of activating or silencing fibers passing close to the nucleus when deciding upon a target location for DBS or lesioning.},
  file = {2013 - Rosa Q. So, Alexander R. Kent - Relative contributions of local cell and passing fiber activation and silencing to changes in tha.pdf},
  journal = {Journal of Computational Neuroscience},
  language = {en},
  number = {3}
}

@article{Sohal2016,
  title = {How {{Close Are We}} to {{Understanding What}} (If {{Anything}}) {{Oscillations Do}} in {{Cortical Circuits}}?},
  author = {Sohal, V. S.},
  year = {2016},
  month = oct,
  volume = {36},
  pages = {10489--10495},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.0990-16.2016},
  file = {Sohal - 2016 - How Close Are We to Understanding What (if Anythin.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {41}
}

@article{Sohn2019,
  title = {Bayesian {{Computation}} through {{Cortical Latent Dynamics}}},
  author = {Sohn, Hansem and Narain, Devika and Meirhaeghe, Nicolas and Jazayeri, Mehrdad},
  year = {2019},
  month = sep,
  volume = {103},
  pages = {934-947.e5},
  issn = {08966273},
  doi = {10.1016/j.neuron.2019.06.012},
  abstract = {Statistical regularities in the environment create prior beliefs that we rely on to optimize our behavior when sensory information is uncertain. Bayesian theory formalizes how prior beliefs can be leveraged and has had a major impact on models of perception, sensorimotor function, and cognition. However, it is not known how recurrent interactions among neurons mediate Bayesian integration. By using a timeinterval reproduction task in monkeys, we found that prior statistics warp neural representations in the frontal cortex, allowing the mapping of sensory inputs to motor outputs to incorporate prior statistics in accordance with Bayesian inference. Analysis of recurrent neural network models performing the task revealed that this warping was enabled by a low-dimensional curved manifold and allowed us to further probe the potential causal underpinnings of this computational strategy. These results uncover a simple and general principle whereby prior beliefs exert their influence on behavior by sculpting cortical latent dynamics.},
  file = {Sohn et al. - 2019 - Bayesian Computation through Cortical Latent Dynam.pdf},
  journal = {Neuron},
  language = {en},
  number = {5}
}

@article{Sokal1962,
  title = {The {{Comparison}} of {{Dendrograms}} by {{Objective Methods}}},
  author = {Sokal, Robert R. and Rohlf, F. James},
  year = {1962},
  month = feb,
  volume = {11},
  pages = {33},
  issn = {00400262},
  doi = {10.2307/1217208},
  file = {2012 - Taxonomy - No Title.pdf},
  journal = {Taxon},
  language = {en},
  number = {2}
}

@article{Solari2011,
  title = {Cognitive Consilience: {{Primate}} Non-Primary Neuroanatomical Circuits Underlying Cognition},
  shorttitle = {Cognitive Consilience},
  author = {{Solari}},
  year = {2011},
  issn = {16625129},
  doi = {10.3389/fnana.2011.00065},
  abstract = {Interactions between the cerebral cortex, thalamus, and basal ganglia form the basis of cognitive information processing in the mammalian brain. Understanding the principles of neuroanatomical organization in these structures is critical to understanding the functions they perform and ultimately how the human brain works. We have manually distilled and synthesized hundreds of primate neuroanatomy facts into a single interactive visualization. The resulting picture represents the fundamental neuroanatomical blueprint upon which cognitive functions must be implemented. Within this framework we hypothesize and detail 7 functional circuits corresponding to psychological perspectives on the brain: consolidated long-term declarative memory, short-term declarative memory, working memory/information processing, behavioral memory selection, behavioral memory output, cognitive control, and cortical information flow regulation. Each circuit is described in terms of distinguishable neuronal groups including the cerebral isocortex (9 pyramidal neuronal groups), parahippocampal gyrus and hippocampus, thalamus (4 neuronal groups), basal ganglia (7 neuronal groups), metencephalon, basal forebrain, and other subcortical nuclei. We focus on neuroanatomy related to primate nonprimary cortical systems to elucidate the basis underlying the distinct homotypical cognitive architecture. To display the breadth of this review, we introduce a novel method of integrating and presenting data in multiple independent visualizations: an interactive website (http://www.frontiersin.org/files/cognitiveconsilience/index.html) and standalone iPhone and iPad applications. With these tools we present a unique, annotated view of neuroanatomical consilience (integration of knowledge).},
  file = {2011 - Solari, Stoner - Cognitive consilience primate non-primary neuroanatomical circuits underlying cognition.pdf},
  journal = {Frontiers in Neuroanatomy},
  language = {en}
}

@article{Soltoggio2013,
  title = {Rare {{Neural Correlations Implement Robotic Conditioning}} with {{Delayed Rewards}} and {{Disturbances}}},
  author = {Soltoggio, Andrea and Lemme, Andre and Reinhart, Felix and Steil, Jochen J.},
  year = {2013},
  volume = {7},
  issn = {1662-5218},
  doi = {10.3389/fnbot.2013.00006},
  abstract = {Neural conditioning associates cues and actions with following rewards. The environments in which robots operate, however, are pervaded by a variety of disturbing stimuli and uncertain timing. In particular, variable reward delays make it difficult to reconstruct which previous actions are responsible for following rewards. Such an uncertainty is handled by biological neural networks, but represents a challenge for computational models, suggesting the lack of a satisfactory theory for robotic neural conditioning. The present study demonstrates the use of rare neural correlations in making correct associations between rewards and previous cues or actions. Rare correlations are functional in selecting sparse synapses to be eligible for later weight updates if a reward occurs. The repetition of this process singles out the associating and reward-triggering pathways, and thereby copes with distal rewards. The neural network displays macro-level classical and operant conditioning, which is demonstrated in an interactive real-life human-robot interaction. The proposed mechanism models realistic conditioning in humans and animals and implements similar behaviors in neuro-robotic platforms.},
  file = {2013 - Soltoggio et al. - Rare neural correlations implement robotic conditioning with delayed rewards and disturbances.pdf;Soltoggio et al. - 2013 - Rare Neural Correlations Implement Robotic Conditi.pdf},
  journal = {Frontiers in Neurorobotics},
  language = {en}
}

@article{Soltoggio2013a,
  title = {Solving the {{Distal Reward Problem}} with {{Rare Correlations}}},
  author = {Soltoggio, Andrea and Steil, Jochen J.},
  year = {2013},
  month = apr,
  volume = {25},
  pages = {940--978},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/NECO_a_00419},
  file = {2013 - Soltoggio, Steil - Solving the distal reward problem with rare correlations.pdf;Soltoggio and Steil - 2013 - Solving the Distal Reward Problem with Rare Correl.pdf},
  journal = {Neural Computation},
  language = {en},
  number = {4}
}

@article{Soman2018,
  title = {An {{Oscillatory Neural Autoencoder Based}} on {{Frequency Modulation}} and {{Multiplexing}}},
  author = {Soman, Karthik and Muralidharan, Vignesh and Chakravarthy, V. Srinivasa},
  year = {2018},
  month = jul,
  volume = {12},
  pages = {52},
  issn = {1662-5188},
  doi = {10.3389/fncom.2018.00052},
  abstract = {Oscillatory phenomena are ubiquitous in the brain. Although there are oscillator-based models of brain dynamics, their universal computational properties have not been explored much unlike in the case of rate-coded and spiking neuron network models. Use of oscillator-based models is often limited to special phenomena like locomotor rhythms and oscillatory attractor-based memories. If neuronal ensembles are taken to be the basic functional units of brain dynamics, it is desirable to develop oscillator-based models that can explain a wide variety of neural phenomena. Autoencoders are a special type of feed forward networks that have been used for construction of large-scale deep networks. Although autoencoders based on rate-coded and spiking neuron networks have been proposed, there are no autoencoders based on oscillators. We propose here an oscillatory neural network model that performs the function of an autoencoder. The model is a hybrid of rate-coded neurons and neural oscillators. Input signals modulate the frequency of the neural encoder oscillators. These signals are then multiplexed using a network of rate-code neurons that has afferent Hebbian and lateral anti-Hebbian connectivity, termed as Lateral Anti Hebbian Network (LAHN). Finally the LAHN output is de-multiplexed using an output neural layer which is a combination of adaptive Hopf and Kuramoto oscillators for the signal reconstruction. The Kuramoto-Hopf combination performing demodulation is a novel way of describing a neural phase-locked loop. The proposed model is tested using both synthetic signals and real world EEG signals. The proposed model arises out of the general motivation to construct biologically inspired, oscillatory versions of some of the standard neural network models, and presents itself as an autoencoder network based on oscillatory neurons applicable to time series signals. As a demonstration, the model is applied to compression of EEG signals.},
  file = {Soman et al. - 2018 - An Oscillatory Neural Autoencoder Based on Frequen.pdf},
  journal = {Front. Comput. Neurosci.},
  language = {en}
}

@article{Sompolinsky1988,
  title = {Chaos in {{Random Neural Networks}}},
  author = {Sompolinsky, H. and Crisanti, A. and Sommers, H. J.},
  year = {1988},
  month = jul,
  volume = {61},
  pages = {259--262},
  issn = {0031-9007},
  doi = {10.1103/PhysRevLett.61.259},
  file = {1988 - Sompolinsky, Crisanti, Sommers - Chaos in random neural networks.pdf;Sompolinsky et al. - 1988 - Chaos in Random Neural Networks.pdf},
  journal = {Physical Review Letters},
  language = {en},
  number = {3}
}

@article{Song,
  title = {Reward-Based Training of Recurrent Neural Networks for Cognitive and Value-Based Tasks},
  author = {Song, H Francis and Yang, Guangyu R and Wang, Xiao-Jing},
  pages = {59},
  abstract = {Trained neural network models, which exhibit many features observed in neural recordings from behaving animals and whose activity and connectivity can be fully analyzed, may provide insights into neural mechanisms. In contrast to commonly used methods for supervised learning from graded error signals, however, animals learn from reward feedback on definite actions through reinforcement learning. Reward maximization is particularly relevant when the optimal behavior depends on an animal's internal judgment of confidence or subjective preferences. Here, we describe reward-based training of recurrent neural networks in which a value network guides learning by using the selected actions and activity of the policy network to predict future reward. We show that such models capture both behavioral and electrophysiological findings from well-known experimental paradigms. Our results provide a unified framework for investigating diverse cognitive and value-based computations, including a role for value representation that is essential for learning, but not executing, a task.},
  file = {Song et al. - Reward-based training of recurrent neural networks.pdf},
  language = {en}
}

@article{Song2000,
  title = {Competitive {{Hebbian}} Learning through Spike-Timing-Dependent Synaptic Plasticity},
  author = {Song, Sen and Miller, Kenneth D. and Abbott, L. F.},
  year = {2000},
  month = sep,
  volume = {3},
  pages = {919--926},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/78829},
  file = {2000 - Song, Miller, Abbott - Competitive Hebbian Learning through Spike-Time Dependent Synaptic Plasticity.pdf;2000 - Song, Miller, Abbott - Competitive Hebbian learning through spike-timing-dependent synaptic plasticity.pdf},
  journal = {Nature Neuroscience},
  language = {en},
  number = {9}
}

@article{Song2005,
  title = {Highly {{Nonrandom Features}} of {{Synaptic Connectivity}} in {{Local Cortical Circuits}}},
  author = {Song, Sen and Sj{\"o}str{\"o}m, Per Jesper and Reigl, Markus and Nelson, Sacha and Chklovskii, Dmitri B},
  editor = {Friston, Karl J.},
  year = {2005},
  month = mar,
  volume = {3},
  pages = {e68},
  issn = {1545-7885},
  doi = {10.1371/journal.pbio.0030068},
  file = {2005 - Song et al. - Highly nonrandom features of synaptic connectivity in local cortical circuits.pdf},
  journal = {PLoS Biology},
  language = {en},
  number = {3}
}

@article{Song2005a,
  title = {Highly {{Nonrandom Features}} of {{Synaptic Connectivity}} in {{Local Cortical Circuits}}},
  author = {Song, Sen and Sj{\"o}str{\"o}m, Per Jesper and Reigl, Markus and Nelson, Sacha and Chklovskii, Dmitri B},
  editor = {Friston, Karl J.},
  year = {2005},
  month = mar,
  volume = {3},
  pages = {e68},
  issn = {1545-7885},
  doi = {10.1371/journal.pbio.0030068},
  file = {Song et al. - 2005 - Highly Nonrandom Features of Synaptic Connectivity.PDF},
  journal = {PLoS Biology},
  language = {en},
  number = {3}
}

@article{Song2016,
  title = {Training {{Excitatory}}-{{Inhibitory Recurrent Neural Networks}} for {{Cognitive Tasks}}: {{A Simple}} and {{Flexible Framework}}},
  shorttitle = {Training {{Excitatory}}-{{Inhibitory Recurrent Neural Networks}} for {{Cognitive Tasks}}},
  author = {Song, H. Francis and Yang, Guangyu R. and Wang, Xiao-Jing},
  editor = {Sporns, Olaf},
  year = {2016},
  month = feb,
  volume = {12},
  pages = {e1004792},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1004792},
  file = {Song et al. - 2016 - Training Excitatory-Inhibitory Recurrent Neural Ne.pdf},
  journal = {PLOS Computational Biology},
  language = {en},
  number = {2}
}

@article{Song2019,
  title = {Sources of Suboptimality in a Minimalistic Explore\textendash Exploit Task},
  author = {Song, Mingyu and Bnaya, Zahy and Ma, Wei Ji},
  year = {2019},
  month = apr,
  volume = {3},
  pages = {361--368},
  issn = {2397-3374},
  doi = {10.1038/s41562-018-0526-x},
  file = {Song et al. - 2019 - Sources of suboptimality in a minimalistic explore.pdf},
  journal = {Nature Human Behaviour},
  language = {en},
  number = {4}
}

@article{Song2019a,
  title = {Sources of Suboptimality in a Minimalistic Explore\textendash Exploit Task},
  author = {Song, Mingyu and Bnaya, Zahy and Ma, Wei Ji},
  year = {2019},
  month = apr,
  volume = {3},
  pages = {361--368},
  issn = {2397-3374},
  doi = {10.1038/s41562-018-0526-x},
  file = {Song et al. - 2019 - Sources of suboptimality in a minimalistic explore.pdf},
  journal = {Nature Human Behaviour},
  language = {en},
  number = {4}
}

@article{Sontakke2020,
  title = {Causal {{Curiosity}}: {{RL Agents Discovering Self}}-Supervised {{Experiments}} for {{Causal Representation Learning}}},
  shorttitle = {Causal {{Curiosity}}},
  author = {Sontakke, Sumedh A. and Mehrjou, Arash and Itti, Laurent and Sch{\"o}lkopf, Bernhard},
  year = {2020},
  month = oct,
  abstract = {Humans show an innate ability to learn the regularities of the world through interaction. By performing experiments in our environment, we are able to discern the causal factors of variation and infer how they affect the dynamics of our world. Analogously, here we attempt to equip reinforcement learning agents with the ability to perform experiments that facilitate a categorization of the rolled-out trajectories, and to subsequently infer the causal factors of the environment in a hierarchical manner. We introduce a novel intrinsic reward, called causal curiosity, and show that it allows our agents to learn optimal sequences of actions, and to discover causal factors in the dynamics. The learned behavior allows the agent to infer a binary quantized representation for the ground-truth causal factors in every environment. Additionally, we find that these experimental behaviors are semantically meaningful (e.g., to differentiate between heavy and light blocks, our agents learn to lift them), and are learnt in a self-supervised manner with approximately 2.5 times less data than conventional supervised planners. We show that these behaviors can be re-purposed and fine-tuned (e.g., from lifting to pushing or other downstream tasks). Finally, we show that the knowledge of causal factor representations aids zero-shot learning for more complex tasks.},
  archiveprefix = {arXiv},
  eprint = {2010.03110},
  eprinttype = {arxiv},
  file = {Sontakke et al. - 2020 - Causal Curiosity RL Agents Discovering Self-super.pdf},
  journal = {arXiv:2010.03110 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  language = {en},
  primaryclass = {cs}
}

@article{Soon2008,
  title = {Unconscious Determinants of Free Decisions in the Human Brain},
  author = {Soon, Chun Siong and Brass, Marcel and Heinze, Hans-Jochen and Haynes, John-Dylan},
  year = {2008},
  month = may,
  volume = {11},
  pages = {543--545},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.2112},
  file = {2008 - Soon et al. - Unconscious determinants of free decisions in the human brain.pdf},
  journal = {Nature Neuroscience},
  language = {en},
  number = {5}
}

@article{Sorribes2011,
  title = {The {{Origin}} of {{Behavioral Bursts}} in {{Decision}}-{{Making Circuitry}}},
  author = {Sorribes, Amanda and Armendariz, Beatriz G. and {Lopez-Pigozzi}, Diego and Murga, Cristina and {de Polavieja}, Gonzalo G.},
  editor = {Sporns, Olaf},
  year = {2011},
  month = jun,
  volume = {7},
  pages = {e1002075},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1002075},
  abstract = {From ants to humans, the timing of many animal behaviors comes in bursts of activity separated by long periods of inactivity. Recently, mathematical modeling has shown that simple algorithms of priority-driven behavioral choice can result in bursty behavior. To experimentally test this link between decision-making circuitry and bursty dynamics, we have turned to Drosophila melanogaster. We have found that the statistics of intervals between activity periods in endogenous activityrest switches of wild-type Drosophila are very well described by the Weibull distribution, a common distribution of bursty dynamics in complex systems. The bursty dynamics of wild-type Drosophila walking activity are shown to be determined by this inter-event distribution alone and not by memory effects, thus resembling human dynamics. Further, using mutant flies that disrupt dopaminergic signaling or the mushroom body, circuitry implicated in decision-making, we show that the degree of behavioral burstiness can be modified. These results are thus consistent with the proposed link between decisionmaking circuitry and bursty dynamics, and highlight the importance of using simple experimental systems to test general theoretical models of behavior. The findings further suggest that analysis of bursts could prove useful for the study and evaluation of decision-making circuitry.},
  file = {Sorribes et al. - 2011 - The Origin of Behavioral Bursts in Decision-Making.pdf},
  journal = {PLoS Comput Biol},
  language = {en},
  number = {6}
}

@techreport{Sorscher2021,
  title = {The {{Geometry}} of {{Concept Learning}}},
  author = {Sorscher, Ben and Ganguli, Surya and Sompolinsky, Haim},
  year = {2021},
  month = mar,
  institution = {{Neuroscience}},
  doi = {10.1101/2021.03.21.436284},
  abstract = {Understanding the neural basis of our remarkable cognitive capacity to accurately learn novel highdimensional naturalistic concepts from just one or a few sensory experiences constitutes a fundamental problem. We propose a simple, biologically plausible, mathematically tractable, and computationally powerful neural mechanism for few-shot learning of naturalistic concepts. We posit that the concepts we can learn given few examples are defined by tightly circumscribed manifolds in the neural firing rate space of higher order sensory areas. We further posit that a single plastic downstream neuron can learn such concepts from few examples using a simple plasticity rule. We demonstrate the computational power of our simple proposal by showing it can achieve high few-shot learning accuracy on natural visual concepts using both macaque inferotemporal cortex representations and deep neural network models of these representations, and can even learn novel visual concepts specified only through language descriptions. Moreover, we develop a mathematical theory of few-shot learning that links neurophysiology to behavior by delineating several fundamental and measurable geometric properties of high-dimensional neural representations that can accurately predict the few-shot learning performance of naturalistic concepts across all our experiments. We discuss several implications of our theory for past and future studies in neuroscience, psychology and machine learning.},
  file = {Sorscher et al. - 2021 - The Geometry of Concept Learning.pdf},
  language = {en},
  type = {Preprint}
}

@article{Sotelo-Lopez2012,
  title = {Conditions under Which a Superdiffusive Random-Search Strategy Is Necessary},
  author = {{Sotelo-L{\'o}pez}, S. A. and Santos, M. C. and Raposo, E. P. and Viswanathan, G. M. and {da Luz}, M. G. E.},
  year = {2012},
  month = sep,
  volume = {86},
  pages = {031133},
  issn = {1539-3755, 1550-2376},
  doi = {10.1103/PhysRevE.86.031133},
  file = {Sotelo-López et al. - 2012 - Conditions under which a superdiffusive random-sea.pdf},
  journal = {Phys. Rev. E},
  language = {en},
  number = {3}
}

@article{Sotero2007,
  title = {Realistically {{Coupled Neural Mass Models Can Generate EEG Rhythms}}},
  author = {Sotero, Roberto C. and {Trujillo-Barreto}, Nelson J. and {Iturria-Medina}, Yasser and Carbonell, Felix and Jimenez, Juan C.},
  year = {2007},
  month = feb,
  volume = {19},
  pages = {478--512},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/neco.2007.19.2.478},
  file = {2007 - Sotero, Trujillo-barreto - Realistically Coupled Neural Mass Models Can Generate EEG Rhythms.pdf},
  journal = {Neural Computation},
  language = {en},
  number = {2}
}

@article{Sotero2015,
  title = {Laminar {{Distribution}} of {{Phase}}-{{Amplitude Coupling}} of {{Spontaneous Current Sources}} and {{Sinks}}},
  author = {Sotero, Roberto C. and Bortel, Aleksandra and Naaman, Shmuel and Mocanu, Victor M. and Kropf, Pascal and Villeneuve, Martin and Shmuel, Amir},
  year = {2015},
  month = dec,
  volume = {9},
  issn = {1662-453X},
  doi = {10.3389/fnins.2015.00454},
  abstract = {Although resting-state functional connectivity is a commonly used neuroimaging paradigm, the underlying mechanisms remain unknown. Thalamo-cortical and cortico-cortical circuits generate oscillations at different frequencies during spontaneous activity. However, it remains unclear how the various rhythms interact and whether their interactions are lamina-specific. Here we investigated intra- and inter-laminar spontaneous phase-amplitude coupling (PAC). We recorded local-field potentials using laminar probes inserted in the forelimb representation of rat area S1. We then computed time-series of frequency-band- and lamina-specific current source density (CSD), and PACs of CSD for all possible pairs of the classical frequency bands in the range of 1\textendash 150 Hz. We observed both intra- and inter-laminar spontaneous PAC. Of 18 possible combinations, 12 showed PAC, with the highest measures of interaction obtained for the pairs of the theta/gamma and delta/gamma bands. Intra- and inter-laminar PACs involving layers 2/3\textendash 5a were higher than those involving layer 6. Current sinks (sources) in the delta band were associated with increased (decreased) amplitudes of high-frequency signals in the beta to fast gamma bands throughout layers 2/3\textendash 6. Spontaneous sinks (sources) of the theta and alpha bands in layers 2/3\textendash 4 were on average linked to dipoles completed by sources (sinks) in layer 6, associated with high (low) amplitudes of the beta to fast-gamma bands in the entire cortical column. Our findings show that during spontaneous activity, delta, theta, and alpha oscillations are associated with periodic excitability, which for the theta and alpha bands is lamina-dependent. They further emphasize the differences between the function of layer 6 and that of the superficial layers, and the role of layer 6 in controlling activity in those layers. Our study links theories on the involvement of PAC in resting-state functional connectivity with previous work that revealed lamina-specific anatomical thalamo-cortico-cortical connections.},
  file = {Sotero et al. - 2015 - Laminar Distribution of Phase-Amplitude Coupling o.pdf},
  journal = {Frontiers in Neuroscience},
  language = {en}
}

@article{Sotero2016,
  title = {Topology, Cross-Frequency, and Same-Frequency Band Interactions Shape the Generation of Phase-Amplitude Coupling in a Neural Mass Model of a Cortical Column},
  author = {Sotero, Roberto},
  year = {2016},
  month = feb,
  doi = {10.1101/023291},
  abstract = {Phase-amplitude coupling (PAC), the phenomenon where the phase of a low-frequency rhythm modulates the amplitude of a higher frequency, is becoming an important neurophysiological indicator of short- and long-range information transmission in the brain. Although recent evidence suggests that PAC might play a functional role during sensorimotor, and cognitive events, the neurobiological mechanisms underlying its generation remain imprecise. Thus, a realistic but simple enough computational model of the phenomenon is needed. Here we propose a neural mass model of a cortical column, comprising fourteen neuronal populations distributed across four layers (L2/3, L4, L5 and L6). While experimental studies often focus in only one or two PAC combinations (e.g., theta-gamma or alpha-gamma) our simulations show that the cortical column can generate almost all possible couplings of phases and amplitudes, which are influenced by connectivity parameters, time constants, and external inputs. Furthermore, our simulations suggest that the effective connectivity between neuronal populations can result in the emergence of PAC combinations with frequencies different from the natural frequencies of the oscillators involved. For instance, simulations of oscillators with natural frequencies in the theta, alpha and gamma bands, were able to produce significant PAC combinations involving delta and beta bands.},
  file = {2015 - Sotero - Generation of phase-amplitude coupling of neurophysiological signals in a neural mass model of a cortical column(2).pdf},
  journal = {bioRxiv},
  language = {en}
}

@article{Spano2020,
  title = {Dreaming with Hippocampal Damage},
  author = {Span{\`o}, Goffredina and Pizzamiglio, Gloria and McCormick, Cornelia and Clark, Ian A and De Felice, Sara and Miller, Thomas D and Edgin, Jamie O and Rosenthal, Clive R and Maguire, Eleanor A},
  year = {2020},
  month = jun,
  volume = {9},
  pages = {e56211},
  issn = {2050-084X},
  doi = {10.7554/eLife.56211},
  abstract = {The hippocampus is linked with both sleep and memory, but there is debate about whether a salient aspect of sleep \textendash{} dreaming \textendash{} requires its input. To address this question, we investigated if human patients with focal bilateral hippocampal damage and amnesia engaged in dreaming. We employed a provoked awakening protocol where participants were woken up at various points throughout the night, including during non-rapid eye movement and rapid eye movement sleep, to report their thoughts in that moment. Despite being roused a similar number of times, dream frequency was reduced in the patients compared to control participants, and the few dreams they reported were less episodic-like in nature and lacked content. These results suggest that hippocampal integrity may be necessary for typical dreaming to occur, and aligns dreaming with other hippocampal-dependent processes such as episodic memory that are central to supporting our mental life.},
  file = {Spanò et al. - 2020 - Dreaming with hippocampal damage.pdf},
  journal = {eLife},
  language = {en}
}

@article{Spencer2004,
  title = {Neural Synchrony Indexes Disordered Perception and Cognition in Schizophrenia},
  author = {Spencer, K. M. and Nestor, P. G. and Perlmutter, R. and Niznikiewicz, M. A. and Klump, M. C. and Frumin, M. and Shenton, M. E. and McCarley, R. W.},
  year = {2004},
  month = dec,
  volume = {101},
  pages = {17288--17293},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.0406074101},
  file = {2004 - Spencer et al. - Neural synchrony indexes disordered perception and cognition in schizophrenia.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {49}
}

@book{Spiegler2012,
  title = {Dynamics of Biologically Informed Neurals Mass Models of the Brain},
  author = {Spiegler, Andreas and Haueisen, Jens and Kn{\"o}sche, Thomas R. and Jirsa, Viktor K.},
  year = {2012},
  publisher = {{Univ.-Verl. Ilmenau}},
  address = {{Ilmenau}},
  abstract = {Die vorliegende Arbeit stellt einen Beitrag zur Entwicklung und Analyse von Computermodellen zum Verst\"andnis von Hirnfunktionen dar. Es wird die mittlere Aktivit\"at eines Hirnareals analytisch einfach und dabei biologisch plausibel modelliert. Auf Grundlage eines Neuronalen Massenmodells (NMM) werden die Wechsel zwischen Oszillationsregimen (z.B. durch pharmakologisch, epilepsie-, schlaf- oder kontextbedingte Zustands\"anderungen) als geordnete Folge beschrieben und Resonanzph\"anomene in einem Photic-Driving-Experiment erkl\"art. Dieses NMM kann sehr komplexe Dynamiken (z.B. Chaos) innerhalb biologisch plausibler Parameterbereiche hervorbringen. Um das Verhalten abzusch\"atzen, wird das NMM als Funktion konstanter Eingangsgr\"o\ss en und charakteristischer Zeitenkonstanten vollst\"andig auf Bifurkationen untersucht und klassifiziert. Dies erm\"oglicht die Beschreibung wechselnder Regime als geordnete Folge durch spezifische Eingangstrajektorien. Es wird ein Prinzip vorgestellt, um komplexe Ph\"anomene durch Prozesse verschiedener Zeitskalen darzustellen. Da aufgrund rhythmischer Stimuli und der intrinsischen Rhythmen von Neuronenverb\"anden die Eingangsgr\"o\ss en h\"aufig periodisch sind, wird das Verhalten des NMM als Funktion der Intensit\"at und Frequenz einer periodischen Stimulation mittels der zugeh\"origen Lyapunov-Spektren und der Zeitreihen charakterisiert. Auf der Basis der gr\"o\ss ten Lyapunov-Exponenten wird das NMM mit dem Photic-Driving-Experiment \"uberein gebracht. Dieses Experiment findet routinem\"a\ss ige Anwendung in der Diagnostik verschiedener Erkrankungen wie Epilepsie, Migr\"ane, Schizophrenie und Depression. Durch die Anwendung des vorgestellten NMM wird der f\"ur die Diagnostik entscheidende Mitnahmeeffekt reproduziert und es werden Vorhersagen f\"ur eine Verbesserung der Indikation getroffen},
  annotation = {OCLC: 855873253},
  file = {2011 - Spiegler - Dynamics of biologically informed neural mass models of the brain.pdf},
  isbn = {978-3-86360-024-2},
  language = {en}
}

@article{Spiegler2013,
  title = {Systematic Approximations of Neural Fields through Networks of Neural Masses in the Virtual Brain},
  author = {Spiegler, A. and Jirsa, V.},
  year = {2013},
  month = dec,
  volume = {83},
  pages = {704--725},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2013.06.018},
  abstract = {Full brain network models comprise a large-scale connectivity (the connectome) and neural mass models as the network's nodes. Neural mass models absorb implicitly a variety of properties in their constant parameters to achieve a reduction in complexity. In situations, where the local network connectivity undergoes major changes, such as in development or epilepsy, it becomes crucial to model local connectivity explicitly. This leads naturally to a description of neural fields on folded cortical sheets with local and global connectivities. The numerical approximation of neural fields in biologically realistic situations as addressed in Virtual Brain simulations (see http://thevirtualbrain.org/app/ (version 1.0)) is challenging and requires a thorough evaluation if the Virtual Brain approach is to be adapted for systematic studies of disease and disorders. Here we analyze the sampling problem of neural fields for arbitrary dimensions and provide explicit results for one, two and three dimensions relevant to realistically folded cortical surfaces. We characterize (i) the error due to sampling of spatial distribution functions; (ii) useful sampling parameter ranges in the context of encephalographic (EEG, MEG, ECoG and functional MRI) signals; (iii) guidelines for choosing the right spatial distribution function for given anatomical and geometrical constraints.},
  file = {Spiegler and Jirsa - 2013 - Systematic approximations of neural fields through.pdf},
  journal = {NeuroImage},
  language = {en}
}

@article{Spiliotis,
  title = {Micro to {{Macro Equation}}-{{Free Bifurcation Analysis}} of {{Neuronal Random Graphs}}: {{Symmetry Breaking}} of {{Majority Rule Dynamics}}},
  author = {Spiliotis, Konstantinos and Russo, Lucia and Siettos, Constantinos I},
  pages = {6},
  file = {2003 - Spiliotis et al. - Micro to Macro Equation-Free Bifurcation Analysis of Neuronal Random Graphs Symmetry Breaking of Majority Rul.pdf},
  language = {en}
}

@article{Spiliotis2010,
  title = {{{MULTISCALE COMPUTATIONS ON NEURAL NETWORKS}}: {{FROM THE INDIVIDUAL NEURON INTERACTIONS TO THE MACROSCOPIC}}-{{LEVEL ANALYSIS}}},
  shorttitle = {{{MULTISCALE COMPUTATIONS ON NEURAL NETWORKS}}},
  author = {Spiliotis, Konstantinos G. and Siettos, Constantinos I.},
  year = {2010},
  month = jan,
  volume = {20},
  pages = {121--134},
  issn = {0218-1274, 1793-6551},
  doi = {10.1142/S0218127410025442},
  abstract = {We show how the ``Equation-Free'' approach for multi-scale computations can be exploited to systematically study the dynamics of neural interactions on a random regular connected graph under a pairwise representation perspective. Using an individual-based microscopic simulator as a black box coarse-grained timestepper and with the aid of simulated annealing we compute the coarse-grained equilibrium bifurcation diagram and analyze the stability of the stationary states sidestepping the necessity of obtaining explicit closures at the macroscopic level. We also exploit the scheme to perform a rare-events analysis by estimating an effective Fokker-Planck describing the evolving probability density function of the corresponding coarse-grained observables.},
  file = {2010 - Spiliotis, Siettos - Multiscale Computations On Neural Networks From The Individual Neuron Interactions To The Macroscopic-Level.pdf},
  journal = {International Journal of Bifurcation and Chaos},
  language = {en},
  number = {01}
}

@article{Spoerer2017,
  title = {Recurrent Convolutional Neural Networks: A Better Model of Biological Object Recognition},
  shorttitle = {Recurrent Convolutional Neural Networks},
  author = {Spoerer, Courtney J and McClure, Patrick and Kriegeskorte, Nikolaus},
  year = {2017},
  month = aug,
  doi = {10.1101/133330},
  abstract = {Feedforward neural networks provide the dominant model of how the brain performs visual object recognition. However, these networks lack the lateral and feedback connections, and the resulting recurrent neuronal dynamics, of the ventral visual pathway in the human and nonhuman primate brain. Here we investigate recurrent convolutional neural networks with bottom-up (B), lateral (L), and top-down (T) connections. Combining these types of connections yields four architectures (B, BT, BL, and BLT), which we systematically test and compare. We hypothesized that recurrent dynamics might improve recognition performance in the challenging scenario of partial occlusion. We introduce two novel occluded object recognition tasks to test the efficacy of the models, digit clutter (where multiple target digits occlude one another) and digit debris (where target digits are occluded by digit fragments). We find that recurrent neural networks outperform feedforward control models (approximately matched in parametric complexity) at recognising objects, both in the absence of occlusion and in all occlusion conditions. Recurrent networks were also found to be more robust to the inclusion of additive Gaussian noise. Recurrent neural networks are better in two respects: (1) they are more neurobiologically realistic than their feedforward counterparts; (2) they are better in terms of their ability to recognise objects, especially under challenging conditions. This work shows that computer vision can benefit from using recurrent convolutional architectures and suggests that the ubiquitous recurrent connections in biological brains are essential for task performance.},
  file = {Spoerer et al. - 2017 - Recurrent convolutional neural networks a better  2.pdf;Spoerer et al. - 2017 - Recurrent convolutional neural networks a better .pdf},
  journal = {bioRxiv},
  language = {en}
}

@article{Sprague2013,
  title = {Attention Modulates Spatial Priority Maps in the Human Occipital, Parietal and Frontal Cortices},
  author = {Sprague, Thomas C and Serences, John T},
  year = {2013},
  month = dec,
  volume = {16},
  pages = {1879--1887},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.3574},
  file = {2013 - Sprague, Serences - Attention modulates spatial priority maps in the human occipital, parietal and frontal cortices.pdf;Sprague and Serences - 2013 - Attention modulates spatial priority maps in the h.pdf},
  journal = {Nature Neuroscience},
  language = {en},
  number = {12}
}

@article{Srivastava,
  title = {Dropout: {{A Simple Way}} to {{Prevent Neural Networks}} from {{Overfitting}}},
  author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  pages = {30},
  abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different ``thinned'' networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
  file = {Srivastava et al. - Dropout A Simple Way to Prevent Neural Networks f.pdf},
  language = {en}
}

@article{Stacy2005,
  title = {Identification of Motor and Nonmotor Wearing-off in {{Parkinson}}'s Disease: {{Comparison}} of a Patient Questionnaire versus a Clinician Assessment},
  shorttitle = {Identification of Motor and Nonmotor Wearing-off in {{Parkinson}}'s Disease},
  author = {Stacy, Mark and Bowron, Annette and Guttman, Mark and Hauser, Robert and Hughes, Kim and Larsen, Jan Petter and LeWitt, Peter and Oertel, Wolfgang and Quinn, Niall and Sethi, Kapil and Stocchi, Fabrizio},
  year = {2005},
  month = jun,
  volume = {20},
  pages = {726--733},
  issn = {0885-3185, 1531-8257},
  doi = {10.1002/mds.20383},
  file = {2005 - Stacy et al. - Identification of motor and nonmotor wearing-off in Parkinson's disease Comparison of a patient questionnaire vers.pdf},
  journal = {Movement Disorders},
  language = {en},
  number = {6}
}

@article{Stamatakis2013,
  title = {A {{Unique Population}} of {{Ventral Tegmental Area Neurons Inhibits}} the {{Lateral Habenula}} to {{Promote Reward}}},
  author = {Stamatakis, Alice M. and Jennings, Joshua H. and Ung, Randall L. and Blair, Grace A. and Weinberg, Richard J. and Neve, Rachael L. and Boyce, Frederick and Mattis, Joanna and Ramakrishnan, Charu and Deisseroth, Karl and Stuber, Garret D.},
  year = {2013},
  month = nov,
  volume = {80},
  pages = {1039--1053},
  issn = {08966273},
  doi = {10.1016/j.neuron.2013.08.023},
  abstract = {Lateral habenula (LHb) neurons convey aversive and negative reward conditions through potent indirect inhibition of ventral tegmental area (VTA) dopaminergic neurons. Although VTA dopaminergic neurons reciprocally project to the LHb, the electrophysiological properties and the behavioral consequences associated with selective manipulations of this circuit are unknown. Here, we identify an inhibitory input to the LHb arising from a unique population of VTA neurons expressing dopaminergic markers. Optogenetic activation of this circuit resulted in no detectable dopamine release in LHb brain slices. Instead, stimulation produced GABA-mediated inhibitory synaptic transmission, which suppressed the firing of postsynaptic LHb neurons in brain slices and increased the spontaneous firing rate of VTA dopaminergic neurons in vivo. Furthermore, in vivo activation of this pathway produced reward-related phenotypes that were dependent on intra-LHb GABAA receptor signaling. These results suggest that noncanonical inhibitory signaling by these hybrid dopaminergic-GABAergic neurons act to suppress LHb output under rewarding conditions.},
  file = {Stamatakis et al. - 2013 - A Unique Population of Ventral Tegmental Area Neur.pdf},
  journal = {Neuron},
  language = {en},
  number = {4}
}

@article{Stanley,
  title = {Evolving {{Neural Networks}} through {{Augmenting Topologies}}},
  author = {Stanley, Kenneth O and Miikkulainen, Risto},
  volume = {10},
  pages = {30},
  abstract = {An important question in neuroevolution is how to gain an advantage from evolving neural network topologies along with weights. We present a method, NeuroEvolution of Augmenting Topologies (NEAT), which outperforms the best fixed-topology method on a challenging benchmark reinforcement learning task. We claim that the increased efficiency is due to (1) employing a principled method of crossover of different topologies, (2) protecting structural innovation using speciation, and (3) incrementally growing from minimal structure. We test this claim through a series of ablation studies that demonstrate that each component is necessary to the system as a whole and to each other. What results is significantly faster learning. NEAT is also an important contribution to GAs because it shows how it is possible for evolution to both optimize and complexify solutions simultaneously, offering the possibility of evolving increasingly complex solutions over generations, and strengthening the analogy with biological evolution.},
  file = {Stanley and Miikkulainen - Evolving Neural Networks through Augmenting Topolo.pdf},
  language = {en},
  number = {2}
}

@incollection{Stanley2004,
  title = {Evolving a {{Roving Eye}} for {{Go}}},
  booktitle = {Genetic and {{Evolutionary Computation}} \textendash{} {{GECCO}} 2004},
  author = {Stanley, Kenneth O. and Miikkulainen, Risto},
  editor = {Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard and Deb, Kalyanmoy},
  year = {2004},
  volume = {3103},
  pages = {1226--1238},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-24855-2_130},
  abstract = {Go remains a challenge for artificial intelligence. Currently, most machine learning methods tackle Go by playing on a specific fixed board size, usually smaller than the standard 19\texttimes 19 board of the complete game. Because such techniques are designed to process only one board size, the knowledge gained through experience cannot be applied on larger boards. In this paper, a roving eye neural network is evolved to solve this problem. The network has a small input field that can scan boards of any size. Experiments demonstrate that (1) The same roving eye architecture can play on different board sizes, and (2) experience gained by playing on a small board provides an advantage for further learning on a larger board. These results suggest a potentially powerful new methodology for computer Go: It may be possible to scale up by learning on incrementally larger boards, each time building on knowledge acquired on the prior board.},
  file = {Stanley and Miikkulainen - 2004 - Evolving a Roving Eye for Go.pdf},
  isbn = {978-3-540-22343-6 978-3-540-24855-2},
  language = {en}
}

@incollection{Stanley2004a,
  title = {Evolving a {{Roving Eye}} for {{Go}}},
  booktitle = {Genetic and {{Evolutionary Computation}} \textendash{} {{GECCO}} 2004},
  author = {Stanley, Kenneth O. and Miikkulainen, Risto},
  editor = {Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard and Deb, Kalyanmoy},
  year = {2004},
  volume = {3103},
  pages = {1226--1238},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-24855-2_130},
  abstract = {Go remains a challenge for artificial intelligence. Currently, most machine learning methods tackle Go by playing on a specific fixed board size, usually smaller than the standard 19\texttimes 19 board of the complete game. Because such techniques are designed to process only one board size, the knowledge gained through experience cannot be applied on larger boards. In this paper, a roving eye neural network is evolved to solve this problem. The network has a small input field that can scan boards of any size. Experiments demonstrate that (1) The same roving eye architecture can play on different board sizes, and (2) experience gained by playing on a small board provides an advantage for further learning on a larger board. These results suggest a potentially powerful new methodology for computer Go: It may be possible to scale up by learning on incrementally larger boards, each time building on knowledge acquired on the prior board.},
  file = {Stanley and Miikkulainen - 2004 - Evolving a Roving Eye for Go 2.pdf},
  isbn = {978-3-540-22343-6 978-3-540-24855-2},
  language = {en}
}

@article{Stanton2018,
  title = {Deep {{Curiosity Search}}: {{Intra}}-{{Life Exploration Can Improve Performance}} on {{Challenging Deep Reinforcement Learning Problems}}},
  shorttitle = {Deep {{Curiosity Search}}},
  author = {Stanton, Christopher and Clune, Jeff},
  year = {2018},
  month = nov,
  abstract = {Traditional exploration methods in reinforcement learning (RL) require agents to perform random actions to find rewards. But these approaches struggle on sparse-reward domains like Montezuma's Revenge where the probability that any random action sequence leads to reward is extremely low. Recent algorithms have performed well on such tasks by encouraging agents to visit new states or perform new actions in relation to all prior training episodes (which we call acrosstraining novelty). But such algorithms do not consider whether an agent exhibits intra-life novelty: doing something new within the current episode, regardless of whether those behaviors have been performed in previous episodes. We hypothesize that across-training novelty might discourage agents from revisiting initially non-rewarding states that could become important stepping stones later in training\textemdash a problem remedied by encouraging intra-life novelty. We introduce Curiosity Search for deep reinforcement learning, or Deep Curiosity Search (DeepCS), which encourages intra-life exploration by rewarding agents for visiting as many different states as possible within each episode, and show that DeepCS matches the performance of current state-of-the-art methods on Montezuma's Revenge. We further show that DeepCS improves exploration on Amidar, Freeway, Gravitar, and Tutankham (many of which are hard exploration games). Surprisingly, DeepCS also doubles A2C performance on Seaquest, a game we would not have expected to benefit from intra-life exploration because the arena is small and already easily navigated by naive exploration techniques. In one run, DeepCS achieves a maximum training score of 80,000 points on Seaquest\textemdash higher than any methods other than Ape-X. The strong performance of DeepCS on these sparse- and dense-reward tasks suggests that encouraging intra-life novelty is an interesting, new approach for improving performance in Deep RL and motivates further research into hybridizing across-training and intra-life exploration methods.},
  archiveprefix = {arXiv},
  eprint = {1806.00553},
  eprinttype = {arxiv},
  file = {Stanton and Clune - 2018 - Deep Curiosity Search Intra-Life Exploration Can .pdf},
  journal = {arXiv:1806.00553 [cs]},
  keywords = {Computer Science - Artificial Intelligence},
  language = {en},
  primaryclass = {cs}
}

@article{Starkweather2017,
  title = {Dopamine Reward Prediction Errors Reflect Hidden-State Inference across Time},
  author = {Starkweather, Clara Kwon and Babayan, Benedicte M and Uchida, Naoshige and Gershman, Samuel J},
  year = {2017},
  month = apr,
  volume = {20},
  pages = {581--589},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.4520},
  file = {Starkweather et al. - 2017 - Dopamine reward prediction errors reflect hidden-s.pdf},
  journal = {Nature Neuroscience},
  language = {en},
  number = {4}
}

@article{Steinberg2013,
  title = {A Causal Link between Prediction Errors, Dopamine Neurons and Learning},
  author = {Steinberg, Elizabeth E and Keiflin, Ronald and Boivin, Josiah R and Witten, Ilana B and Deisseroth, Karl and Janak, Patricia H},
  year = {2013},
  month = jul,
  volume = {16},
  pages = {966--973},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.3413},
  file = {Steinberg et al. - 2013 - A causal link between prediction errors, dopamine .pdf},
  journal = {Nature Neuroscience},
  language = {en},
  number = {7}
}

@article{Steinmetz2018,
  title = {Distributed Correlates of Visually-Guided Behavior across the Mouse Brain},
  author = {Steinmetz, Nicholas and {Zatka-Haas}, Peter and Carandini, Matteo and Harris, Kenneth},
  year = {2018},
  month = nov,
  doi = {10.1101/474437},
  abstract = {Behavior arises from neuronal activity, but it is not known how the active neurons are distributed across brain regions and how their activity unfolds in time. Here, we used high-density Neuropixels probes to record from \textasciitilde 30,000 neurons in mice performing a visual contrast discrimination task. The task activated 60\% of the neurons, involving nearly all 42 recorded brain regions, well beyond the regions activated by passive visual stimulation. However, neurons selective for choice (left vs. right) were rare, and found mostly in midbrain, striatum, and frontal cortex. Those in midbrain were typically activated prior to contralateral choices and suppressed prior to ipsilateral choices, consistent with a competitive midbrain circuit for adjudicating the subject's choice. A brain-wide state shift distinguished trials in which visual stimuli led to movement. These results reveal concurrent representations of movement and choice in neurons widely distributed across the brain.},
  file = {Steinmetz et al. - 2018 - Distributed correlates of visually-guided behavior.pdf},
  journal = {bioRxiv},
  language = {en}
}

@article{Steinmetz2019,
  title = {Distributed Coding of Choice, Action and Engagement across the Mouse Brain},
  author = {Steinmetz, Nicholas A. and {Zatka-Haas}, Peter and Carandini, Matteo and Harris, Kenneth D.},
  year = {2019},
  month = dec,
  volume = {576},
  pages = {266--273},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/s41586-019-1787-x},
  file = {Steinmetz et al. - 2019 - Distributed coding of choice, action and engagemen.pdf},
  journal = {Nature},
  language = {en},
  number = {7786}
}

@article{Stephens2008,
  title = {Decision Ecology: {{Foraging}} and the Ecology of Animal Decision Making},
  shorttitle = {Decision Ecology},
  author = {Stephens, D. W.},
  year = {2008},
  month = dec,
  volume = {8},
  pages = {475--484},
  issn = {1530-7026, 1531-135X},
  doi = {10.3758/CABN.8.4.475},
  file = {Stephens - 2008 - Decision ecology Foraging and the ecology of anim.pdf},
  journal = {Cognitive, Affective, \& Behavioral Neuroscience},
  language = {en},
  number = {4}
}

@article{Stephenson-Jones2011,
  title = {Evolutionary {{Conservation}} of the {{Basal Ganglia}} as a {{Common Vertebrate Mechanism}} for {{Action Selection}}},
  author = {{Stephenson-Jones}, Marcus and Samuelsson, Ebba and Ericsson, Jesper and Robertson, Brita and Grillner, Sten},
  year = {2011},
  month = jul,
  volume = {21},
  pages = {1081--1091},
  issn = {09609822},
  doi = {10.1016/j.cub.2011.05.001},
  abstract = {Background: Although the basal ganglia are thought to play a key role in action selection in mammals, it is unknown whether this mammalian circuitry is present in lower vertebrates as a conserved selection mechanism. We aim here, using lamprey, to elucidate the basal ganglia circuitry in the phylogenetically oldest group of vertebrates (cyclostomes) and determine how this selection architecture evolved to accommodate the increased behavioral repertoires of advanced vertebrates. Results: We show, using immunohistochemistry, tract tracing, and whole-cell recordings, that all parts of the mammalian basal ganglia (striatum, globus pallidus interna [GPi] and externa [GPe], and subthalamic nucleus [STN]) are present in the lamprey forebrain. In addition, the circuit features, molecular markers, and physiological activity patterns are conserved. Thus, GABAergic striatal neurons expressing substance P project directly to the pallidal output layer, whereas enkephalin-expressing striatal neurons project indirectly via nuclei homologous to the GPe and STN. Moreover, pallidal output neurons tonically inhibit tectum, mesencephalic, and diencephalic motor regions. Conclusions: These results show that the detailed basal ganglia circuitry is present in the phylogenetically oldest vertebrates and has been conserved, most likely as a mechanism for action selection used by all vertebrates, for over 560 million years. Our data also suggest that the mammalian basal ganglia evolved through a process of exaptation, where the ancestral core unit has been co-opted for multiple functions, allowing them to process cognitive, emotional, and motor information in parallel and control a broader range of behaviors.},
  file = {Stephenson-Jones et al. - 2011 - Evolutionary Conservation of the Basal Ganglia as .pdf},
  journal = {Current Biology},
  language = {en},
  number = {13}
}

@article{Stiefel2008,
  title = {Cholinergic {{Neuromodulation Changes Phase Response Curve Shape}} and {{Type}} in {{Cortical Pyramidal Neurons}}},
  author = {Stiefel, Klaus M. and Gutkin, Boris S. and Sejnowski, Terrence J.},
  editor = {Ermentrout, Bard},
  year = {2008},
  month = dec,
  volume = {3},
  pages = {e3947},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0003947},
  abstract = {Spike generation in cortical neurons depends on the interplay between diverse intrinsic conductances. The phase response curve (PRC) is a measure of the spike time shift caused by perturbations of the membrane potential as a function of the phase of the spike cycle of a neuron. Near the rheobase, purely positive (type I) phase-response curves are associated with an onset of repetitive firing through a saddle-node bifurcation, whereas biphasic (type II) phase-response curves point towards a transition based on a Hopf-Andronov bifurcation. In recordings from layer 2/3 pyramidal neurons in cortical slices, cholinergic action, consistent with down-regulation of slow voltage-dependent potassium currents such as the M-current, switched the PRC from type II to type I. This is the first report showing that cholinergic neuromodulation may cause a qualitative switch in the PRCs type implying a change in the fundamental dynamical mechanism of spike generation.},
  file = {2008 - Stiefel, Gutkin, Sejnowski - Cholinergic neuromodulation changes phase response curve shape and type in cortical pyramidal neuron.pdf},
  journal = {PLoS ONE},
  language = {en},
  number = {12}
}

@article{Stiefel2009,
  title = {The Effects of Cholinergic Neuromodulation on Neuronal Phase-Response Curves of Modeled Cortical Neurons},
  author = {Stiefel, Klaus M. and Gutkin, Boris S. and Sejnowski, Terrence J.},
  year = {2009},
  month = apr,
  volume = {26},
  pages = {289--301},
  issn = {0929-5313, 1573-6873},
  doi = {10.1007/s10827-008-0111-9},
  file = {2009 - Stiefel, Gutkin, Sejnowski - The effects of cholinergic neuromodulation on neuronal phase-response curves of modeled cortical neu.pdf},
  journal = {Journal of Computational Neuroscience},
  language = {en},
  number = {2}
}

@article{Still2012,
  title = {The Thermodynamics of Prediction},
  author = {Still, Susanne and Sivak, David A. and Bell, Anthony J. and Crooks, Gavin E.},
  year = {2012},
  month = sep,
  volume = {109},
  issn = {0031-9007, 1079-7114},
  doi = {10.1103/PhysRevLett.109.120604},
  abstract = {A system responding to a stochastic driving signal can be interpreted as computing, by means of its dynamics, an implicit model of the environmental variables. The system's state retains information about past environmental fluctuations, and a fraction of this information is predictive of future ones. The remaining nonpredictive information reflects model complexity that does not improve predictive power, and thus represents the ineffectiveness of the model. We expose the fundamental equivalence between this model inefficiency and thermodynamic inefficiency, measured by dissipation. Our results hold arbitrarily far from thermodynamic equilibrium and are applicable to a wide range of systems, including biomolecular machines. They highlight a profound connection between the effective use of information and efficient thermodynamic operation: any system constructed to keep memory about its environment and to operate with maximal energetic efficiency has to be predictive.},
  archiveprefix = {arXiv},
  eprint = {1203.3271},
  eprinttype = {arxiv},
  file = {2012 - Still et al. - Thermodynamics of prediction.pdf},
  journal = {Physical Review Letters},
  keywords = {Computer Science - Information Theory,Condensed Matter - Statistical Mechanics,Quantitative Biology - Quantitative Methods},
  language = {en},
  number = {12}
}

@article{Stimberg2014,
  title = {Equation-Oriented Specification of Neural Models for Simulations},
  author = {Stimberg, Marcel and Goodman, Dan F. M. and Benichoux, Victor and Brette, Romain},
  year = {2014},
  volume = {8},
  issn = {1662-5196},
  doi = {10.3389/fninf.2014.00006},
  abstract = {Simulating biological neuronal networks is a core method of research in computational neuroscience. A full specification of such a network model includes a description of the dynamics and state changes of neurons and synapses, as well as the synaptic connectivity patterns and the initial values of all parameters. A standard approach in neuronal modeling software is to build network models based on a library of pre-defined components and mechanisms; if a model component does not yet exist, it has to be defined in a special-purpose or general low-level language and potentially be compiled and linked with the simulator. Here we propose an alternative approach that allows flexible definition of models by writing textual descriptions based on mathematical notation. We demonstrate that this approach allows the definition of a wide range of models with minimal syntax. Furthermore, such explicit model descriptions allow the generation of executable code for various target languages and devices, since the description is not tied to an implementation. Finally, this approach also has advantages for readability and reproducibility, because the model description is fully explicit, and because it can be automatically parsed and transformed into formatted descriptions. The presented approach has been implemented in the Brian2 simulator.},
  file = {Stimberg et al. - 2014 - Equation-oriented specification of neural models f.pdf},
  journal = {Frontiers in Neuroinformatics},
  language = {en}
}

@article{Stimberg2017,
  title = {Modeling Neuron\textendash Glia Interactions with the                          Simulator},
  author = {Stimberg, Marcel and Goodman, Dan F. M. and Brette, Romain and De Pitt{\`a}, Maurizio},
  year = {2017},
  month = oct,
  doi = {10.1101/198366},
  abstract = {Despite compelling evidence that glial cells could crucially regulate neural network activity, the vast majority of available neural simulators ignores the possible contribution of glia to neuronal physiology. Here, we show how to model glial physiology and neuron-glia interactions in the Brian 2 simulator. Brian 2 offers facilities to explicitly describe any model in mathematical terms with limited and simple simulator-specific syntax, automatically generating high-performance code from the user-provided descriptions. The flexibility of this approach allows us to model not only networks of neurons, but also individual glial cells, electrical coupling of glial cells, and the interaction between glial cells and synapses. We therefore conclude that Brian 2 provides an ideal platform to efficiently simulate glial physiology, and specifically, the influence of astrocytes on neural activity.},
  file = {Stimberg et al. - 2017 - Modeling neuron–glia interactions with the        .pdf},
  journal = {bioRxiv},
  language = {en}
}

@article{Stocchi2005,
  title = {Intermittent vs {{Continuous Levodopa Administration}} in {{Patients With Advanced Parkinson Disease}}: {{A Clinical}} and {{Pharmacokinetic Study}}},
  shorttitle = {Intermittent vs {{Continuous Levodopa Administration}} in {{Patients With Advanced Parkinson Disease}}},
  author = {Stocchi, Fabrizio and Vacca, Laura and Ruggieri, Stefano and Olanow, C. Warren},
  year = {2005},
  month = jun,
  volume = {62},
  issn = {0003-9942},
  doi = {10.1001/archneur.62.6.905},
  file = {2013 - Study - in Patients With Advanced Parkinson Disease.pdf;Stocchi et al. - 2005 - Intermittent vs Continuous Levodopa Administration.pdf},
  journal = {Archives of Neurology},
  language = {en},
  number = {6}
}

@article{Stokes2009,
  title = {Top-{{Down Activation}} of {{Shape}}-{{Specific Population Codes}} in {{Visual Cortex}} during {{Mental Imagery}}},
  author = {Stokes, M. and Thompson, R. and Cusack, R. and Duncan, J.},
  year = {2009},
  month = feb,
  volume = {29},
  pages = {1565--1572},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.4657-08.2009},
  file = {2009 - Stokes et al. - Top-down activation of shape-specific population codes in visual cortex during mental imagery.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {5}
}

@article{Stokes2015,
  title = {`{{Activity}}-Silent' Working Memory in Prefrontal Cortex: A Dynamic Coding Framework},
  shorttitle = {`{{Activity}}-Silent' Working Memory in Prefrontal Cortex},
  author = {Stokes, Mark G.},
  year = {2015},
  month = jul,
  volume = {19},
  pages = {394--405},
  issn = {13646613},
  doi = {10.1016/j.tics.2015.05.004},
  file = {2015 - Stokes - ‘Activity-silent’ working memory in prefrontal cortex a dynamic coding framework.pdf;Stokes - 2015 - ‘Activity-silent’ working memory in prefrontal cor.pdf},
  journal = {Trends in Cognitive Sciences},
  language = {en},
  number = {7}
}

@article{Storch2013,
  title = {Nonmotor Fluctuations in {{Parkinson}} Disease: {{Severity}} and Correlation with Motor Complications},
  shorttitle = {Nonmotor Fluctuations in {{Parkinson}} Disease},
  author = {Storch, A. and Schneider, C. B. and Wolz, M. and Sturwald, Y. and Nebe, A. and Odin, P. and Mahler, A. and Fuchs, G. and Jost, W. H. and Chaudhuri, K. R. and Koch, R. and Reichmann, H. and Ebersbach, G.},
  year = {2013},
  month = feb,
  volume = {80},
  pages = {800--809},
  issn = {0028-3878, 1526-632X},
  doi = {10.1212/WNL.0b013e318285c0ed},
  abstract = {Objective: To evaluate frequency, severity, and correlation of nonmotor symptoms (NMS) with motor complications in fluctuating Parkinson disease (PD). Methods: The Multicenter NonMotor Fluctuations in PD cross-sectional study used clinical examination of 10 NMS (dysphagia, anxiety, depression, fatigue, excessive sweating, inner restlessness, pain, concentration/attention, dizziness, bladder urgency) quantified using a visual analogue scale (VAS) in motor-defined on (NMSOn) and off state (NMSOff) combined with motor assessments and self-ratings at home in 100 patients with advanced PD. Results: All NMS except dysphagia, excessive sweating, and bladder urgency fluctuated in conjunction to motor fluctuations with more frequent and severe symptoms in off compared to on state. The proportions of patients experiencing autonomic/sensory NMS in both motor states were similar to those with these NMS exclusively in off state (ratios 0.4\textendash 1.3), while for mental/psychic NMS the proportions with exclusive manifestation in off state were higher (ratios 1.8\textendash 3.1). Demographic and clinical characteristics correlated neither with NMS frequency patterns and severities nor with DNMSOn/Off severities (defined as the differences of VAS scores between on and off). Severities of NMSon, NMSOff, and DNMSOn/Off did not correlate with motor function. Presence of anxiety, depression, fatigue, and pain had negative impact on health-related quality of life (HRQOL) measured by Parkinson's Disease Questionnaire\textendash 8 scoring independent of their occurrence with respect to motor state. Fluctuations of these NMS but not of fatigue deteriorated HRQOL. Conclusion: Patterns of NMS fluctuations are heterogeneous and complex, but psychic NMS fluctuate more frequently and severely. Demographic parameters and motor function do not correlate with NMS or nonmotor fluctuation severities in fluctuating PD. Neurology\^a 2013;80:800\textendash 809},
  file = {2013 - Storch et al. - Nonmotor fluctuations in Parkinson disease Severity and correlation with motor complications.pdf;Storch et al. - 2013 - Nonmotor fluctuations in Parkinson disease Severi.pdf},
  journal = {Neurology},
  language = {en},
  number = {9}
}

@article{Strehl,
  title = {Reinforcement {{Learning}} in {{Finite MDPs}}: {{PAC Analysis}}},
  author = {Strehl, Alexander L and Li, Lihong and Littman, Michael L},
  pages = {32},
  abstract = {We study the problem of learning near-optimal behavior in finite Markov Decision Processes (MDPs) with a polynomial number of samples. These ``PAC-MDP'' algorithms include the wellknown E3 and R-MAX algorithms as well as the more recent Delayed Q-learning algorithm. We summarize the current state-of-the-art by presenting bounds for the problem in a unified theoretical framework. A more refined analysis for upper and lower bounds is presented to yield insight into the differences between the model-free Delayed Q-learning and the model-based R-MAX.},
  file = {Strehl et al. - Reinforcement Learning in Finite MDPs PAC Analysi 2.pdf},
  language = {en}
}

@article{Strehl2009,
  title = {Reinforcement {{Learning}} in {{Finite MDPs}}: {{PAC Analysis}}},
  author = {Strehl, Alexander L and Li, Lihong and Littman, Michael L},
  year = {2009},
  volume = {10},
  pages = {1--32},
  abstract = {We study the problem of learning near-optimal behavior in finite Markov Decision Processes (MDPs) with a polynomial number of samples. These ``PAC-MDP'' algorithms include the wellknown E3 and R-MAX algorithms as well as the more recent Delayed Q-learning algorithm. We summarize the current state-of-the-art by presenting bounds for the problem in a unified theoretical framework. A more refined analysis for upper and lower bounds is presented to yield insight into the differences between the model-free Delayed Q-learning and the model-based R-MAX.},
  file = {Strehl et al. - Reinforcement Learning in Finite MDPs PAC Analysi.pdf},
  journal = {Journal of Machine Learning Research},
  language = {en}
}

@article{Stringer2016,
  title = {Inhibitory Control of Shared Variability in Cortical Networks},
  author = {Stringer, Carsen and Pachitariu, Marius and Okun, Michael and Bartho, Peter and Harris, Kenneth and Latham, Peter and Sahani, Maneesh and Lesica, Nicholas},
  year = {2016},
  month = jul,
  doi = {10.1101/041103},
  abstract = {Cortical networks exhibit intrinsic dynamics that drive coordinated, large-scale fluctuations across neuronal populations and create noise correlations that impact sensory coding. To investigate the network-level mechanisms that underlie these dynamics, we developed novel computational techniques to fit a deterministic spiking network model directly to multi-neuron recordings from different species, sensory modalities, and behavioral states. The model accurately reproduced the wide variety of activity patterns in our recordings, and analysis of its parameters suggested that differences in noise correlations across recordings were due primarily to differences in the strength of feedback inhibition. Further analysis of our recordings confirmed that putative inhibitory interneurons were indeed more active during desynchronized cortical states with weak noise correlations. Our results demonstrate the power of fitting spiking network models directly to multi-neuron recordings and suggest that inhibition modulates the interactions between intrinsic dynamics and sensory inputs by controlling network stability.},
  file = {Stringer et al. - 2016 - Inhibitory control of shared variability in cortic 2.pdf;Stringer et al. - 2016 - Inhibitory control of shared variability in cortic.pdf},
  journal = {bioRxiv},
  language = {en}
}

@book{Strogatz1994,
  title = {Nonlinear Dynamics and {{Chaos}}: With Applications to Physics, Biology, Chemistry, and Engineering},
  shorttitle = {Nonlinear Dynamics and {{Chaos}}},
  author = {Strogatz, Steven H.},
  year = {1994},
  publisher = {{Addison-Wesley Pub}},
  address = {{Reading, Mass}},
  file = {Strogatz - 1994 - Nonlinear dynamics and Chaos with applications to.pdf},
  isbn = {978-0-201-54344-5},
  keywords = {Chaotic behavior in systems,Dynamics,Nonlinear theories},
  language = {en},
  lccn = {Q172.5.C45 S767 1994},
  series = {Studies in Nonlinearity}
}

@article{Strouse2017,
  title = {The {{Deterministic Information Bottleneck}}},
  author = {Strouse, Dj and Schwab, David J.},
  year = {2017},
  month = jun,
  volume = {29},
  pages = {1611--1630},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/NECO_a_00961},
  abstract = {Lossy compression and clustering fundamentally involve a decision about what features are relevant and which are not. The information bottleneck method (IB) by Tishby, Pereira, and Bialek formalized this notion as an information-theoretic optimization problem and proposed an optimal tradeoff between throwing away as many bits as possible, and selectively keeping those that are most important. In the IB, compression is measure my mutual information. Here, we introduce an alternative formulation that replaces mutual information with entropy, which we call the deterministic information bottleneck (DIB), that we argue better captures this notion of compression. As suggested by its name, the solution to the DIB problem turns out to be a deterministic encoder, or hard clustering, as opposed to the stochastic encoder, or soft clustering, that is optimal under the IB. We compare the IB and DIB on synthetic data, showing that the IB and DIB perform similarly in terms of the IB cost function, but that the DIB significantly outperforms the IB in terms of the DIB cost function. We also empirically find that the DIB offers a considerable gain in computational efficiency over the IB, over a range of convergence parameters. Our derivation of the DIB also suggests a method for continuously interpolating between the soft clustering of the IB and the hard clustering of the DIB.},
  file = {Strouse and Schwab - 2017 - The Deterministic Information Bottleneck 2.pdf;Strouse and Schwab - 2017 - The Deterministic Information Bottleneck.pdf},
  journal = {Neural Computation},
  language = {en},
  number = {6}
}

@article{Suarez2019,
  title = {Neural {{MMO}}: {{A Massively Multiagent Game Environment}} for {{Training}} and {{Evaluating Intelligent Agents}}},
  shorttitle = {Neural {{MMO}}},
  author = {Suarez, Joseph and Du, Yilun and Isola, Phillip and Mordatch, Igor},
  year = {2019},
  month = mar,
  abstract = {The emergence of complex life on Earth is often attributed to the arms race that ensued from a huge number of organisms all competing for finite resources. We present an artificial intelligence research environment, inspired by the human game genre of MMORPGs (Massively Multiplayer Online Role-Playing Games, a.k.a. MMOs), that aims to simulate this setting in microcosm. As with MMORPGs and the real world alike, our environment is persistent and supports a large and variable number of agents. Our environment is well suited to the study of large-scale multiagent interaction: it requires that agents learn robust combat and navigation policies in the presence of large populations attempting to do the same. Baseline experiments reveal that population size magnifies and incentivizes the development of skillful behaviors and results in agents that outcompete agents trained in smaller populations. We further show that the policies of agents with unshared weights naturally diverge to fill different niches in order to avoid competition.},
  archiveprefix = {arXiv},
  eprint = {1903.00784},
  eprinttype = {arxiv},
  file = {Suarez et al. - 2019 - Neural MMO A Massively Multiagent Game Environmen.pdf},
  journal = {arXiv:1903.00784 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Computer Science - Multiagent Systems,Statistics - Machine Learning},
  primaryclass = {cs, stat}
}

@article{Such,
  title = {Deep {{Neuroevolution}}: {{Genetic Algorithms}} Are a {{Competitive Alternative}} for {{Training Deep Neural Networks}} for {{Reinforcement Learning}}},
  author = {Such, Felipe Petroski and Madhavan, Vashisht and Conti, Edoardo and Lehman, Joel and Stanley, Kenneth O and Clune, Jeff},
  pages = {15},
  abstract = {Deep artificial neural networks (DNNs) are typically trained via gradient-based learning algorithms, namely backpropagation. Evolution strategies (ES) can rival backprop-based algorithms such as Q-learning and policy gradients on challenging deep reinforcement learning (RL) problems. However, ES can be considered a gradient-based algorithm because it performs stochastic gradient descent via an operation similar to a finite-difference approximation of the gradient. That raises the question of whether non-gradient-based evolutionary algorithms can work at DNN scales. Here we demonstrate they can: we evolve the weights of a DNN with a simple, gradient-free, population-based genetic algorithm (GA) and it performs well on hard deep RL problems, including Atari and humanoid locomotion. The Deep GA successfully evolves networks with over four million free parameters, the largest neural networks ever evolved with a traditional evolutionary algorithm. These results (1) expand our sense of the scale at which GAs can operate, (2) suggest intriguingly that in some cases following the gradient is not the best choice for optimizing performance, and (3) make immediately available the multitude of techniques that have been developed in the neuroevolution community to improve performance on RL problems. To demonstrate the latter, we show that combining DNNs with novelty search, which was designed to encourage exploration on tasks with deceptive or sparse reward functions, can solve a high-dimensional problem on which reward-maximizing algorithms (e.g. DQN, A3C, ES, and the GA) fail. Additionally, the Deep GA parallelizes better than ES, A3C, and DQN, and enables a state-of-the-art compact encoding technique that can represent million-parameter DNNs in thousands of bytes.},
  file = {Such et al. - Deep Neuroevolution Genetic Algorithms are a Comp.pdf},
  language = {en}
}

@article{Suda2020,
  title = {Calcium Dynamics during Trap Closure Visualized in Transgenic {{Venus}} Flytrap},
  author = {Suda, Hiraku and Mano, Hiroaki and Toyota, Masatsugu and Fukushima, Kenji and Mimura, Tetsuro and Tsutsui, Izuo and Hedrich, Rainer and Tamada, Yosuke and Hasebe, Mitsuyasu},
  year = {2020},
  month = oct,
  volume = {6},
  pages = {1219--1224},
  issn = {2055-0278},
  doi = {10.1038/s41477-020-00773-1},
  file = {Suda et al. - 2020 - Calcium dynamics during trap closure visualized in.pdf},
  journal = {Nat. Plants},
  language = {en},
  number = {10}
}

@article{Suda2020a,
  title = {Calcium Dynamics during Trap Closure Visualized in Transgenic {{Venus}} Flytrap},
  author = {Suda, Hiraku and Mano, Hiroaki and Toyota, Masatsugu and Fukushima, Kenji and Mimura, Tetsuro and Tsutsui, Izuo and Hedrich, Rainer and Tamada, Yosuke and Hasebe, Mitsuyasu},
  year = {2020},
  month = oct,
  volume = {6},
  pages = {1219--1224},
  issn = {2055-0278},
  doi = {10.1038/s41477-020-00773-1},
  file = {Suda et al. - 2020 - Calcium dynamics during trap closure visualized in.pdf},
  journal = {Nat. Plants},
  language = {en},
  number = {10}
}

@article{Sui2009,
  title = {An {{ICA}}-Based Method for the Identification of Optimal {{FMRI}} Features and Components Using Combined Group-Discriminative Techniques},
  author = {Sui, Jing and Adali, T{\"u}lay and Pearlson, Godfrey D. and Calhoun, Vince D.},
  year = {2009},
  month = may,
  volume = {46},
  pages = {73--86},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2009.01.026},
  abstract = {Extraction of relevant features from multitask functional MRI (fMRI) data in order to identify potential biomarkers for disease, is an attractive goal. In this paper, we introduce a novel feature-based framework, which is sensitive and accurate in detecting group differences (e.g. controls vs. patients) by proposing three key ideas. First, we integrate two goal-directed techniques: coefficient-constrained independent component analysis (CC-ICA) and principal component analysis with reference (PCA-R), both of which improve sensitivity to group differences. Secondly, an automated artifact-removal method is developed for selecting components of interest derived from CC-ICA, with an average accuracy of 91\%. Finally, we propose a strategy for optimal feature/component selection, aiming to identify optimal group-discriminative brain networks as well as the tasks within which these circuits are engaged. The group-discriminating performance is evaluated on 15 fMRI feature combinations (5 single features and 10 joint features) collected from 28 healthy control subjects and 25 schizophrenia patients. Results show that a feature from a sensorimotor task and a joint feature from a Sternberg working memory (probe) task and an auditory oddball (target) task are the top two feature combinations distinguishing groups. We identified three optimal features that best separate patients from controls, including brain networks consisting of temporal lobe, default mode and occipital lobe circuits, which when grouped together provide improved capability in classifying group membership. The proposed framework provides a general approach for selecting optimal brain networks which may serve as potential biomarkers of several brain diseases and thus has wide applicability in the neuroimaging research community.},
  file = {2009 - Sui et al. - An ICA-based method for the identification of optimal FMRI features and components using combined group-discriminati.pdf},
  journal = {NeuroImage},
  language = {en},
  number = {1}
}

@article{Sumner2019,
  title = {The {{Exploration Advantage}}: {{Children}}'s Instinct to Explore Allows Them to Find Information That Adults Miss},
  author = {Sumner, Emily S and Li, Amy X and Perfors, Amy and Hayes, Brett K and Navarro, Danielle J and Sarnecka, Barbara W},
  year = {2019},
  volume = {h437v},
  pages = {11},
  abstract = {Humans have a long childhood in comparison to all other species. Across disciplines, researchers agree that humans' prolonged immaturity is integral to our unique intelligence. The studies presented here support the hypothesis that human beings' extended childhood pays off in the form of an ability to learn more about changing environments. Across two studies (n = 213), children and adults played a game where they chose among four different cartoon monsters yielding different numbers of star rewards. Adults focused on maximizing reward, while children chose to explore longer, even at the cost of earning fewer stars. As a result, adults won significantly more stars than children did. However, in the `dynamic' version of the task, the rewards given out by the monsters changed halfway through: the monster that had been giving out the fewest stars began giving out the most. Because children continued to explore whereas adults ignored the low-reward monster, children were much more likely than adults to detect the change. This illustrates that while exploration may be costly in the short term, it leads to a more flexible understanding of the world in the long term, particularly when that world is changing.},
  file = {Sumner et al. - The Exploration Advantage.pdf},
  journal = {PsyArxiv},
  language = {en}
}

@article{Sun2011,
  title = {Gamma Oscillations in Schizophrenia: {{Mechanisms}} and Clinical Significance},
  shorttitle = {Gamma Oscillations in Schizophrenia},
  author = {Sun, Yinming and Farzan, Faranak and Barr, Mera S. and Kirihara, Kenji and Fitzgerald, Paul B. and Light, Gregory A. and Daskalakis, Zafiris J.},
  year = {2011},
  month = sep,
  volume = {1413},
  pages = {98--114},
  issn = {00068993},
  doi = {10.1016/j.brainres.2011.06.065},
  abstract = {Brain oscillations are increasingly used for understanding complex psychiatric disorders. Gamma (30\textendash 50 Hz) oscillations have warranted special attention due to their omnipresence in cognitive tasks. For patients with schizophrenia (SCZ), a disease associated with poor cognition, abnormal gamma oscillations have been reported in many experimental paradigms. The goal of this paper is to review the literature on gamma oscillations in SCZ. The review is structured into four sections. First, the functional role, neurobiology, and analysis of brain oscillations, especially gamma oscillations will be outlined. Second, the neurobiological abnormalities of SCZ in relation to gamma oscillations will be reviewed. Third, selected paradigms for investigating irregular gamma oscillations in SCZ will be discussed in detail. Finally, a discussion on the limitations of current findings and potential future research directions will be provided. The reviewed evidence suggests that gamma oscillations are disrupted in SCZ and could account for cognitive disturbances in this disorder. With additional analysis and experimentation, these indices may ultimately serve as endophenotypes that facilitate the development of etiologically based diagnostic methods, foster early identification and treatment, and advance our understanding of the complex genetic mechanisms involved in this disorder.},
  file = {2011 - Sun et al. - Gamma oscillations in schizophrenia Mechanisms and clinical significance.pdf},
  journal = {Brain Research},
  language = {en}
}

@article{Sundararajan2017,
  title = {Axiomatic {{Attribution}} for {{Deep Networks}}},
  author = {Sundararajan, Mukund and Taly, Ankur and Yan, Qiqi},
  year = {2017},
  month = jun,
  abstract = {We study the problem of attributing the prediction of a deep network to its input features, a problem previously studied by several other works. We identify two fundamental axioms\textemdash Sensitivity and Implementation Invariance that attribution methods ought to satisfy. We show that they are not satisfied by most known attribution methods, which we consider to be a fundamental weakness of those methods. We use the axioms to guide the design of a new attribution method called Integrated Gradients. Our method requires no modification to the original network and is extremely simple to implement; it just needs a few calls to the standard gradient operator. We apply this method to a couple of image models, a couple of text models and a chemistry model, demonstrating its ability to debug networks, to extract rules from a network, and to enable users to engage with models better.},
  archiveprefix = {arXiv},
  eprint = {1703.01365},
  eprinttype = {arxiv},
  file = {Sundararajan et al. - 2017 - Axiomatic Attribution for Deep Networks.pdf},
  journal = {arXiv:1703.01365 [cs]},
  keywords = {Computer Science - Machine Learning},
  language = {en},
  primaryclass = {cs}
}

@book{Sundstrom2014,
  title = {Mathematical {{Reasoning}}: {{Writing}} and {{Proof}}},
  author = {Sundstrom, Ted},
  year = {2014},
  edition = {1.1},
  publisher = {{Pearson Education}},
  file = {Sundstrom - Mathematical Reasoning Writing and Proof.pdf},
  language = {en}
}

@article{Sussillo2009,
  title = {Generating {{Coherent Patterns}} of {{Activity}} from {{Chaotic Neural Networks}}},
  author = {Sussillo, David and Abbott, L.F.},
  year = {2009},
  month = aug,
  volume = {63},
  pages = {544--557},
  issn = {08966273},
  doi = {10.1016/j.neuron.2009.07.018},
  abstract = {Neural circuits display complex activity patterns both spontaneously and when responding to a stimulus or generating a motor output. How are these two forms of activity related? We develop a procedure called FORCE learning for modifying synaptic strengths either external to or within a model neural network to change chaotic spontaneous activity into a wide variety of desired activity patterns. FORCE learning works even though the networks we train are spontaneously chaotic and we leave feedback loops intact and unclamped during learning. Using this approach, we construct networks that produce a wide variety of complex output patterns, input-output transformations that require memory, multiple outputs that can be switched by control inputs, and motor patterns matching human motion capture data. Our results reproduce data on premovement activity in motor and premotor cortex, and suggest that synaptic plasticity may be a more rapid and powerful modulator of network activity than generally appreciated.},
  file = {2009 - Sussillo, Abbott - Generating Coherent Patterns of Activity from Chaotic Neural Networks.pdf},
  journal = {Neuron},
  language = {en},
  number = {4}
}

@article{Sussillo2013,
  title = {Opening the {{Black Box}}: {{Low}}-{{Dimensional Dynamics}} in {{High}}-{{Dimensional Recurrent Neural Networks}}},
  shorttitle = {Opening the {{Black Box}}},
  author = {Sussillo, David and Barak, Omri},
  year = {2013},
  month = mar,
  volume = {25},
  pages = {626--649},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/NECO_a_00409},
  file = {2013 - Sussillo, Barak - Opening the black box low-dimensional dynamics in high-dimensional recurrent neural networks.pdf;Sussillo and Barak - 2013 - Opening the Black Box Low-Dimensional Dynamics in.pdf},
  journal = {Neural Computation},
  language = {en},
  number = {3}
}

@article{Sussillo2015,
  title = {A Neural Network That Finds a Naturalistic Solution for the Production of Muscle Activity},
  author = {Sussillo, David and Churchland, Mark M and Kaufman, Matthew T and Shenoy, Krishna V},
  year = {2015},
  month = jul,
  volume = {18},
  pages = {1025--1033},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.4042},
  file = {2015 - Sussillo et al. - A neural network that finds a naturalistic solution for the production of muscle activity.pdf;Sussillo et al. - 2015 - A neural network that finds a naturalistic solutio.pdf},
  journal = {Nature Neuroscience},
  language = {en},
  number = {7}
}

@article{Sutskever2008,
  title = {Deep, {{Narrow Sigmoid Belief Networks Are Universal Approximators}}},
  author = {Sutskever, Ilya and Hinton, Geoffrey E.},
  year = {2008},
  month = nov,
  volume = {20},
  pages = {2629--2636},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/neco.2008.12-07-661},
  abstract = {In this note, we show that exponentially deep belief networks can approximate any distribution over binary vectors to arbitrary accuracy, even when the width of each layer is limited to the dimensionality of the data. We further show that such networks can be greedily learned in an easy yet impractical way.},
  file = {Sutskever and Hinton - 2008 - Deep, Narrow Sigmoid Belief Networks Are Universal.pdf},
  journal = {Neural Computation},
  language = {en},
  number = {11}
}

@article{Sutton,
  title = {{{IntegraBteadseAd rocnhiAtepctpurroexsimfoartiLnegaDrnyinnga}},{{mPiclaPnrnoingrga}}, {{manmdinRgeacting}}},
  author = {Sutton, Richard S},
  pages = {9},
  abstract = {This paper extends previous work with Dyna, a class of architectures for intelligent systems based on approximating dynamic programming methods. Dyna architectures integrate trial-and-error (reinforcement) learning and execution-time planning into a single process operating alternately on the world and on a learned model of the world. In this paper, I present and show results for two Dyna architectures. The Dyna-PI architecture is based on dynamic programming's policy iteration method and can be related to existing AI ideas such as evaluation functions and universal plans (reactive systems). Using a navigation task, results are shown for a simple Dyna-PI system that simultaneously learns by trial and error, learns a world model, and plans optimal routes using the evolving world model. The Dyna-Q architecture is based on Watkins's Q-learning, a new kind of reinforcement learning. Dyna-Q uses a less familiar set of data structures than does Dyna-PI, but is arguably simpler to implement and use. We show that Dyna-Q architectures are easy to adapt for use in changing environments.},
  file = {1990 - Sutton - Integrated Architectures for Learning, Planning, and Reacting Based on Approximating Dynamic Programming.pdf},
  language = {en}
}

@book{Sutton1998,
  title = {Reinforcement Learning: An Introduction},
  shorttitle = {Reinforcement Learning},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  year = {1998},
  publisher = {{MIT Press}},
  address = {{Cambridge, Mass}},
  file = {Sutton and Barto - 1998 - Reinforcement learning an introduction.pdf},
  isbn = {978-0-262-19398-6},
  keywords = {Reinforcement learning},
  language = {en},
  lccn = {Q325.6 .S88 1998},
  series = {Adaptive Computation and Machine Learning}
}

@book{Sutton2018,
  title = {Reinforcement Learning: An Introduction},
  shorttitle = {Reinforcement Learning},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  year = {2018},
  edition = {Second edition},
  publisher = {{The MIT Press}},
  address = {{Cambridge, Massachusetts}},
  abstract = {"Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms."--},
  file = {Sutton and Barto - 2018 - Reinforcement learning an introduction.pdf},
  isbn = {978-0-262-03924-6},
  keywords = {Reinforcement learning},
  language = {en},
  lccn = {Q325.6 .R45 2018},
  series = {Adaptive Computation and Machine Learning Series}
}

@article{Suttona,
  title = {Policy {{Gradient Methods}} for {{Reinforcement Learning}} with {{Function Approximation}}},
  author = {Sutton, Richard S and McAllester, David A and Singh, Satinder P and Mansour, Yishay},
  pages = {7},
  abstract = {Function approximation is essential to reinforcement learning, but the standard approach of approximating a value function and determining a policy from it has so far proven theoretically intractable. In this paper we explore an alternative approach in which the policy is explicitly represented by its own function approximator, independent of the value function, and is updated according to the gradient of expected reward with respect to the policy parameters. Williams's REINFORCE method and actor-critic methods are examples of this approach. Our main new result is to show that the gradient can be written in a form suitable for estimation from experience aided by an approximate action-value or advantage function. Using this result, we prove for the first time that a version of policy iteration with arbitrary differentiable function approximation is convergent to a locally optimal policy.},
  file = {1999 - Sutton et al. - Policy Gradient Methods for Reinforcement Learning with Function Approximation.pdf;Sutton et al. - Policy Gradient Methods for Reinforcement Learning.pdf},
  language = {en}
}

@article{Suzuki2011,
  title = {Astrocyte-{{Neuron Lactate Transport Is Required}} for {{Long}}-{{Term Memory Formation}}},
  author = {Suzuki, Akinobu and Stern, Sarah A. and Bozdagi, Ozlem and Huntley, George W. and Walker, Ruth H. and Magistretti, Pierre J. and Alberini, Cristina M.},
  year = {2011},
  month = mar,
  volume = {144},
  pages = {810--823},
  issn = {00928674},
  doi = {10.1016/j.cell.2011.02.018},
  abstract = {We report that, in the rat hippocampus, learning leads to a significant increase in extracellular lactate levels that derive from glycogen, an energy reserve selectively localized in astrocytes. Astrocytic glycogen breakdown and lactate release are essential for long-term but not short-term memory formation, and for the maintenance of long-term potentiation (LTP) of synaptic strength elicited in vivo. Disrupting the expression of the astrocytic lactate transporters monocarboxylate transporter 4 (MCT4) or MCT1 causes amnesia, which, like LTP impairment, is rescued by L-lactate but not equicaloric glucose. Disrupting the expression of the neuronal lactate transporter MCT2 also leads to amnesia that is unaffected by either L-lactate or glucose, suggesting that lactate import into neurons is necessary for long-term memory. Glycogenolysis and astrocytic lactate transporters are also critical for the induction of molecular changes required for memory formation, including the induction of phospho-CREB, Arc, and phospho-cofilin. We conclude that astrocyte-neuron lactate transport is required for long-term memory formation.},
  file = {Suzuki et al. - 2011 - Astrocyte-Neuron Lactate Transport Is Required for.pdf},
  journal = {Cell},
  language = {en},
  number = {5}
}

@article{Swisher2010,
  title = {Multiscale {{Pattern Analysis}} of {{Orientation}}-{{Selective Activity}} in the {{Primary Visual Cortex}}},
  author = {Swisher, J. D. and Gatenby, J. C. and Gore, J. C. and Wolfe, B. A. and Moon, C.-H. and Kim, S.-G. and Tong, F.},
  year = {2010},
  month = jan,
  volume = {30},
  pages = {325--330},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.4811-09.2010},
  file = {2010 - Swisher et al. - Multiscale pattern analysis of orientation-selective activity in the primary visual cortex.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {1}
}

@article{Szegedy2013,
  title = {Intriguing Properties of Neural Networks},
  author = {Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},
  year = {2013},
  month = dec,
  abstract = {Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties.},
  archiveprefix = {arXiv},
  eprint = {1312.6199},
  eprinttype = {arxiv},
  file = {2013 - Szegedy, Zaremba, Sutskever - Intriguing properties of neural networks.pdf;Szegedy et al. - 2013 - Intriguing properties of neural networks.pdf},
  journal = {arXiv:1312.6199 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  language = {en},
  primaryclass = {cs}
}

@article{Szepesvari2010,
  title = {Algorithms for {{Reinforcement Learning}}},
  author = {Szepesv{\'a}ri, Csaba},
  year = {2010},
  month = jan,
  volume = {4},
  pages = {1--103},
  issn = {1939-4608, 1939-4616},
  doi = {10.2200/S00268ED1V01Y201005AIM009},
  file = {2010 - Szepesvári - Algorithms for reinforcement learning.pdf},
  journal = {Synthesis Lectures on Artificial Intelligence and Machine Learning},
  language = {en},
  number = {1}
}

@inproceedings{Szerlip2013,
  title = {Indirectly {{Encoded Sodarace}} for {{Artificial Life}}},
  booktitle = {Advances in {{Artificial Life}}, {{ECAL}} 2013},
  author = {Szerlip, Paul and Stanley, Kenneth},
  year = {2013},
  month = sep,
  pages = {218--225},
  publisher = {{MIT Press}},
  doi = {10.7551/978-0-262-31709-2-ch033},
  abstract = {The aim of this paper is to introduce a lightweight twodimensional domain for evolving diverse and interesting artificial creatures. The hope is that this domain will fill a need for such an easily-accessible option for researchers who wish to focus more on the evolutionary dynamics of artificial life scenarios than on building simulators and creature encodings. The proposed domain is inspired by Sodarace, a construction set for two-dimensional creatures made of masses and springs. However, unlike the original Sodarace, the indirectly encoded Sodarace (IESoR) system introduced in this paper allows evolution to discover a wide range of complex and regular ambulating creature morphologies by encoding them with compositional pattern producing networks (CPPNs), which are an established indirect encoding originally introduced for encoding large-scale neural networks. The result, demonstrated through a technique called novelty search with local competition (which are combined through multiobjective search), is that IESoR can discover a wide breadth of interesting and functional creatures, suggesting its potential utility for future experiments in artificial life.},
  file = {Szerlip and Stanley - 2013 - Indirectly Encoded Sodarace for Artificial Life.pdf},
  isbn = {978-0-262-31709-2},
  language = {en}
}

@article{Tachet2017,
  title = {Scaling {{Law}} of {{Urban Ride Sharing}}},
  author = {Tachet, R. and Sagarra, O. and Santi, P. and Resta, G. and Szell, M. and Strogatz, S. H. and Ratti, C.},
  year = {2017},
  month = dec,
  volume = {7},
  issn = {2045-2322},
  doi = {10.1038/srep42868},
  file = {Tachet et al. - 2017 - Scaling Law of Urban Ride Sharing.pdf},
  journal = {Scientific Reports},
  language = {en},
  number = {1}
}

@article{Tachibana2011,
  title = {Subthalamo-Pallidal Interactions Underlying Parkinsonian Neuronal Oscillations in the Primate Basal Ganglia: {{BG}} Oscillations in {{Parkinson}}'s Disease},
  shorttitle = {Subthalamo-Pallidal Interactions Underlying Parkinsonian Neuronal Oscillations in the Primate Basal Ganglia},
  author = {Tachibana, Yoshihisa and Iwamuro, Hirokazu and Kita, Hitoshi and Takada, Masahiko and Nambu, Atsushi},
  year = {2011},
  month = nov,
  volume = {34},
  pages = {1470--1484},
  issn = {0953816X},
  doi = {10.1111/j.1460-9568.2011.07865.x},
  abstract = {Parkinson's disease is characterized by degeneration of nigral dopaminergic neurons, leading to a wide variety of psychomotor dysfunctions. Accumulated evidence suggests that abnormally synchronized oscillations in the basal ganglia contribute to the expression of parkinsonian motor symptoms. However, the mechanism that generates abnormal oscillations in a dopamine-depleted state remains poorly understood. We addressed this question by examining basal ganglia neuronal activity in two 1-methyl-4-phenyl1,2,3,6-tetrahydropyridine-treated parkinsonian monkeys. We found that systemic administration of l-3,4-dihydroxyphenylalanine (l-DOPA; dopamine precursor) decreased abnormal neuronal oscillations (8\textendash 15 Hz) in the internal segment of the globus pallidus (GPi) and the subthalamic nucleus (STN) during the ON state when parkinsonian signs were alleviated and during l-DOPA-induced dyskinesia. GPi oscillations and parkinsonian signs were suppressed by silencing of the STN with infusion of muscimol (GABAA receptor agonist). Intrapallidal microinjection of a mixture of 3-(2-carboxypiperazin-4-yl)-propyl-1-phosphonic acid (CPP; N-methyl-daspartate receptor antagonist) and 1,2,3,4-tetrahydro-6-nitro-2,3-dioxo-benzo[f]quinoxaline-7-sulfonamide (NBQX; AMPA {$\fracslash$} kainate receptor antagonist) also decreased the oscillations in the GPi and the external segment of the globus pallidus (GPe). Neuronal oscillations in the STN were suppressed after intrasubthalamic microinjection of CPP {$\fracslash$} NBQX to block glutamatergic afferents of the STN. The STN oscillations were further reduced by muscimol inactivation of the GPe to block GABAergic inputs from the GPe. These results suggest that, in the dopamine-depleted state, glutamatergic inputs to the STN and reciprocal GPe\textendash STN interconnections are both important for the generation and amplification of the oscillatory activity of STN neurons, which is subsequently transmitted to the GPi, thus contributing to the symptomatic expression of Parkinson's disease.},
  file = {2011 - Tachibana et al. - Subthalamo-pallidal interactions underlying parkinsonian neuronal oscillations in the primate basal ganglia.pdf},
  journal = {European Journal of Neuroscience},
  language = {en},
  number = {9}
}

@article{Talwar,
  title = {Computational {{Separations}} between {{Sampling}} and {{Optimization}}},
  author = {Talwar, Kunal},
  pages = {11},
  abstract = {Two commonly arising computational tasks in Bayesian learning are Optimization (Maximum A Posteriori estimation) and Sampling (from the posterior distribution). In the convex case these two problems are efficiently reducible to each other. Recent work [Ma et al., 2019] shows that in the non-convex case, sampling can sometimes be provably faster. We present a simpler and stronger separation. We then compare sampling and optimization in more detail and show that they are provably incomparable: there are families of continuous functions for which optimization is easy but sampling is NP-hard, and vice versa. Further, we show function families that exhibit a sharp phase transition in the computational complexity of sampling, as one varies the natural temperature parameter. Our results draw on a connection to analogous separations in the discrete setting which are well-studied.},
  file = {Talwar - Computational Separations between Sampling and Opt.pdf},
  language = {en}
}

@article{Tan2016,
  title = {Decoding Gripping Force Based on Local Field Potentials Recorded from Subthalamic Nucleus in Humans},
  author = {Tan, Huiling and Pogosyan, Alek and Ashkan, Keyoumars and Green, Alexander L and Aziz, Tipu and Foltynie, Thomas and Limousin, Patricia and Zrinzo, Ludvic and Hariz, Marwan and Brown, Peter},
  year = {2016},
  month = nov,
  volume = {5},
  issn = {2050-084X},
  doi = {10.7554/eLife.19089},
  file = {Tan et al. - 2016 - Decoding gripping force based on local field poten.pdf},
  journal = {eLife},
  language = {en}
}

@article{Tang,
  title = {Control of {{Dynamics}} in {{Brain Networks}}},
  author = {Tang, Evelyn and Bassett, Danielle S},
  pages = {21},
  file = {Tang and Bassett - Control of Dynamics in Brain Networks.pdf},
  language = {en}
}

@article{Tang2017,
  title = {\#{{Exploration}}: {{A Study}} of {{Count}}-{{Based Exploration}} for {{Deep Reinforcement Learning}}},
  shorttitle = {\#{{Exploration}}},
  author = {Tang, Haoran and Houthooft, Rein and Foote, Davis and Stooke, Adam and Chen, Xi and Duan, Yan and Schulman, John and De Turck, Filip and Abbeel, Pieter},
  year = {2017},
  month = dec,
  abstract = {Count-based exploration algorithms are known to perform near-optimally when used in conjunction with tabular reinforcement learning (RL) methods for solving small discrete Markov decision processes (MDPs). It is generally thought that count-based methods cannot be applied in high-dimensional state spaces, since most states will only occur once. Recent deep RL exploration strategies are able to deal with high-dimensional continuous state spaces through complex heuristics, often relying on optimism in the face of uncertainty or intrinsic motivation. In this work, we describe a surprising finding: a simple generalization of the classic count-based approach can reach near state-of-the-art performance on various highdimensional and/or continuous deep RL benchmarks. States are mapped to hash codes, which allows to count their occurrences with a hash table. These counts are then used to compute a reward bonus according to the classic count-based exploration theory. We find that simple hash functions can achieve surprisingly good results on many challenging tasks. Furthermore, we show that a domaindependent learned hash code may further improve these results. Detailed analysis reveals important aspects of a good hash function: 1) having appropriate granularity and 2) encoding information relevant to solving the MDP. This exploration strategy achieves near state-of-the-art performance on both continuous control tasks and Atari 2600 games, hence providing a simple yet powerful baseline for solving MDPs that require considerable exploration.},
  archiveprefix = {arXiv},
  eprint = {1611.04717},
  eprinttype = {arxiv},
  file = {Tang et al. - 2017 - #Exploration A Study of Count-Based Exploration f.pdf},
  journal = {arXiv:1611.04717 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  language = {en},
  primaryclass = {cs}
}

@article{Tao2010,
  title = {Outliers in the Spectrum of Iid Matrices with Bounded Rank Perturbations},
  author = {Tao, Terence},
  year = {2010},
  month = dec,
  abstract = {It is known that if one perturbs a large iid random matrix by a bounded rank error, then the majority of the eigenvalues will remain distributed according to the circular law. However, the bounded rank perturbation may also create one or more outlier eigenvalues. We show that if the perturbation is small, then the outlier eigenvalues are created next to the outlier eigenvalues of the bounded rank perturbation; but if the perturbation is large, then many more outliers can be created, and their law is governed by the zeroes of a random Laurent series with Gaussian coefficients. On the other hand, these outliers may be eliminated by enforcing a row sum condition on the final matrix.},
  archiveprefix = {arXiv},
  eprint = {1012.4818},
  eprinttype = {arxiv},
  file = {2013 - Tao - Outliers in the spectrum of iid matrices with bounded rank perturbations.pdf;Tao - 2010 - Outliers in the spectrum of iid matrices with boun.pdf},
  journal = {arXiv:1012.4818 [math]},
  keywords = {60B20,Mathematics - Probability},
  language = {en},
  primaryclass = {math}
}

@article{Tao2019,
  title = {Parallel {{Processing}} of {{Two Mechanosensory Modalities}} by a {{Single Neuron}} in {{C}}. Elegans},
  author = {Tao, Li and Porto, Daniel and Li, Zhaoyu and Fechner, Sylvia and Lee, Sol Ah and Goodman, Miriam B. and Xu, X.Z. Shawn and Lu, Hang and Shen, Kang},
  year = {2019},
  month = dec,
  volume = {51},
  pages = {617-631.e3},
  issn = {15345807},
  doi = {10.1016/j.devcel.2019.10.008},
  abstract = {Neurons convert synaptic or sensory inputs into cellular outputs. It is not well understood how a single neuron senses, processes multiple stimuli, and generates distinct neuronal outcomes. Here, we describe the mechanism by which the C. elegans PVD neurons sense two mechanical stimuli: external touch and proprioceptive body movement. These two stimuli are detected by distinct mechanosensitive DEG/ENaC/ASIC channels, which trigger distinct cellular outputs linked to mechanonociception and proprioception. Mechanonociception depends on DEGT-1 and activates PVD's downstream command interneurons through its axon, while proprioception depends on DEL-1, UNC-8, and MEC-10 to induce local dendritic Ca2+ increase and dendritic release of a neuropeptide NLP-12. NLP-12 directly modulates neuromuscular junction activity through the cholecystokinin receptor homolog on motor axons, setting muscle tone and movement vigor. Thus, the same neuron simultaneously uses both its axon and dendrites as output apparatus to drive distinct sensorimotor outcomes.},
  file = {Tao et al. - 2019 - Parallel Processing of Two Mechanosensory Modaliti.pdf},
  journal = {Developmental Cell},
  language = {en},
  number = {5}
}

@article{Tasic2017,
  title = {Shared and Distinct Transcriptomic Cell Types across Neocortical Areas},
  author = {Tasic, Bosiljka and Yao, Zizhen and Smith, Kimberly A and Graybuck, Lucas and Nguyen, Thuc Nghi and Bertagnolli, Darren and Goldy, Jeff and Garren, Emma and Economo, Michael N and Viswanathan, Sarada and Penn, Osnat and Bakken, Trygve and Menon, Vilas and Miller, Jeremy A and Fong, Olivia and Hirokawa, Karla E and Lathia, Kanan and Rimorin, Christine and Tieu, Michael and Larsen, Rachael and Casper, Tamara and Barkan, Eliza and Kroll, Matthew and Parry, Seana and Shapovalova, Nadiya V and Hirchstein, Daniel and Pendergraft, Julie and Kim, Tae Kyung and Szafer, Aaron and Dee, Nick and Groblewski, Peter and Wickersham, Ian and Cetin, Ali and Harris, Julie A and Levi, Boaz P and Sunkin, Susan M and Madisen, Linda and Daigle, Tanya L and Looger, Loren and Bernard, Amy and Phillips, John and Lein, Ed and Hawrylycz, Michael and Svoboda, Karel and Jones, Allan R and Koch, Christof and Zeng, Hongkui},
  year = {2017},
  month = dec,
  doi = {10.1101/229542},
  abstract = {Neocortex contains a multitude of cell types segregated into layers and functionally distinct regions. To investigate the diversity of cell types across the mouse neocortex, we analyzed 12,714 cells from the primary visual cortex (VISp), and 9,035 cells from the anterior lateral motor cortex (ALM) by deep single-cell RNA-sequencing (scRNA-seq), identifying 116 transcriptomic cell types. These two regions represent distant poles of the neocortex and perform distinct functions. We define 50 inhibitory transcriptomic cell types, all of which are shared across both cortical regions. In contrast, 49 of 52 excitatory transcriptomic types were found in either VISp or ALM, with only three present in both. By combining single cell RNA-seq and retrograde labeling, we demonstrate correspondence between excitatory transcriptomic types and their region-specific long-range target specificity. This study establishes a combined transcriptomic and projectional taxonomy of cortical cell types from functionally distinct regions of the mouse cortex.},
  file = {Tasic et al. - 2017 - Shared and distinct transcriptomic cell types acro.pdf},
  journal = {bioRxiv},
  language = {en}
}

@article{Taylor1975,
  title = {Discriminability and the Contrafreeloading Phenomenon.},
  author = {Taylor, George T.},
  year = {1975},
  volume = {88},
  pages = {104--109},
  issn = {0021-9940},
  doi = {10.1037/h0076222},
  file = {Taylor - 1975 - Discriminability and the contrafreeloading phenome.pdf},
  journal = {Journal of Comparative and Physiological Psychology},
  language = {en},
  number = {1}
}

@article{Taylor2007,
  title = {{{TRANSFORMING THE DILEMMA}}},
  author = {Taylor, Christine and Nowak, Martin A.},
  year = {2007},
  month = oct,
  volume = {61},
  pages = {2281--2292},
  issn = {0014-3820, 1558-5646},
  doi = {10.1111/j.1558-5646.2007.00196.x},
  abstract = {How does natural selection lead to cooperation between competing individuals? The Prisoner's Dilemma captures the essence of this problem. Two players can either cooperate or defect. The payoff for mutual cooperation, R, is greater than the payoff for mutual defection, P. But a defector versus a cooperator receives the highest payoff, T, while the cooperator obtains the lowest payoff, S. Hence, the Prisoner's Dilemma is defined by the payoff ranking T {$>$} R {$>$} P {$>$} S. In a well-mixed population, defectors always have a higher expected payoff than cooperators, and therefore natural selection favors defectors. The evolution of cooperation requires specific mechanisms. Here we discuss five mechanisms for the evolution of cooperation: direct reciprocity, indirect reciprocity, kin selection, group selection and network reciprocity (or graph selection). Each mechanism leads to a transformation of the Prisoner's Dilemma payoff matrix. From the transformed matrices, we derive the fundamental conditions for the evolution of cooperation. The transformed matrices can be used in standard frameworks of evolutionary dynamics such as the replicator equation or stochastic processes of game dynamics in finite populations.},
  file = {Taylor and Nowak - 2007 - TRANSFORMING THE DILEMMA.pdf},
  journal = {Evolution},
  language = {en},
  number = {10}
}

@article{Taylor2007a,
  title = {{{TRANSFORMING THE DILEMMA}}},
  author = {Taylor, Christine and Nowak, Martin A.},
  year = {2007},
  month = oct,
  volume = {61},
  pages = {2281--2292},
  issn = {0014-3820, 1558-5646},
  doi = {10.1111/j.1558-5646.2007.00196.x},
  abstract = {How does natural selection lead to cooperation between competing individuals? The Prisoner's Dilemma captures the essence of this problem. Two players can either cooperate or defect. The payoff for mutual cooperation, R, is greater than the payoff for mutual defection, P. But a defector versus a cooperator receives the highest payoff, T, while the cooperator obtains the lowest payoff, S. Hence, the Prisoner's Dilemma is defined by the payoff ranking T {$>$} R {$>$} P {$>$} S. In a well-mixed population, defectors always have a higher expected payoff than cooperators, and therefore natural selection favors defectors. The evolution of cooperation requires specific mechanisms. Here we discuss five mechanisms for the evolution of cooperation: direct reciprocity, indirect reciprocity, kin selection, group selection and network reciprocity (or graph selection). Each mechanism leads to a transformation of the Prisoner's Dilemma payoff matrix. From the transformed matrices, we derive the fundamental conditions for the evolution of cooperation. The transformed matrices can be used in standard frameworks of evolutionary dynamics such as the replicator equation or stochastic processes of game dynamics in finite populations.},
  file = {Taylor and Nowak - 2007 - TRANSFORMING THE DILEMMA 2.pdf},
  journal = {Evolution},
  language = {en},
  number = {10}
}

@article{Teglas2011,
  title = {Pure {{Reasoning}} in 12-{{Month}}-{{Old Infants}} as {{Probabilistic Inference}}},
  author = {Teglas, E. and Vul, E. and Girotto, V. and Gonzalez, M. and Tenenbaum, J. B. and Bonatti, L. L.},
  year = {2011},
  month = may,
  volume = {332},
  pages = {1054--1059},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1196404},
  file = {Teglas et al. - 2011 - Pure Reasoning in 12-Month-Old Infants as Probabil.pdf},
  journal = {Science},
  language = {en},
  number = {6033}
}

@article{Tejedor2012,
  title = {Optimizing {{Persistent Random Searches}}},
  author = {Tejedor, Vincent and Voituriez, Raphael and B{\'e}nichou, Olivier},
  year = {2012},
  month = feb,
  volume = {108},
  pages = {088103},
  issn = {0031-9007, 1079-7114},
  doi = {10.1103/PhysRevLett.108.088103},
  file = {Tejedor et al. - 2012 - Optimizing Persistent Random Searches.pdf},
  journal = {Phys. Rev. Lett.},
  language = {en},
  number = {8}
}

@article{Tenenbaum2006,
  title = {Theory-Based {{Bayesian}} Models of Inductive Learning and Reasoning},
  author = {Tenenbaum, Joshua B. and Griffiths, Thomas L. and Kemp, Charles},
  year = {2006},
  month = jul,
  volume = {10},
  pages = {309--318},
  issn = {13646613},
  doi = {10.1016/j.tics.2006.05.009},
  file = {Tenenbaum et al. - 2006 - Theory-based Bayesian models of inductive learning.pdf},
  journal = {Trends in Cognitive Sciences},
  language = {en},
  number = {7}
}

@article{Tercariol2007,
  title = {The Influence of Memory in Deterministic Walks in Random Media: Analytical Calculation within a Mean Field Approximation},
  shorttitle = {The Influence of Memory in Deterministic Walks in Random Media},
  author = {Tercariol, Cesar Augusto Sangaletti and Martinez, Alexandre Souto},
  year = {2007},
  month = jun,
  volume = {75},
  pages = {061117},
  issn = {1539-3755, 1550-2376},
  doi = {10.1103/PhysRevE.75.061117},
  abstract = {Consider a random medium consisting of points randomly distributed so that there is no correlation among the distances. This is the random link model, which is the high dimensionality limit (mean field approximation) for the euclidean random point structure. In the random link model, at discrete time steps, the walker moves to the nearest site, which has not been visited in the last \$\textbackslash mu\$ steps (memory), producing a deterministic partially self avoiding walk (the tourist walk). We have obtained analitically the distribution of the number \$n\$ of points explored by a walker with memory \$\textbackslash mu = 2\$, as well as the transient and period joint distribution. This result enables to explain the abrupt change in the exploratory behavior between the cases \$\textbackslash mu = 1\$ (memoryless, driven by extremal statistics) and \$\textbackslash mu = 2\$ (with memory, driven by combinatorial statistics). In the \$\textbackslash mu = 1\$ case, the mean newly visited points in the thermodynamic limit \$(N \textbackslash gg 1)\$ is just \${$<$}n {$>$} = e = 2.72 ...\$ while in the \$\textbackslash mu = 2\$ case, the mean number \${$<$}n{$>\$$} of visited points is proportional to \$N\^\{1/2\}\$. Also, this result allows us to stabilish an equivalence between the random link model with \$\textbackslash mu=2\$ and random map (uncorrelated back and forth distances) with \$\textbackslash mu=0\$ and the drastic change between the cases where the transient time is null compared to non-null transient times.},
  archiveprefix = {arXiv},
  eprint = {0806.2557},
  eprinttype = {arxiv},
  file = {Tercariol and Martinez - 2007 - The influence of memory in deterministic walks in .pdf},
  journal = {Phys. Rev. E},
  keywords = {Condensed Matter - Disordered Systems and Neural Networks,Condensed Matter - Statistical Mechanics},
  language = {en},
  number = {6}
}

@article{Tercariol2008,
  title = {Influence of Memory in Deterministic Walks in Random Media: {{Analytical}} Calculation within a Mean-Field Approximation},
  shorttitle = {Influence of Memory in Deterministic Walks in Random Media},
  author = {Ter{\c c}ariol, C{\'e}sar Augusto Sangaletti and Martinez, Alexandre Souto},
  year = {2008},
  month = sep,
  volume = {78},
  pages = {031111},
  issn = {1539-3755, 1550-2376},
  doi = {10.1103/PhysRevE.78.031111},
  file = {Terçariol and Martinez - 2008 - Influence of memory in deterministic walks in rand.pdf},
  journal = {Phys. Rev. E},
  language = {en},
  number = {3}
}

@article{Terman2002,
  title = {Activity {{Patterns}} in a {{Model}} for the {{Subthalamopallidal Network}} of the {{Basal Ganglia}}},
  author = {Terman, D. and Rubin, J. E. and Yew, A. C. and Wilson, C. J.},
  year = {2002},
  month = apr,
  volume = {22},
  pages = {2963--2976},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.22-07-02963.2002},
  file = {2002 - Terman et al. - Activity patterns in a model for the subthalamopallidal network of the basal ganglia.pdf},
  journal = {The Journal of Neuroscience},
  language = {en},
  number = {7}
}

@article{Thalmeier2016,
  title = {Learning Universal Computations with Spikes},
  author = {Thalmeier, Dominik and Uhlmann, Marvin and Kappen, Hilbert J. and Memmesheimer, Raoul-Martin},
  year = {2016},
  month = jun,
  volume = {12},
  pages = {e1004895},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1004895},
  abstract = {Providing the neurobiological basis of information processing in higher animals, spiking neural networks must be able to learn a variety of complicated computations, including the generation of appropriate, possibly delayed reactions to inputs and the self-sustained generation of complex activity patterns, e.g. for locomotion. Many such computations require previous building of intrinsic world models. Here we show how spiking neural networks may solve these different tasks. Firstly, we derive constraints under which classes of spiking neural networks lend themselves to substrates of powerful general purpose computing. The networks contain dendritic or synaptic nonlinearities and have a constrained connectivity. We then combine such networks with learning rules for outputs or recurrent connections. We show that this allows to learn even difficult benchmark tasks such as the self-sustained generation of desired low-dimensional chaotic dynamics or memory-dependent computations. Furthermore, we show how spiking networks can build models of external world systems and use the acquired knowledge to control them.},
  archiveprefix = {arXiv},
  eprint = {1505.07866},
  eprinttype = {arxiv},
  file = {2015 - Thalmeier et al. - Learning universal computations with spikes.pdf;Thalmeier et al. - 2016 - Learning universal computations with spikes.pdf},
  journal = {PLOS Computational Biology},
  keywords = {Quantitative Biology - Neurons and Cognition},
  language = {en},
  number = {6}
}

@article{Thirion2006,
  title = {Inverse Retinotopy: {{Inferring}} the Visual Content of Images from Brain Activation Patterns},
  shorttitle = {Inverse Retinotopy},
  author = {Thirion, Bertrand and Duchesnay, Edouard and Hubbard, Edward and Dubois, Jessica and Poline, Jean-Baptiste and Lebihan, Denis and Dehaene, Stanislas},
  year = {2006},
  month = dec,
  volume = {33},
  pages = {1104--1116},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2006.06.062},
  file = {2006 - Thirion et al. - Inverse retinotopy inferring the visual content of images from brain activation patterns.pdf},
  journal = {NeuroImage},
  language = {en},
  number = {4}
}

@article{Thomson2003,
  title = {Interlaminar {{Connections}} in the {{Neocortex}}},
  author = {Thomson, A. M.},
  year = {2003},
  month = jan,
  volume = {13},
  pages = {5--14},
  issn = {14602199},
  doi = {10.1093/cercor/13.1.5},
  file = {2003 - Thomson, Bannister - Interlaminar connections in the neocortex.pdf},
  journal = {Cerebral Cortex},
  language = {en},
  number = {1}
}

@article{Thomson2010,
  title = {Neocortical Layer 6, a Review},
  author = {{Thomson}},
  year = {2010},
  issn = {16625129},
  doi = {10.3389/fnana.2010.00013},
  abstract = {This review attempts to summarise some of the major areas of neocortical research as it pertains to neocortical layer 6. After a brief summary of the development of this intriguing layer, the major pyramidal cell classes to be found in layer 6 are described and compared.The connections made and received by these different classes of neurones are then discussed and the possible functions of these connections, with particular reference to the shaping of responses in visual cortex and thalamus. Inhibition in layer 6 is discussed where appropriate, but not in great detail. Many types of interneurones are to be found in each cortical layer and layer 6 is no exception, but the functions of each type remain to be elucidated (Gonchar et al., 2007).},
  file = {2010 - Thomson - Neocortical layer 6, a review.pdf},
  journal = {Frontiers in Neuroanatomy},
  language = {en}
}

@techreport{Thornquist2020,
  title = {Biochemical Computation Underlying Behavioral Decision-Making},
  author = {Thornquist, Stephen C. and Pitsch, Maximilian J. and Auth, Charlotte S. and Crickmore, Michael A.},
  year = {2020},
  month = mar,
  institution = {{Neuroscience}},
  doi = {10.1101/2020.03.14.992057},
  abstract = {Abstract           Computations in the brain are broadly assumed to emerge from patterns of fast electrical activity. Challenging this view, we show that a male fly's decision to persist in mating, even through a potentially lethal threat, hinges on biochemical computations that enable processing over minutes to hours. Each neuron in a recurrent network measuring time into mating contains slightly different internal molecular estimates of elapsed time. Protein Kinase A (PKA) activity contrasts this internal measurement with input from the other neurons to represent evidence that the network's goal has been achieved. When consensus is reached, PKA pushes the network toward a large-scale and synchronized burst of calcium influx, which we call an eruption. The eruption functions like an action potential at the level of the network, transforming deliberation within the network into an all-or-nothing output, after which the male will no longer sacrifice his life to continue mating. We detail the continuous transformation between interwoven molecular and electrical information over long timescales in this system, showing how biochemical activity, invisible to most large scale recording techniques, is the key computational currency directing a life-or-death decision.},
  file = {Thornquist et al. - 2020 - Biochemical computation underlying behavioral deci.pdf},
  language = {en},
  type = {Preprint}
}

@article{Thrun,
  title = {Is {{Learning The}} N-Th {{Thing Any Easier Than Learning The First}}?},
  author = {Thrun, Sebastian},
  pages = {7},
  abstract = {This paper investigates learning in a lifelong context. Lifelong learning addresses situations in which a learner faces a whole stream of learning tasks. Such scenarios provide the opportunity to transfer knowledge across multiple learning tasks, in order to generalize more accurately from less training data. In this paper, several different approaches to lifelong learning are described, and applied in an object recognition domain. It is shown that across the board, lifelong learning approaches generalize consistently more accurately from less training data, by their ability to transfer knowledge across learning tasks.},
  file = {Thrun - Is Learning The n-th Thing Any Easier Than Learnin.pdf},
  language = {en}
}

@article{Thrun1992,
  title = {Eficient {{Exploration In Reinforcement Learning}}},
  author = {Thrun, Sebastian B},
  year = {1992},
  pages = {44},
  abstract = {Exploration plays a fundamental role in any active learning system. This study evaluates the role of exploration in active learning and describes several local techniques for exploration in nite, discrete domains, embedded in a reinforcement learning framework (delayed reinforcement).},
  file = {Thrun - E cient Exploration In Reinforcement Learning.pdf},
  journal = {NIPS},
  language = {en}
}

@article{Thrun1992a,
  title = {Active {{Exploration}} in {{Dynamic Environments}}},
  author = {Thrun, Sebastian and M{\"o}ller, Knut},
  year = {1992},
  pages = {531--538},
  abstract = {Vhenever an agent learns to control an unknown environment, two opposing principles have to be combined, namely: exploration (long-term optimization) and exploitation (short-term optimization). Many real-valued connectionist approaches to learning control realize exploration by randomness in action selection. This might be disadvantageous when costs are assigned to "negative experiences" . The basic idea presented in this paper is to make an agent explore unknown regions in a more directed manner. This is achieved by a so-called competence map, which is trained to predict the controller's accuracy, and is used for guiding exploration. Based on this, a bistable system enables smoothly switching attention between two behaviors - exploration and exploitation - depending on expected costs and knowledge gain.},
  file = {Thrun and Möller - Active Exploration in Dynamic Environments.pdf},
  journal = {Advances in neural information processing systems},
  language = {en}
}

@article{Thrun1992b,
  title = {Efficient {{Exploration In Reinforcement Learning}}},
  author = {Thrun, Sebastian B},
  year = {1992},
  pages = {1--44},
  abstract = {Exploration plays a fundamental role in any active learning system. This study evaluates the role of exploration in active learning and describes several local techniques for exploration in nite, discrete domains, embedded in a reinforcement learning framework (delayed reinforcement).},
  file = {Thrun - E cient Exploration In Reinforcement Learning 2.pdf},
  journal = {Technical Report},
  language = {en}
}

@incollection{Thrun1998,
  title = {Lifelong {{Learning Algorithms}}},
  booktitle = {Learning to {{Learn}}},
  author = {Thrun, Sebastian},
  editor = {Thrun, Sebastian and Pratt, Lorien},
  year = {1998},
  pages = {181--209},
  publisher = {{Springer US}},
  address = {{Boston, MA}},
  doi = {10.1007/978-1-4615-5529-2_8},
  abstract = {Machine learning has not yet succeeded in the design of robust learning algorithms that generalize well from very small datasets. In contrast, humans often generalize correctly from only a single training example, even if the number of potentially relevant features is large. To do so, they successfully exploit knowledge acquired in previous learning tasks, to bias subsequent learning.},
  file = {Thrun - 1998 - Lifelong Learning Algorithms.pdf},
  isbn = {978-1-4613-7527-2 978-1-4615-5529-2},
  language = {en}
}

@article{Thruna,
  title = {Exploration in {{Active Learning}}},
  author = {Thrun, Sebastian},
  pages = {10},
  file = {Thrun - Exploration in Active Learning.pdf},
  language = {en}
}

@article{Thura2017,
  title = {The {{Basal Ganglia Do Not Select Reach Targets}} but {{Control}} the {{Urgency}} of {{Commitment}}},
  author = {Thura, David and Cisek, Paul},
  year = {2017},
  month = aug,
  volume = {95},
  pages = {1160-1170.e5},
  issn = {08966273},
  doi = {10.1016/j.neuron.2017.07.039},
  abstract = {Prominent theories of decision making suggest that the basal ganglia (BG) play a causal role in deliberation between action choices. An alternative hypothesis is that deliberation occurs in cortical regions, while the BG control the speed-accuracy trade-off (SAT) between committing to a choice versus continuing to deliberate. Here, we test these hypotheses by recording activity in the internal and external segments of the globus pallidus (GPi/GPe) while monkeys perform a task dissociating the process of deliberation, the moment of commitment, and adjustment of the SAT. Our data suggest that unlike premotor and motor cortical regions, pallidal output does not contribute to the process of deliberation but instead provides a time-varying signal that controls the SAT and reflects the growing urgency to commit to a choice. Once a target is selected by cortical regions, GP activity confirms commitment to the decision and invigorates the subsequent movement.},
  file = {Thura and Cisek - 2017 - The Basal Ganglia Do Not Select Reach Targets but .pdf},
  journal = {Neuron},
  language = {en},
  number = {5}
}

@article{Thut2006,
  title = {-{{Band Electroencephalographic Activity}} over {{Occipital Cortex Indexes Visuospatial Attention Bias}} and {{Predicts Visual Target Detection}}},
  author = {Thut, G.},
  year = {2006},
  month = sep,
  volume = {26},
  pages = {9494--9502},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.0875-06.2006},
  file = {2006 - Thut - -Band Electroencephalographic Activity over Occipital Cortex Indexes Visuospatial Attention Bias and Predicts Visual Targe.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {37}
}

@article{Tian2010,
  title = {Cortical Depth-Specific Microvascular Dilation Underlies Laminar Differences in Blood Oxygenation Level-Dependent Functional {{MRI}} Signal},
  author = {Tian, P. and Teng, I. C. and May, L. D. and Kurz, R. and Lu, K. and Scadeng, M. and Hillman, E. M. C. and De Crespigny, A. J. and D'Arceuil, H. E. and Mandeville, J. B. and Marota, J. J. A. and Rosen, B. R. and Liu, T. T. and Boas, D. A. and Buxton, R. B. and Dale, A. M. and Devor, A.},
  year = {2010},
  month = aug,
  volume = {107},
  pages = {15246--15251},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1006735107},
  file = {2010 - Tian et al. - Cortical depth-specific microvascular dilation underlies laminar differences in blood oxygenation level-dependent f.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {34}
}

@article{Tian2019,
  title = {Luck {{Matters}}: {{Understanding Training Dynamics}} of {{Deep ReLU Networks}}},
  shorttitle = {Luck {{Matters}}},
  author = {Tian, Yuandong and Jiang, Tina and Gong, Qucheng and Morcos, Ari},
  year = {2019},
  month = jun,
  abstract = {We analyze the dynamics of training deep ReLU networks and their implications on generalization capability. Using a teacher-student setting, we discovered a novel relationship between the gradient received by hidden student nodes and the activations of teacher nodes for deep ReLU networks. With this relationship and the assumption of small overlapping teacher node activations, we prove that (1) student nodes whose weights are initialized to be close to teacher nodes converge to them at a faster rate, and (2) in over-parameterized regimes and 2-layer case, while a small set of lucky nodes do converge to the teacher nodes, the fanout weights of other nodes converge to zero. This framework provides insight into multiple puzzling phenomena in deep learning like over-parameterization, implicit regularization, lottery tickets, etc. We verify our assumption by showing that the majority of BatchNorm biases of pre-trained VGG11/13/16/19 models are negative. Experiments on (1) random deep teacher networks with Gaussian inputs, (2) teacher network pre-trained on CIFAR-10 and (3) extensive ablation studies validate our multiple theoretical predictions. Code is available at https://github.com/facebookresearch/luckmatters.},
  archiveprefix = {arXiv},
  eprint = {1905.13405},
  eprinttype = {arxiv},
  file = {Tian et al. - 2019 - Luck Matters Understanding Training Dynamics of D.pdf},
  journal = {arXiv:1905.13405 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryclass = {cs, stat}
}

@article{Tibshirani,
  title = {Valerie and {{Patrick Hastie}}},
  author = {Tibshirani, Sami and Friedman, Harry},
  pages = {764},
  file = {Tibshirani and Friedman - Valerie and Patrick Hastie.pdf},
  language = {en}
}

@article{Tiesinga2001,
  title = {Optimal Information Transfer in Synchronized Neocortical Neurons},
  author = {Tiesinga, P.H.E. and Fellous, J.-M. and Jos{\'e}, J.V. and Sejnowski, T.J.},
  year = {2001},
  month = jun,
  volume = {38-40},
  pages = {397--402},
  issn = {09252312},
  doi = {10.1016/S0925-2312(01)00464-7},
  abstract = {The output precision and information transnl\textasciitilde ssionwas studied in a model neocortical neuron that was driven by a periodic presynaptic spike train w \textasciitilde{} t ha variable number of inhibitory inputs on each cycle. Spike-timing precision was maintained during feedforward propagation during entrainment. The range of presynaptic firing rates and precision for entrainment was determined. During entrainment the Shannon information of the output spike phase was reduced but the amount of information the neuron transmitted about the synaptic input was increased. We quantify how robust information transmission is against intrinsic neuronal noise. We propose how neurotnodulation, via entrainment, can regulate the information transfer in neocortical networks. 0 2001 Elsevier Science B.V. All rights reserved.},
  file = {Tiesinga et al. - 2001 - Optimal information transfer in synchronized neoco.pdf},
  journal = {Neurocomputing},
  language = {en}
}

@article{Tiesinga2008,
  title = {Regulation of Spike Timing in Visual Cortical Circuits},
  author = {Tiesinga, Paul and Fellous, Jean-Marc and Sejnowski, Terrence J.},
  year = {2008},
  month = feb,
  volume = {9},
  pages = {97--107},
  issn = {1471-003X, 1471-0048},
  doi = {10.1038/nrn2315},
  abstract = {A train of action potentials (a spike train) can carry information in both the average firing rate and the pattern of spikes in the train. But can such a spike-pattern code be supported by cortical circuits? Neurons in vitro produce a spike pattern in response to the injection of a fluctuating current. However, cortical neurons in vivo are modulated by local oscillatory neuronal activity and by top-down inputs. In a cortical circuit, precise spike patterns thus reflect the interaction between internally generated activity and sensory information encoded by input spike trains. We review the evidence for precise and reliable spike timing in the cortex and discuss its computational role.},
  file = {2008 - Tiesinga, Fellous, Sejnowski - Regulation of spike timing in visual cortical circuits.pdf},
  journal = {Nature Reviews Neuroscience},
  language = {en},
  number = {2}
}

@article{Tiesinga2010,
  title = {Mechanisms for {{Phase Shifting}} in {{Cortical Networks}} and Their {{Role}} in {{Communication}} through {{Coherence}}},
  author = {Tiesinga, Paul H. and Sejnowski, Terrence J.},
  year = {2010},
  volume = {4},
  issn = {1662-5161},
  doi = {10.3389/fnhum.2010.00196},
  abstract = {In the primate visual cortex, the phase of spikes relative to oscillations in the local field potential (LFP) in the gamma frequency range (30\textendash 80 Hz) can be shifted by stimulus features such as orientation and thus the phase may carry information about stimulus identity. According to the principle of communication through coherence (CTC), the relative LFP phase between the LFPs in the sending and receiving circuits affects the effectiveness of the transmission. CTC predicts that phase shifting can be used for stimulus selection. We review and investigate phase shifting in models of periodically driven single neurons and compare it with phase shifting in models of cortical networks. In a single neuron, as the driving current is increased, the spike phase varies systematically while the firing rate remains constant. In a network model of reciprocally connected excitatory (E) and inhibitory (I) cells phase shifting occurs in response to both injection of constant depolarizing currents and to brief pulses to I cells. These simple models provide an account for phase-shifting observed experimentally and suggest a mechanism for implementing CTC. We discuss how this hypothesis can be tested experimentally using optogenetic techniques.},
  file = {2010 - Tiesinga, Sejnowski - Mechanisms for Phase Shifting in Cortical Networks and their Role in Communication through Coherence.pdf},
  journal = {Frontiers in Human Neuroscience},
  language = {en}
}

@article{Timms2014,
  title = {Synchronization in Phase-Coupled {{Kuramoto}} Oscillator Networks with Axonal Delay and Synaptic Plasticity},
  author = {Timms, L. and English, L. Q.},
  year = {2014},
  month = mar,
  volume = {89},
  issn = {1539-3755, 1550-2376},
  doi = {10.1103/PhysRevE.89.032906},
  file = {2014 - Timms, English - Synchronization in phase-coupled Kuramoto oscillator networks with axonal delay and synaptic plasticity.pdf;Timms and English - 2014 - Synchronization in phase-coupled Kuramoto oscillat.pdf},
  journal = {Physical Review E},
  language = {en},
  number = {3}
}

@article{Tingley2016,
  title = {Transformation of {{Independent Oscillatory Inputs}} into {{Temporally Precise Rate Codes}}},
  author = {Tingley, David and Alexander, Andrew and Quinn, Laleh and Chiba, Andrea and Nitz, Douglas},
  year = {2016},
  month = may,
  doi = {10.1101/054163},
  abstract = {Complex behaviors demand temporal coordination among functionally distinct brain regions. The basal forebrain's afferent and efferent structure suggests a capacity for mediating such coordination. During performance of a selective attention task, synaptic activity in this region was dominated by four amplitude-oscillations temporally organized by the phase of the slowest, a theta rhythm. Further, oscillatory amplitudes were precisely organized by task epoch and a robust input/output transform, from synchronous synaptic activity to spiking rates of basal forebrain neurons, was identified. For many neurons, spiking was temporally organized as phase precessing sequences against theta band field potential oscillations. Remarkably, theta phase precession advanced in parallel to task progression, rather than absolute spatial location or time. Together, the findings reveal a process by which associative brain regions can integrate independent oscillatory inputs and transform them into sequence-specific, rate-coded outputs that are adaptive to the pace with which organisms interact with their environment.},
  file = {2013 - Tobergte, Curtis - Transformation!of!Independent!Oscillatory!Inputs!into!Temporally!Precise!Rate!Codes'.pdf;Tingley et al. - 2016 - Transformation of Independent Oscillatory Inputs i.pdf},
  journal = {bioRxiv},
  language = {en}
}

@article{Tinkhauser2017,
  title = {Beta Burst Dynamics in {{Parkinson}}'s Disease {{OFF}} and {{ON}} Dopaminergic Medication},
  author = {Tinkhauser, Gerd and Pogosyan, Alek and Tan, Huiling and Herz, Damian M and K{\"u}hn, Andrea A and Brown, Peter},
  year = {2017},
  month = nov,
  volume = {140},
  pages = {2968--2981},
  issn = {0006-8950, 1460-2156},
  doi = {10.1093/brain/awx252},
  file = {Tinkhauser et al. - 2017 - Beta burst dynamics in Parkinson’s disease OFF and.pdf},
  journal = {Brain},
  language = {en},
  number = {11}
}

@article{Tishby2000,
  title = {The {{Information Bottleneck Method}}},
  author = {Tishby, Naftali and Pereira, Fernando C and Bialek, William},
  year = {2000},
  volume = {0004057},
  pages = {11},
  abstract = {We define the relevant information in a signal x {$\in$} X as being the information that this signal provides about another signal y {$\in$} Y . Examples include the information that face images provide about the names of the people portrayed, or the information that speech sounds provide about the words spoken. Understanding the signal x requires more than just predicting y, it also requires specifying which features of X play a role in the prediction. We formalize the problem as that of finding a short code for X that preserves the maximum information about Y . That is, we squeeze the information that X provides about Y through a `bottleneck' formed by a limited set of codewords X\texttildelow{} . This constrained optimization problem can be seen as a generalization of rate distortion theory in which the distortion measure d(x, x\texttildelow ) emerges from the joint statistics of X and Y . The approach yields an exact set of self-consistent equations for the coding rules X \textrightarrow{} X\texttildelow{} and X\texttildelow{} \textrightarrow{} Y . Solutions to these equations can be found by a convergent re-estimation method that generalizes the Blahut\textendash Arimoto algorithm. Our variational principle provides a surprisingly rich framework for discussing a variety of problems in signal processing and learning, as will be described in detail elsewhere.},
  file = {2000 - Tishby, Pereira, Bialek - The information bottleneck method.pdf;2011 - Thomas - The Knowledge Link How Firms Compete through Strategic Alliances.pdf;Tishby et al. - The Information Bottleneck Method.pdf},
  journal = {Arxiv},
  keywords = {Computer Science - Machine Learning,Condensed Matter - Disordered Systems and Neural Networks,Nonlinear Sciences - Adaptation and Self-Organizing Systems,Physics - Data Analysis; Statistics and Probability},
  language = {en}
}

@article{Tishby2015,
  title = {Deep {{Learning}} and the {{Information Bottleneck Principle}}},
  author = {Tishby, Naftali and Zaslavsky, Noga},
  year = {2015},
  month = mar,
  abstract = {Deep Neural Networks (DNNs) are analyzed via the theoretical framework of the information bottleneck (IB) principle. We first show that any DNN can be quantified by the mutual information between the layers and the input and output variables. Using this representation we can calculate the optimal information theoretic limits of the DNN and obtain finite sample generalization bounds. The advantage of getting closer to the theoretical limit is quantifiable both by the generalization bound and by the network's simplicity. We argue that both the optimal architecture, number of layers and features/connections at each layer, are related to the bifurcation points of the information bottleneck tradeoff, namely, relevant compression of the input layer with respect to the output layer. The hierarchical representations at the layered network naturally correspond to the structural phase transitions along the information curve. We believe that this new insight can lead to new optimality bounds and deep learning algorithms.},
  archiveprefix = {arXiv},
  eprint = {1503.02406},
  eprinttype = {arxiv},
  file = {Tishby and Zaslavsky - 2015 - Deep Learning and the Information Bottleneck Princ.pdf},
  journal = {arXiv:1503.02406 [cs]},
  keywords = {Computer Science - Machine Learning},
  language = {en},
  primaryclass = {cs}
}

@article{Tognoli2014,
  title = {Enlarging the Scope: Grasping Brain Complexity},
  shorttitle = {Enlarging the Scope},
  author = {Tognoli, Emmanuelle and Kelso, J. A. Scott},
  year = {2014},
  month = jun,
  volume = {8},
  issn = {1662-5137},
  doi = {10.3389/fnsys.2014.00122},
  file = {Tognoli and Kelso - 2014 - Enlarging the scope grasping brain complexity.pdf},
  journal = {Frontiers in Systems Neuroscience},
  language = {en}
}

@article{Tognoli2014a,
  title = {The {{Metastable Brain}}},
  author = {Tognoli, Emmanuelle and Kelso, J. A. Scott},
  year = {2014},
  month = jan,
  volume = {81},
  pages = {35--48},
  issn = {08966273},
  doi = {10.1016/j.neuron.2013.12.022},
  file = {Tognoli and Kelso - 2014 - The Metastable Brain.pdf},
  journal = {Neuron},
  language = {en},
  number = {1}
}

@article{Tokdar2010,
  title = {Detection of Bursts in Extracellular Spike Trains Using Hidden Semi-{{Markov}} Point Process Models},
  author = {Tokdar, Surya and Xi, Peiyi and Kelly, Ryan C. and Kass, Robert E.},
  year = {2010},
  month = aug,
  volume = {29},
  pages = {203--212},
  issn = {0929-5313, 1573-6873},
  doi = {10.1007/s10827-009-0182-2},
  file = {2010 - Tokdar et al. - Detection of bursts in extracellular spike trains using hidden semi-Markov point process models.pdf},
  journal = {Journal of Computational Neuroscience},
  language = {en},
  number = {1-2}
}

@article{Tomasello2019,
  title = {Thirty Years of Great Ape Gestures},
  author = {Tomasello, Michael and Call, Josep},
  year = {2019},
  month = jul,
  volume = {22},
  pages = {461--469},
  issn = {1435-9448, 1435-9456},
  doi = {10.1007/s10071-018-1167-1},
  abstract = {We and our colleagues have been doing studies of great ape gestural communication for more than 30 years. Here we attempt to spell out what we have learned. Some aspects of the process have been reliably established by multiple researchers, for example, its intentional structure and its sensitivity to the attentional state of the recipient. Other aspects are more controversial. We argue here that it is a mistake to assimilate great ape gestures to the species-typical displays of other mammals by claiming that they are fixed action patterns, as there are many differences, including the use of attention-getters. It is also a mistake, we argue, to assimilate great ape gestures to human gestures by claiming that they are used referentially and declaratively in a human-like manner, as apes' ``pointing'' gesture has many limitations and they do not gesture iconically. Great ape gestures constitute a unique form of primate communication with their own unique qualities.},
  file = {Tomasello and Call - 2019 - Thirty years of great ape gestures.pdf},
  journal = {Anim Cogn},
  language = {en},
  number = {4}
}

@article{Tomasello2019a,
  title = {Thirty Years of Great Ape Gestures},
  author = {Tomasello, Michael and Call, Josep},
  year = {2019},
  month = jul,
  volume = {22},
  pages = {461--469},
  issn = {1435-9448, 1435-9456},
  doi = {10.1007/s10071-018-1167-1},
  abstract = {We and our colleagues have been doing studies of great ape gestural communication for more than 30 years. Here we attempt to spell out what we have learned. Some aspects of the process have been reliably established by multiple researchers, for example, its intentional structure and its sensitivity to the attentional state of the recipient. Other aspects are more controversial. We argue here that it is a mistake to assimilate great ape gestures to the species-typical displays of other mammals by claiming that they are fixed action patterns, as there are many differences, including the use of attention-getters. It is also a mistake, we argue, to assimilate great ape gestures to human gestures by claiming that they are used referentially and declaratively in a human-like manner, as apes' ``pointing'' gesture has many limitations and they do not gesture iconically. Great ape gestures constitute a unique form of primate communication with their own unique qualities.},
  file = {Tomasello and Call - 2019 - Thirty years of great ape gestures 2.pdf},
  journal = {Anim Cogn},
  language = {en},
  number = {4}
}

@article{Tompson2014,
  title = {Efficient {{Object Localization Using Convolutional Networks}}},
  author = {Tompson, Jonathan and Goroshin, Ross and Jain, Arjun and LeCun, Yann and Bregler, Christopher},
  year = {2014},
  month = nov,
  abstract = {Recent state-of-the-art performance on human-body pose estimation has been achieved with Deep Convolutional Networks (ConvNets). Traditional ConvNet architectures include pooling layers which reduce computational requirements, introduce invariance and prevent over-training. These benefits of pooling come at the cost of reduced localization accuracy. We introduce a novel architecture which includes an efficient `position refinement' model that is trained to estimate the joint offset location within a small region of the image. This refinement model is jointly trained in cascade with a state-of-the-art ConvNet model [21] to achieve improved accuracy in human joint location estimation. We show that the variance of our detector approaches the variance of human annotations on the FLIC [20] dataset and outperforms all existing approaches on the MPII-human-pose dataset [1].},
  archiveprefix = {arXiv},
  eprint = {1411.4280},
  eprinttype = {arxiv},
  file = {Tompson et al. - 2014 - Efficient Object Localization Using Convolutional .pdf},
  journal = {arXiv:1411.4280 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  language = {en},
  primaryclass = {cs}
}

@article{Tort2010,
  title = {Measuring {{Phase}}-{{Amplitude Coupling Between Neuronal Oscillations}} of {{Different Frequencies}}},
  author = {Tort, Adriano B. L. and Komorowski, Robert and Eichenbaum, Howard and Kopell, Nancy},
  year = {2010},
  month = aug,
  volume = {104},
  pages = {1195--1210},
  issn = {0022-3077, 1522-1598},
  doi = {10.1152/jn.00106.2010},
  file = {2010 - Tort et al. - Measuring phase-amplitude coupling between neuronal oscillations of different frequencies(2).pdf},
  journal = {Journal of Neurophysiology},
  language = {en},
  number = {2}
}

@article{Tort2018,
  title = {Parallel Detection of Theta and Respiration-Coupled Oscillations throughout the Mouse Brain},
  author = {Tort, Adriano B. L. and Ponsel, Simon and Jessberger, Jakob and Yanovsky, Yevgenij and Branka{\v c}k, Jurij and Draguhn, Andreas},
  year = {2018},
  month = dec,
  volume = {8},
  issn = {2045-2322},
  doi = {10.1038/s41598-018-24629-z},
  file = {Tort et al. - 2018 - Parallel detection of theta and respiration-couple.pdf},
  journal = {Scientific Reports},
  language = {en},
  number = {1}
}

@article{Toth2011,
  title = {Dynamical Estimation of Neuron and Network Properties {{I}}: Variational Methods},
  shorttitle = {Dynamical Estimation of Neuron and Network Properties {{I}}},
  author = {Toth, Bryan A. and Kostuk, Mark and Meliza, C. Daniel and Margoliash, Daniel and Abarbanel, Henry D. I.},
  year = {2011},
  month = oct,
  volume = {105},
  pages = {217--237},
  issn = {0340-1200, 1432-0770},
  doi = {10.1007/s00422-011-0459-1},
  file = {2011 - Toth et al. - Dynamical estimation of neuron and network properties I Variational methods.pdf},
  journal = {Biological Cybernetics},
  language = {en},
  number = {3-4}
}

@article{Touboul2008,
  title = {Bifurcation {{Analysis}} of a {{General Class}} of {{Nonlinear Integrate}}-and-{{Fire Neurons}}},
  author = {Touboul, Jonathan},
  year = {2008},
  month = jan,
  volume = {68},
  pages = {1045--1079},
  issn = {0036-1399, 1095-712X},
  doi = {10.1137/070687268},
  abstract = {In this paper we define a class of formal neuron models being computationally efficient and biologically plausible, i.e., able to reproduce a wide range of behaviors observed in in vivo or in vitro recordings of cortical neurons. This class includes, for instance, two models widely used in computational neuroscience, the Izhikevich and the Brette\textendash Gerstner models. These models consist of a 4-parameter dynamical system. We provide the full local bifurcation diagram of the members of this class and show that they all present the same bifurcations: an Andronov\textendash Hopf bifurcation manifold, a saddle-node bifurcation manifold, a Bogdanov\textendash Takens bifurcation, and possibly a Bautin bifurcation, i.e., all codimension two local bifurcations in a two-dimensional phase space except the cusp. Among other global bifurcations, this system shows a saddle homoclinic bifurcation curve. We show how this bifurcation diagram generates the most prominent cortical neuron behaviors. This study leads us to introduce a new neuron model, the quartic model, able to reproduce among all the behaviors of the Izhikevich and Brette\textendash Gerstner models self-sustained subthreshold oscillations, which are of great interest in neuroscience.},
  file = {2008 - Touboul - Bifurcation Analysis of a General Class of Nonlinear Integrate-and-Fire Neurons.pdf},
  journal = {SIAM Journal on Applied Mathematics},
  language = {en},
  number = {4}
}

@article{Touboul2008a,
  title = {Dynamics and Bifurcations of the Adaptive Exponential Integrate-and-Fire Model},
  author = {Touboul, Jonathan and Brette, Romain},
  year = {2008},
  month = nov,
  volume = {99},
  pages = {319--334},
  issn = {0340-1200, 1432-0770},
  doi = {10.1007/s00422-008-0267-4},
  abstract = {Recently, several two-dimensional spiking neuron models have been introduced, with the aim of reproducing the diversity of electrophysiological features displayed by real neurons while keeping a simple model, for simulation and analysis purposes. Among these models, the adaptive integrate-and-fire model is physiologically relevant in that its parameters can be easily related to physiological quantities. The interaction of the differential equations with the reset results in a rich and complex dynamical structure. We relate the subthreshold features of the model to the dynamical properties of the differential system and the spike patterns to the properties of a Poincare\textasciiacute{} map defined by the sequence of spikes. We find a complex bifurcation structure which has a direct interpretation in terms of spike trains. For some parameter values, spike patterns are chaotic.},
  file = {2008 - Touboul, Brette - Dynamics and bifurcations of the adaptive exponential integrate-and-fire model.pdf},
  journal = {Biological Cybernetics},
  language = {en},
  number = {4-5}
}

@article{Touboul2011,
  title = {Finite-Size and Correlation-Induced Effects in Mean-Field Dynamics},
  author = {Touboul, Jonathan D. and Ermentrout, G. Bard},
  year = {2011},
  month = nov,
  volume = {31},
  pages = {453--484},
  issn = {0929-5313, 1573-6873},
  doi = {10.1007/s10827-011-0320-5},
  file = {2011 - Touboul, Ermentrout - Finite-size and correlation-induced effects in mean-field dynamics(2).pdf},
  journal = {Journal of Computational Neuroscience},
  language = {en},
  number = {3}
}

@article{Touboul2012,
  title = {Mean-Field Equations for Stochastic Firing-Rate Neural Fields with Delays: {{Derivation}} and Noise-Induced Transitions},
  shorttitle = {Mean-Field Equations for Stochastic Firing-Rate Neural Fields with Delays},
  author = {Touboul, Jonathan},
  year = {2012},
  month = aug,
  volume = {241},
  pages = {1223--1244},
  issn = {01672789},
  doi = {10.1016/j.physd.2012.03.010},
  abstract = {In this manuscript we analyze the collective behavior of mean-field limits of large-scale, spatially extended stochastic neuronal networks with delays. Rigorously, the asymptotic regime of such systems is characterized by a very intricate stochastic delayed integro-differential McKean\textendash Vlasov equation that remain impenetrable, leaving the stochastic collective dynamics of such networks poorly understood. In order to study these macroscopic dynamics, we analyze networks of firing-rate neurons, i.e. with linear intrinsic dynamics and sigmoidal interactions. In that case, we prove that the solution of the mean-field equation is Gaussian, hence characterized by its two first moments, and that these two quantities satisfy a set of coupled delayed integro-differential equations. These equations are similar to usual neural field equations, and incorporate noise levels as a parameter, allowing analysis of noise-induced transitions. We identify through bifurcation analysis several qualitative transitions due to noise in the mean-field limit. In particular, stabilization of spatially homogeneous solutions, synchronized oscillations, bumps, chaotic dynamics, wave or bump splitting are exhibited and arise from static or dynamic Turing\textendash Hopf bifurcations. These surprising phenomena allow further exploring the role of noise in the nervous system. \textcopyright{} 2012 Elsevier B.V. All rights reserved.},
  file = {2012 - Touboul - Mean-field equations for stochastic firing-rate neural fields with delays Derivation and noise-induced transitions.pdf},
  journal = {Physica D: Nonlinear Phenomena},
  language = {en},
  number = {15}
}

@article{Touboul2017,
  title = {Power-Law Statistics and Universal Scaling in the Absence of Criticality},
  author = {Touboul, Jonathan and Destexhe, Alain},
  year = {2017},
  month = jan,
  volume = {95},
  pages = {012413},
  issn = {2470-0045, 2470-0053},
  doi = {10.1103/PhysRevE.95.012413},
  abstract = {Critical states are sometimes identified experimentally through power-law statistics or universal scaling functions. We show here that such features naturally emerge from networks in self-sustained irregular regimes away from criticality. In these regimes, statistical physics theory of large interacting systems predict a regime where the nodes have independent and identically distributed dynamics. We thus investigated the statistics of a system in which units are replaced by independent stochastic surrogates, and found the same power-law statistics, indicating that these are not sufficient to establish criticality. We rather suggest that these are universal features of large-scale networks when considered macroscopically. These results put caution on the interpretation of scaling laws found in nature.},
  archiveprefix = {arXiv},
  eprint = {1503.08033},
  eprinttype = {arxiv},
  file = {Touboul and Destexhe - 2017 - Power-law statistics and universal scaling in the .pdf},
  journal = {Phys. Rev. E},
  keywords = {Quantitative Biology - Neurons and Cognition},
  language = {en},
  number = {1}
}

@article{Toupo2014,
  title = {Limit {{Cycles Sparked}} by {{Mutation}} in the {{Repeated Prisoner}}'s {{Dilemma}}},
  author = {Toupo, Danielle F. P. and Rand, David G. and Strogatz, Steven H.},
  year = {2014},
  month = dec,
  volume = {24},
  pages = {1430035},
  issn = {0218-1274, 1793-6551},
  doi = {10.1142/S0218127414300353},
  file = {Toupo et al. - 2014 - Limit Cycles Sparked by Mutation in the Repeated P.pdf},
  journal = {International Journal of Bifurcation and Chaos},
  language = {en},
  number = {12}
}

@article{Toupo2015,
  title = {Evolutionary Game Dynamics of Controlled and Automatic Decision-Making},
  author = {Toupo, Danielle F. P. and Strogatz, Steven H. and Cohen, Jonathan D. and Rand, David G.},
  year = {2015},
  month = jul,
  volume = {25},
  pages = {073120},
  issn = {1054-1500, 1089-7682},
  doi = {10.1063/1.4927488},
  file = {Toupo et al. - 2015 - Evolutionary game dynamics of controlled and autom.pdf},
  journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
  language = {en},
  number = {7}
}

@article{Toupo2015a,
  title = {Nonlinear Dynamics of the Rock-Paper-Scissors Game with Mutations},
  author = {Toupo, Danielle F. P. and Strogatz, Steven H.},
  year = {2015},
  month = may,
  volume = {91},
  issn = {1539-3755, 1550-2376},
  doi = {10.1103/PhysRevE.91.052907},
  file = {Toupo and Strogatz - 2015 - Nonlinear dynamics of the rock-paper-scissors game.pdf},
  journal = {Physical Review E},
  language = {en},
  number = {5}
}

@article{Toyoizumi2006,
  title = {Fisher {{Information}} for {{Spike}}-{{Based Population Decoding}}},
  author = {Toyoizumi, Taro and Aihara, Kazuyuki and Amari, Shun-ichi},
  year = {2006},
  month = aug,
  volume = {97},
  issn = {0031-9007, 1079-7114},
  doi = {10.1103/PhysRevLett.97.098102},
  file = {2006 - Toyoizumi, Aihara, Amari - Fisher information for spike-based population decoding.pdf},
  journal = {Physical Review Letters},
  language = {en},
  number = {9}
}

@article{Toyoizumi2011,
  title = {Beyond the Edge of Chaos: {{Amplification}} and Temporal Integration by Recurrent Networks in the Chaotic Regime},
  shorttitle = {Beyond the Edge of Chaos},
  author = {Toyoizumi, T. and Abbott, L. F.},
  year = {2011},
  month = nov,
  volume = {84},
  issn = {1539-3755, 1550-2376},
  doi = {10.1103/PhysRevE.84.051908},
  file = {2011 - Toyoizumi, Abbott - Beyond the edge of chaos Amplification and temporal integration by recurrent networks in the chaotic regime.pdf},
  journal = {Physical Review E},
  language = {en},
  number = {5}
}

@article{Tran2016,
  title = {Alpha Phase Dynamics Predict Age-Related Visual Working Memory Decline},
  author = {Tran, Tam T. and Hoffner, Nicole C. and LaHue, Sara C. and Tseng, Lisa and Voytek, Bradley},
  year = {2016},
  month = dec,
  volume = {143},
  pages = {196--203},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2016.08.052},
  abstract = {Alpha oscillations (7-14 Hz) are modulated in response to visual temporal and spatial cues. However, the neural response to alerting cues is less explored, as is how this response is affected by healthy aging. Using scalp EEG, we examined how visual cortical alpha activity relates to working memory performance. Younger (20-30 years) and older (60-70 years) participants were presented with a visual alerting cue uninformative of the position or size of a lateralized working memory array. Older adults showed longer response times overall and reduced accuracy when memory load was high. Older adults had less consistent cue-evoked alpha phase resetting than younger adults, which predicted worse performance. Alpha phase prior to memory array presentation predicted response time, but the relationship between phase and response time was weaker in older adults. These results suggest that changes in alpha phase dynamics, especially prior to presentation of task-relevant stimuli, potentially contribute to age-related cognitive decline.},
  file = {Tran et al. - 2016 - Alpha phase dynamics predict age-related visual wo.pdf},
  journal = {NeuroImage},
  language = {en}
}

@article{Tran2017,
  title = {Ionic {{Current Correlations Are Ubiquitous Across Phyla}}},
  author = {Tran, Trinh and Unal, Cagri T. and Zaborszky, Laszlo and Rotstein, Horacio G. and Kirkwood, Alfredo and Golowasch, Jorge P.},
  year = {2017},
  month = may,
  doi = {10.1101/137133},
  abstract = {Ionic currents, whether measured as conductance amplitude or as ion channel transcript levels, can vary many-fold within a population of identified neurons. This variability has been observed in multiple invertebrate neuronal types, but they do so in a coordinated manner such that their magnitudes are correlated. These conductance correlations are thought to reflect a tight homeostasis of cellular excitability that enhances the robustness and stability of neuronal activity over long stretches of time. Notably, although such ionic current correlations are well documented in invertebrates, they have not been reported in vertebrates. Here we demonstrate with two examples, identified mouse hippocampal granule cells and cholinergic basal forebrain neurons, that ionic current correlations is a ubiquitous phenomenon expressed by a number of species across phyla.},
  file = {Tran et al. - 2017 - Ionic Current Correlations Are Ubiquitous Across P 2.pdf;Tran et al. - 2017 - Ionic Current Correlations Are Ubiquitous Across P.pdf},
  journal = {bioRxiv},
  language = {en}
}

@article{Tran2019,
  title = {Ionic Current Correlations Are Ubiquitous across Phyla},
  author = {Tran, Trinh and Unal, Cagri T. and Severin, Daniel and Zaborszky, Laszlo and Rotstein, Horacio G. and Kirkwood, Alfredo and Golowasch, Jorge},
  year = {2019},
  month = dec,
  volume = {9},
  pages = {1687},
  issn = {2045-2322},
  doi = {10.1038/s41598-018-38405-6},
  file = {Tran et al. - 2019 - Ionic current correlations are ubiquitous across p.pdf},
  journal = {Sci Rep},
  language = {en},
  number = {1}
}

@article{Trautmann2017,
  title = {Accurate Estimation of Neural Population Dynamics without Spike Sorting},
  author = {Trautmann, Eric and Stavisky, Sergey and Lahiri, Subhaneil and Ames, Katherine and Kaufman, Matthew and Ryu, Stephen and Ganguli, Surya and Shenoy, Krishna},
  year = {2017},
  month = dec,
  doi = {10.1101/229252},
  abstract = {A central goal of systems neuroscience is to relate an organism's neural activity to behavior. Neural population analysis often begins by reducing the dimensionality of the data to focus on the patterns most relevant to a given task. A major practical hurdle to data analysis is spike sorting, and this problem is growing rapidly as the number of neurons measured increases. Here, we investigate whether spike sorting is necessary to estimate neural dynamics. The theory of random projections suggests that we can accurately estimate the geometry of low-dimensional manifolds from a small number of linear projections of the data. We re-analyzed data from three previous studies and found that neural dynamics and scientific conclusions are quite similar using multi-unit threshold crossings in place of sorted neurons. This finding unlocks existing data for new analyses and informs the design and use of new electrode arrays for laboratory and clinical use.},
  file = {Trautmann et al. - 2017 - Accurate estimation of neural population dynamics .pdf},
  journal = {bioRxiv},
  language = {en}
}

@article{Tria2015,
  title = {The Dynamics of Correlated Novelties},
  author = {Tria, F. and Loreto, V. and Servedio, V. D. P. and Strogatz, S. H.},
  year = {2015},
  month = may,
  volume = {4},
  issn = {2045-2322},
  doi = {10.1038/srep05890},
  file = {2014 - Tria et al. - The dynamics of correlated novelties.pdf;Tria et al. - 2015 - The dynamics of correlated novelties.pdf},
  journal = {Scientific Reports},
  language = {en},
  number = {1}
}

@article{Tripathy,
  title = {Transcriptomic Correlates of Neuron Electrophysiological Diversity},
  author = {Tripathy, Shreejoy J and Toker, Lilah and Li, Brenna and Crichlow, Cindy-Lee and Tebaykin, Dmitry and Mancarci, B Ogan and Pavlidis, Paul},
  pages = {28},
  abstract = {How neuronal diversity emerges from complex patterns of gene expression remains poorly understood. Here we present an approach to understand electrophysiological diversity through gene expression by integrating pooled- and single-cell transcriptomics with intracellular electrophysiology. Using neuroinformatics methods, we compiled a brain-wide dataset of 34 neuron types with paired gene expression and intrinsic electrophysiological features from publically accessible sources, the largest such collection to date. We identified 420 genes whose expression levels significantly correlated with variability in one or more of 11 physiological parameters. We next trained statistical models to infer cellular features from multivariate gene expression patterns. Such models were predictive of gene-electrophysiological relationships in an independent collection of 12 visual cortex cell types from the Allen Institute, suggesting that these correlations might reflect general principles relating expression patterns to phenotypic diversity across very different cell types. Many associations reported here have the potential to provide new insights into how neurons generate functional diversity, and correlations of ion channel genes like Gabrd and Scn1a (Nav1.1) with resting potential and spiking frequency are consistent with known causal mechanisms. Our work highlights the promise and inherent challenges in using cell type-specific transcriptomics to understand the mechanistic origins of neuronal diversity.},
  file = {Tripathy et al. - Transcriptomic correlates of neuron electrophysiol.pdf},
  language = {en}
}

@article{Tripp2007,
  title = {Neural {{Populations Can Induce Reliable Postsynaptic Currents}} without {{Observable Spike Rate Changes}} or {{Precise Spike Timing}}},
  author = {Tripp, B. and Eliasmith, C.},
  year = {2007},
  month = aug,
  volume = {17},
  pages = {1830--1840},
  issn = {1047-3211, 1460-2199},
  doi = {10.1093/cercor/bhl092},
  file = {2007 - Tripp, Eliasmith - Neural populations can induce reliable postsynaptic currents without observable spike rate changes or precise.pdf},
  journal = {Cerebral Cortex},
  language = {en},
  number = {8}
}

@article{Tripp2016,
  title = {Function Approximation in Inhibitory Networks},
  author = {Tripp, Bryan and Eliasmith, Chris},
  year = {2016},
  month = may,
  volume = {77},
  pages = {95--106},
  issn = {08936080},
  doi = {10.1016/j.neunet.2016.01.010},
  abstract = {In performance-optimized artificial neural networks, such as convolutional networks, each neuron makes excitatory connections with some of its targets and inhibitory connections with others. In contrast, physiological neurons are typically either excitatory or inhibitory, not both. This is a puzzle, because it seems to constrain computation, and because there are several counter-examples that suggest that it may not be a physiological necessity. Parisien et al. (2008) showed that any mixture of excitatory and inhibitory functional connections could be realized by a purely excitatory projection in parallel with a two-synapse projection through an inhibitory population. They showed that this works well with ratios of excitatory and inhibitory neurons that are realistic for the neocortex, suggesting that perhaps the cortex efficiently works around this apparent computational constraint. Extending this work, we show here that mixed excitatory and inhibitory functional connections can also be realized in networks that are dominated by inhibition, such as those of the basal ganglia. Further, we show that the function-approximation capacity of such connections is comparable to that of idealized mixed-weight connections. We also study whether such connections are viable in recurrent networks, and find that such recurrent networks can flexibly exhibit a wide range of dynamics. These results offer a new perspective on computation in the basal ganglia, and also perhaps on inhibitory networks within the cortex.},
  file = {Tripp and Eliasmith - 2016 - Function approximation in inhibitory networks.pdf},
  journal = {Neural Networks},
  language = {en}
}

@article{Trosch2020,
  title = {Horses Feel Emotions When They Watch Positive and Negative Horse\textendash Human Interactions in a Video and Transpose What They Saw to Real Life},
  author = {Tr{\"o}sch, Mil{\'e}na and Pellon, Sophie and Cuzol, Florent and Parias, C{\'e}line and Nowak, Raymond and Calandreau, Ludovic and Lansade, L{\'e}a},
  year = {2020},
  month = jul,
  volume = {23},
  pages = {643--653},
  issn = {1435-9448, 1435-9456},
  doi = {10.1007/s10071-020-01369-0},
  abstract = {Animals can indirectly gather meaningful information about other individuals by eavesdropping on their third-party interactions. In particular, eavesdropping can be used to indirectly attribute a negative or positive valence to an individual and to adjust one's future behavior towards that individual. Few studies have focused on this ability in nonhuman animals, especially in nonprimate species. Here, we investigated this ability for the first time in domestic horses (Equus caballus) by projecting videos of positive and negative interactions between an unknown human experimenter (a ``positive'' experimenter or a ``negative'' experimenter) and an actor horse. The horses reacted emotionally while watching the videos, expressing behavioral (facial expressions and contact-seeking behavior) and physiological (heart rate) cues of positive emotions while watching the positive video and of negative emotions while watching the negative video. This result shows that the horses perceived the content of the videos and suggests an emotional contagion between the actor horse and the subjects. After the videos were projected, the horses took a choice test, facing the positive and negative experimenters in real life. The horses successfully used the interactions seen in the videos to discriminate between the experimenters. They touched the negative experimenter significantly more, which seems counterintuitive but can be interpreted as an appeasement attempt, based on the existing literature. This result suggests that horses can indirectly attribute a valence to a human experimenter by eavesdropping on a previous third-party interaction with a conspecific.},
  file = {Trösch et al. - 2020 - Horses feel emotions when they watch positive and .pdf},
  journal = {Anim Cogn},
  language = {en},
  number = {4}
}

@article{Truccolo2014,
  title = {Neuronal {{Ensemble Synchrony}} during {{Human Focal Seizures}}},
  author = {Truccolo, W. and Ahmed, O. J. and Harrison, M. T. and Eskandar, E. N. and Cosgrove, G. R. and Madsen, J. R. and Blum, A. S. and Potter, N. S. and Hochberg, L. R. and Cash, S. S.},
  year = {2014},
  month = jul,
  volume = {34},
  pages = {9927--9944},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.4567-13.2014},
  file = {Truccolo et al. - 2014 - Neuronal Ensemble Synchrony during Human Focal Sei.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {30}
}

@article{Trujillo2013,
  title = {Altered Cortical Spectrotemporal Processing with Age-Related Hearing Loss},
  author = {Trujillo, Michael and Razak, Khaleel A.},
  year = {2013},
  month = dec,
  volume = {110},
  pages = {2873--2886},
  issn = {0022-3077, 1522-1598},
  doi = {10.1152/jn.00423.2013},
  file = {Trujillo and Razak - 2013 - Altered cortical spectrotemporal processing with a.pdf},
  journal = {Journal of Neurophysiology},
  language = {en},
  number = {12}
}

@article{Tschopp2018,
  title = {A {{Connectome Based Hexagonal Lattice Convolutional Network Model}} of the {{Drosophila Visual System}}},
  author = {Tschopp, Fabian David and Reiser, Michael B. and Turaga, Srinivas C.},
  year = {2018},
  month = jun,
  abstract = {What can we learn from a connectome? We constructed a simplified model of the first two stages of the fly visual system, the lamina and medulla. The resulting hexagonal lattice convolutional network was trained using backpropagation through time to perform object tracking in natural scene videos. Networks initialized with weights from connectome reconstructions automatically discovered well-known orientation and direction selectivity properties in T4 neurons and their inputs, while networks initialized at random did not. Our work is the first demonstration, that knowledge of the connectome can enable in silico predictions of the functional properties of individual neurons in a circuit, leading to an understanding of circuit function from structure alone.},
  archiveprefix = {arXiv},
  eprint = {1806.04793},
  eprinttype = {arxiv},
  file = {Tschopp et al. - 2018 - A Connectome Based Hexagonal Lattice Convolutional.pdf},
  journal = {arXiv:1806.04793 [cs, q-bio]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Quantitative Biology - Neurons and Cognition},
  language = {en},
  primaryclass = {cs, q-bio}
}

@techreport{Tseng2019,
  title = {Distinct {{Oscillation Dynamics Selectively Coordinate Excitatory}} and {{Inhibitory Neurons}} in {{Prefrontal Cortex}} during {{Sensory Discrimination}}},
  author = {Tseng, Hua-an and Han, Xue},
  year = {2019},
  month = may,
  institution = {{Neuroscience}},
  doi = {10.1101/629659},
  abstract = {The prefrontal cortex (PFC) is crucial for many cognitive functions. PFC individual neuron activity and ensemble oscillation dynamics have been linked to unique aspects of behavior. However, it remains largely unclear how different neuron types relate to oscillation features. To understand how excitatory and inhibitory neurons in PFC are coordinated by distinct oscillation signatures, we designed a 3-choice auditory discrimination task and used tetrode devices to examine individual PFC neuron activity and ensemble LFP oscillations in task performing mice. We found that PFC neurons and ensemble LFP oscillations exhibited complex sensory evoked responses that are context and task-progression dependent. While both excitatory and inhibitory neurons were transiently modulated at different phases of the task, inhibitory neurons were increasingly recruited as trial progressed compared to excitatory neurons. Inhibitory neurons in general showed higher spike-field coherence with LFP oscillations than excitatory neurons throughout the task period, first at higher frequencies at the beginning of the task, and then transitioned to lower frequencies in the middle of the task that sustained beyond task completion. Together, our results demonstrate that excitatory and inhibitory neurons selectively engage distinct oscillation dynamics during sensory discrimination in mice.},
  file = {Tseng and Han - 2019 - Distinct Oscillation Dynamics Selectively Coordina.pdf},
  language = {en},
  type = {Preprint}
}

@article{Tsirogiannis2010,
  title = {A Population Level Computational Model of the Basal Ganglia That Generates Parkinsonian Local Field Potential Activity},
  author = {Tsirogiannis, George L. and Tagaris, George A. and Sakas, Damianos and Nikita, Konstantina S.},
  year = {2010},
  month = feb,
  volume = {102},
  pages = {155--176},
  issn = {0340-1200, 1432-0770},
  doi = {10.1007/s00422-009-0360-3},
  abstract = {Recordings from the basal ganglia's subthalamic nucleus are acquired via microelectrodes immediately prior to the application of Deep Brain Stimulation (DBS) treatment for Parkinson's Disease (PD) to assist in the selection of the final point for the implantation of the DBS electrode. The acquired recordings reveal a persistent characteristic beta band peak in the power spectral density function of the Local Field Potential (LFP) signals. This peak is considered to lie at the core of the causality\textendash effect relationships of the parkinsonian pathophysiology. Based on LFPs acquired from human subjects during DBS for PD, we constructed a computational model of the basal ganglia on the population level that generates LFPs to identify the critical pathophysiological alterations that lead to the expression of the beta band peak. To this end, we used experimental data reporting that the strengths of the synaptic connections are modified under dopamine depletion. The hypothesis that the altered dopaminergic modulation may affect both the amplitude and the time course of the postsynaptic potentials is validated by the model. The results suggest a pivotal role of both of these parameters to the pathophysiology of PD.},
  file = {2010 - Tsirogiannis et al. - A population level computational model of the basal ganglia that generates parkinsonian local field potenti.pdf},
  journal = {Biological Cybernetics},
  language = {en},
  number = {2}
}

@article{Tulving2002,
  title = {Episodic {{Memory}}: {{From Mind}} to {{Brain}}},
  shorttitle = {Episodic {{Memory}}},
  author = {Tulving, Endel},
  year = {2002},
  month = feb,
  volume = {53},
  pages = {1--25},
  issn = {0066-4308, 1545-2085},
  doi = {10.1146/annurev.psych.53.100901.135114},
  file = {Tulving - 2002 - Episodic Memory From Mind to Brain.pdf},
  journal = {Annu. Rev. Psychol.},
  language = {en},
  number = {1}
}

@article{Turner2012,
  title = {Spatiotemporal Activity Estimation for Multivoxel Pattern Analysis with Rapid Event-Related Designs},
  author = {Turner, Benjamin O. and Mumford, Jeanette A. and Poldrack, Russell A. and Ashby, F. Gregory},
  year = {2012},
  month = sep,
  volume = {62},
  pages = {1429--1438},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2012.05.057},
  abstract = {Despite growing interest in multi-voxel pattern analysis (MVPA) methods for fMRI, a major problem remains \textemdash that of generating estimates in rapid event-related (ER) designs, where the BOLD responses of temporally adjacent events will overlap. While this problem has been investigated for methods that reduce each event to a single parameter per voxel (Mumford et al., 2012), most of these methods make strong parametric assumptions about the shape of the hemodynamic response, and require exact knowledge of the temporal profile of the underlying neural activity. A second class of methods uses multiple parameters per event (per voxel) to capture temporal information more faithfully. In addition to enabling a more accurate estimate of ER responses, this allows for the extension of the standard classification paradigm into the temporal domain (e.g., Mour\~ao-Miranda et al., 2007). However, existing methods in this class were developed for use with block and slow ER data, and there has not yet been an exploration of how to adapt such methods to data collected using rapid ER designs. Here, we demonstrate that the use of multiple parameters preserves or improves classification accuracy, while additionally providing information on the evolution of class discrimination. Additionally, we explore an alternative to the method of Mour\~ao-Miranda et al. tailored to use in rapid ER designs that yields equivalent classification accuracies, but is better at unmixing responses to temporally adjacent events. The current work paves the way for wider adoption of spatiotemporal classification analyses, and greater use of MVPA with rapid ER designs.},
  file = {2012 - Turner et al. - Spatiotemporal activity estimation for multivoxel pattern analysis with rapid event-related designs.pdf},
  journal = {NeuroImage},
  language = {en},
  number = {3}
}

@article{Turnquist2017,
  title = {Quadratization: {{From Conductance}}-{{Based Models To Caricature Models With Parabolic Nonlinearities}}},
  shorttitle = {Quadratization},
  author = {Turnquist, Axel G. R. and Rotstein, Horacio},
  year = {2017},
  month = may,
  doi = {10.1101/137422},
  abstract = {Quadratization of biophysical (conductance-based) models having a parabolic-like voltage nullcline in the subthreshold voltage regime refers to the process by which these models are substituted by ``caricature'' models having a strictly parabolic voltage nullcline and a linear nullcline  for the recovery variable. We refer to the latter as quadratic or parabolic models. The parabolic-like and strictly parabolic voltage nullclines coincide at their extrema (minima or maxima) and  are well approximated by each other in vicinities of these extrema whose size depend on the model parameters. Quadratic models are simplified by a change of variables that translates these extrema into the origin of the phase-plane diagram. A further simplification (parameter reduction) can be achieved by nondimensionalizing  the quadratic models. This procedure can be extended to three-dimensional models having a parabolic-cylinder-like shaped voltage nullsurface and to models having time-dependent inputs and synaptic currents.},
  file = {Turnquist and Rotstein - 2017 - Quadratization From Conductance-Based Models To C.pdf},
  journal = {bioRxiv},
  language = {en}
}

@article{Tuyls2007,
  title = {What Evolutionary Game Theory Tells Us about Multiagent Learning},
  author = {Tuyls, Karl and Parsons, Simon},
  year = {2007},
  month = may,
  volume = {171},
  pages = {406--416},
  issn = {00043702},
  doi = {10.1016/j.artint.2007.01.004},
  abstract = {This paper discusses If multi-agent learning is the answer, what is the question? [Y. Shoham, R. Powers, T. Grenager, If multiagent learning is the answer, what is the question? Artificial Intelligence 171 (7) (2007) 365\textendash 377, this issue] from the perspective of evolutionary game theory. We briefly discuss the concepts of evolutionary game theory, and examine the main conclusions from [Y. Shoham, R. Powers, T. Grenager, If multi-agent learning is the answer, what is the question? Artificial Intelligence 171 (7) (2007) 365\textendash 377, this issue] with respect to some of our previous work. Overall we find much to agree with, concluding, however, that the central concerns of multiagent learning are rather narrow compared with the broad variety of work identified in [Y. Shoham, R. Powers, T. Grenager, If multi-agent learning is the answer, what is the question? Artificial Inteligence 171 (7) (2007) 365\textendash 377, this issue].},
  file = {2007 - Tuyls, Parsons - What evolutionary game theory tells us about multiagent learning.pdf},
  journal = {Artificial Intelligence},
  language = {en},
  number = {7}
}

@article{Tuyls2018,
  title = {Symmetric {{Decomposition}} of {{Asymmetric Games}}},
  author = {Tuyls, Karl and P{\'e}rolat, Julien and Lanctot, Marc and Ostrovski, Georg and Savani, Rahul and Leibo, Joel Z and Ord, Toby and Graepel, Thore and Legg, Shane},
  year = {2018},
  month = dec,
  volume = {8},
  issn = {2045-2322},
  doi = {10.1038/s41598-018-19194-4},
  file = {Tuyls et al. - 2018 - Symmetric Decomposition of Asymmetric Games.pdf},
  journal = {Scientific Reports},
  language = {en},
  number = {1}
}

@article{Uhlhaas2008,
  title = {The {{Role}} of {{Oscillations}} and {{Synchrony}} in {{Cortical Networks}} and {{Their Putative Relevance}} for the {{Pathophysiology}} of {{Schizophrenia}}},
  author = {Uhlhaas, P. J. and Haenschel, C. and Nikolic, D. and Singer, W.},
  year = {2008},
  month = jul,
  volume = {34},
  pages = {927--943},
  issn = {0586-7614, 1745-1701},
  doi = {10.1093/schbul/sbn062},
  file = {2008 - Uhlhaas et al. - The role of oscillations and synchrony in cortical networks and their putative relevance for the pathophysiology.pdf},
  journal = {Schizophrenia Bulletin},
  language = {en},
  number = {5}
}

@article{Uhlhaas2009,
  title = {Neural Synchrony in Cortical Networks: History, Concept and Current Status},
  shorttitle = {Neural Synchrony in Cortical Networks},
  author = {Uhlhaas, Peter},
  year = {2009},
  volume = {3},
  issn = {16625145},
  doi = {10.3389/neuro.07.017.2009},
  abstract = {Following the discovery of context-dependent synchronization of oscillatory neuronal responses in the visual system, the role of neural synchrony in cortical networks has been expanded to provide a general mechanism for the coordination of distributed neural activity patterns. In the current paper, we present an update of the status of this hypothesis through summarizing recent results from our laboratory that suggest important new insights regarding the mechanisms, function and relevance of this phenomenon. In the first part, we present recent results derived from animal experiments and mathematical simulations that provide novel explanations and mechanisms for zero and nero-zero phase lag synchronization. In the second part, we shall discuss the role of neural synchrony for expectancy during perceptual organization and its role in conscious experience. This will be followed by evidence that indicates that in addition to supporting conscious cognition, neural synchrony is abnormal in major brain disorders, such as schizophrenia and autism spectrum disorders. We conclude this paper with suggestions for further research as well as with critical issues that need to be addressed in future studies.},
  file = {2009 - Neuroscience et al. - Neural synchrony in cortical networks history , concept and current status.pdf},
  journal = {Frontiers in Integrative Neuroscience},
  language = {en}
}

@article{Uhlhaas2012,
  title = {Neuronal {{Dynamics}} and {{Neuropsychiatric Disorders}}: {{Toward}} a {{Translational Paradigm}} for {{Dysfunctional Large}}-{{Scale Networks}}},
  shorttitle = {Neuronal {{Dynamics}} and {{Neuropsychiatric Disorders}}},
  author = {Uhlhaas, Peter J. and Singer, Wolf},
  year = {2012},
  month = sep,
  volume = {75},
  pages = {963--980},
  issn = {08966273},
  doi = {10.1016/j.neuron.2012.09.004},
  file = {2012 - Uhlhaas, Singer - Neuronal Dynamics and Neuropsychiatric Disorders Toward a Translational Paradigm for Dysfunctional Large-Scale.pdf},
  journal = {Neuron},
  language = {en},
  number = {6}
}

@article{Ujfalussy2015,
  title = {Dendritic Nonlinearities Are Tuned for Efficient Spike-Based Computations in Cortical Circuits},
  author = {Ujfalussy, Bal{\'a}zs B and Makara, Judit K and Branco, Tiago and Lengyel, M{\'a}t{\'e}},
  year = {2015},
  month = dec,
  volume = {4},
  issn = {2050-084X},
  doi = {10.7554/eLife.10056},
  file = {Ujfalussy et al. - 2015 - Dendritic nonlinearities are tuned for efficient s.pdf},
  journal = {eLife},
  language = {en}
}

@techreport{Ujfalussy2017,
  title = {Global, Multiplexed Dendritic Computations under in Vivo-like Conditions},
  author = {Ujfalussy, Balazs B and Lengyel, Mate and Branco, Tiago},
  year = {2017},
  month = dec,
  institution = {{Neuroscience}},
  doi = {10.1101/235259},
  abstract = {Dendrites integrate inputs in highly non-linear ways, but it is unclear how these non-linearities contribute to the overall input-output transformation of single neurons. Here, we developed statistically principled methods using a hierarchical cascade of linear-nonlinear subunits (hLN) to model the dynamically evolving somatic response of neurons receiving complex spatio-temporal synaptic input patterns. We used the hLN to predict the membrane potential of a detailed biophysical model of a L2/3 pyramidal cell receiving in vivo-like synaptic input and reproducing in vivo dendritic recordings. We found that more than 90\% of the somatic response could be captured by linear integration followed a single global non-linearity. Multiplexing inputs into parallel processing channels could improve prediction accuracy by as much as additional layers of local non-linearities. These results provide a data-driven characterisation of a key building block of cortical circuit computations: dendritic integration and the input-output transformation of single neurons during in vivo-like conditions.},
  file = {Ujfalussy et al. - 2017 - Global, multiplexed dendritic computations under i.pdf},
  language = {en},
  type = {Preprint}
}

@article{UniversidadTecnologicadePereira2016,
  title = {Estimation of the Neuromodulation Parameters from the Planned Volume of Tissue Activated in Deep Brain Stimulation},
  author = {{Universidad Tecnol\'ogica de Pereira} and {G{\'o}mez-Orozco}, Viviana and {\'A}lvarez-L{\'o}pez, Mauricio Alexander and {Universidad Tecnol\'ogica de Pereira} and {Henao-Gallo}, {\'O}scar Alberto and {Universidad Tecnol\'ogica de Pereira} and {Daza-Santacoloma}, Genaro and {Instituto de Epilepsia y Parkinson del Eje Cafetero - Neurocentro} and {Orozco-Guti{\'e}rrez}, {\'A}lvaro {\'A}ngel and {Universidad Tecnol\'ogica de Pereira}},
  year = {2016},
  month = jun,
  issn = {01206230},
  doi = {10.17533/udea.redin.n79a02},
  abstract = {Deep brain stimulation (DBS) is a therapy with promissory results for the treatment of movement disorders. It delivers electric stimulation via an electrode to a specific target brain region. The spatial extent of neural response to this stimulation is known as volume of tissue activated (VTA). Changes in stimulation parameters that control VTA, such as amplitude, pulse width and electrode configuration can affect the effectiveness of the DBS therapy. In this study, we develop a novel methodology for estimating suitable DBS neuromodulation parameters, from planned VTA, that attempts to maximize the therapeutic effects, and to minimize the adverse effects of DBS. For estimating the continuous outputs (amplitude and pulse width), we use multi-output support vector regression, taking the geometry of the VTA as input space. For estimating the electrode polarity configuration, we perform several classification problems, also using support vector machines from the same input space. Our methodology attains promising results for both the regression setting, and for predicting electrode active contacts and their polarity. Combining biological neural modeling techniques together with machine learning, we introduce a novel area of research where parameters of neuromodulation in DBS can be tuned by manually specifying a desired geometric volume.},
  file = {Universidad Tecnológica de Pereira et al. - 2016 - Estimation of the neuromodulation parameters from .pdf},
  journal = {Revista Facultad de Ingenier\'ia Universidad de Antioquia},
  language = {en},
  number = {79}
}

@article{Unsworth2007,
  title = {On the Division of Short-Term and Working Memory: {{An}} Examination of Simple and Complex Span and Their Relation to Higher Order Abilities.},
  shorttitle = {On the Division of Short-Term and Working Memory},
  author = {Unsworth, Nash and Engle, Randall W.},
  year = {2007},
  volume = {133},
  pages = {1038--1066},
  issn = {1939-1455, 0033-2909},
  doi = {10.1037/0033-2909.133.6.1038},
  abstract = {Research has suggested that short-term memory and working memory (as measured by simple and complex span tasks, respectively) are separate constructs that are differentially related to higher order cognitive abilities. This claim is critically evaluated by reviewing research that has compared simple and complex span tasks in both experimental and correlational studies. In addition, a meta-analysis and re-analyses of key data sets were conducted. The review and analyses suggest that simple and complex span tasks largely measure the same basic subcomponent processes (e.g., rehearsal, maintenance, updating, controlled search) but differ in the extent to which these processes operate in a particular task. These differences largely depend on the extent to which phonological processes are maximized and variability from long list lengths is present. Potential methodological, psychometric, and assessment implications are discussed and a theoretical account of the data is proposed.},
  file = {2007 - Unsworth, Engle - On the division of short-term and working memory An examination of simple and complex span and their relatio(2).pdf;2007 - Unsworth, Engle - On the division of short-term and working memory An examination of simple and complex span and their relation t.pdf},
  journal = {Psychological Bulletin},
  language = {en},
  number = {6}
}

@article{Urban2015,
  title = {Real-Time Imaging of Brain Activity in Freely Moving Rats Using Functional Ultrasound},
  author = {Urban, Alan and Dussaux, Clara and Martel, Guillaume and Brunner, Cl{\'e}ment and Mace, Emilie and Montaldo, Gabriel},
  year = {2015},
  month = sep,
  volume = {12},
  pages = {873--878},
  issn = {1548-7091, 1548-7105},
  doi = {10.1038/nmeth.3482},
  file = {Urban et al. - 2015 - Real-time imaging of brain activity in freely movi.pdf},
  journal = {Nature Methods},
  language = {en},
  number = {9}
}

@article{Ursino2010,
  title = {The Generation of Rhythms within a Cortical Region: {{Analysis}} of a Neural Mass Model},
  shorttitle = {The Generation of Rhythms within a Cortical Region},
  author = {Ursino, Mauro and Cona, Filippo and Zavaglia, Melissa},
  year = {2010},
  month = sep,
  volume = {52},
  pages = {1080--1094},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2009.12.084},
  abstract = {Rhythms in brain electrical activity are assumed to play a significant role in many cognitive and perceptual processes. It is thus of great value to analyze these rhythms and their mutual relationships in large scale models of cortical regions. In the present work, we modified the neural mass model by Wendling et al. (Eur. J. Neurosci. 15 (2002) 1499\textendash 1508) by including a new inhibitory self-loop among GABAA,fast interneurons. A theoretical analysis was performed to demonstrate that, thanks to this loop, GABAA,fast interneurons can produce a {$\gamma$} rhythm in the power spectral density (PSD) even without the participation of the other neural populations. Then, the model of a whole cortical region, built upon four interconnected neural populations (pyramidal cells, excitatory, GABAA,slow and GABAA,fast interneurons) was investigated by changing the internal connectivity parameters. Results show that different rhythm combinations ({$\beta$} and {$\gamma$}, {$\alpha$} and {$\gamma$}, or a wide spectrum) can be obtained within the same region by simply altering connectivity values, without the need to change synaptic kinetics. Finally, two or three cortical regions were connected by using different topologies of long range connections. Results show that long-range connections directed from pyramidal neurons to GABAA,fast interneurons are the most efficient to transmit rhythms from one region to another. In this way, PSD with three or four peaks can be obtained using simple connectivity patterns. The model can be of value to gain a deeper insight into the mechanisms involved in the generation of {$\gamma$} rhythms and provide a better understanding of cortical EEG spectra.},
  file = {2010 - Ursino, Cona, Zavaglia - The generation of rhythms within a cortical region Analysis of a neural mass model.pdf},
  journal = {NeuroImage},
  language = {en},
  number = {3}
}

@article{Uva2015,
  title = {Synchronous {{Inhibitory Potentials Precede Seizure}}-{{Like Events}} in {{Acute Models}} of {{Focal Limbic Seizures}}},
  author = {Uva, L. and Breschi, G. L. and Gnatkovsky, V. and Taverna, S. and {de Curtis}, M.},
  year = {2015},
  month = feb,
  volume = {35},
  pages = {3048--3055},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.3692-14.2015},
  file = {Uva et al. - 2015 - Synchronous Inhibitory Potentials Precede Seizure-.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {7}
}

@article{Valiant1984a,
  title = {A Theory of the Learnable},
  author = {Valiant, L. G.},
  year = {1984},
  month = nov,
  volume = {27},
  pages = {1134--1142},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/1968.1972},
  file = {Valiant - 1984 - A theory of the learnable.pdf},
  journal = {Commun. ACM},
  language = {en},
  number = {11}
}

@article{vanderWesthuizen2018,
  title = {The Unreasonable Effectiveness of the Forget Gate},
  author = {{van der Westhuizen}, Jos and Lasenby, Joan},
  year = {2018},
  month = apr,
  abstract = {Given the success of the gated recurrent unit, a natural question is whether all the gates of the long short-term memory (LSTM) network are necessary. Previous research has shown that the forget gate is one of the most important gates in the LSTM. Here we show that a forget-gate-only version of the LSTM with chronoinitialized biases, not only provides computational savings but outperforms the standard LSTM on multiple benchmark datasets and competes with some of the best contemporary models. Our proposed network, the JANET, achieves accuracies of 99\% and 92.5\% on the MNIST and pMNIST datasets, outperforming the standard LSTM which yields accuracies of 98.5\% and 91\%.},
  archiveprefix = {arXiv},
  eprint = {1804.04849},
  eprinttype = {arxiv},
  file = {../../Zotero/storage/ZVD94T7I/van der Westhuizen and Lasenby - 2018 - The unreasonable effectiveness of the forget gate.pdf;van der Westhuizen and Lasenby - 2018 - The unreasonable effectiveness of the forget gate.pdf},
  journal = {arXiv:1804.04849 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  language = {en},
  primaryclass = {cs, stat}
}

@article{vanDijk2008,
  title = {Prestimulus {{Oscillatory Activity}} in the {{Alpha Band Predicts Visual Discrimination Ability}}},
  author = {{van Dijk}, H. and Schoffelen, J.-M. and Oostenveld, R. and Jensen, O.},
  year = {2008},
  month = feb,
  volume = {28},
  pages = {1816--1823},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.1853-07.2008},
  file = {2008 - Van Dijk et al. - Prestimulus Oscillatory Activity in the Alpha Band Predicts Visual Discrimination Ability.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {8}
}

@article{vanEde2018,
  title = {Neural {{Oscillations}}: {{Sustained Rhythms}} or {{Transient Burst}}-{{Events}}?},
  shorttitle = {Neural {{Oscillations}}},
  author = {{van Ede}, Freek and Quinn, Andrew J. and Woolrich, Mark W. and Nobre, Anna C.},
  year = {2018},
  month = jul,
  volume = {41},
  pages = {415--417},
  issn = {01662236},
  doi = {10.1016/j.tins.2018.04.004},
  file = {van Ede et al. - 2018 - Neural Oscillations Sustained Rhythms or Transien.pdf},
  journal = {Trends in Neurosciences},
  language = {en},
  number = {7}
}

@article{vanEde2018a,
  title = {Neural {{Oscillations}}: {{Sustained Rhythms}} or {{Transient Burst}}-{{Events}}?},
  shorttitle = {Neural {{Oscillations}}},
  author = {{van Ede}, Freek and Quinn, Andrew J. and Woolrich, Mark W. and Nobre, Anna C.},
  year = {2018},
  month = jul,
  volume = {41},
  pages = {415--417},
  issn = {01662236},
  doi = {10.1016/j.tins.2018.04.004},
  file = {van Ede et al. - 2018 - Neural Oscillations Sustained Rhythms or Transien 2.pdf},
  journal = {Trends in Neurosciences},
  language = {en},
  number = {7}
}

@article{Vangel2015,
  title = {Randomly {{Spiking Dynamic Neural Fields}}},
  author = {Vangel, Beno{\^i}t Chappet De and {Torres-huitzil}, Cesar and Girau, Bernard},
  year = {2015},
  month = apr,
  volume = {11},
  pages = {1--26},
  issn = {15504832},
  doi = {10.1145/2629517},
  file = {2014 - de Vangel, Torres-Huitzil, Girau - Randomly spiking dynamic neural fields.pdf;Vangel et al. - 2015 - Randomly Spiking Dynamic Neural Fields.pdf},
  journal = {ACM Journal on Emerging Technologies in Computing Systems},
  language = {en},
  number = {4}
}

@article{vanGelder1998,
  title = {The Dynamical Hypothesis in Cognitive Science},
  author = {{van Gelder}, Tim},
  year = {1998},
  month = oct,
  volume = {21},
  pages = {615--628},
  issn = {0140-525X, 1469-1825},
  doi = {10.1017/S0140525X98001733},
  abstract = {According to the dominant computational approach in cognitive science, cognitive agents are digital computers; according to the alternative approach, they are dynamical systems. This target article attempts to articulate and support the dynamical hypothesis. The dynamical hypothesis has two major components: the nature hypothesis (cognitive agents are dynamical systems) and the knowledge hypothesis (cognitive agents can be understood dynamically). A wide range of objections to this hypothesis can be rebutted. The conclusion is that cognitive systems may well be dynamical systems, and only sustained empirical research in cognitive science will determine the extent to which that is true.},
  file = {van Gelder - 1998 - The dynamical hypothesis in cognitive science.pdf},
  journal = {Behav Brain Sci},
  language = {en},
  number = {5}
}

@article{vanHasselt2015,
  title = {Deep {{Reinforcement Learning}} with {{Double Q}}-Learning},
  author = {{van Hasselt}, Hado and Guez, Arthur and Silver, David},
  year = {2015},
  month = dec,
  abstract = {The popular Q-learning algorithm is known to overestimate action values under certain conditions. It was not previously known whether, in practice, such overestimations are common, whether they harm performance, and whether they can generally be prevented. In this paper, we answer all these questions affirmatively. In particular, we first show that the recent DQN algorithm, which combines Q-learning with a deep neural network, suffers from substantial overestimations in some games in the Atari 2600 domain. We then show that the idea behind the Double Q-learning algorithm, which was introduced in a tabular setting, can be generalized to work with large-scale function approximation. We propose a specific adaptation to the DQN algorithm and show that the resulting algorithm not only reduces the observed overestimations, as hypothesized, but that this also leads to much better performance on several games.},
  archiveprefix = {arXiv},
  eprint = {1509.06461},
  eprinttype = {arxiv},
  file = {van Hasselt et al. - 2015 - Deep Reinforcement Learning with Double Q-learning.pdf},
  journal = {arXiv:1509.06461 [cs]},
  keywords = {Computer Science - Machine Learning},
  language = {en},
  primaryclass = {cs}
}

@article{VanRooij2008,
  title = {The {{Tractable Cognition Thesis}}},
  author = {Van Rooij, Iris},
  year = {2008},
  month = sep,
  volume = {32},
  pages = {939--984},
  issn = {03640213},
  doi = {10.1080/03640210801897856},
  abstract = {The recognition that human minds/brains are finite systems with limited resources for computation has led some researchers to advance the Tractable Cognition thesis: Human cognitive capacities are constrained by computational tractability. This thesis, if true, serves cognitive psychology by constraining the space of computational-level theories of cognition. To utilize this constraint, a precise and workable definition of ``computational tractability'' is needed. Following computer science tradition, many cognitive scientists and psychologists define computational tractability as polynomial-time computability, leading to the P-Cognition thesis. This article explains how and why the P-Cognition thesis may be overly restrictive, risking the exclusion of veridical computational-level theories from scientific investigation. An argument is made to replace the P-Cognition thesis by the FPT-Cognition thesis as an alternative formalization of the Tractable Cognition thesis (here, FPT stands for fixed-parameter tractable). Possible objections to the Tractable Cognition thesis, and its proposed formalization, are discussed, and existing misconceptions are clarified.},
  file = {Van Rooij - 2008 - The Tractable Cognition Thesis.pdf},
  journal = {Cognitive Science},
  language = {en},
  number = {6}
}

@article{Vanschoren2018,
  title = {Meta-{{Learning}}: {{A Survey}}},
  shorttitle = {Meta-{{Learning}}},
  author = {Vanschoren, Joaquin},
  year = {2018},
  month = oct,
  abstract = {Meta-learning, or learning to learn, is the science of systematically observing how different machine learning approaches perform on a wide range of learning tasks, and then learning from this experience, or meta-data, to learn new tasks much faster than otherwise possible. Not only does this dramatically speed up and improve the design of machine learning pipelines or neural architectures, it also allows us to replace hand-engineered algorithms with novel approaches learned in a data-driven way. In this chapter, we provide an overview of the state of the art in this fascinating and continuously evolving field.},
  archiveprefix = {arXiv},
  eprint = {1810.03548},
  eprinttype = {arxiv},
  file = {Vanschoren - 2018 - Meta-Learning A Survey.pdf},
  journal = {arXiv:1810.03548 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryclass = {cs, stat}
}

@article{Vapnik1971,
  title = {On the {{Uniform Convergence}} of {{Relative Frequencies}} of {{Events}} to {{Their Probabilities}}},
  author = {Vapnik, V and Chervonenkis, A},
  year = {1971},
  volume = {16},
  pages = {17},
  file = {Vapnik and Chervonenkis - On the Uniform Convergence of Relative Frequencies.pdf},
  journal = {Theory of Probability \& Its Applications},
  language = {en},
  number = {2}
}

@article{Varela2001,
  title = {The Brainweb: {{Phase}} Synchronization and Large-Scale Integration},
  shorttitle = {The Brainweb},
  author = {Varela, Francisco and Lachaux, Jean-Philippe and Rodriguez, Eugenio and Martinerie, Jacques},
  year = {2001},
  month = apr,
  volume = {2},
  pages = {229--239},
  issn = {1471-003X, 1471-0048},
  doi = {10.1038/35067550},
  file = {Varela et al. - 2001 - The brainweb Phase synchronization and large-scal.pdf},
  journal = {Nat Rev Neurosci},
  language = {en},
  number = {4}
}

@article{Varma2006,
  title = {Bias in Error Estimation When Using Cross-Validation for Model Selection},
  author = {Varma, Sudhir and Simon, Richard},
  year = {2006},
  pages = {8},
  abstract = {Background: Cross-validation (CV) is an effective method for estimating the prediction error of a classifier. Some recent articles have proposed methods for optimizing classifiers by choosing classifier parameter values that minimize the CV error estimate. We have evaluated the validity of using the CV error estimate of the optimized classifier as an estimate of the true error expected on independent data. Results: We used CV to optimize the classification parameters for two kinds of classifiers; Shrunken Centroids and Support Vector Machines (SVM). Random training datasets were created, with no difference in the distribution of the features between the two classes. Using these "null" datasets, we selected classifier parameter values that minimized the CV error estimate. 10-fold CV was used for Shrunken Centroids while Leave-One-Out-CV (LOOCV) was used for the SVM. Independent test data was created to estimate the true error. With "null" and "non null" (with differential expression between the classes) data, we also tested a nested CV procedure, where an inner CV loop is used to perform the tuning of the parameters while an outer CV is used to compute an estimate of the error. The CV error estimate for the classifier with the optimal parameters was found to be a substantially biased estimate of the true error that the classifier would incur on independent data. Even though there is no real difference between the two classes for the "null" datasets, the CV error estimate for the Shrunken Centroid with the optimal parameters was less than 30\% on 18.5\% of simulated training data-sets. For SVM with optimal parameters the estimated error rate was less than 30\% on 38\% of "null" data-sets. Performance of the optimized classifiers on the independent test set was no better than chance. The nested CV procedure reduces the bias considerably and gives an estimate of the error that is very close to that obtained on the independent testing set for both Shrunken Centroids and SVM classifiers for "null" and "non-null" data distributions. Conclusion: We show that using CV to compute an error estimate for a classifier that has itself been tuned using CV gives a significantly biased estimate of the true error. Proper use of CV for estimating true error of a classifier developed using a well defined algorithm requires that all steps of the algorithm, including classifier parameter tuning, be repeated in each CV loop. A nested CV procedure provides an almost unbiased estimate of the true error.},
  file = {2006 - Varma, Simon - Bias in error estimation when using cross-validation for model selection.pdf},
  journal = {BMC Bioinformatics},
  language = {en}
}

@article{Vegue2017,
  title = {On {{The Structure Of Cortical Micro}}-{{Circuits Inferred From Small Sample Sizes}}},
  author = {Vegue, Marina and Perin, Rodrigo and Roxin, Alex},
  year = {2017},
  month = apr,
  doi = {10.1101/118471},
  abstract = {The structure in cortical micro-circuits deviates from what would be expected in a purely random network, which has been seen as evidence of clustering. To address this issue we sought to reproduce the non-random features of cortical circuits by considering several distinct classes of network topology, including clustered networks, networks with distance-dependent connectivity and those with broad degree distributions. To our surprise we found that all these qualitatively distinct topologies could account equally well for all reported non-random features, despite being easily distinguishable from one another at the network level. This apparent paradox was a consequence of estimating network properties given only small sample sizes. In other words, networks which differ markedly in their global structure can look quite similar locally. This makes inferring network structure from small sample sizes, a necessity given the technical difficulty inherent in simultaneous intracellular recordings, problematic. We found that a network statistic called the sample degree correlation (SDC) overcomes this difficulty. The SDC depends only on parameters which can be reliably estimated given small sample sizes, and is an accurate fingerprint of every topological family. We applied the SDC criterion to data from rat visual and somatosensory cortex and discovered that the connectivity was not consistent with any of these main topological classes. However, we were able to fit the experimental data with a more general network class, of which all previous topologies were special cases. The resulting network topology could be interpreted as a combination of physical spatial dependence and non-spatial, hierarchical clustering.},
  file = {Vegue et al. - 2017 - On The Structure Of Cortical Micro-Circuits Inferr.pdf},
  journal = {bioRxiv},
  language = {en}
}

@inproceedings{Velez2014,
  title = {Novelty Search Creates Robots with General Skills for Exploration},
  booktitle = {Proceedings of the 2014 Conference on {{Genetic}} and Evolutionary Computation - {{GECCO}} '14},
  author = {Velez, Roby and Clune, Jeff},
  year = {2014},
  pages = {737--744},
  publisher = {{ACM Press}},
  address = {{Vancouver, BC, Canada}},
  doi = {10.1145/2576768.2598225},
  abstract = {Novelty Search, a new type of Evolutionary Algorithm, has shown much promise in the last few years. Instead of selecting for phenotypes that are closer to an objective, Novelty Search assigns rewards based on how different the phenotypes are from those already generated. A common criticism of Novelty Search is that it is effectively random or exhaustive search because it tries solutions in an unordered manner until a correct one is found. Its creators respond that over time Novelty Search accumulates information about the environment in the form of skills relevant to reaching uncharted territory, but to date no evidence for that hypothesis has been presented. In this paper we test that hypothesis by transferring robots evolved under Novelty Search to new environments (here, mazes) to see if the skills they've acquired generalize. Three lines of evidence support the claim that Novelty Search agents do indeed learn general exploration skills. First, robot controllers evolved via Novelty Search in one maze and then transferred to a new maze explore significantly more of the new environment than nonevolved (randomly generated) agents. Second, a Novelty Search process to solve the new mazes works significantly faster when seeded with the transferred controllers versus randomly-generated ones. Third, no significant difference exists when comparing two types of transferred agents: those evolved in the original maze under (1) Novelty Search vs. (2) a traditional, objective-based fitness function. The evidence gathered suggests that, like traditional Evolutionary Algorithms with objective-based fitness functions, Novelty Search is not a random or exhaustive search process, but instead is accumulating information about the environment, resulting in phenotypes possessing skills needed to explore their world.},
  file = {Velez and Clune - 2014 - Novelty search creates robots with general skills .pdf},
  isbn = {978-1-4503-2662-9},
  language = {en}
}

@article{Veness,
  title = {Online {{Learning}} with {{Gated Linear Networks}}},
  author = {Veness, Joel and Lattimore, Tor and Bhoopchand, Avishkar and {Grabska-Barwinska}, Agnieszka and Mattern, Christopher and Toth, Peter},
  pages = {40},
  abstract = {This paper describes a family of probabilistic architectures designed for online learning under the logarithmic loss. Rather than relying on non-linear transfer functions, our method gains representational power by the use of data conditioning. We state under general conditions a learnable capacity theorem that shows this approach can in principle learn any bounded Borel-measurable function on a compact subset of euclidean space; the result is stronger than many universality results for connectionist architectures because we provide both the model and the learning procedure for which convergence is guaranteed.},
  file = {Veness et al. - Online Learning with Gated Linear Networks.pdf},
  language = {en}
}

@article{Vergassola2007,
  title = {`{{Infotaxis}}' as a Strategy for Searching without Gradients},
  author = {Vergassola, Massimo and Villermaux, Emmanuel and Shraiman, Boris I.},
  year = {2007},
  month = jan,
  volume = {445},
  pages = {406--409},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature05464},
  file = {Vergassola et al. - 2007 - ‘Infotaxis’ as a strategy for searching without gr.pdf},
  journal = {Nature},
  language = {en},
  number = {7126}
}

@article{Vezhnevets,
  title = {{{FeUdal Networks}} for {{Hierarchical Reinforcement Learning}}},
  author = {Vezhnevets, Alexander Sasha and Osindero, Simon and Schaul, Tom and Heess, Nicolas and Jaderberg, Max and Silver, David and Kavukcuoglu, Koray},
  pages = {12},
  abstract = {We introduce FeUdal Networks (FuNs): a novel architecture for hierarchical reinforcement learning. Our approach is inspired by the feudal reinforcement learning proposal of Dayan and Hinton, and gains power and efficacy by decoupling end-to-end learning across multiple levels \textendash{} allowing it to utilise different resolutions of time. Our framework employs a Manager module and a Worker module. The Manager operates at a lower temporal resolution and sets abstract goals which are conveyed to and enacted by the Worker. The Worker generates primitive actions at every tick of the environment. The decoupled structure of FuN conveys several benefits \textendash{} in addition to facilitating very long timescale credit assignment it also encourages the emergence of sub-policies associated with different goals set by the Manager. These properties allow FuN to dramatically outperform a strong baseline agent on tasks that involve long-term credit assignment or memorisation. We demonstrate the performance of our proposed system on a range of tasks from the ATARI suite and also from a 3D DeepMind Lab environment.},
  file = {Vezhnevets et al. - FeUdal Networks for Hierarchical Reinforcement Lea.pdf},
  language = {en}
}

@article{Vicente2008,
  title = {Dynamical Relaying Can Yield Zero Time Lag Neuronal Synchrony despite Long Conduction Delays},
  author = {Vicente, R. and Gollo, L. L. and Mirasso, C. R. and Fischer, I. and Pipa, G.},
  year = {2008},
  month = nov,
  volume = {105},
  pages = {17157--17162},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.0809353105},
  file = {2008 - Vicente et al. - Dynamical relaying can yield zero time lag neuronal synchrony despite long conduction delays.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {44}
}

@article{Vicente2012,
  title = {Looking at the {{Trees}} in the {{Central Forest}}: {{A New Pallidal}}-{{Striatal Cell Type}}},
  shorttitle = {Looking at the {{Trees}} in the {{Central Forest}}},
  author = {Vicente, Ana M. and Costa, Rui M.},
  year = {2012},
  month = jun,
  volume = {74},
  pages = {967--969},
  issn = {08966273},
  doi = {10.1016/j.neuron.2012.06.003},
  file = {2012 - Vicente, Costa - Looking at the Trees in the Central Forest A New Pallidal-Striatal Cell Type.pdf},
  journal = {Neuron},
  language = {en},
  number = {6}
}

@article{Vijayan2012,
  title = {Thalamic Model of Awake Alpha Oscillations and Implications for Stimulus Processing},
  author = {Vijayan, S. and Kopell, N. J.},
  year = {2012},
  month = nov,
  volume = {109},
  pages = {18553--18558},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1215385109},
  file = {2012 - Vijayan, Kopell - Thalamic model of awake alpha oscillations and implications for stimulus processing.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {45}
}

@article{Vilares2017,
  title = {Dopaminergic Medication Increases Reliance on Current Information in {{Parkinson}}'s Disease},
  author = {Vilares, Iris and Kording, Konrad P.},
  year = {2017},
  month = aug,
  volume = {1},
  issn = {2397-3374},
  doi = {10.1038/s41562-017-0129},
  file = {Vilares and Kording - 2017 - Dopaminergic medication increases reliance on curr.pdf},
  journal = {Nature Human Behaviour},
  language = {en},
  number = {8}
}

@article{Vinyals,
  title = {{{StarCraft II}}: {{A New Challenge}} for {{Reinforcement Learning}}},
  author = {Vinyals, Oriol and Ewalds, Timo and Bartunov, Sergey and Georgiev, Petko and Vezhnevets, Alexander Sasha and Yeo, Michelle and Makhzani, Alireza and Agapiou, John and Schrittwieser, Julian and Quan, John and Gaffney, Stephen and Petersen, Stig and Simonyan, Karen and Schaul, Tom and {van Hasselt}, Hado and Silver, David and Lillicrap, Timothy and Calderone, Kevin and Keet, Paul and Brunasso, Anthony and Lawrence, David and Ekermo, Anders and Repp, Jacob and Tsing, Rodney},
  pages = {20},
  abstract = {This paper introduces SC2LE (StarCraft II Learning Environment), a reinforcement learning environment based on the game StarCraft II. This domain poses a new grand challenge for reinforcement learning, representing a more difficult class of problems than considered in most prior work. It is a multi-agent problem with multiple players interacting; there is imperfect information due to a partially observed map; it has a large action space involving the selection and control of hundreds of units; it has a large state space that must be observed solely from raw input feature planes; and it has delayed credit assignment requiring long-term strategies over thousands of steps. We describe the observation, action, and reward specification for the StarCraft II domain and provide an open source Python-based interface for communicating with the game engine. In addition to the main game maps, we provide a suite of mini-games focusing on different elements of StarCraft II gameplay. For the main game maps, we also provide an accompanying dataset of game replay data from human expert players. We give initial baseline results for neural networks trained from this data to predict game outcomes and player actions. Finally, we present initial baseline results for canonical deep reinforcement learning agents applied to the StarCraft II domain. On the mini-games, these agents learn to achieve a level of play that is comparable to a novice player. However, when trained on the main game, these agents are unable to make significant progress. Thus, SC2LE offers a new and challenging environment for exploring deep reinforcement learning algorithms and architectures.},
  file = {Vinyals et al. - StarCraft II A New Challenge for Reinforcement Le.pdf},
  language = {en}
}

@article{Viriyopase2012,
  title = {When {{Long}}-{{Range Zero}}-{{Lag Synchronization}} Is {{Feasible}} in {{Cortical Networks}}},
  author = {Viriyopase, Atthaphon and Bojak, Ingo and Zeitler, Magteld and Gielen, Stan},
  year = {2012},
  volume = {6},
  issn = {1662-5188},
  doi = {10.3389/fncom.2012.00049},
  file = {2012 - Viriyopase et al. - When Long-Range Zero-Lag Synchronization is Feasible in Cortical Networks.pdf},
  journal = {Frontiers in Computational Neuroscience},
  language = {en}
}

@article{Viswanathan1996,
  title = {L\'evy Flight Search Patterns of Wandering Albatrosses},
  author = {Viswanathan, G. M. and Afanasyev, V. and Buldyrev, S. V. and Murphy, E. J. and Prince, P. A. and Stanley, H. E.},
  year = {1996},
  month = may,
  volume = {381},
  pages = {413--415},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/381413a0},
  file = {1996-nature-v381-413.pdf},
  journal = {Nature},
  language = {en},
  number = {6581}
}

@article{Viswanathan1999,
  title = {Optimizing the Success of Random Searches},
  author = {Viswanathan, G. M. and Buldyrev, Sergey V. and Havlin, Shlomo and {da Luz}, M. G. E. and Raposo, E. P. and Stanley, H. Eugene},
  year = {1999},
  month = oct,
  volume = {401},
  pages = {911--914},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/44831},
  file = {Viswanathan et al. - 1999 - Optimizing the success of random searches.pdf},
  journal = {Nature},
  language = {en},
  number = {6756}
}

@article{Viswanathan1999a,
  title = {Optimizing the Success of Random Searches},
  author = {Viswanathan, G. M. and Buldyrev, Sergey V. and Havlin, Shlomo and {da Luz}, M. G. E. and Raposo, E. P. and Stanley, H. Eugene},
  year = {1999},
  month = oct,
  volume = {401},
  pages = {911--914},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/44831},
  file = {Viswanathan et al. - 1999 - Optimizing the success of random searches 2.pdf},
  journal = {Nature},
  language = {en},
  number = {6756}
}

@article{Viswanathan1999b,
  title = {Optimizing the Success of Random Searches},
  author = {Viswanathan, G. M. and Buldyrev, Sergey V. and Havlin, Shlomo and {da Luz}, M. G. E. and Raposo, E. P. and Stanley, H. Eugene},
  year = {1999},
  month = oct,
  volume = {401},
  pages = {911--914},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/44831},
  file = {Viswanathan et al. - 1999 - Optimizing the success of random searches 3.pdf},
  journal = {Nature},
  language = {en},
  number = {6756}
}

@article{Viswanathan2000,
  title = {Levy Flights in Random Searches},
  author = {Viswanathan, G M and Afanasyev, V and Buldyrev, Sergey V and Havlin, Shlomo and Stanley, H Eugene},
  year = {2000},
  pages = {12},
  abstract = {We review the general search problem of how to \"ynd randomly located objects that can only be detected in the limited vicinity of a forager, and discuss its quantitative description using the theory of random walks. We illustrate Le\~Avy ight foraging by comparison to Brownian random walks and discuss experimental observations of Le\~Avy ights in biological foraging. We review recent \"yndings suggesting that an inverse square probability density distribution P(`) {$\sim$} `-2 of step lengths ` can lead to optimal searches. Finally, we survey the explanations put forth to account for these unexpected \"yndings. c 2000 Published by Elsevier Science B.V. All rights reserved.},
  file = {Viswanathan et al. - 2000 - LeÃvy ights in random searches.pdf},
  journal = {Physica A},
  language = {en}
}

@article{Viswanathan2000a,
  title = {{{Le\~Avy}} Ights in Random Searches},
  author = {Viswanathan, G M and Afanasyev, V and Buldyrev, Sergey V and Havlin, Shlomo and Stanley, H Eugene},
  year = {2000},
  pages = {12},
  abstract = {We review the general search problem of how to \"ynd randomly located objects that can only be detected in the limited vicinity of a forager, and discuss its quantitative description using the theory of random walks. We illustrate Le\~Avy ight foraging by comparison to Brownian random walks and discuss experimental observations of Le\~Avy ights in biological foraging. We review recent \"yndings suggesting that an inverse square probability density distribution P(`) {$\sim$} `-2 of step lengths ` can lead to optimal searches. Finally, we survey the explanations put forth to account for these unexpected \"yndings. c 2000 Published by Elsevier Science B.V. All rights reserved.},
  file = {Viswanathan et al. - 2000 - LeÃvy ights in random searches 2.pdf},
  journal = {Physica A},
  language = {en}
}

@article{Viswanathan2000b,
  title = {{{Le\~Avy}} Ights in Random Searches},
  author = {Viswanathan, G M and Afanasyev, V and Buldyrev, Sergey V and Havlin, Shlomo and Stanley, H Eugene},
  year = {2000},
  pages = {12},
  abstract = {We review the general search problem of how to \"ynd randomly located objects that can only be detected in the limited vicinity of a forager, and discuss its quantitative description using the theory of random walks. We illustrate Le\~Avy ight foraging by comparison to Brownian random walks and discuss experimental observations of Le\~Avy ights in biological foraging. We review recent \"yndings suggesting that an inverse square probability density distribution P(`) {$\sim$} `-2 of step lengths ` can lead to optimal searches. Finally, we survey the explanations put forth to account for these unexpected \"yndings. c 2000 Published by Elsevier Science B.V. All rights reserved.},
  file = {Viswanathan et al. - 2000 - LeÃvy ights in random searches 3.pdf},
  journal = {Physica A},
  language = {en}
}

@article{Viswanathan2008,
  title = {L\'evy Flights and Superdiffusion in the Context of Biological Encounters and Random Searches},
  author = {Viswanathan, G.M. and Raposo, E.P. and {da Luz}, M.G.E.},
  year = {2008},
  month = sep,
  volume = {5},
  pages = {133--150},
  issn = {15710645},
  doi = {10.1016/j.plrev.2008.03.002},
  abstract = {We review the general problem of random searches in the context of biological encounters. We analyze deterministic and stochastic aspects of searching in general and address the destructive and nondestructive cases specifically. We discuss the concepts of L\'evy walks as adaptive strategies and explore possible examples. We also review L\'evy searches in other media and spaces, including lattices and networks as opposed to continuous environments. We analyze empirical evidence supporting the L\'evy flight foraging hypothesis, as well as the more general idea of superdiffusive foraging. We compare these hypothesis with alternative theories of random searches. Finally, we comment on several issues relevant to the practical application of models of L\'evy and superdiffusive strategies to the general question of biological foraging.},
  file = {Viswanathan et al. - 2008 - Lévy flights and superdiffusion in the context of .pdf},
  journal = {Physics of Life Reviews},
  language = {en},
  number = {3}
}

@article{Vogel2004,
  title = {Neural Activity Predicts Individual Differences in Visual Working Memory Capacity},
  author = {Vogel, Edward K. and Machizawa, Maro G.},
  year = {2004},
  month = apr,
  volume = {428},
  pages = {748--751},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature02447},
  file = {2004 - Vogel, Machizawa - Neural activity predicts individual differences in visual working memory capacity.pdf},
  journal = {Nature},
  language = {en},
  number = {6984}
}

@article{Vogels2009,
  title = {Gating Multiple Signals through Detailed Balance of Excitation and Inhibition in Spiking Networks},
  author = {Vogels, Tim P and Abbott, L F},
  year = {2009},
  month = apr,
  volume = {12},
  pages = {483--491},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.2276},
  file = {2009 - Vogels, Abbott - Gating Multiple Signals through Detailed Balance of Excitation and Inhibition in Spiking Networks T.P.pdf;2009 - Vogels, Abbott - Gating Multiple Signals through Detailed Balance of Excitation and Inhibition in Spiking Networks T.P(2).pdf},
  journal = {Nature Neuroscience},
  language = {en},
  number = {4}
}

@article{Vogelstein2014,
  title = {Discovery of {{Brainwide Neural}}-{{Behavioral Maps}} via {{Multiscale Unsupervised Structure Learning}}},
  author = {Vogelstein, J. T. and Park, Y. and Ohyama, T. and Kerr, R. A. and Truman, J. W. and Priebe, C. E. and Zlatic, M.},
  year = {2014},
  month = apr,
  volume = {344},
  pages = {386--392},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1250298},
  file = {2014 - Vogelstein et al. - Discovery of Brainwide Neural-Behavioral Maps via Multiscale Unsupervised Structure Learning.pdf;Vogelstein et al. - 2014 - Discovery of Brainwide Neural-Behavioral Maps via .pdf},
  journal = {Science},
  language = {en},
  number = {6182}
}

@article{vonNicolai2014,
  title = {Corticostriatal {{Coordination}} through {{Coherent Phase}}-{{Amplitude Coupling}}},
  author = {{von Nicolai}, C. and Engler, G. and Sharott, A. and Engel, A. K. and Moll, C. K. and Siegel, M.},
  year = {2014},
  month = apr,
  volume = {34},
  pages = {5938--5948},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.5007-13.2014},
  file = {von Nicolai et al. - 2014 - Corticostriatal Coordination through Coherent Phas.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {17}
}

@article{Voytek2013,
  title = {A Method for Event-Related Phase/Amplitude Coupling},
  author = {Voytek, Bradley and D'Esposito, Mark and Crone, Nathan and Knight, Robert T.},
  year = {2013},
  month = jan,
  volume = {64},
  pages = {416--424},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2012.09.023},
  abstract = {Phase/amplitude coupling (PAC) is emerging as an important electrophysiological measure of local and long-distance neuronal communication. Current techniques for calculating PAC provide a numerical index that represents an average value across an arbitrarily long time period. This requires researchers to rely on block design experiments and temporal concatenation at the cost of the sub-second temporal resolution afforded by electrophysiological recordings. Here we present a method for calculating event-related phase/ amplitude coupling (ERPAC) designed to capture the temporal evolution of task-related changes in PAC across events or between distant brain regions that is applicable to human or animal electromagnetic recording.},
  file = {Voytek et al. - 2013 - A method for event-related phaseamplitude couplin.pdf},
  journal = {NeuroImage},
  language = {en}
}

@article{Voytek2013a,
  title = {Stimulating the Aging Brain},
  author = {Voytek, Bradley and Gazzaley, Adam},
  year = {2013},
  month = jan,
  volume = {73},
  pages = {1--3},
  issn = {03645134},
  doi = {10.1002/ana.23790},
  file = {Voytek and Gazzaley - 2013 - Stimulating the aging brain.pdf},
  journal = {Annals of Neurology},
  language = {en},
  number = {1}
}

@article{Voytek2015,
  title = {Dynamic {{Network Communication}} as a {{Unifying Neural Basis}} for {{Cognition}}, {{Development}}, {{Aging}}, and {{Disease}}},
  author = {Voytek, Bradley and Knight, Robert T.},
  year = {2015},
  month = jun,
  volume = {77},
  pages = {1089--1097},
  issn = {00063223},
  doi = {10.1016/j.biopsych.2015.04.016},
  file = {Voytek and Knight - 2015 - Dynamic Network Communication as a Unifying Neural 2.pdf;Voytek and Knight - 2015 - Dynamic Network Communication as a Unifying Neural.pdf},
  journal = {Biological Psychiatry},
  language = {en},
  number = {12}
}

@article{Voytek2015a,
  title = {Oscillatory Dynamics Coordinating Human Frontal Networks in Support of Goal Maintenance},
  author = {Voytek, Bradley and Kayser, Andrew S and Badre, David and Fegen, David and Chang, Edward F and Crone, Nathan E and Parvizi, Josef and Knight, Robert T and D'Esposito, Mark},
  year = {2015},
  month = sep,
  volume = {18},
  pages = {1318--1324},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.4071},
  file = {Voytek et al. - 2015 - Oscillatory dynamics coordinating human frontal ne.pdf},
  journal = {Nature Neuroscience},
  language = {en},
  number = {9}
}

@article{Voytek2015c,
  title = {Age-{{Related Changes}} in 1/f {{Neural Electrophysiological Noise}}},
  author = {Voytek, B. and Kramer, M. A. and Case, J. and Lepage, K. Q. and Tempesta, Z. R. and Knight, R. T. and Gazzaley, A.},
  year = {2015},
  month = sep,
  volume = {35},
  pages = {13257--13265},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.2332-14.2015},
  file = {Voytek et al. - 2015 - Age-Related Changes in 1f Neural Electrophysiolog.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {38}
}

@article{Vreeswijk1996,
  title = {Chaos in {{Neuronal Networks}} with {{Balanced Excitatory}} and {{Inhibitory Activity}}},
  author = {v. Vreeswijk, C. and Sompolinsky, H.},
  year = {1996},
  month = dec,
  volume = {274},
  pages = {1724--1726},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.274.5293.1724},
  file = {1996 - van Vreeswijk, Sompolinsky - Chaos in neuronal networks with balanced excitatory and inhibitory activity.pdf},
  journal = {Science},
  language = {en},
  number = {5293}
}

@article{Vul2008,
  title = {Measuring the {{Crowd Within}}: {{Probabilistic Representations Within Individuals}}},
  shorttitle = {Measuring the {{Crowd Within}}},
  author = {Vul, Edward and Pashler, Harold},
  year = {2008},
  month = jul,
  volume = {19},
  pages = {645--647},
  issn = {0956-7976, 1467-9280},
  doi = {10.1111/j.1467-9280.2008.02136.x},
  file = {2008 - Vul, Pashler - Measuring the crowd within probabilistic representations within individuals.pdf},
  journal = {Psychological Science},
  language = {en},
  number = {7}
}

@article{Wach2013,
  title = {The Effect of 10 {{Hz}} Transcranial Alternating Current Stimulation ({{tACS}}) on Corticomuscular Coherence},
  author = {Wach, Claudia and Krause, Vanessa and Moliadze, Vera and Paulus, Walter and Schnitzler, Alfons and Pollok, Bettina},
  year = {2013},
  volume = {7},
  issn = {1662-5161},
  doi = {10.3389/fnhum.2013.00511},
  file = {2013 - Wach et al. - The effect of 10 Hz transcranial alternating current stimulation (tACS) on corticomuscular coherence.pdf;Wach et al. - 2013 - The effect of 10 Hz transcranial alternating curre.pdf},
  journal = {Frontiers in Human Neuroscience},
  language = {en}
}

@article{Wagenmakers2004,
  title = {{{AIC}} Model Selection Using {{Akaike}} Weights},
  author = {Wagenmakers, Eric-Jan and Farrell, Simon},
  year = {2004},
  month = feb,
  volume = {11},
  pages = {192--196},
  issn = {1069-9384, 1531-5320},
  doi = {10.3758/BF03206482},
  file = {2004 - Wagenmakers, Farrell - AIC model selection using Akaike weights.pdf},
  journal = {Psychonomic Bulletin \& Review},
  language = {en},
  number = {1}
}

@article{Wainrib2013,
  title = {Topological and {{Dynamical Complexity}} of {{Random Neural Networks}}},
  author = {Wainrib, Gilles and Touboul, Jonathan},
  year = {2013},
  month = mar,
  volume = {110},
  issn = {0031-9007, 1079-7114},
  doi = {10.1103/PhysRevLett.110.118101},
  abstract = {Random neural networks are dynamical descriptions of randomly interconnected neural units. These show a phase transition to chaos as a disorder parameter is increased. The microscopic mechanisms underlying this phase transition are unknown, and similarly to spin-glasses, shall be fundamentally related to the behavior of the system. In this Letter we investigate the explosion of complexity arising near that phase transition. We show that the mean number of equilibria undergoes a sharp transition from one equilibrium to a very large number scaling exponentially with the dimension on the system. Near criticality, we compute the exponential rate of divergence, called topological complexity. Strikingly, we show that it behaves exactly as the maximal Lyapunov exponent, a classical measure of dynamical complexity. This relationship unravels a microscopic mechanism leading to chaos which we further demonstrate on a simpler class of disordered systems, suggesting a deep and underexplored link between topological and dynamical complexity.},
  archiveprefix = {arXiv},
  eprint = {1210.5082},
  eprinttype = {arxiv},
  file = {2013 - Wainrib, Touboul - Topological and dynamical complexity of random neural networks.pdf;Wainrib and Touboul - 2013 - Topological and Dynamical Complexity of Random Neu.pdf},
  journal = {Physical Review Letters},
  keywords = {Condensed Matter - Disordered Systems and Neural Networks,Mathematical Physics,Quantitative Biology - Neurons and Cognition},
  language = {en},
  number = {11}
}

@article{Wald1947,
  title = {Foundations of a {{General Theory}} of {{Sequential Decision Functions}}},
  author = {Wald, Abraham},
  year = {1947},
  month = oct,
  volume = {15},
  pages = {279},
  issn = {00129682},
  doi = {10.2307/1905331},
  file = {2012 - Unknown - No Title.pdf},
  journal = {Econometrica},
  language = {en},
  number = {4}
}

@article{Walker2001,
  title = {Minimax {{Play}} at {{Wimbledon}}},
  author = {Walker, Mark and Wooders, John},
  year = {2001},
  month = dec,
  volume = {91},
  pages = {1521--1538},
  issn = {0002-8282},
  doi = {10.1257/aer.91.5.1521},
  abstract = {We use data from classic professional tennis matches to provide an empirical test of the theory of mixed strategy equilibrium. We \TH nd that the serve-andreturn play of John McEnroe, Bjorn Borg, Boris Becker, Pete Sampras and others is consistent with equilibrium play. The same statistical tests soundly reject the assumption of equilibrium play in experimental data, including the data from Barry O'Neill's celebrated experiment.},
  file = {Walker and Wooders - 2001 - Minimax Play at Wimbledon.pdf},
  journal = {American Economic Review},
  language = {en},
  number = {5}
}

@article{Walther2009,
  title = {Natural {{Scene Categories Revealed}} in {{Distributed Patterns}} of {{Activity}} in the {{Human Brain}}},
  author = {Walther, D. B. and Caddigan, E. and {Fei-Fei}, L. and Beck, D. M.},
  year = {2009},
  month = aug,
  volume = {29},
  pages = {10573--10581},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.0559-09.2009},
  file = {../../Zotero/storage/B245PG9T/2015 - Biphenyls - HHS Public Access.pdf;2009 - Walther et al. - Natural scene categories revealed in distributed patterns of activity in the human brain.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {34}
}

@article{Wang1996,
  title = {Gamma {{Oscillation}} by {{Synaptic Inhibition}} in a {{Hippocampal Interneuronal Network Model}}},
  author = {Wang, Xiao-Jing and Buzs{\'a}ki, Gy{\"o}rgy},
  year = {1996},
  month = oct,
  volume = {16},
  pages = {6402--6413},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.16-20-06402.1996},
  file = {Wang and Buzsáki - 1996 - Gamma Oscillation by Synaptic Inhibition in a Hipp.pdf},
  journal = {The Journal of Neuroscience},
  language = {en},
  number = {20}
}

@article{Wang1998,
  title = {Calcium {{Coding}} and {{Adaptive Temporal Computation}} in {{Cortical Pyramidal Neurons}}},
  author = {Wang, Xiao-Jing},
  year = {1998},
  month = mar,
  volume = {79},
  pages = {1549--1566},
  issn = {0022-3077, 1522-1598},
  doi = {10.1152/jn.1998.79.3.1549},
  file = {1998 - Wang - Calcium coding and adaptive temporal computation in cortical pyramidal neurons.pdf},
  journal = {Journal of Neurophysiology},
  language = {en},
  number = {3}
}

@article{Wang2002,
  title = {Probabilistic {{Decision Making}} by {{Slow Reverberation}} in {{Cortical Circuits}}},
  author = {Wang, Xiao-Jing},
  year = {2002},
  month = dec,
  volume = {36},
  pages = {955--968},
  issn = {08966273},
  doi = {10.1016/S0896-6273(02)01092-9},
  abstract = {Recent physiological studies of alert primates have revealed cortical neural correlates of key steps in a perceptual decision-making process. To elucidate synaptic mechanisms of decision making, I investigated a biophysically realistic cortical network model for a visual discrimination experiment. In the model, slow recurrent excitation and feedback inhibition produce attractor dynamics that amplify the difference between conflicting inputs and generates a binary choice. The model is shown to account for salient characteristics of the observed decision-correlated neural activity, as well as the animal's psychometric function and reaction times. These results suggest that recurrent excitation mediated by NMDA receptors provides a candidate cellular mechanism for the slow time integration of sensory stimuli and the formation of categorical choices in a decision-making neocortical network.},
  file = {2002 - Wang - by Slow Reverberation in Cortical Circuits.pdf},
  journal = {Neuron},
  language = {en},
  number = {5}
}

@article{Wang2010,
  title = {Neurophysiological and {{Computational Principles}} of {{Cortical Rhythms}} in {{Cognition}}},
  author = {Wang, Xiao-Jing},
  year = {2010},
  month = jul,
  volume = {90},
  pages = {1195--1268},
  issn = {0031-9333, 1522-1210},
  doi = {10.1152/physrev.00035.2008},
  file = {2010 - Wang - Neurophysiological and computational principles of cortical rhythms in cognition.pdf;2010 - Wang - Neurophysiological and computational principles of cortical rhythms in cognition(2).pdf},
  journal = {Physiological Reviews},
  language = {en},
  number = {3}
}

@article{Wang2010a,
  title = {Synchrony of {{Thalamocortical Inputs Maximizes Cortical Reliability}}},
  author = {Wang, H. P. and Spencer, D. and Fellous, J. M. and Sejnowski, T. J.},
  year = {2010},
  month = apr,
  volume = {328},
  pages = {106--109},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1183108},
  file = {2010 - Wang et al. - Synchrony of Thalamocortical Inputs Maximizes Cortical Reliability.pdf},
  journal = {Science},
  language = {en},
  number = {5974}
}

@article{Wang2013,
  title = {A {{Realistic Neural Mass Model}} of the {{Cortex}} with {{Laminar}}-{{Specific Connections}} and {{Synaptic Plasticity}} \textendash{} {{Evaluation}} with {{Auditory Habituation}}},
  author = {Wang, Peng and Kn{\"o}sche, Thomas R.},
  editor = {Lytton, William W.},
  year = {2013},
  month = oct,
  volume = {8},
  pages = {e77876},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0077876},
  abstract = {In this work we propose a biologically realistic local cortical circuit model (LCCM), based on neural masses, that incorporates important aspects of the functional organization of the brain that have not been covered by previous models: (1) activity dependent plasticity of excitatory synaptic couplings via depleting and recycling of neurotransmitters and (2) realistic interlaminar dynamics via laminar-specific distribution of and connections between neural populations. The potential of the LCCM was demonstrated by accounting for the process of auditory habituation. The model parameters were specified using Bayesian inference. It was found that: (1) besides the major serial excitatory information pathway (layer 4 to layer 2/3 to layer 5/6), there exists a parallel ``short-cut'' pathway (layer 4 to layer 5/6), (2) the excitatory signal flow from the pyramidal cells to the inhibitory interneurons seems to be more intra-laminar while, in contrast, the inhibitory signal flow from inhibitory interneurons to the pyramidal cells seems to be both intra- and inter-laminar, and (3) the habituation rates of the connections are unsymmetrical: forward connections (from layer 4 to layer 2/3) are more strongly habituated than backward connections (from Layer 5/6 to layer 4). Our evaluation demonstrates that the novel features of the LCCM are of crucial importance for mechanistic explanations of brain function. The incorporation of these features into a mass model makes them applicable to modeling based on macroscopic data (like EEG or MEG), which are usually available in human experiments. Our LCCM is therefore a valuable building block for future realistic models of human cognitive function.},
  file = {2013 - Wang, KnÃ¶sche - A Realistic Neural Mass Model of the Cortex with Laminar-Specific Connections and Synaptic Plasticity Evaluati.pdf;Wang and Knösche - 2013 - A Realistic Neural Mass Model of the Cortex with L.pdf},
  journal = {PLoS ONE},
  language = {en},
  number = {10}
}

@article{Wang2015,
  title = {Social Cycling and Conditional Responses in the {{Rock}}-{{Paper}}-{{Scissors}} Game},
  author = {Wang, Zhijian and Xu, Bin and Zhou, Hai-Jun},
  year = {2015},
  month = may,
  volume = {4},
  pages = {5830},
  issn = {2045-2322},
  doi = {10.1038/srep05830},
  file = {Wang et al. - 2015 - Social cycling and conditional responses in the Ro.pdf},
  journal = {Sci Rep},
  language = {en},
  number = {1}
}

@article{Wang2016,
  title = {Brain Structure and Dynamics across Scales: In Search of Rules},
  shorttitle = {Brain Structure and Dynamics across Scales},
  author = {Wang, Xiao-Jing and Kennedy, Henry},
  year = {2016},
  month = apr,
  volume = {37},
  pages = {92--98},
  issn = {09594388},
  doi = {10.1016/j.conb.2015.12.010},
  file = {Wang and Kennedy - 2016 - Brain structure and dynamics across scales in sea.pdf},
  journal = {Current Opinion in Neurobiology},
  language = {en}
}

@article{Wang2017,
  title = {{{SAMPLE EFFICIENT ACTOR}}-{{CRITIC WITH EXPERIENCE REPLAY}}},
  author = {Wang, Ziyu and Bapst, Victor and Mnih, Volodymyr and Munos, Remi and {de Freitas}, Nando and Heess, Nicolas and Kavukcuoglu, Koray},
  year = {2017},
  pages = {20},
  abstract = {This paper presents an actor-critic deep reinforcement learning agent with experience replay that is stable, sample efficient, and performs remarkably well on challenging environments, including the discrete 57-game Atari domain and several continuous control problems. To achieve this, the paper introduces several innovations, including truncated importance sampling with bias correction, stochastic dueling network architectures, and a new trust region policy optimization method.},
  file = {Wang et al. - 2017 - SAMPLE EFFICIENT ACTOR-CRITIC WITH EXPERIENCE REPL.pdf},
  language = {en}
}

@article{Wang2019,
  title = {Monkeys Are Curious about Counterfactual Outcomes},
  author = {Wang, Maya Zhe and Hayden, Benjamin Y.},
  year = {2019},
  month = aug,
  volume = {189},
  pages = {1--10},
  issn = {00100277},
  doi = {10.1016/j.cognition.2019.03.009},
  abstract = {Many non-human animals show exploratory behaviors. It remains unclear whether any possess human-like curiosity. We previously proposed three criteria for applying the term curiosity to animal behavior: (1) the subject is willing to sacrifice reward to obtain information, (2) the information provides no immediate instrumental or strategic benefit, and (3) the amount the subject is willing to pay depends systematically on the amount of information available. In previous work on information-seeking in animals, information generally predicts upcoming rewards, and animals' decisions may therefore be a byproduct of reinforcement processes. Here we get around this potential confound by taking advantage of macaques' ability to reason counterfactually (that is, about outcomes that could have occurred had the subject chosen differently). Specifically, macaques sacrificed fluid reward to obtain information about counterfactual outcomes. Moreover, their willingness to pay scaled with the information (Shannon entropy) offered by the counterfactual option. These results demonstrate the existence of human-like curiosity in non-human primates according to our criteria, which circumvent several confounds associated with less stringent criteria.},
  file = {Wang and Hayden - 2019 - Monkeys are curious about counterfactual outcomes.pdf},
  journal = {Cognition},
  language = {en}
}

@article{Wang2020,
  title = {Microglia Mediate Forgetting via Complement-Dependent Synaptic Elimination},
  author = {Wang, Chao and Yue, Huimin and Hu, Zhechun and Shen, Yuwen and Ma, Jiao and Li, Jie and Wang, Xiao-Dong and Wang, Liang and Sun, Binggui and Shi, Peng and Wang, Lang and Gu, Yan},
  year = {2020},
  month = feb,
  volume = {367},
  pages = {688--694},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aaz2288},
  abstract = {Synapses between engram cells are believed to be substrates for memory storage, and the weakening or loss of these synapses leads to the forgetting of related memories. We found engulfment of synaptic components by microglia in the hippocampi of healthy adult mice. Depletion of microglia or inhibition of microglial phagocytosis prevented forgetting and the dissociation of engram cells. By introducing CD55 to inhibit complement pathways, specifically in engram cells, we further demonstrated that microglia regulated forgetting in a complement- and activity-dependent manner. Additionally, microglia were involved in both neurogenesis-related and neurogenesis-unrelated memory degradation. Together, our findings revealed complement-dependent synapse elimination by microglia as a mechanism underlying the forgetting of remote memories.},
  file = {Wang et al. - 2020 - Microglia mediate forgetting via complement-depend.pdf},
  journal = {Science},
  language = {en},
  number = {6478}
}

@techreport{Wang2020a,
  title = {Curiosity, Latent Learning, and Cognitive Maps},
  author = {Wang, Maya Zhe and Hayden, Benjamin Y.},
  year = {2020},
  month = jun,
  institution = {{Neuroscience}},
  doi = {10.1101/2020.05.31.123380},
  abstract = {ABSTRACT           Curiosity refers to a desire for information that is not driven by immediate strategic or instrumental concerns. Latent earning refers to a form of learning that is not directly driven by standard reinforcement learning processes. We propose that curiosity serves the purpose of motivating latent learning. Thus, while latent learning is often treated as an incidental or passive process, in practice it most often reflects a strong evolved pressure to consume large amounts of information. That large volume of information in turn allows curious decision makers to generate sophisticated representations of the structure of their environment, known as cognitive maps. Cognitive maps facilitate adaptive and flexible behavior while maintaining its adaptivity and flexibility via map updates based on new information. Here we describe data supporting the idea that orbitofrontal cortex (OFC) and dorsal anterior cingulate cortex (dACC) play complementary roles in curiosity-driven learning. Specifically, we propose that (1) OFC tracks the innate value of information and incorporates new information into a detailed cognitive map; and (2) dACC tracks the environmental demands and information availability to then use the cognitive map for guiding behavior.},
  file = {Wang and Hayden - 2020 - Curiosity, latent learning, and cognitive maps.pdf},
  language = {en},
  type = {Preprint}
}

@techreport{Wang2021,
  title = {Evolving the {{Olfactory System}} with {{Machine Learning}}},
  author = {Wang, Peter Y. and Sun, Yi and Axel, Richard and Abbott, L.F. and Yang, Guangyu Robert},
  year = {2021},
  month = apr,
  institution = {{Neuroscience}},
  doi = {10.1101/2021.04.15.439917},
  abstract = {Summary                        The convergent evolution of the fly and mouse olfactory system led us to ask whether the anatomic connectivity and functional logic             in vivo             would evolve in artificial neural networks constructed to perform olfactory tasks. Artificial networks trained to classify odor identity recapitulate the connectivity inherent in the olfactory system. Input units are driven by a single receptor type, and units driven by the same receptor converge to form a glomerulus. Glomeruli exhibit sparse, unstructured connectivity to a larger, expansion layer. When trained to both classify odor and impart innate valence on odors, the network develops independent pathways for innate output and odor classification. Thus, artificial networks evolve even without the biological mechanisms necessary to build these systems             in vivo             , providing a rationale for the convergent evolution of olfactory circuits.},
  file = {Wang et al. - 2021 - Evolving the Olfactory System with Machine Learnin.pdf},
  language = {en},
  type = {Preprint}
}

@article{Waschke2017,
  title = {States and Traits of Neural Irregularity in the Age-Varying Human Brain},
  author = {Waschke, Leonhard and Woestmann, Malte and Obleser, Jonas},
  year = {2017},
  month = may,
  doi = {10.1101/103432},
  abstract = {Humans sometimes do perceive differences where physically there are none. It is thus tenable that perception is susceptible to seemingly random fluctuations in brain activity or ``neural noise''. Here, we demonstrate the potency of both trial-aggregated as well as trial-by-trial measures in the human electroencephalogram (EEG) to characterize neural noise as (i) a trait of individuals of varying age (n = 19; 19\textendash 74 years), and (ii) a brain state that predicts an individual's impending perceptual decision. Human participants were instructed to discriminate two identical, consecutively presented pure tones. Behaviorally, all participants reported perceiving pitch differences of first versus second tone. Neurally, decisions for the first versus the second tone were preceded by more consistently phase-locked responses to the first tone in the theta (4\textendash 9 Hz) band at central scalp electrodes. Second, a trial-wise information-theoretic measure quantifying the irregularity of broadband EEG, Weighted Permutation Entropy (WPE), prior to stimulus onset allowed to classify a listener's impending decision on this trial. Average entropy not only increased with participants' age, but correlated with previously suggested measures of an altered excitation\textendash inhibition balance in the aging brain. Therefore, neural noise is best conceived not only as a state variable that can shape perceptual decisions but moreover can capture trait-like changes with age.},
  file = {Waschke et al. - 2017 - States and traits of neural irregularity in the ag 2.pdf;Waschke et al. - 2017 - States and traits of neural irregularity in the ag.pdf},
  journal = {bioRxiv},
  language = {en}
}

@article{Watanabe2011,
  title = {Prediction of Subsequent Recognition Performance Using Brain Activity in the Medial Temporal Lobe},
  author = {Watanabe, Takamitsu and Hirose, Satoshi and Wada, Hiroyuki and Katsura, Masaki and Chikazoe, Junichi and Jimura, Koji and Imai, Yoshio and Machida, Toru and Shirouzu, Ichiro and Miyashita, Yasushi and Konishi, Seiki},
  year = {2011},
  month = feb,
  volume = {54},
  pages = {3085--3092},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2010.10.066},
  abstract = {Application of multivoxel pattern analysis (MVPA) to functional magnetic resonance imaging (fMRI) data enables reconstruction and classification of cognitive status from brain activity. However, previous studies using MVPA have extracted information about cognitive status that is experienced simultaneously with fMRI scanning, but not one that will be observed after the scanning. In this study, by focusing on activity in the medial temporal lobe (MTL), we demonstrate that MVPA on fMRI data is capable of predicting subsequent recognition performance. In this experiment, six runs of fMRI signals were acquired during encoding of phonogram stimuli. In the analysis, using data acquired in runs 1\textendash 3, we first conducted MVPA-based voxelwise search for the clusters in the MTL whose signals contained the most information about subsequent recognition performance. Next, using the fMRI signals acquired in runs 1\textendash 3 from the selected clusters, we trained a classifier function in MVPA. Finally, the trained classifier function was applied to fMRI signals acquired in runs 4\textendash 6. Consequently, we succeeded in predicting the subsequent recognition performance for stimuli studied in runs 4\textendash 6 with significant accuracy. This accurate prediction suggests that MVPA can extract information that is associated not only with concurrent cognitive status, but also with behavior in the near future.},
  file = {2011 - Watanabe et al. - Prediction of subsequent recognition performance using brain activity in the medial temporal lobe.pdf},
  journal = {NeuroImage},
  language = {en},
  number = {4}
}

@article{Watrous2013,
  title = {Frequency-Specific Network Connectivity Increases Underlie Accurate Spatiotemporal Memory Retrieval},
  author = {Watrous, Andrew J and Tandon, Nitin and Conner, Chris R and Pieters, Thomas and Ekstrom, Arne D},
  year = {2013},
  month = mar,
  volume = {16},
  pages = {349--356},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.3315},
  file = {Watrous et al. - 2013 - Frequency-specific network connectivity increases .pdf},
  journal = {Nature Neuroscience},
  language = {en},
  number = {3}
}

@article{Watson2016,
  title = {How {{Can Evolution Learn}}?},
  author = {Watson, Richard A. and Szathm{\'a}ry, E{\"o}rs},
  year = {2016},
  month = feb,
  volume = {31},
  pages = {147--157},
  issn = {01695347},
  doi = {10.1016/j.tree.2015.11.009},
  file = {Watson and Szathmáry - 2016 - How Can Evolution Learn.pdf},
  journal = {Trends in Ecology \& Evolution},
  language = {en},
  number = {2}
}

@article{Watts1998,
  title = {Collective Dynamics of `Small-World' Networks},
  author = {Watts, Duncan J and Strogatz, Steven H},
  year = {1998},
  volume = {393},
  pages = {3},
  file = {1998 - Watts, Strogatz - Collective dynamics of ‘small-world’ networks.pdf},
  language = {en}
}

@article{Wayne,
  title = {Unsupervised {{Predictive Memory}} in a {{Goal}}-{{Directed Agent}}},
  author = {Wayne, Greg and Hung, Chia-Chun and Amos, David and Mirza, Mehdi and Ahuja, Arun and {Grabska-Barwinska}, Agnieszka and Rae, Jack and Mirowski, Piotr and Leibo, Joel Z and Santoro, Adam and Gemici, Mevlana and Reynolds, Malcolm and Harley, Tim and Abramson, Josh and Mohamed, Shakir and Rezende, Danilo and Saxton, David and Cain, Adam and Hillier, Chloe and Silver, David and Kavukcuoglu, Koray and Botvinick, Matt and Hassabis, Demis and Lillicrap, Timothy},
  pages = {57},
  file = {Wayne et al. - Unsupervised Predictive Memory in a Goal-Directed .pdf},
  language = {en}
}

@article{Wearmouth2014,
  title = {Scaling Laws of Ambush Predator `Waiting' Behaviour Are Tuned to a Common Ecology},
  author = {Wearmouth, Victoria J. and McHugh, Matthew J. and Humphries, Nicolas E. and Naegelen, Aurore and Ahmed, Mohammed Z. and Southall, Emily J. and Reynolds, Andrew M. and Sims, David W.},
  year = {2014},
  month = may,
  volume = {281},
  pages = {20132997},
  issn = {0962-8452, 1471-2954},
  doi = {10.1098/rspb.2013.2997},
  abstract = {The decisions animals make about how long to wait between activities can determine the success of diverse behaviours such as foraging, group formation or risk avoidance. Remarkably, for diverse animal species, including humans, spontaneous patterns of waiting times show random `burstiness' that appears scale-invariant across a broad set of scales. However, a general theory linking this phenomenon across the animal kingdom currently lacks an ecological basis. Here, we demonstrate from tracking the activities of 15 sympatric predator species (cephalopods, sharks, skates and teleosts) under natural and controlled conditions that bursty waiting times are an intrinsic spontaneous behaviour well approximated by heavy-tailed (power-law) models over data ranges up to four orders of magnitude. Scaling exponents quantifying ratios of frequent short to rare very long waits are species-specific, being determined by traits such as foraging mode (active versus ambush predation), body size and prey preference. A stochastic\textendash deterministic decision model reproduced the empirical waiting time scaling and species-specific exponents, indicating that apparently complex scaling can emerge from simple decisions. Results indicate temporal power-law scaling is a behavioural `rule of thumb' that is tuned to species' ecological traits, implying a common pattern may have naturally evolved that optimizes move\textendash wait decisions in less predictable natural environments.},
  file = {Wearmouth et al. - 2014 - Scaling laws of ambush predator ‘waiting’ behaviou.pdf},
  journal = {Proc. R. Soc. B.},
  language = {en},
  number = {1782}
}

@article{Weber,
  title = {Imagination-{{Augmented Agents}} for {{Deep Reinforcement Learning}}},
  author = {Weber, Th{\'e}ophane and Racani{\`e}re, S{\'e}bastien and Reichert, David P and Buesing, Lars and Guez, Arthur and Heess, Nicolas and Li, Yujia and Pascanu, Razvan and Battaglia, Peter and Hassabis, Demis and Silver, David and Wierstra, Daan},
  pages = {20},
  abstract = {We introduce Imagination-Augmented Agents (I2As), a novel architecture for deep reinforcement learning combining model-free and model-based aspects. In contrast to most existing model-based reinforcement learning and planning methods, which prescribe how a model should be used to arrive at a policy, I2As learn to interpret predictions from a learned environment model to construct implicit plans in arbitrary ways, by using the predictions as additional context in deep policy networks. I2As show improved data efficiency, performance, and robustness to model misspecification compared to several baselines.},
  file = {Weber et al. - Imagination-Augmented Agents for Deep Reinforcemen.pdf},
  language = {en}
}

@article{Weibull,
  title = {{{WHAT HAVE WE LEARNED FROM EVOLUTIONARY GAME THEORY SO FAR}}?},
  author = {Weibull, J{\"o}rgen W},
  pages = {30},
  file = {1997 - Wiebull - What have we learned from Evolutionary Game Theory so far.pdf},
  language = {en}
}

@article{Weichwald2015,
  title = {Causal Interpretation Rules for Encoding and Decoding Models in Neuroimaging},
  author = {Weichwald, Sebastian and Meyer, Timm and {\"O}zdenizci, Ozan and Sch{\"o}lkopf, Bernhard and Ball, Tonio and {Grosse-Wentrup}, Moritz},
  year = {2015},
  month = apr,
  volume = {110},
  pages = {48--59},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2015.01.036},
  abstract = {Causal terminology is often introduced in the interpretation of encoding and decoding models trained on neuroimaging data. In this article, we investigate which causal statements are warranted and which ones are not supported by empirical evidence. We argue that the distinction between encoding and decoding models is not sufficient for this purpose: relevant features in encoding and decoding models carry a different meaning in stimulus- and in response-based experimental paradigms.We show that only encoding models in the stimulus-based setting support unambiguous causal interpretations. By combining encoding and decoding models trained on the same data, however, we obtain insights into causal relations beyond those that are implied by each individual model type. We illustrate the empirical relevance of our theoretical findings on EEG data recorded during a visuo-motor learning task.},
  file = {Weichwald et al. - 2015 - Causal interpretation rules for encoding and decod 2.pdf;Weichwald et al. - 2015 - Causal interpretation rules for encoding and decod.pdf},
  journal = {NeuroImage},
  keywords = {Computer Science - Machine Learning,Quantitative Biology - Neurons and Cognition,Statistics - Applications,Statistics - Machine Learning},
  language = {en}
}

@article{Weinstein,
  title = {Structure {{Learning}} in {{Motor Control}}: {{A Deep Reinforcement Learning Model}}},
  author = {Weinstein, Ari and Botvinick, Matthew M},
  pages = {6},
  abstract = {Motor adaptation displays a structure-learning effect: adaptation to a new perturbation occurs more quickly when the subject has prior exposure to perturbations with related structure. Although this `learning-to-learn' effect is well documented, its underlying computational mechanisms are poorly understood. We present a new model of motor structure learning, approaching it from the point of view of deep reinforcement learning. Previous work outside of motor control has shown how recurrent neural networks can account for learning-to-learn effects. We leverage this insight to address motor learning, by importing it into the setting of model-based reinforcement learning. We apply the resulting processing architecture to empirical findings from a landmark study of structure learning in targetdirected reaching (Braun et al., 2009), and discuss its implications for a wider range of learning-to-learn phenomena.},
  file = {Weinstein and Botvinick - Structure Learning in Motor Control A Deep Reinfo.pdf},
  language = {en}
}

@article{Weisbecker2010,
  title = {Brain Size, Life History, and Metabolism at the Marsupial/Placental Dichotomy},
  author = {Weisbecker, V. and Goswami, A.},
  year = {2010},
  month = sep,
  volume = {107},
  pages = {16216--16221},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.0906486107},
  file = {Weisbecker and Goswami - 2010 - Brain size, life history, and metabolism at the ma.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {37}
}

@article{Weissman2004,
  title = {Calcium {{Waves Propagate}} through {{Radial Glial Cells}} and {{Modulate Proliferation}} in the {{Developing Neocortex}}},
  author = {Weissman, Tamily A. and Riquelme, Patricio A. and Ivic, Lidija and Flint, Alexander C. and Kriegstein, Arnold R.},
  year = {2004},
  month = sep,
  volume = {43},
  pages = {647--661},
  issn = {08966273},
  doi = {10.1016/j.neuron.2004.08.015},
  abstract = {The majority of neurons in the adult neocortex are produced embryonically during a brief but intense period of neuronal proliferation. The radial glial cell, a transient embryonic cell type known for its crucial role in neuronal migration, has recently been shown to function as a neuronal progenitor cell and appears to produce most cortical pyramidal neurons. Radial glial cell modulation could thus affect neuron production, neuronal migration, and overall cortical architecture; however, signaling mechanisms among radial glia have not been studied directly. We demonstrate here that calcium waves propagate through radial glial cells in the proliferative cortical ventricular zone (VZ). Radial glial calcium waves occur spontaneously and require connexin hemichannels, P2Y1 ATP receptors, and intracellular IP3-mediated calcium release. Furthermore, we show that wave disruption decreases VZ proliferation during the peak of embryonic neurogenesis. Taken together, these results demonstrate a radial glial signaling mechanism that may regulate cortical neuronal production.},
  file = {Weissman et al. - 2004 - Calcium Waves Propagate through Radial Glial Cells.pdf},
  journal = {Neuron},
  language = {en},
  number = {5}
}

@article{Welford1962,
  title = {Note on a {{Method}} for {{Calculating Corrected Sums}} of {{Squares}} and {{Products}}},
  author = {Welford, B. P.},
  year = {1962},
  month = aug,
  volume = {4},
  pages = {419--420},
  issn = {0040-1706, 1537-2723},
  doi = {10.1080/00401706.1962.10490022},
  file = {1962 - Welford - Note on a Method for Calculating Corrected Sums of Squares and Products.pdf;Welford - 1962 - Note on a Method for Calculating Corrected Sums of.pdf},
  journal = {Technometrics},
  language = {en},
  number = {3}
}

@article{Wen2016,
  title = {Separating {{Fractal}} and {{Oscillatory Components}} in the {{Power Spectrum}} of {{Neurophysiological Signal}}},
  author = {Wen, Haiguang and Liu, Zhongming},
  year = {2016},
  month = jan,
  volume = {29},
  pages = {13--26},
  issn = {0896-0267, 1573-6792},
  doi = {10.1007/s10548-015-0448-0},
  file = {Wen and Liu - 2016 - Separating Fractal and Oscillatory Components in t 2.pdf;Wen and Liu - 2016 - Separating Fractal and Oscillatory Components in t.pdf},
  journal = {Brain Topography},
  language = {en},
  number = {1}
}

@article{Wendling2002,
  title = {Epileptic Fast Activity Can Be Explained by a Model of Impaired {{GABAergic}} Dendritic Inhibition: {{Epileptic}} Activity Explained by Dendritic Dis-Inhibition},
  shorttitle = {Epileptic Fast Activity Can Be Explained by a Model of Impaired {{GABAergic}} Dendritic Inhibition},
  author = {Wendling, F. and Bartolomei, F. and Bellanger, J. J. and Chauvel, P.},
  year = {2002},
  month = may,
  volume = {15},
  pages = {1499--1508},
  issn = {0953816X},
  doi = {10.1046/j.1460-9568.2002.01985.x},
  abstract = {This paper focuses on high-frequency (gamma band) EEG activity, the most characteristic electrophysiological pattern in focal seizures of human epilepsy. It starts with recent hypotheses about: (i) the behaviour of inhibitory interneurons in hippocampal or neocortical networks in the generation of gamma frequency oscillations; (ii) the nonuniform alteration of GABAergic inhibition in experimental epilepsy (reduced dendritic inhibition and increased somatic inhibition); and (iii) the possible depression of GABAA,fast circuit activity by GABAA,slow inhibitory postsynaptic currents. In particular, these hypotheses are introduced in a new computational macroscopic model of EEG activity that includes a physiologically relevant fast inhibitory feedback loop. Results show that strikingly realistic activity is produced by the model when compared to real EEG signals recorded with intracerebral electrodes. They show that, in the model, the transition from interictal to fast ictal activity is explained by the impairment of dendritic inhibition.},
  file = {2002 - Wendling et al. - Epileptic fast activity can be explained by a model of impaired GABAergic dendritic inhibition.pdf},
  journal = {European Journal of Neuroscience},
  language = {en},
  number = {9}
}

@article{Wessel2020,
  title = {{$\beta$}-{{Bursts Reveal}} the {{Trial}}-to-{{Trial Dynamics}} of {{Movement Initiation}} and {{Cancellation}}},
  author = {Wessel, Jan R.},
  year = {2020},
  month = jan,
  volume = {40},
  pages = {411--423},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.1887-19.2019},
  file = {Wessel - 2020 - β-Bursts Reveal the Trial-to-Trial Dynamics of Mov.pdf},
  journal = {J. Neurosci.},
  language = {en},
  number = {2}
}

@article{West2002,
  title = {Sexual {{Selection}}, {{Temperature}}, and the {{Lion}}'s {{Mane}}},
  author = {West, P. M.},
  year = {2002},
  month = aug,
  volume = {297},
  pages = {1339--1343},
  issn = {00368075, 10959203},
  doi = {10.1126/science.1073257},
  file = {West - 2002 - Sexual Selection, Temperature, and the Lion's Mane.pdf},
  journal = {Science},
  language = {en},
  number = {5585}
}

@article{West2005,
  title = {Neither a Token of Royalty nor a Shield for Fighting, the Mane Is a Signal of Quality to Mates and Rivals, but One That Comes with Consequences},
  author = {West, Peyton M},
  year = {2005},
  pages = {11},
  file = {West - 2005 - Neither a token of royalty nor a shield for fighti.pdf},
  language = {en}
}

@article{White2012,
  title = {Perceptual {{Criteria}} in the {{Human Brain}}},
  author = {White, C. N. and Mumford, J. A. and Poldrack, R. A.},
  year = {2012},
  month = nov,
  volume = {32},
  pages = {16716--16724},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.1744-12.2012},
  file = {2012 - White, Mumford, Poldrack - Perceptual Criteria in the Human Brain.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {47}
}

@techreport{White2019,
  title = {A Neural Network for Information Seeking},
  author = {White, J Kael and {Bromberg-Martin}, Ethan S and Heilbronner, Sarah R and Zhang, Kaining and Pai, Julia and Haber, Suzanne N and Monosov, Ilya E},
  year = {2019},
  month = aug,
  institution = {{Neuroscience}},
  doi = {10.1101/720433},
  abstract = {Humans and other animals often show a strong desire to know the uncertain rewards their future has in store, even when they cannot use this information to influence the outcome. However, it is unknown how the brain predicts opportunities to gain information and motivates this information seeking behavior. Here we show that neurons in a network of interconnected subregions of primate anterior cingulate cortex and basal ganglia predict the moment of gaining information about uncertain rewards. Spontaneous increases in their information prediction signals are followed by gaze shifts toward objects associated with resolving uncertainty, and pharmacologically disrupting this network reduces the motivation to seek information. These findings demonstrate a cortico-basal ganglia mechanism responsible for motivating actions to resolve uncertainty by seeking knowledge about the future.},
  file = {White et al. - 2019 - A neural network for information seeking.pdf},
  language = {en},
  type = {Preprint}
}

@article{White2019b,
  title = {A Neural Network for Information Seeking},
  author = {White, J. Kael and {Bromberg-Martin}, Ethan S. and Heilbronner, Sarah R. and Zhang, Kaining and Pai, Julia and Haber, Suzanne N. and Monosov, Ilya E.},
  year = {2019},
  month = dec,
  volume = {10},
  pages = {5168},
  issn = {2041-1723},
  doi = {10.1038/s41467-019-13135-z},
  abstract = {Abstract             Humans and other animals often show a strong desire to know the uncertain rewards their future has in store, even when they cannot use this information to influence the outcome. However, it is unknown how the brain predicts opportunities to gain information and motivates this information-seeking behavior. Here we show that neurons in a network of interconnected subregions of primate anterior cingulate cortex and basal ganglia predict the moment of gaining information about uncertain rewards. Spontaneous increases in their information prediction signals are followed by gaze shifts toward objects associated with resolving uncertainty, and pharmacologically disrupting this network reduces the motivation to seek information. These findings demonstrate a cortico-basal ganglia mechanism responsible for motivating actions to resolve uncertainty by seeking knowledge about the future.},
  file = {White et al. - 2019 - A neural network for information seeking 3.pdf},
  journal = {Nat Commun},
  language = {en},
  number = {1}
}

@article{Whitten2011,
  title = {A Better Oscillation Detection Method Robustly Extracts {{EEG}} Rhythms across Brain State Changes: {{The}} Human Alpha Rhythm as a Test Case},
  shorttitle = {A Better Oscillation Detection Method Robustly Extracts {{EEG}} Rhythms across Brain State Changes},
  author = {Whitten, Tara A. and Hughes, Adam M. and Dickson, Clayton T. and Caplan, Jeremy B.},
  year = {2011},
  month = jan,
  volume = {54},
  pages = {860--874},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2010.08.064},
  abstract = {Oscillatory activity is a principal mode of operation in the brain. Despite an intense resurgence of interest in the mechanisms and functions of brain rhythms, methods for the detection and analysis of oscillatory activity in neurophysiological recordings are still highly variable across studies. We recently proposed a method for detecting oscillatory activity from time series data, which we call the BOSC (Better OSCillation detection) method. This method produces systematic, objective, and consistent results across frequencies, brain regions and tasks. It does so by modeling the functional form of the background spectrum by fitting the empirically observed spectrum at the recording site. This minimizes bias in oscillation detection across frequency, region and task. Here we show that the method is also robust to dramatic changes in state that are known to influence the shape of the power spectrum, namely, the presence versus absence of the alpha rhythm, and can be applied to independent components, which are thought to reflect underlying sources, in addition to individual raw signals. This suggests that the BOSC method is an effective tool for measuring changes in rhythmic activity in the more common research scenario wherein state is unknown.},
  file = {2011 - Whitten et al. - A better oscillation detection method robustly extracts EEG rhythms across brain state changes The human alpha r.pdf},
  journal = {NeuroImage},
  language = {en},
  number = {2}
}

@article{Whittington2019,
  title = {Theories of {{Error Back}}-{{Propagation}} in the {{Brain}}},
  author = {Whittington, James C.R. and Bogacz, Rafal},
  year = {2019},
  month = mar,
  volume = {23},
  pages = {235--250},
  issn = {13646613},
  doi = {10.1016/j.tics.2018.12.005},
  file = {Whittington and Bogacz - 2019 - Theories of Error Back-Propagation in the Brain.pdf},
  journal = {Trends in Cognitive Sciences},
  language = {en},
  number = {3}
}

@article{Whittington2019a,
  title = {Theories of {{Error Back}}-{{Propagation}} in the {{Brain}}},
  author = {Whittington, James C.R. and Bogacz, Rafal},
  year = {2019},
  month = mar,
  volume = {23},
  pages = {235--250},
  issn = {13646613},
  doi = {10.1016/j.tics.2018.12.005},
  file = {Whittington and Bogacz - 2019 - Theories of Error Back-Propagation in the Brain 2.pdf},
  journal = {Trends in Cognitive Sciences},
  language = {en},
  number = {3}
}

@article{Wickham,
  title = {A Grammar of Graphics: Past, Present, and Future},
  author = {Wickham, Hadley},
  pages = {45},
  file = {Wickham - A grammar of graphics past, present, and future.pdf},
  language = {en}
}

@article{Widmann2015,
  title = {Digital Filter Design for Electrophysiological Data \textendash{} a Practical Approach},
  author = {Widmann, Andreas and Schr{\"o}ger, Erich and Maess, Burkhard},
  year = {2015},
  month = jul,
  volume = {250},
  pages = {34--46},
  issn = {01650270},
  doi = {10.1016/j.jneumeth.2014.08.002},
  abstract = {Background: Filtering is a ubiquitous step in the preprocessing of electroencephalographic (EEG) and magnetoencephalographic (MEG) data. Besides the intended effect of the attenuation of signal components considered as noise, filtering can also result in various unintended adverse filter effects (distortions such as smoothing) and filter artifacts. Method: We give some practical guidelines for the evaluation of filter responses (impulse and frequency response) and the selection of filter types (high-pass/low-pass/band-pass/band-stop; finite/infinite impulse response, FIR/IIR) and filter parameters (cutoff frequencies, filter order and roll-off, ripple, delay and causality) to optimize signal-to-noise ratio and avoid or reduce signal distortions for selected electrophysiological applications. Results: Various filter implementations in common electrophysiology software packages are introduced and discussed. Resulting filter responses are compared and evaluated. Conclusion: We present strategies for recognizing common adverse filter effects and filter artifacts and demonstrate them in practical examples. Best practices and recommendations for the selection and reporting of filter parameters, limitations, and alternatives to filtering are discussed.},
  file = {Widmann et al. - 2015 - Digital filter design for electrophysiological dat.pdf},
  journal = {Journal of Neuroscience Methods},
  language = {en}
}

@article{Williams2018,
  title = {Escape {{Dynamics}} in {{Learning Models}}},
  author = {Williams, Noah},
  year = {2018},
  month = jun,
  issn = {0034-6527, 1467-937X},
  doi = {10.1093/restud/rdy033},
  file = {2014 - Williams - Escape Dynamics in Learning Models.pdf;Williams - 2018 - Escape Dynamics in Learning Models.pdf},
  journal = {The Review of Economic Studies},
  language = {en}
}

@techreport{Williams2019a,
  title = {Discovering Precise Temporal Patterns in Large-Scale Neural Recordings through Robust and Interpretable Time Warping},
  author = {Williams, Alex H. and Poole, Ben and Maheswaranathan, Niru and Dhawale, Ashesh K. and Fisher, Tucker and Wilson, Christopher D. and Brann, David H. and Trautmann, Eric and Ryu, Stephen and Shusterman, Roman and Rinberg, Dmitry and {\"O}lveczky, Bence P. and Shenoy, Krishna V. and Ganguli, Surya},
  year = {2019},
  month = jun,
  institution = {{Neuroscience}},
  doi = {10.1101/661165},
  abstract = {Abstract           Though the temporal precision of neural computation has been studied intensively, a data-driven determination of this precision remains a fundamental challenge. Reproducible spike time patterns may be obscured on single trials by uncontrolled temporal variability in behavior and cognition, or may not even be time locked to measurable signatures in either behavior or local field potentials (LFP). To overcome these challenges, we describe a general-purpose time warping framework that reveals precise spike-time patterns in an unsupervised manner, even when spiking is decoupled from behavior or is temporally stretched across single trials. We demonstrate this method across diverse systems: cued reaching in nonhuman primates, motor sequence production in rats, and olfaction in mice. This approach flexibly uncovers diverse dynamical firing patterns, including pulsatile responses to behavioral events, LFP-aligned oscillatory spiking, and even unanticipated patterns, like 7 Hz oscillations in rat motor cortex that are not time-locked to measured behaviors or LFP.},
  file = {Williams et al. - 2019 - Discovering precise temporal patterns in large-sca 2.pdf},
  language = {en},
  type = {Preprint}
}

@article{Williams2021,
  title = {Increased {{Complexity}} and {{Fitness}} of {{Artificial Cells}} That {{Reproduce Using Spatially Distributed Asynchronous Parallel Processes}}},
  author = {Williams, Lance R.},
  year = {2021},
  month = mar,
  abstract = {Replication time is among the most important components of a bacterial cell's reproductive fitness. Paradoxically, larger cells replicate in less time than smaller cells despite the fact that building a larger cell requires increased quantities of raw materials and energy. This feat is primarily accomplished by the massive over expression of ribosomes, which permits translation of mRNA into protein, the limiting step in reproduction, to occur at a scale that would be impossible were it not for the use of parallel processing. In computer science, spatial parallelism is the distribution of work across the nodes of a distributed-memory multicomputer system. Despite the fact that a non-negligible fraction of artificial life research is grounded in formulations based on spatially parallel substrates, there have been no examples of artificial organisms that use spatial parallelism to replicate in less time than smaller organisms. This paper describes artificial cells defined using a combinator-based artificial chemistry that replicate in less time than smaller cells. This is achieved by employing extra copies of programs implementing the limiting steps in the process used by the cells to synthesize their component parts. Significant speedup is demonstrated, despite the increased complexity of control and export processes necessitated by the use of a parallel replication strategy.},
  archiveprefix = {arXiv},
  eprint = {2103.08406},
  eprinttype = {arxiv},
  file = {Williams - 2021 - Increased Complexity and Fitness of Artificial Cel.pdf},
  journal = {arXiv:2103.08406 [cs]},
  keywords = {Computer Science - Neural and Evolutionary Computing},
  language = {en},
  primaryclass = {cs}
}

@article{Wilson1972,
  title = {Excitatory and {{Inhibitory Interactions}} in {{Localized Populations}} of {{Model Neurons}}},
  author = {Wilson, Hugh R. and Cowan, Jack D.},
  year = {1972},
  month = jan,
  volume = {12},
  pages = {1--24},
  issn = {00063495},
  doi = {10.1016/S0006-3495(72)86068-5},
  file = {1972 - Wilson, Cowan - Excitatory and inhibitory interactions in localized populations of model neurons.pdf},
  journal = {Biophysical Journal},
  language = {en},
  number = {1}
}

@article{Wilson1973,
  title = {A Mathematical Theory of the Functional Dynamics of Cortical and Thalamic Nervous Tissue},
  author = {Wilson, H. R. and Cowan, J. D.},
  year = {1973},
  month = sep,
  volume = {13},
  pages = {55--80},
  issn = {0340-1200, 1432-0770},
  doi = {10.1007/BF00288786},
  abstract = {It is proposed that distinct anatomical regions of cerebral cortex and of thalamic nuclei are functionally two-dimensional. On this view, the third (radial) dimension of cortical and thalamic structures is associated with a redundancy of circuits and functions so that reliable signal processing obtains in the presence of noisy or ambiguous stimuli.},
  file = {1973 - Wilson, Cowan - A mathematical theory of the functional dynamics of cortical and thalamic nervous tissue.pdf},
  journal = {Kybernetik},
  language = {en},
  number = {2}
}

@article{Wilson2014,
  title = {Humans Use Directed and Random Exploration to Solve the Explore\textendash Exploit Dilemma.},
  author = {Wilson, Robert C. and Geana, Andra and White, John M. and Ludvig, Elliot A. and Cohen, Jonathan D.},
  year = {2014},
  volume = {143},
  pages = {2074--2081},
  issn = {1939-2222, 0096-3445},
  doi = {10.1037/a0038199},
  file = {Wilson et al. - 2014 - Humans use directed and random exploration to solv.pdf},
  journal = {Journal of Experimental Psychology: General},
  language = {en},
  number = {6}
}

@article{Wilson2014a,
  title = {Humans Use Directed and Random Exploration to Solve the Explore\textendash Exploit Dilemma.},
  author = {Wilson, Robert C. and Geana, Andra and White, John M. and Ludvig, Elliot A. and Cohen, Jonathan D.},
  year = {2014},
  volume = {143},
  pages = {2074--2081},
  issn = {1939-2222, 0096-3445},
  doi = {10.1037/a0038199},
  abstract = {All adaptive organisms face the fundamental tradeoff between pursuing a known reward (exploitation) and sampling lesser-known options in search of something better (exploration). Theory suggests at least two strategies for solving this dilemma: a directed strategy in which choices are explicitly biased toward information seeking, and a random strategy in which decision noise leads to exploration by chance. In this work we investigated the extent to which humans use these two strategies. In our ``Horizon task,'' participants made explore\textendash{} exploit decisions in two contexts that differed in the number of choices that they would make in the future (the time horizon). Participants were allowed to make either a single choice in each game (horizon 1), or 6 sequential choices (horizon 6), giving them more opportunity to explore. By modeling the behavior in these two conditions, we were able to measure exploration-related changes in decision making and quantify the contributions of the two strategies to behavior. We found that participants were more information seeking and had higher decision noise with the longer horizon, suggesting that humans use both strategies to solve the exploration\textendash{} exploitation dilemma. We thus conclude that both information seeking and choice variability can be controlled and put to use in the service of exploration.},
  file = {Wilson et al. - 2014 - Humans use directed and random exploration to solv 2.pdf},
  journal = {Journal of Experimental Psychology: General},
  language = {en},
  number = {6}
}

@article{Wilson2015,
  title = {Clustered {{Desynchronization}} from {{High}}-{{Frequency Deep Brain Stimulation}}},
  author = {Wilson, Dan and Moehlis, Jeff},
  editor = {Diedrichsen, J{\"o}rn},
  year = {2015},
  month = dec,
  volume = {11},
  pages = {e1004673},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1004673},
  file = {Wilson and Moehlis - 2015 - Clustered Desynchronization from High-Frequency De.pdf},
  journal = {PLOS Computational Biology},
  language = {en},
  number = {12}
}

@article{Wilson2015a,
  title = {Is {{Model Fitting Necessary}} for {{Model}}-{{Based fMRI}}?},
  author = {Wilson, Robert C. and Niv, Yael},
  editor = {Boorman, Erie Dell},
  year = {2015},
  month = jun,
  volume = {11},
  pages = {e1004237},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1004237},
  abstract = {OPEN ACCESS Citation: Wilson RC, Niv Y (2015) Is Model Fitting Necessary for Model-Based fMRI?. PLoS Comput Biol 11(6): e1004237. doi:10.1371/journal.},
  file = {Wilson and Niv - 2015 - Is Model Fitting Necessary for Model-Based fMRI.pdf},
  journal = {PLOS Computational Biology},
  language = {en},
  number = {6}
}

@inproceedings{Wilson2018,
  title = {Evolving Simple Programs for Playing Atari Games},
  booktitle = {Proceedings of the {{Genetic}} and {{Evolutionary Computation Conference}} on   - {{GECCO}} '18},
  author = {Wilson, Dennis G and {Cussat-Blanc}, Sylvain and Luga, Herv{\'e} and Miller, Julian F},
  year = {2018},
  pages = {229--236},
  publisher = {{ACM Press}},
  address = {{Kyoto, Japan}},
  doi = {10.1145/3205455.3205578},
  abstract = {Cartesian Genetic Programming (CGP) has previously shown capabilities in image processing tasks by evolving programs with a function set specialized for computer vision. A similar approach can be applied to Atari playing. Programs are evolved using mixed type CGP with a function set suited for matrix operations, including image processing, but allowing for controller behavior to emerge. While the programs are relatively small, many controllers are competitive with state of the art methods for the Atari benchmark set and require less training time. By evaluating the programs of the best evolved individuals, simple but e ective strategies can be found.},
  file = {Wilson et al. - 2018 - Evolving simple programs for playing atari games.pdf},
  isbn = {978-1-4503-5618-3},
  language = {en}
}

@techreport{Wilson2020,
  title = {Balancing Exploration and Exploitation with Information and Randomization},
  author = {Wilson, Robert C and Bonawitz, Elizabeth and Costa, Vincent and Ebitz, Becket},
  year = {2020},
  month = jul,
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/e9azw},
  abstract = {Explore-exploit decisions require us to trade off the benefits of exploring unknown options to learn more about them, with exploiting known options, for immediate reward. Such decisions are ubiquitous in nature, but from a computational perspective, they are notoriously hard. There is therefore much interest in how humans and animals make these decisions and recently there has been an explosion of research in this area. Here we provide a biased and incomplete snapshot of this field focusing on the major finding that many organisms use two distinct strategies to solve the explore-exploit dilemma: a bias for information (`directed exploration') and the randomization of choice (`random exploration'). We review evidence for the existence of these strategies, their computational properties, their neural implementations, as well as how directed and random exploration vary over the lifespan. We conclude by highlighting open questions in this field that are ripe to both explore and exploit.},
  file = {Wilson et al. - 2020 - Balancing exploration and exploitation with inform.pdf},
  language = {en},
  type = {Preprint}
}

@article{Wilson2021,
  title = {Balancing Exploration and Exploitation with Information and Randomization},
  author = {Wilson, Robert C and Bonawitz, Elizabeth and Costa, Vincent D and Ebitz, R Becket},
  year = {2021},
  month = apr,
  volume = {38},
  pages = {49--56},
  issn = {23521546},
  doi = {10.1016/j.cobeha.2020.10.001},
  file = {Wilson et al. - 2021 - Balancing exploration and exploitation with inform.pdf},
  journal = {Current Opinion in Behavioral Sciences},
  language = {en}
}

@article{Wilson2021a,
  title = {Balancing Exploration and Exploitation with Information and Randomization},
  author = {Wilson, Robert C and Bonawitz, Elizabeth and Costa, Vincent D and Ebitz, R Becket},
  year = {2021},
  month = apr,
  volume = {38},
  pages = {49--56},
  issn = {23521546},
  doi = {10.1016/j.cobeha.2020.10.001},
  file = {Wilson et al. - 2021 - Balancing exploration and exploitation with inform.pdf},
  journal = {Current Opinion in Behavioral Sciences},
  language = {en}
}

@article{Wilson2021b,
  title = {Balancing Exploration and Exploitation with Information and Randomization},
  author = {Wilson, Robert C and Bonawitz, Elizabeth and Costa, Vincent D and Ebitz, R Becket},
  year = {2021},
  month = apr,
  volume = {38},
  pages = {49--56},
  issn = {23521546},
  doi = {10.1016/j.cobeha.2020.10.001},
  file = {Wilson et al. - 2021 - Balancing exploration and exploitation with inform 2.pdf},
  journal = {Current Opinion in Behavioral Sciences},
  language = {en}
}

@article{Wiltschko2015,
  title = {Mapping {{Sub}}-{{Second Structure}} in {{Mouse Behavior}}},
  author = {Wiltschko, Alexander B. and Johnson, Matthew J. and Iurilli, Giuliano and Peterson, Ralph E. and Katon, Jesse M. and Pashkovski, Stan L. and Abraira, Victoria E. and Adams, Ryan P. and Datta, Sandeep Robert},
  year = {2015},
  month = dec,
  volume = {88},
  pages = {1121--1135},
  issn = {08966273},
  doi = {10.1016/j.neuron.2015.11.031},
  abstract = {Complex animal behaviors are likely built from simpler modules, but their systematic identification in mammals remains a significant challenge. Here we use depth imaging to show that 3D mouse pose dynamics are structured at the sub-second timescale. Computational modeling of these fast dynamics effectively describes mouse behavior as a series of reused and stereotyped modules with defined transition probabilities. We demonstrate this combined 3D imaging and machine learning method can be used to unmask potential strategies employed by the brain to adapt to the environment, to capture both predicted and previously hidden phenotypes caused by genetic or neural manipulations, and to systematically expose the global structure of behavior within an experiment. This work reveals that mouse body language is built from identifiable components and is organized in a predictable fashion; deciphering this language establishes an objective framework for characterizing the influence of environmental cues, genes and neural activity on behavior.},
  file = {Wiltschko et al. - 2015 - Mapping Sub-Second Structure in Mouse Behavior.pdf},
  journal = {Neuron},
  language = {en},
  number = {6}
}

@article{Wimmer2014,
  title = {Bump Attractor Dynamics in Prefrontal Cortex Explains Behavioral Precision in Spatial Working Memory},
  author = {Wimmer, Klaus and Nykamp, Duane Q and Constantinidis, Christos and Compte, Albert},
  year = {2014},
  month = mar,
  volume = {17},
  pages = {431--439},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.3645},
  file = {2014 - Wimmer et al. - Bump attractor dynamics in prefrontal cortex explains behavioral precision in spatial working memory.pdf;Wimmer et al. - 2014 - Bump attractor dynamics in prefrontal cortex expla.pdf},
  journal = {Nature Neuroscience},
  language = {en},
  number = {3}
}

@article{Wimmer2016,
  title = {Transitions between {{Multiband Oscillatory Patterns Characterize Memory}}-{{Guided Perceptual Decisions}} in {{Prefrontal Circuits}}},
  author = {Wimmer, K. and Ramon, M. and Pasternak, T. and Compte, A.},
  year = {2016},
  month = jan,
  volume = {36},
  pages = {489--505},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.3678-15.2016},
  file = {Wimmer et al. - 2016 - Transitions between Multiband Oscillatory Patterns.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {2}
}

@article{Winder2017,
  title = {Weak Correlations between Hemodynamic Signals and Ongoing Neural Activity during the Resting State},
  author = {Winder, Aaron T. and Echagarruga, Christina and Zhang, Qingguang and Drew, Patrick J.},
  year = {2017},
  month = dec,
  volume = {20},
  pages = {1761--1769},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/s41593-017-0007-y},
  file = {Winder et al. - 2017 - Weak correlations between hemodynamic signals and .pdf},
  journal = {Nature Neuroscience},
  language = {en},
  number = {12}
}

@article{Winning2018,
  title = {Rethinking {{Causality}} in {{Biological}} and {{Neural Mechanisms}}: {{Constraints}} and {{Control}}},
  shorttitle = {Rethinking {{Causality}} in {{Biological}} and {{Neural Mechanisms}}},
  author = {Winning, Jason and Bechtel, William},
  year = {2018},
  month = jun,
  volume = {28},
  pages = {287--310},
  issn = {0924-6495, 1572-8641},
  doi = {10.1007/s11023-018-9458-5},
  abstract = {Existing accounts of mechanistic causation are not suited for understanding causation in biological and neural mechanisms because they do not have the resources to capture the unique causal structure of control heterarchies. In this paper, we provide a new account on which the causal powers of mechanisms are grounded by time-dependent, variable constraints. Constraints can also serve as a key bridge concept between the mechanistic approach to explanation and underappreciated work in theoretical biology that sheds light on how biological systems channel energy to actively respond to the environment in adaptive ways, perform work, and fulfill the requirements to maintain themselves far from equilibrium. We show how the framework applies to several concrete examples of control in simple organisms as well as the nervous system of complex organisms.},
  file = {Winning and Bechtel - 2018 - Rethinking Causality in Biological and Neural Mech.pdf},
  journal = {Minds \& Machines},
  language = {en},
  number = {2}
}

@article{Winterer2003,
  title = {Cortical Signal-to-Noise Ratio: Insight into the Pathophysiology and Genetics of Schizophrenia},
  shorttitle = {Cortical Signal-to-Noise Ratio},
  author = {Winterer, Georg and Weinberger, Daniel R.},
  year = {2003},
  month = may,
  volume = {3},
  pages = {55--66},
  issn = {15662772},
  doi = {10.1016/S1566-2772(03)00019-7},
  abstract = {During the past two decades, it has been convincingly demonstrated that schizophrenic patients and subjects genetically at risk for schizophrenia show abnormalities of cortical and particularly prefrontal function. Depending on clinical state and task conditions, hypo- and hyperfrontality have been frequently described with functional neuroimaging and electrophysiological techniques; however, the underlying neurophysiological deficits remained largely obscure. There is now growing empirical evidence that cortical signal-to-noise ratio (SNR) during information processing is fundamentally disturbed and may be key to a further understanding of schizophrenic pathophysiology. The evidence comes from animal and human electrophysiological and neuroimaging investigations as well as neuropsychological and computational simulation studies. This research has also shown that dopamine signaling in prefrontal cortex is a critical factor in modulation of cortical SNR and in neurocognitive performance. Moreover, it was recently demonstrated that genetically determined variations in dopamine signaling, mediated by a functional polymorphism in the gene for the enzyme catechol-o-methyltransferase, has a significant impact on the cortical SNR, prefrontal information processing, and as a result, is a susceptibility gene for schizophrenia. This review summarizes the current state of research on the pathophysiology of schizophrenia with emphasis on cortical-SNR and the involvement of potentially relevant, molecular and genetic determinants of the cortical dopaminergic signaling.},
  file = {2003 - Winterer, Weinberger - Cortical signal-to-noise ratio Insight into the pathophysiology and genetics of schizophrenia.pdf},
  journal = {Clinical Neuroscience Research},
  language = {en},
  number = {1-2}
}

@article{Wolff2020,
  title = {Bored {{Into Depletion}}? {{Toward}} a {{Tentative Integration}} of {{Perceived Self}}-{{Control Exertion}} and {{Boredom}} as {{Guiding Signals}} for {{Goal}}-{{Directed Behavior}}},
  shorttitle = {Bored {{Into Depletion}}?},
  author = {Wolff, Wanja and Martarelli, Corinna S.},
  year = {2020},
  month = sep,
  volume = {15},
  pages = {1272--1283},
  issn = {1745-6916, 1745-6924},
  doi = {10.1177/1745691620921394},
  abstract = {During the past two decades, self-control research has been dominated by the strength model of self-control, which is built on the premise that the capacity for self-control is a limited global resource that can become temporarily depleted, resulting in a state called ego depletion. The foundations of ego depletion have recently been questioned. Thus, although self-control is among the most researched psychological concepts with high societal relevance, an inconsistent body of literature limits our understanding of how self-control operates. Here, we propose that the inconsistencies are partly due to a confound that has unknowingly and systematically been introduced into the ego-depletion research: boredom. We propose that boredom might affect results of self-control research by placing an unwanted demand on self-control and signaling that one should explore behavioral alternatives. To account for boredom in self-controlled behavior, we provide a working model that integrates evidence from reward-based models of self-control and recent theorizing on boredom to explain the effects of both self-control exertion and boredom on subsequent self-control performance. We propose that task-induced boredom should be systematically monitored in self-control research to assess the validity of the ego-depletion effect.},
  file = {Wolff and Martarelli - 2020 - Bored Into Depletion Toward a Tentative Integrati.pdf},
  journal = {Perspect Psychol Sci},
  language = {en},
  number = {5}
}

@article{Wolff2020a,
  title = {Bored {{Into Depletion}}? {{Toward}} a {{Tentative Integration}} of {{Perceived Self}}-{{Control Exertion}} and {{Boredom}} as {{Guiding Signals}} for {{Goal}}-{{Directed Behavior}}},
  shorttitle = {Bored {{Into Depletion}}?},
  author = {Wolff, Wanja and Martarelli, Corinna S.},
  year = {2020},
  month = sep,
  volume = {15},
  pages = {1272--1283},
  issn = {1745-6916, 1745-6924},
  doi = {10.1177/1745691620921394},
  abstract = {During the past two decades, self-control research has been dominated by the strength model of self-control, which is built on the premise that the capacity for self-control is a limited global resource that can become temporarily depleted, resulting in a state called ego depletion. The foundations of ego depletion have recently been questioned. Thus, although self-control is among the most researched psychological concepts with high societal relevance, an inconsistent body of literature limits our understanding of how self-control operates. Here, we propose that the inconsistencies are partly due to a confound that has unknowingly and systematically been introduced into the ego-depletion research: boredom. We propose that boredom might affect results of self-control research by placing an unwanted demand on self-control and signaling that one should explore behavioral alternatives. To account for boredom in self-controlled behavior, we provide a working model that integrates evidence from reward-based models of self-control and recent theorizing on boredom to explain the effects of both self-control exertion and boredom on subsequent self-control performance. We propose that task-induced boredom should be systematically monitored in self-control research to assess the validity of the ego-depletion effect.},
  file = {Wolff and Martarelli - 2020 - Bored Into Depletion Toward a Tentative Integrati.pdf},
  journal = {Perspect Psychol Sci},
  language = {en},
  number = {5}
}

@article{Wolff2020b,
  title = {Bored {{Into Depletion}}? {{Toward}} a {{Tentative Integration}} of {{Perceived Self}}-{{Control Exertion}} and {{Boredom}} as {{Guiding Signals}} for {{Goal}}-{{Directed Behavior}}},
  shorttitle = {Bored {{Into Depletion}}?},
  author = {Wolff, Wanja and Martarelli, Corinna S.},
  year = {2020},
  month = sep,
  volume = {15},
  pages = {1272--1283},
  issn = {1745-6916, 1745-6924},
  doi = {10.1177/1745691620921394},
  abstract = {During the past two decades, self-control research has been dominated by the strength model of self-control, which is built on the premise that the capacity for self-control is a limited global resource that can become temporarily depleted, resulting in a state called ego depletion. The foundations of ego depletion have recently been questioned. Thus, although self-control is among the most researched psychological concepts with high societal relevance, an inconsistent body of literature limits our understanding of how self-control operates. Here, we propose that the inconsistencies are partly due to a confound that has unknowingly and systematically been introduced into the ego-depletion research: boredom. We propose that boredom might affect results of self-control research by placing an unwanted demand on self-control and signaling that one should explore behavioral alternatives. To account for boredom in self-controlled behavior, we provide a working model that integrates evidence from reward-based models of self-control and recent theorizing on boredom to explain the effects of both self-control exertion and boredom on subsequent self-control performance. We propose that task-induced boredom should be systematically monitored in self-control research to assess the validity of the ego-depletion effect.},
  file = {Wolff and Martarelli - 2020 - Bored Into Depletion Toward a Tentative Integrati 2.pdf},
  journal = {Perspect Psychol Sci},
  language = {en},
  number = {5}
}

@article{Wolpert,
  title = {What the No Free Lunch Theorems Really Mean; How to Improve Search Algorithms},
  author = {Wolpert, David H},
  pages = {14},
  file = {Wolpert - What the no free lunch theorems really mean\; how t.pdf},
  language = {en}
}

@article{Wolpert1997,
  title = {No Free Lunch Theorems for Optimization},
  author = {Wolpert, D.H. and Macready, W.G.},
  year = {1997},
  month = apr,
  volume = {1},
  pages = {67--82},
  issn = {1089778X},
  doi = {10.1109/4235.585893},
  abstract = {A framework is developed to explore the connection between effective optimization algorithms and the problems they are solving. A number of ``no free lunch'' (NFL) theorems are presented which establish that for any algorithm, any elevated performance over one class of problems is offset by performance over another class. These theorems result in a geometric interpretation of what it means for an algorithm to be well suited to an optimization problem. Applications of the NFL theorems to information-theoretic aspects of optimization and benchmark measures of performance are also presented. Other issues addressed include time-varying optimization problems and a priori ``head-to-head'' minimax distinctions between optimization algorithms, distinctions that result despite the NFL theorems' enforcing of a type of uniformity over all algorithms.},
  file = {Wolpert and Macready - 1997 - No free lunch theorems for optimization.pdf},
  journal = {IEEE Trans. Evol. Computat.},
  language = {en},
  number = {1}
}

@article{Wolpert1997a,
  title = {No Free Lunch Theorems for Optimization},
  author = {Wolpert, D.H. and Macready, W.G.},
  year = {1997},
  month = apr,
  volume = {1},
  pages = {67--82},
  issn = {1089778X},
  doi = {10.1109/4235.585893},
  abstract = {A framework is developed to explore the connection between effective optimization algorithms and the problems they are solving. A number of ``no free lunch'' (NFL) theorems are presented which establish that for any algorithm, any elevated performance over one class of problems is offset by performance over another class. These theorems result in a geometric interpretation of what it means for an algorithm to be well suited to an optimization problem. Applications of the NFL theorems to information-theoretic aspects of optimization and benchmark measures of performance are also presented. Other issues addressed include time-varying optimization problems and a priori ``head-to-head'' minimax distinctions between optimization algorithms, distinctions that result despite the NFL theorems' enforcing of a type of uniformity over all algorithms.},
  file = {Wolpert and Macready - 1997 - No free lunch theorems for optimization 2.pdf},
  journal = {IEEE Trans. Evol. Computat.},
  language = {en},
  number = {1}
}

@article{Wolpert2005,
  title = {Coevolutionary {{Free Lunches}}},
  author = {Wolpert, D.H. and Macready, W.G.},
  year = {2005},
  month = dec,
  volume = {9},
  pages = {721--735},
  issn = {1089-778X},
  doi = {10.1109/TEVC.2005.856205},
  abstract = {Recent work on the foundational underpinnings of black-box optimization has begun to uncover a rich mathematical structure. In particular, it is now known that an inner product between the optimization algorithm and the distribution of optimization problems likely to be encountered fixes the distribution over likely performances in running that algorithm. One ramification of this is the ``No Free Lunch'' (NFL) theorems, which state that any two algorithms are equivalent when their performance is averaged across all possible problems. This highlights the need for exploiting problem-specific knowledge to achieve better than random performance. In this paper we present a general framework covering most optimization scenarios. In addition to the optimization scenarios addressed in the NFL results, this framework covers multi-armed bandit problems and evolution of multiple co-evolving players. As a particular instance of the latter, it covers ``self-play'' problems. In these problems the set of players work together to produce a champion, who then engages one or more antagonists in a subsequent multi-player game. In contrast to the traditional optimization case where the NFL results hold, we show that in self-play there are free lunches: in coevolution some algorithms have better performance than other algorithms, averaged across all possible problems. However in the typical coevolutionary scenarios encountered in biology, where there is no champion, the NFL theorems still hold.},
  file = {Wolpert and Macready - 2005 - Coevolutionary Free Lunches.pdf},
  journal = {IEEE Trans. Evol. Computat.},
  language = {en},
  number = {6}
}

@article{Wolpert2015,
  title = {Optimal High-Level Descriptions of Dynamical Systems},
  author = {Wolpert, David H. and Grochow, Joshua A. and Libby, Eric and DeDeo, Simon},
  year = {2015},
  month = jun,
  abstract = {To analyze high-dimensional systems, many fields in science and engineering rely on highlevel descriptions, sometimes called ``macrostates,'' ``coarse-grainings,'' or ``effective theories''. Examples of such descriptions include the thermodynamic properties of a large collection of point particles undergoing reversible dynamics, the variables in a macroeconomic model describing the individuals that participate in an economy, and the summary state of a cell composed of a large set of biochemical networks.},
  archiveprefix = {arXiv},
  eprint = {1409.7403},
  eprinttype = {arxiv},
  file = {Wolpert et al. - 2015 - Optimal high-level descriptions of dynamical syste.pdf},
  journal = {arXiv:1409.7403 [cond-mat, q-bio]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computational Engineering; Finance; and Science,Computer Science - Information Theory,Condensed Matter - Statistical Mechanics,Quantitative Biology - Populations and Evolution},
  language = {en},
  primaryclass = {cond-mat, q-bio}
}

@article{Wolpert2020,
  title = {What Is Important about the {{No Free Lunch}} Theorems?},
  author = {Wolpert, David H.},
  year = {2020},
  month = jul,
  abstract = {The No Free Lunch theorems prove that under a uniform distribution over induction problems (search problems or learning problems), all induction algorithms perform equally. As I discuss in this chapter, the importance of the theorems arises by using them to analyze scenarios involving non-uniform distributions, and to compare different algorithms, without any assumption about the distribution over problems at all. In particular, the theorems prove that anti-cross-validation (choosing among a set of candidate algorithms based on which has worst out-of-sample behavior) performs as well as cross-validation, unless one makes an assumption \textemdash{} which has never been formalized \textemdash{} about how the distribution over induction problems, on the one hand, is related to the set of algorithms one is choosing among using (anti-)cross validation, on the other. In addition, they establish strong caveats concerning the significance of the many results in the literature which establish the strength of a particular algorithm without assuming a particular distribution. They also motivate a ``dictionary'' between supervised learning and improve blackbox optimization, which allows one to ``translate'' techniques from supervised learning into the domain of blackbox optimization, thereby strengthening blackbox optimization algorithms. In addition to these topics, I also briefly discuss their implications for philosophy of science.},
  archiveprefix = {arXiv},
  eprint = {2007.10928},
  eprinttype = {arxiv},
  file = {Wolpert - 2020 - What is important about the No Free Lunch theorems.pdf},
  journal = {arXiv:2007.10928 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  language = {en},
  primaryclass = {cs, stat}
}

@article{Wolpert2020a,
  title = {What Is Important about the {{No Free Lunch}} Theorems?},
  author = {Wolpert, David H.},
  year = {2020},
  month = jul,
  abstract = {The No Free Lunch theorems prove that under a uniform distribution over induction problems (search problems or learning problems), all induction algorithms perform equally. As I discuss in this chapter, the importance of the theorems arises by using them to analyze scenarios involving non-uniform distributions, and to compare different algorithms, without any assumption about the distribution over problems at all. In particular, the theorems prove that anti-cross-validation (choosing among a set of candidate algorithms based on which has worst out-of-sample behavior) performs as well as cross-validation, unless one makes an assumption \textemdash{} which has never been formalized \textemdash{} about how the distribution over induction problems, on the one hand, is related to the set of algorithms one is choosing among using (anti-)cross validation, on the other. In addition, they establish strong caveats concerning the significance of the many results in the literature which establish the strength of a particular algorithm without assuming a particular distribution. They also motivate a ``dictionary'' between supervised learning and improve blackbox optimization, which allows one to ``translate'' techniques from supervised learning into the domain of blackbox optimization, thereby strengthening blackbox optimization algorithms. In addition to these topics, I also briefly discuss their implications for philosophy of science.},
  archiveprefix = {arXiv},
  eprint = {2007.10928},
  eprinttype = {arxiv},
  file = {Wolpert - 2020 - What is important about the No Free Lunch theorems 2.pdf},
  journal = {arXiv:2007.10928 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  language = {en},
  primaryclass = {cs, stat}
}

@article{Womelsdorf2014,
  title = {Dynamic Circuit Motifs Underlying Rhythmic Gain Control, Gating and Integration},
  author = {Womelsdorf, Thilo and Valiante, Taufik A and Sahin, Ned T and Miller, Kai J and Tiesinga, Paul},
  year = {2014},
  month = aug,
  volume = {17},
  pages = {1031--1039},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.3764},
  file = {Womelsdorf et al. - 2014 - Dynamic circuit motifs underlying rhythmic gain co.pdf},
  journal = {Nature Neuroscience},
  language = {en},
  number = {8}
}

@article{Wong2006,
  title = {A {{Recurrent Network Mechanism}} of {{Time Integration}} in {{Perceptual Decisions}}},
  author = {Wong, K.-F.},
  year = {2006},
  month = jan,
  volume = {26},
  pages = {1314--1328},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.3733-05.2006},
  file = {2006 - Wong - A Recurrent Network Mechanism of Time Integration in Perceptual Decisions(2).pdf;2006 - Wong, Wang - A Recurrent Network Mechanism of Time Integration in Perceptual Decisions.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {4}
}

@article{Wongsarnpigoon2010,
  title = {Efficiency {{Analysis}} of {{Waveform Shape}} for {{Electrical Excitation}} of {{Nerve Fibers}}},
  author = {Wongsarnpigoon, Amorn and Woock, John P and Grill, Warren M},
  year = {2010},
  month = jun,
  volume = {18},
  pages = {319--328},
  issn = {1534-4320, 1558-0210},
  doi = {10.1109/TNSRE.2010.2047610},
  abstract = {Stimulation efficiency is an important consideration in the stimulation parameters of implantable neural stimulators. The objective of this study was to analyze the effects of waveform shape and duration on the charge, power, and energy efficiency of neural stimulation. Using a population model of mammalian axons and in vivo experiments on cat sciatic nerve, we analyzed the stimulation efficiency of four waveform shapes: square, rising exponential, decaying exponential, and rising ramp. No waveform was simultaneously energy-, charge-, and power-optimal, and differences in efficiency among waveform shapes varied with pulse width (PW) For short PWs ({$\leq$} 0.1 ms), square waveforms were no less energy-efficient than exponential waveforms, and the most charge-efficient shape was the ramp. For long PWs ({$\geq$}0.5 ms), the square was the least energy-efficient and charge-efficient shape, but across most PWs, the square was the most powerefficient shape. Rising exponentials provided no practical gains in efficiency over the other shapes, and our results refute previous claims that the rising exponential is the energy-optimal shape. An improved understanding of how stimulation parameters affect stimulation efficiency will help improve the design and programming of implantable stimulators to minimize tissue damage and extend battery life.},
  file = {2010 - Wongsarnpigoon, Woock, Grill - Efficiency analysis of waveform shape for electrical excitation of nerve fibers.pdf},
  journal = {IEEE Transactions on Neural Systems and Rehabilitation Engineering},
  language = {en},
  number = {3}
}

@article{Wongsarnpigoon2010a,
  title = {Energy-Efficient Waveform Shapes for Neural Stimulation Revealed with a Genetic Algorithm},
  author = {Wongsarnpigoon, Amorn and Grill, Warren M},
  year = {2010},
  month = aug,
  volume = {7},
  pages = {046009},
  issn = {1741-2560, 1741-2552},
  doi = {10.1088/1741-2560/7/4/046009},
  abstract = {The energy efficiency of stimulation is an important consideration for battery-powered implantable stimulators. We used a genetic algorithm (GA) to determine the energy-optimal waveform shape for neural stimulation. The GA was coupled to a computational model of extracellular stimulation of a mammalian myelinated axon. As the GA progressed, waveforms became increasingly energy-efficient and converged upon an energy-optimal shape. The results of the GA were consistent across several trials, and resulting waveforms resembled truncated Gaussian curves. When constrained to monophasic cathodic waveforms, the GA produced waveforms that were symmetric about the peak, which occurred approximately during the middle of the pulse. However, when the cathodic waveforms were coupled to rectangular chargebalancing anodic pulses, the location and sharpness of the peak varied with the duration and timing (i.e., before or after cathodic phase) of the anodic phase. In a model of a population of mammalian axons and in vivo experiments on cat sciatic nerve, the GA-optimized waveforms were more energy-efficient and charge-efficient than several conventional waveform shapes used in neural stimulation. If used in implantable neural stimulators, GA-optimized waveforms could prolong battery life, thereby reducing the frequency of recharge intervals, the volume of implanted pulse generators, and the costs and risks of battery-replacement surgeries.},
  file = {2011 - Wongsarnpigoon, Grill - Energy-efficient waveform shapes for neural stimulation revealed with genetic algorithm.pdf},
  journal = {Journal of Neural Engineering},
  language = {en},
  number = {4}
}

@article{Woodgate2017,
  title = {Continuous {{Radar Tracking Illustrates}} the {{Development}} of {{Multi}}-Destination {{Routes}} of {{Bumblebees}}},
  author = {Woodgate, Joseph L. and Makinson, James C. and Lim, Ka S. and Reynolds, Andrew M. and Chittka, Lars},
  year = {2017},
  month = dec,
  volume = {7},
  issn = {2045-2322},
  doi = {10.1038/s41598-017-17553-1},
  file = {Woodgate et al. - 2017 - Continuous Radar Tracking Illustrates the Developm.pdf},
  journal = {Scientific Reports},
  language = {en},
  number = {1}
}

@article{Worden,
  title = {Anticipatory {{Biasing}} of {{Visuospatial Attention Indexed}} by {{Retinotopically Specific}} \textvisiblespace -{{Band Electroencephalography Increases}} over {{Occipital Cortex}}},
  author = {Worden, Michael S and Foxe, John J and Wang, Norman and Simpson, Gregory V},
  pages = {6},
  file = {2000 - Worden et al. - Anticipatory biasing of visuospatial attention indexed by retinotopically specific alpha-band electroencephalogra.pdf},
  language = {en}
}

@article{Worthy2014,
  title = {A Comparison Model of Reinforcement-Learning and Win-Stay-Lose-Shift Decision-Making Processes: {{A}} Tribute to {{W}}.{{K}}. {{Estes}}},
  shorttitle = {A Comparison Model of Reinforcement-Learning and Win-Stay-Lose-Shift Decision-Making Processes},
  author = {Worthy, Darrell A. and Todd Maddox, W.},
  year = {2014},
  month = apr,
  volume = {59},
  pages = {41--49},
  issn = {00222496},
  doi = {10.1016/j.jmp.2013.10.001},
  abstract = {W.K. Estes often championed an approach to model development whereby an existing model was augmented by the addition of one or more free parameters, and a comparison between the simple and more complex, augmented model determined whether the additions were justified. Following this same approach we utilized Estes' (1950) own augmented learning equations to improve the fit and plausibility of a win-stay-lose-shift (WSLS) model that we have used in much of our recent work. Estes also championed models that assumed a comparison between multiple concurrent cognitive processes. In line with this, we develop a WSLS-Reinforcement Learning (RL) model that assumes that the output of a WSLS process that provides a probability of staying or switching to a different option based on the last two decision outcomes is compared with the output of an RL process that determines a probability of selecting each option based on a comparison of the expected value of each option. Fits to data from three different decision-making experiments suggest that the augmentations to the WSLS and RL models lead to a better account of decisionmaking behavior. Our results also support the assertion that human participants weigh the output of WSLS and RL processes during decision-making.},
  file = {Worthy and Todd Maddox - 2014 - A comparison model of reinforcement-learning and w.pdf},
  journal = {Journal of Mathematical Psychology},
  language = {en}
}

@article{Wosniack2015,
  title = {A Parallel Algorithm for Random Searches},
  author = {Wosniack, M.E. and Raposo, E.P. and Viswanathan, G.M. and {da Luz}, M.G.E.},
  year = {2015},
  month = nov,
  volume = {196},
  pages = {390--397},
  issn = {00104655},
  doi = {10.1016/j.cpc.2015.07.014},
  abstract = {We discuss a parallelization procedure for a two-dimensional random search of a single individual, a typical sequential process. To assure the same features of the sequential random search in the parallel version, we analyze the former spatial patterns of the encountered targets for different search strategies and densities of homogeneously distributed targets. We identify a lognormal tendency for the distribution of distances between consecutively detected targets. Then, by assigning the distinct mean and standard deviation of this distribution for each corresponding configuration in the parallel simulations (constituted by parallel random walkers), we are able to recover important statistical properties, e.g., the target detection efficiency, of the original problem. The proposed parallel approach presents a speedup of nearly one order of magnitude compared with the sequential implementation. This algorithm can be easily adapted to different instances, as searches in three dimensions. Its possible range of applicability covers problems in areas as diverse as automated computer searchers in high-capacity databases and animal foraging.},
  file = {Wosniack et al. - 2015 - A parallel algorithm for random searches.pdf},
  journal = {Computer Physics Communications},
  language = {en}
}

@article{Wosniack2017,
  title = {The Evolutionary Origins of {{L\'evy}} Walk Foraging},
  author = {Wosniack, Marina E. and Santos, Marcos C. and Raposo, Ernesto P. and Viswanathan, Gandhi M. and {da Luz}, Marcos G. E.},
  editor = {Bartumeus, Frederic},
  year = {2017},
  month = oct,
  volume = {13},
  pages = {e1005774},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1005774},
  abstract = {We study through a reaction-diffusion algorithm the influence of landscape diversity on the efficiency of search dynamics. Remarkably, the identical optimal search strategy arises in a wide variety of environments, provided the target density is sparse and the searcher's information is restricted to its close vicinity. Our results strongly impact the current debate on the emergentist vs. evolutionary origins of animal foraging. The inherent character of the optimal solution (i.e., independent on the landscape for the broad scenarios assumed here) suggests an interpretation favoring the evolutionary view, as originally implied by the Le\textasciiacute{} vy flight foraging hypothesis. The latter states that, under conditions of scarcity of information and sparse resources, some organisms must have evolved to exploit optimal strategies characterized by heavy-tailed truncated power-law distributions of move lengths. These results strongly suggest that Le\textasciiacute{} vy strategies\textemdash and hence the selection pressure for the relevant adaptations\textemdash are robust with respect to large changes in habitat. In contrast, the usual emergentist explanation seems not able to explain how very similar Le\textasciiacute vy walks can emerge from all the distinct non-Le\textasciiacute{} vy foraging strategies that are needed for the observed large variety of specific environments. We also report that deviations from Le\textasciiacute{} vy can take place in plentiful ecosystems, where locomotion truncation is very frequent due to high encounter rates. So, in this case normal diffusion strategies\textemdash performing as effectively as the optimal one\textemdash can naturally emerge from Le\textasciiacute{} vy. Our results constitute the strongest theoretical evidence to date supporting the evolutionary origins of experimentally observed Le\textasciiacute vy walks.},
  file = {Wosniack et al. - 2017 - The evolutionary origins of Lévy walk foraging.pdf},
  journal = {PLoS Comput Biol},
  language = {en},
  number = {10}
}

@article{Wosniack2017a,
  title = {The Evolutionary Origins of {{L\'evy}} Walk Foraging},
  author = {Wosniack, Marina E. and Santos, Marcos C. and Raposo, Ernesto P. and Viswanathan, Gandhi M. and {da Luz}, Marcos G. E.},
  editor = {Bartumeus, Frederic},
  year = {2017},
  month = oct,
  volume = {13},
  pages = {e1005774},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1005774},
  abstract = {We study through a reaction-diffusion algorithm the influence of landscape diversity on the efficiency of search dynamics. Remarkably, the identical optimal search strategy arises in a wide variety of environments, provided the target density is sparse and the searcher's information is restricted to its close vicinity. Our results strongly impact the current debate on the emergentist vs. evolutionary origins of animal foraging. The inherent character of the optimal solution (i.e., independent on the landscape for the broad scenarios assumed here) suggests an interpretation favoring the evolutionary view, as originally implied by the Le\textasciiacute{} vy flight foraging hypothesis. The latter states that, under conditions of scarcity of information and sparse resources, some organisms must have evolved to exploit optimal strategies characterized by heavy-tailed truncated power-law distributions of move lengths. These results strongly suggest that Le\textasciiacute{} vy strategies\textemdash and hence the selection pressure for the relevant adaptations\textemdash are robust with respect to large changes in habitat. In contrast, the usual emergentist explanation seems not able to explain how very similar Le\textasciiacute vy walks can emerge from all the distinct non-Le\textasciiacute{} vy foraging strategies that are needed for the observed large variety of specific environments. We also report that deviations from Le\textasciiacute{} vy can take place in plentiful ecosystems, where locomotion truncation is very frequent due to high encounter rates. So, in this case normal diffusion strategies\textemdash performing as effectively as the optimal one\textemdash can naturally emerge from Le\textasciiacute{} vy. Our results constitute the strongest theoretical evidence to date supporting the evolutionary origins of experimentally observed Le\textasciiacute vy walks.},
  file = {Wosniack et al. - 2017 - The evolutionary origins of Lévy walk foraging 2.pdf},
  journal = {PLoS Comput Biol},
  language = {en},
  number = {10}
}

@article{Wosniack2017b,
  title = {The Evolutionary Origins of {{L\'evy}} Walk Foraging},
  author = {Wosniack, Marina E. and Santos, Marcos C. and Raposo, Ernesto P. and Viswanathan, Gandhi M. and {da Luz}, Marcos G. E.},
  editor = {Bartumeus, Frederic},
  year = {2017},
  month = oct,
  volume = {13},
  pages = {e1005774},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1005774},
  abstract = {We study through a reaction-diffusion algorithm the influence of landscape diversity on the efficiency of search dynamics. Remarkably, the identical optimal search strategy arises in a wide variety of environments, provided the target density is sparse and the searcher's information is restricted to its close vicinity. Our results strongly impact the current debate on the emergentist vs. evolutionary origins of animal foraging. The inherent character of the optimal solution (i.e., independent on the landscape for the broad scenarios assumed here) suggests an interpretation favoring the evolutionary view, as originally implied by the Le\textasciiacute{} vy flight foraging hypothesis. The latter states that, under conditions of scarcity of information and sparse resources, some organisms must have evolved to exploit optimal strategies characterized by heavy-tailed truncated power-law distributions of move lengths. These results strongly suggest that Le\textasciiacute{} vy strategies\textemdash and hence the selection pressure for the relevant adaptations\textemdash are robust with respect to large changes in habitat. In contrast, the usual emergentist explanation seems not able to explain how very similar Le\textasciiacute vy walks can emerge from all the distinct non-Le\textasciiacute{} vy foraging strategies that are needed for the observed large variety of specific environments. We also report that deviations from Le\textasciiacute{} vy can take place in plentiful ecosystems, where locomotion truncation is very frequent due to high encounter rates. So, in this case normal diffusion strategies\textemdash performing as effectively as the optimal one\textemdash can naturally emerge from Le\textasciiacute{} vy. Our results constitute the strongest theoretical evidence to date supporting the evolutionary origins of experimentally observed Le\textasciiacute vy walks.},
  file = {Wosniack et al. - 2017 - The evolutionary origins of Lévy walk foraging 3.pdf},
  journal = {PLoS Comput Biol},
  language = {en},
  number = {10}
}

@article{Wu2018,
  title = {Generalization Guides Human Exploration in Vast Decision Spaces},
  author = {Wu, Charley M. and Schulz, Eric and Speekenbrink, Maarten and Nelson, Jonathan D. and Meder, Bj{\"o}rn},
  year = {2018},
  month = dec,
  volume = {2},
  pages = {915--924},
  issn = {2397-3374},
  doi = {10.1038/s41562-018-0467-4},
  file = {Wu et al. - 2018 - Generalization guides human exploration in vast de.pdf},
  journal = {Nature Human Behaviour},
  language = {en},
  number = {12}
}

@article{Wullimann2011,
  title = {Basal {{Ganglia}}: {{Insights}} into {{Origins}} from {{Lamprey Brains}}},
  shorttitle = {Basal {{Ganglia}}},
  author = {Wullimann, Mario F.},
  year = {2011},
  month = jul,
  volume = {21},
  pages = {R497-R500},
  issn = {09609822},
  doi = {10.1016/j.cub.2011.05.052},
  file = {Wullimann - 2011 - Basal Ganglia Insights into Origins from Lamprey .pdf},
  journal = {Current Biology},
  language = {en},
  number = {13}
}

@article{Wybo2019,
  title = {Electrical {{Compartmentalization}} in {{Neurons}}},
  author = {Wybo, Willem A.M. and {Torben-Nielsen}, Benjamin and Nevian, Thomas and Gewaltig, Marc-Oliver},
  year = {2019},
  month = feb,
  volume = {26},
  pages = {1759-1773.e7},
  issn = {22111247},
  doi = {10.1016/j.celrep.2019.01.074},
  abstract = {The dendritic tree of neurons plays an important role in information processing in the brain. While it is thought that dendrites require independent subunits to perform most of their computations, it is still not understood how they compartmentalize into functional subunits. Here, we show how these subunits can be deduced from the properties of dendrites. We devised a formalism that links the dendritic arborization to an impedance-based tree graph and show how the topology of this graph reveals independent subunits. This analysis reveals that cooperativity between synapses decreases slowly with increasing electrical separation and thus that few independent subunits coexist. We nevertheless find that balanced inputs or shunting inhibition can modify this topology and increase the number and size of the subunits in a context-dependent manner. We also find that this dynamic recompartmentalization can enable branch-specific learning of stimulus features. Analysis of dendritic patch-clamp recording experiments confirmed our theoretical predictions.},
  file = {Wybo et al. - 2019 - Electrical Compartmentalization in Neurons.pdf},
  journal = {Cell Reports},
  language = {en},
  number = {7}
}

@article{Wynn2015,
  title = {{{EEG Findings}} of {{Reduced Neural Synchronization}} during {{Visual Integration}} in {{Schizophrenia}}},
  author = {Wynn, Jonathan K. and Roach, Brian J. and Lee, Junghee and Horan, William P. and Ford, Judith M. and Jimenez, Amy M. and Green, Michael F.},
  editor = {Kotz, Sonja},
  year = {2015},
  month = mar,
  volume = {10},
  pages = {e0119849},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0119849},
  file = {Wynn et al. - 2015 - EEG Findings of Reduced Neural Synchronization dur.pdf},
  journal = {PLOS ONE},
  language = {en},
  number = {3}
}

@article{Xiong2016,
  title = {Dynamic {{Memory Networks}} for {{Visual}} and {{Textual Question Answering}}},
  author = {Xiong, Caiming and Merity, Stephen and Socher, Richard},
  year = {2016},
  month = mar,
  abstract = {Neural network architectures with memory and attention mechanisms exhibit certain reasoning capabilities required for question answering. One such architecture, the dynamic memory network (DMN), obtained high accuracy on a variety of language tasks. However, it was not shown whether the architecture achieves strong results for question answering when supporting facts are not marked during training or whether it could be applied to other modalities such as images. Based on an analysis of the DMN, we propose several improvements to its memory and input modules. Together with these changes we introduce a novel input module for images in order to be able to answer visual questions. Our new DMN+ model improves the state of the art on both the Visual Question Answering dataset and the bAbI-10k text question-answering dataset without supporting fact supervision.},
  archiveprefix = {arXiv},
  eprint = {1603.01417},
  eprinttype = {arxiv},
  file = {Xiong et al. - 2016 - Dynamic Memory Networks for Visual and Textual Que.pdf},
  journal = {arXiv:1603.01417 [cs]},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Neural and Evolutionary Computing},
  language = {en},
  primaryclass = {cs}
}

@inproceedings{Xu2012,
  title = {Regularized Hyperalignment of Multi-Set {{fMRI}} Data},
  booktitle = {2012 {{IEEE Statistical Signal Processing Workshop}} ({{SSP}})},
  author = {Xu, Hao and Lorbert, Alexander and Ramadge, Peter J. and Guntupalli, J. Swaroop and Haxby, James V.},
  year = {2012},
  month = aug,
  pages = {229--232},
  publisher = {{IEEE}},
  address = {{Ann Arbor, MI, USA}},
  doi = {10.1109/SSP.2012.6319668},
  abstract = {Inter-subject correspondence is an important aspect of multisubject fMRI studies. Recently, a new approach, called hyperalignment, has shown very promising results in fMRI functional alignment. Hyperalignment is based on Procrustean rotations and is connected, mathematically, to canonical correlation analysis. We review the core details of each approach, relate them through an SVD analysis, and indicate why they can yield different levels of performance. We then examine the effectiveness of regularization in mediating between the extremes of these methods. An inter-subject classification experiment based on functional aligned fMRI datasets illustrates the resulting improved performance.},
  file = {2012 - Xu et al. - Regularized Hyperalignment of Multi-set FMRI Data.pdf},
  isbn = {978-1-4673-0182-4 978-1-4673-0181-7},
  language = {en}
}

@article{Xu2013,
  title = {Reduction in {{LFP}} Cross-Frequency Coupling between Theta and Gamma Rhythms Associated with Impaired {{STP}} and {{LTP}} in a Rat Model of Brain Ischemia},
  author = {Xu, Xiaxia and Zheng, Chenguang and Zhang, Tao},
  year = {2013},
  volume = {7},
  issn = {1662-5188},
  doi = {10.3389/fncom.2013.00027},
  file = {Xu et al. - 2013 - Reduction in LFP cross-frequency coupling between .pdf},
  journal = {Frontiers in Computational Neuroscience},
  language = {en}
}

@article{Xu2020,
  title = {Accelerating {{Reinforcement Learning Agent}} with {{EEG}}-Based {{Implicit Human Feedback}}},
  author = {Xu, Duo and Agarwal, Mohit and Gupta, Ekansh and Fekri, Faramarz and Sivakumar, Raghupathy},
  year = {2020},
  month = oct,
  abstract = {Providing Reinforcement Learning (RL) agents with human feedback can dramatically improve various aspects of learning. However, previous methods require human observer to give inputs explicitly (e.g., press buttons, voice interface), burdening the human in the loop of RL agent's learning process. Further, providing explicit human advise (feedback) continuously is not always possible or too restrictive, e.g., autonomous driving, disabled rehabilitation, etc. In this work, we investigate capturing human's intrinsic reactions as implicit (and natural) feedback through EEG in the form of error-related potentials (ErrP), providing a natural and direct way for humans to improve the RL agent learning. As such, the human intelligence can be integrated via implicit feedback with RL algorithms to accelerate the learning of RL agent. We develop three reasonably complex 2D discrete navigational games to experimentally evaluate the overall performance of the proposed work. And the motivation of using ErrPs as feedbacks is also verified by subjective experiments. Major contributions of our work are as follows, (i) we propose and experimentally validate the zero-shot learning of ErrPs, where the ErrPs can be learned for one game, and transferred to other unseen games, (ii) we propose a novel RL framework for integrating implicit human feedbacks via ErrPs with RL agent, improving the label efficiency and robustness to human mistakes, and (iii) compared to prior works, we scale the application of ErrPs to reasonably complex environments, and demonstrate the significance of our approach for accelerated learning through real user experiments.},
  archiveprefix = {arXiv},
  eprint = {2006.16498},
  eprinttype = {arxiv},
  file = {Xu et al. - 2020 - Accelerating Reinforcement Learning Agent with EEG.pdf},
  journal = {arXiv:2006.16498 [cs, eess]},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Electrical Engineering and Systems Science - Signal Processing},
  language = {en},
  primaryclass = {cs, eess}
}

@techreport{Xu2020a,
  title = {Novelty Is Not {{Surprise}}: {{Human}} Exploratory and Adaptive Behavior in Sequential Decision-Making},
  shorttitle = {Novelty Is Not {{Surprise}}},
  author = {Xu, He A. and Modirshanechi, Alireza and Lehmann, Marco P. and Gerstner, Wulfram and Herzog, Michael H.},
  year = {2020},
  month = sep,
  institution = {{Neuroscience}},
  doi = {10.1101/2020.09.24.311084},
  abstract = {Abstract           Classic reinforcement learning (RL) theories cannot explain human behavior in response to changes in the environment or in the absence of external reward. Here, we design a deep sequential decision-making paradigm with sparse reward and abrupt environmental changes. To explain the behavior of human participants in these environments, we show that RL theories need to include surprise and novelty, each with a distinct role. While novelty drives exploration before the first encounter of a reward, surprise increases the rate of learning of a world-model as well as of model-free action-values. Even though the world-model is available for model-based RL, we find that human decisions are dominated by model-free action choices. The world-model is only marginally used for planning but is important to detect surprising events. Our theory predicts human action choices with high probability and allows us to dissociate surprise, novelty, and reward in EEG signals.},
  file = {Xu et al. - 2020 - Novelty is not Surprise Human exploratory and ada.pdf},
  language = {en},
  type = {Preprint}
}

@article{Xue2014,
  title = {Equalizing Excitation\textendash Inhibition Ratios across Visual Cortical Neurons},
  author = {Xue, Mingshan and Atallah, Bassam V. and Scanziani, Massimo},
  year = {2014},
  month = jun,
  volume = {511},
  pages = {596--600},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature13321},
  file = {Xue et al. - 2014 - Equalizing excitation–inhibition ratios across vis.pdf},
  journal = {Nature},
  language = {en},
  number = {7511}
}

@article{Xue2017,
  title = {Coevolution {{Maintains Diversity}} in the {{Stochastic}} ``{{Kill}} the {{Winner}}'' {{Model}}},
  author = {Xue, Chi and Goldenfeld, Nigel},
  year = {2017},
  month = dec,
  volume = {119},
  pages = {268101},
  issn = {0031-9007, 1079-7114},
  doi = {10.1103/PhysRevLett.119.268101},
  file = {Xue and Goldenfeld - 2017 - Coevolution Maintains Diversity in the Stochastic .pdf},
  journal = {Phys. Rev. Lett.},
  language = {en},
  number = {26}
}

@article{Yagodin1994,
  title = {Nonlinear Propagation of Agonist-Induced Cytoplasmic Calcium Waves in Single Astrocytes},
  author = {Yagodin, Sergey V. and Holtzclaw, Lynne and Sheppard, Carol A. and Russell, James T.},
  year = {1994},
  month = mar,
  volume = {25},
  pages = {265--280},
  issn = {0022-3034, 1097-4695},
  doi = {10.1002/neu.480250307},
  file = {Yagodin et al. - 1994 - Nonlinear propagation of agonist-induced cytoplasm.pdf},
  journal = {J. Neurobiol.},
  language = {en},
  number = {3}
}

@article{Yamazaki2007,
  title = {The Cerebellum as a Liquid State Machine},
  author = {Yamazaki, Tadashi and Tanaka, Shigeru},
  year = {2007},
  month = apr,
  volume = {20},
  pages = {290--297},
  issn = {08936080},
  doi = {10.1016/j.neunet.2007.04.004},
  abstract = {We examined closely the cerebellar circuit model that we have proposed previously. The model granular layer generates a finite but very long sequence of active neuron populations without recurrence, which is able to represent the passage of time. For all the possible binary patterns fed into mossy fibres, the circuit generates the same number of different sequences of active neuron populations. Model Purkinje cells that receive parallel fiber inputs from neurons in the granular layer learn to stop eliciting spikes at the timing instructed by the arrival of signals from the inferior olive. These functional roles of the granular layer and Purkinje cells are regarded as a liquid state generator and readout neurons, respectively. Thus, the cerebellum that has been considered to date as a biological counterpart of a perceptron is reinterpreted to be a liquid state machine that possesses powerful information processing capability more than a perceptron.},
  file = {2007 - Yamazaki, Tanaka - The cerebellum as a liquid state machine.pdf},
  journal = {Neural Networks},
  language = {en},
  number = {3}
}

@article{Yamazaki2013,
  title = {Realtime Cerebellum: {{A}} Large-Scale Spiking Network Model of the Cerebellum That Runs in Realtime Using a Graphics Processing Unit},
  shorttitle = {Realtime Cerebellum},
  author = {Yamazaki, Tadashi and Igarashi, Jun},
  year = {2013},
  month = nov,
  volume = {47},
  pages = {103--111},
  issn = {08936080},
  doi = {10.1016/j.neunet.2013.01.019},
  abstract = {The cerebellum plays an essential role in adaptive motor control. Once we are able to build a cerebellar model that runs in realtime, which means that a computer simulation of 1 s in the simulated world completes within 1 s in the real world, the cerebellar model could be used as a realtime adaptive neural controller for physical hardware such as humanoid robots. In this paper, we introduce ``Realtime Cerebellum (RC)'', a new implementation of our large-scale spiking network model of the cerebellum, which was originally built to study cerebellar mechanisms for simultaneous gain and timing control and acted as a general-purpose supervised learning machine of spatiotemporal information known as reservoir computing, on a graphics processing unit (GPU). Owing to the massive parallel computing capability of a GPU, RC runs in realtime, while reproducing qualitatively the same simulation results of the Pavlovian delay eyeblink conditioning with the previous version. RC is adopted as a realtime adaptive controller of a humanoid robot, which is instructed to learn a proper timing to swing a bat to hit a flying ball online. These results suggest that RC provides a means to apply the computational power of the cerebellum as a versatile supervised learning machine towards engineering applications.},
  file = {2013 - Yamazaki, Igarashi - Realtime cerebellum A large-scale spiking network model of the cerebellum that runs in realtime using a grap.pdf;Yamazaki and Igarashi - 2013 - Realtime cerebellum A large-scale spiking network.pdf},
  journal = {Neural Networks},
  language = {en}
}

@article{Yan2014,
  title = {{{HD}}-{{CNN}}: {{Hierarchical Deep Convolutional Neural Network}} for {{Large Scale Visual Recognition}}},
  shorttitle = {{{HD}}-{{CNN}}},
  author = {Yan, Zhicheng and Zhang, Hao and Piramuthu, Robinson and Jagadeesh, Vignesh and DeCoste, Dennis and Di, Wei and Yu, Yizhou},
  year = {2014},
  month = oct,
  abstract = {In image classification, visual separability between different object categories is highly uneven, and some categories are more difficult to distinguish than others. Such difficult categories demand more dedicated classifiers. However, existing deep convolutional neural networks (CNN) are trained as flat N-way classifiers, and few efforts have been made to leverage the hierarchical structure of categories. In this paper, we introduce hierarchical deep CNNs (HD-CNNs) by embedding deep CNNs into a category hierarchy. An HD-CNN separates easy classes using a coarse category classifier while distinguishing difficult classes using fine category classifiers. During HD-CNN training, component-wise pretraining is followed by global finetuning with a multinomial logistic loss regularized by a coarse category consistency term. In addition, conditional executions of fine category classifiers and layer parameter compression make HD-CNNs scalable for large-scale visual recognition. We achieve state-of-the-art results on both CIFAR100 and large-scale ImageNet 1000-class benchmark datasets. In our experiments, we build up three different HD-CNNs and they lower the top-1 error of the standard CNNs by 2.65\%, 3.1\% and 1.1\%, respectively.},
  archiveprefix = {arXiv},
  eprint = {1410.0736},
  eprinttype = {arxiv},
  file = {2014 - Yan et al. - HD-CNN Hierarchical Deep Convolutional Neural Network for Large Scale Visual Recognition.pdf;Yan et al. - 2014 - HD-CNN Hierarchical Deep Convolutional Neural Net.pdf},
  journal = {arXiv:1410.0736 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  language = {en},
  primaryclass = {cs, stat}
}

@article{Yang,
  title = {Deep {{Neural Decision Trees}}},
  author = {Yang, Yongxin and Morillo, Irene Garcia and Hospedales, Timothy M},
  pages = {7},
  abstract = {Deep neural networks have been proven powerful at processing perceptual data, such as images and audio. However for tabular data, tree-based models are more popular. A nice property of tree-based models is their natural interpretability. In this work, we present Deep Neural Decision Trees (DNDT) \textendash{} tree models realised by neural networks. A DNDT is intrinsically interpretable, as it is a tree. Yet as it is also a neural network (NN), it can be easily implemented in NN toolkits, and trained with gradient descent rather than greedy splitting. We evaluate DNDT on several tabular datasets, verify its efficacy, and investigate similarities and differences between DNDT and vanilla decision trees. Interestingly, DNDT self-prunes at both split and feature-level.},
  file = {Yang et al. - Deep Neural Decision Trees.pdf},
  language = {en}
}

@article{Yang2018,
  title = {Mean {{Field Multi}}-{{Agent Reinforcement Learning}}},
  author = {Yang, Yaodong and Luo, Rui and Li, Minne and Zhou, Ming and Zhang, Weinan and Wang, Jun},
  year = {2018},
  month = feb,
  abstract = {Existing multi-agent reinforcement learning methods are limited typically to a small number of agents. When the agent number increases largely, the learning becomes intractable due to the curse of the dimensionality and the exponential growth of agent interactions. In this paper, we present Mean Field Reinforcement Learning where the interactions within the population of agents are approximated by those between a single agent and the average effect from the overall population or neighboring agents; the interplay between the two entities is mutually reinforced: the learning of the individual agent's optimal policy depends on the dynamics of the population, while the dynamics of the population change according to the collective patterns of the individual policies. We develop practical mean field Q-learning and mean field Actor-Critic algorithms and analyze the convergence of the solution to Nash equilibrium. Experiments on Gaussian squeeze, Ising model, and battle games justify the learning effectiveness of our mean field approaches. In addition, we report the first result to solve the Ising model via model-free reinforcement learning methods.},
  archiveprefix = {arXiv},
  eprint = {1802.05438},
  eprinttype = {arxiv},
  file = {Yang et al. - 2018 - Mean Field Multi-Agent Reinforcement Learning.pdf},
  journal = {arXiv:1802.05438 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Multiagent Systems},
  language = {en},
  primaryclass = {cs}
}

@article{Yang2019,
  title = {Exploration via {{Flow}}-{{Based Intrinsic Rewards}}},
  author = {Yang, Hsuan-Kung and Chiang, Po-Han and Hong, Min-Fong and Lee, Chun-Yi},
  year = {2019},
  month = may,
  abstract = {Exploration bonuses derived from the novelty of observations in an environment have become a popular approach to motivate exploration for reinforcement learning (RL) agents in the past few years. Recent methods such as curiosity-driven exploration usually estimate the novelty of new observations by the prediction errors of their system dynamics models. In this paper, we introduce the concept of optical flow estimation from the field of computer vision to the RL domain and utilize the errors from optical flow estimation to evaluate the novelty of new observations. We introduce a flow-based intrinsic curiosity module (FICM) capable of learning the motion features and understanding the observations in a more comprehensive and efficient fashion. We evaluate our method and compare it with a number of baselines on several benchmark environments, including Atari games, Super Mario Bros., and ViZDoom. Our results show that the proposed method is superior to the baselines in certain environments, especially for those featuring sophisticated moving patterns or with high-dimensional observation spaces. We further analyze the hyper-parameters used in the training phase and discuss our insights into them.},
  archiveprefix = {arXiv},
  eprint = {1905.10071},
  eprinttype = {arxiv},
  file = {Yang et al. - 2019 - Exploration via Flow-Based Intrinsic Rewards.pdf},
  journal = {arXiv:1905.10071 [cs, stat]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryclass = {cs, stat}
}

@article{Yavuz2016,
  title = {{{GeNN}}: A Code Generation Framework for Accelerated Brain Simulations},
  shorttitle = {{{GeNN}}},
  author = {Yavuz, Esin and Turner, James and Nowotny, Thomas},
  year = {2016},
  month = may,
  volume = {6},
  issn = {2045-2322},
  doi = {10.1038/srep18854},
  file = {Yavuz et al. - 2016 - GeNN a code generation framework for accelerated .pdf},
  journal = {Scientific Reports},
  language = {en},
  number = {1}
}

@article{Ye2014,
  title = {Estimating the Biophysical Properties of Neurons with Intracellular Calcium Dynamics},
  author = {Ye, Jingxin and Rozdeba, Paul J. and Morone, Uriel I. and Daou, Arij and Abarbanel, Henry D. I.},
  year = {2014},
  month = jun,
  volume = {89},
  issn = {1539-3755, 1550-2376},
  doi = {10.1103/PhysRevE.89.062714},
  file = {2014 - Ye, Rozdeba, Morone - Estimating the biophysical properties of neurons with intracellular calcium dynamics.pdf;Ye et al. - 2014 - Estimating the biophysical properties of neurons w.pdf},
  journal = {Physical Review E},
  language = {en},
  number = {6}
}

@article{Yeung1999,
  title = {Time {{Delay}} in the {{Kuramoto Model}} of {{Coupled Oscillators}}},
  author = {Yeung, M. K. Stephen and Strogatz, Steven H.},
  year = {1999},
  month = jan,
  volume = {82},
  pages = {648--651},
  issn = {0031-9007, 1079-7114},
  doi = {10.1103/PhysRevLett.82.648},
  file = {1999 - Yeung, Strogatz - Time Delay in the Kuramoto Model of Coupled Oscillators.pdf},
  journal = {Physical Review Letters},
  language = {en},
  number = {3}
}

@article{Yi2015,
  title = {Spike-Frequency Adaptation of a Two-Compartment Neuron Modulated by Extracellular Electric Fields},
  author = {Yi, Guosheng and Wang, Jiang and Tsang, Kai-Ming and Wei, Xile and Deng, Bin and Han, Chunxiao},
  year = {2015},
  month = jun,
  volume = {109},
  pages = {287--306},
  issn = {0340-1200, 1432-0770},
  doi = {10.1007/s00422-014-0642-2},
  file = {Yi et al. - 2015 - Spike-frequency adaptation of a two-compartment ne.pdf},
  journal = {Biological Cybernetics},
  language = {en},
  number = {3}
}

@article{Yi2017,
  title = {Morphology Controls How Hippocampal {{CA1}} Pyramidal Neuron Responds to Uniform Electric Fields: A Biophysical Modeling Study},
  shorttitle = {Morphology Controls How Hippocampal {{CA1}} Pyramidal Neuron Responds to Uniform Electric Fields},
  author = {Yi, Guo-Sheng and Wang, Jiang and Deng, Bin and Wei, Xi-Le},
  year = {2017},
  month = dec,
  volume = {7},
  issn = {2045-2322},
  doi = {10.1038/s41598-017-03547-6},
  file = {Yi et al. - 2017 - Morphology controls how hippocampal CA1 pyramidal .pdf},
  journal = {Scientific Reports},
  language = {en},
  number = {1}
}

@article{Yochum2018,
  title = {Reconstruction of Post-Synaptic Potentials by Reverse Modeling of Local Field Potentials},
  author = {Yochum, Maxime and Modolo, Julien and Benquet, Pascal and Wendling, Fabrice},
  year = {2018},
  month = sep,
  doi = {10.1101/346148},
  abstract = {Among electrophysiological signals, Local Field Potentials (LFPs) are extensively used to study brain activity, either in vivo or in vitro. LFPs are recorded with extracellular electrodes implanted in brain tissue. They reflect intermingled excitatory and inhibitory processes in neuronal assemblies. In cortical structures, LFPs mainly originate from the summation of post-synaptic potentials (PSPs), either excitatory (ePSPs) and inhibitory (iPSPs) generated at the level of pyramidal cells. The challenging issue, addressed in this paper, is to estimate, from a single extracellularly-recorded signal, both ePSP and iPSP components of the LFP. The proposed method is based on a model-based reverse engineering approach in which the measured LFP is fed into a physiologically-grounded neural mass model (mesoscopic level) in order to estimate the synaptic activity of a sub-population of pyramidal cells interacting with local GABAergic interneurons. The method was first validated using simulated LFPs for which excitatory and inhibitory components are known a priori and can thus serve as a ground truth. It was then evaluated on in vivo data (PTZ-induced seizures, rat; PTZ-induced excitability increase, mouse; epileptiform discharges, mouse) and on in clinico data (human seizures recorded with depth-EEG electrodes). Under these various conditions, results showed that the proposed reverse engineering method provides a reliable estimation of the average excitatory and inhibitory post-synaptic potentials at the origin of the measured LFPs. They also indicated that the method allows for monitoring of the excitation/inhibition ratio. The method has potential for multiple applications in neuroscience, typically when a time tracking of local excitability changes is required.},
  file = {Yochum et al. - 2018 - Reconstruction of post-synaptic potentials by reve.pdf},
  journal = {bioRxiv},
  language = {en}
}

@article{Yoon2008,
  title = {Multivariate {{Pattern Analysis}} of {{Functional Magnetic Resonance Imaging Data Reveals Deficits}} in {{Distributed Representations}} in {{Schizophrenia}}},
  author = {Yoon, Jong H. and Tamir, Diana and Minzenberg, Michael J. and Ragland, J. Daniel and Ursu, Stefan and Carter, Cameron S.},
  year = {2008},
  month = dec,
  volume = {64},
  pages = {1035--1041},
  issn = {00063223},
  doi = {10.1016/j.biopsych.2008.07.025},
  abstract = {Background: Multivariate pattern analysis is an alternative method of analyzing functional magnetic resonance imaging (fMRI) data, which is capable of decoding distributed neural representations. We applied this method to test the hypothesis of the impairment in distributed representations in schizophrenia. We also compared the results of this method with traditional general linear model (GLM)-based univariate analysis. Methods: Nineteen schizophrenia and 15 control subjects viewed two runs of stimuli\textemdash{} exemplars of faces, scenes, objects, and scrambled images. To verify engagement with stimuli, subjects completed a 1-back matching task. A multivoxel pattern classifier was trained to identify category-specific activity patterns on one run of fMRI data. Classification testing was conducted on the remaining run. Correlation of voxelwise activity across runs evaluated variance over time in activity patterns. Results: Patients performed the task less accurately. This group difference was reflected in the pattern analysis results with diminished classification accuracy in patients compared with control subjects, 59\% and 72\%, respectively. In contrast, there was no group difference in GLM-based univariate measures. In both groups, classification accuracy was significantly correlated with behavioral measures. Both groups showed highly significant correlation between interrun correlations and classification accuracy. Conclusions: Distributed representations of visual objects are impaired in schizophrenia. This impairment is correlated with diminished task performance, suggesting that decreased integrity of cortical activity patterns is reflected in impaired behavior. Comparisons with univariate results suggest greater sensitivity of pattern analysis in detecting group differences in neural activity and reduced likelihood of nonspecific factors driving these results.},
  file = {2008 - Yoon et al. - Multivariate pattern analysis of functional magnetic resonance imaging data reveals deficits in distributed represe.pdf},
  journal = {Biological Psychiatry},
  language = {en},
  number = {12}
}

@article{Yoon2018,
  title = {Control of Movement Vigor and Decision Making during Foraging},
  author = {Yoon, Tehrim and Geary, Robert B. and Ahmed, Alaa A. and Shadmehr, Reza},
  year = {2018},
  month = oct,
  volume = {115},
  pages = {E10476-E10485},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1812979115},
  abstract = {During foraging, animals decide how long to stay at a patch and harvest reward, and then, they move with certain vigor to another location. How does the brain decide when to leave, and how does it determine the speed of the ensuing movement? Here, we considered the possibility that both the decision-making and the motor control problems aimed to maximize a single normative utility: the sum of all rewards acquired minus all efforts expended divided by total time. This optimization could be achieved if the brain compared a local measure of utility with its history. To test the theory, we examined behavior of people as they gazed at images: they chose how long to look at the image (harvesting information) and then moved their eyes to another image, controlling saccade speed. We varied reward via image content and effort via image eccentricity, and then, we measured how these changes affected decision making (gaze duration) and motor control (saccade speed). After a history of low rewards, people increased gaze duration and decreased saccade speed. In anticipation of future effort, they lowered saccade speed and increased gaze duration. After a history of high effort, they elevated their saccade speed and increased gaze duration. Therefore, the theory presented a principled way with which the brain may control two aspects of behavior: movement speed and harvest duration. Our experiments confirmed many (but not all) of the predictions, suggesting that harvest duration and movement speed, fundamental aspects of behavior during foraging, may be governed by a shared principle of control.},
  file = {Yoon et al. - 2018 - Control of movement vigor and decision making duri.pdf},
  journal = {Proc Natl Acad Sci USA},
  language = {en},
  number = {44}
}

@article{Yoshimura2005,
  title = {Fine-Scale Specificity of Cortical Networks Depends on Inhibitory Cell Type and Connectivity},
  author = {Yoshimura, Yumiko and Callaway, Edward M},
  year = {2005},
  month = nov,
  volume = {8},
  pages = {1552--1559},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn1565},
  file = {2005 - Yoshimura, Callaway - Fine-scale specificity of cortical networks depends on inhibitory cell type and connectivity.pdf},
  journal = {Nature Neuroscience},
  language = {en},
  number = {11}
}

@article{Yosinski2014,
  title = {How Transferable Are Features in Deep Neural Networks?},
  author = {Yosinski, Jason and Clune, Jeff and Bengio, Yoshua and Lipson, Hod},
  year = {2014},
  month = nov,
  abstract = {Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters and color blobs. Such first-layer features appear not to be specific to a particular dataset or task, but general in that they are applicable to many datasets and tasks. Features must eventually transition from general to specific by the last layer of the network, but this transition has not been studied extensively. In this paper we experimentally quantify the generality versus specificity of neurons in each layer of a deep convolutional neural network and report a few surprising results. Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected. In an example network trained on ImageNet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network. We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features. A final surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after fine-tuning to the target dataset.},
  archiveprefix = {arXiv},
  eprint = {1411.1792},
  eprinttype = {arxiv},
  file = {2014 - Yosinski et al. - How transferable are features in deep neural networks.pdf;Yosinski et al. - 2014 - How transferable are features in deep neural netwo.pdf},
  journal = {arXiv:1411.1792 [cs]},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  language = {en},
  primaryclass = {cs}
}

@article{Young2014,
  title = {Differential Effects of Aging on Dendritic Spines in Visual Cortex and Prefrontal Cortex of the Rhesus Monkey},
  author = {Young, M.E. and Ohm, D.T. and Dumitriu, D. and Rapp, P.R. and Morrison, J.H.},
  year = {2014},
  month = aug,
  volume = {274},
  pages = {33--43},
  issn = {03064522},
  doi = {10.1016/j.neuroscience.2014.05.008},
  file = {Young et al. - 2014 - Differential effects of aging on dendritic spines .pdf},
  journal = {Neuroscience},
  language = {en}
}

@article{Yttri2016,
  title = {Opponent and Bidirectional Control of Movement Velocity in the Basal Ganglia},
  author = {Yttri, Eric A. and Dudman, Joshua T.},
  year = {2016},
  month = may,
  volume = {533},
  pages = {402--406},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature17639},
  file = {Yttri and Dudman - 2016 - Opponent and bidirectional control of movement vel.pdf},
  journal = {Nature},
  language = {en},
  number = {7603}
}

@article{Yu,
  title = {Sequential Effects: {{Superstition}} or Rational Behavior?},
  author = {Yu, Angela J and Cohen, Jonathan D},
  pages = {8},
  abstract = {In a variety of behavioral tasks, subjects exhibit an automatic and apparently suboptimal sequential effect: they respond more rapidly and accurately to a stimulus if it reinforces a local pattern in stimulus history, such as a string of repetitions or alternations, compared to when it violates such a pattern. This is often the case even if the local trends arise by chance in the context of a randomized design, such that stimulus history has no real predictive power. In this work, we use a normative Bayesian framework to examine the hypothesis that such idiosyncrasies may reflect the inadvertent engagement of mechanisms critical for adapting to a changing environment. We show that prior belief in non-stationarity can induce experimentally observed sequential effects in an otherwise Bayes-optimal algorithm. The Bayesian algorithm is shown to be well approximated by linear-exponential filtering of past observations, a feature also apparent in the behavioral data. We derive an explicit relationship between the parameters and computations of the exact Bayesian algorithm and those of the approximate linear-exponential filter. Since the latter is equivalent to a leaky-integration process, a commonly used model of neuronal dynamics underlying perceptual decision-making and trial-to-trial dependencies, our model provides a principled account of why such dynamics are useful. We also show that parameter-tuning of the leaky-integration process is possible, using stochastic gradient descent based only on the noisy binary inputs. This is a proof of concept that not only can neurons implement near-optimal prediction based on standard neuronal dynamics, but that they can also learn to tune the processing parameters without explicitly representing probabilities.},
  file = {2009 - Yu, Cohen - Sequential effects Superstition or rational behavior.pdf},
  language = {en}
}

@article{Yu2009,
  title = {Gaussian-{{Process Factor Analysis}} for {{Low}}-{{Dimensional Single}}-{{Trial Analysis}} of {{Neural Population Activity}}},
  author = {Yu, Byron M. and Cunningham, John P. and Santhanam, Gopal and Ryu, Stephen I. and Shenoy, Krishna V. and Sahani, Maneesh},
  year = {2009},
  month = jul,
  volume = {102},
  pages = {614--635},
  issn = {0022-3077, 1522-1598},
  doi = {10.1152/jn.90941.2008},
  file = {2009 - Yu et al. - Gaussian-Process Factor Analysis for Low-Dimensional Single-Trial Analysis of Neural Population Activity (vol 102, pg.pdf},
  journal = {Journal of Neurophysiology},
  language = {en},
  number = {1}
}

@article{Yu2014,
  title = {Sparse {{Coding}} and {{Lateral Inhibition Arising}} from {{Balanced}} and {{Unbalanced Dendrodendritic Excitation}} and {{Inhibition}}},
  author = {Yu, Yuguo and Migliore, Michele and Hines, Michael L. and Shepherd, Gordon M.},
  year = {2014},
  month = oct,
  volume = {34},
  pages = {13701--13713},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.1834-14.2014},
  file = {2014 - Yu et al. - Sparse Coding and Lateral Inhibition Arising from Balanced and Unbalanced Dendrodendritic Excitation and Inhibition.pdf;Yu et al. - 2014 - Sparse Coding and Lateral Inhibition Arising from .pdf},
  journal = {The Journal of Neuroscience},
  language = {en},
  number = {41}
}

@article{Yu2020,
  title = {Playing the Lottery with Rewards and Multiple Languages: Lottery Tickets in {{RL}} and {{NLP}}},
  shorttitle = {Playing the Lottery with Rewards and Multiple Languages},
  author = {Yu, Haonan and Edunov, Sergey and Tian, Yuandong and Morcos, Ari S.},
  year = {2020},
  month = feb,
  abstract = {The lottery ticket hypothesis proposes that over-parameterization of deep neural networks (DNNs) aids training by increasing the probability of a ``lucky'' sub-network initialization being present rather than by helping the optimization process (Frankle \& Carbin, 2019). Intriguingly, this phenomenon suggests that initialization strategies for DNNs can be improved substantially, but the lottery ticket hypothesis has only previously been tested in the context of supervised learning for natural image tasks. Here, we evaluate whether ``winning ticket'' initializations exist in two different domains: natural language processing (NLP) and reinforcement learning (RL). For NLP, we examined both recurrent LSTM models and large-scale Transformer models (Vaswani et al., 2017). For RL, we analyzed a number of discrete-action space tasks, including both classic control and pixel control. Consistent with work in supervised image classification, we confirm that winning ticket initializations generally outperform parameter-matched random initializations, even at extreme pruning rates for both NLP and RL. Notably, we are able to find winning ticket initializations for Transformers which enable models one-third the size to achieve nearly equivalent performance. Together, these results suggest that the lottery ticket hypothesis is not restricted to supervised learning of natural images, but rather represents a broader phenomenon in DNNs.},
  archiveprefix = {arXiv},
  eprint = {1906.02768},
  eprinttype = {arxiv},
  file = {Yu et al. - 2020 - Playing the lottery with rewards and multiple lang.pdf},
  journal = {arXiv:1906.02768 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  language = {en},
  primaryclass = {cs, stat}
}

@article{Yu2020a,
  title = {Gradient {{Surgery}} for {{Multi}}-{{Task Learning}}},
  author = {Yu, Tianhe and Kumar, Saurabh and Gupta, Abhishek and Levine, Sergey and Hausman, Karol and Finn, Chelsea},
  year = {2020},
  month = jan,
  abstract = {While deep learning and deep reinforcement learning (RL) systems have demonstrated impressive results in domains such as image classification, game playing, and robotic control, data efficiency remains a major challenge. Multi-task learning has emerged as a promising approach for sharing structure across multiple tasks to enable more efficient learning. However, the multi-task setting presents a number of optimization challenges, making it difficult to realize large efficiency gains compared to learning tasks independently. The reasons why multi-task learning is so challenging compared to single-task learning are not fully understood. In this work, we identify a set of three conditions of the multi-task optimization landscape that cause detrimental gradient interference, and develop a simple yet general approach for avoiding such interference between task gradients. We propose a form of gradient surgery that projects a task's gradient onto the normal plane of the gradient of any other task that has a conflicting gradient. On a series of challenging multi-task supervised and multi-task RL problems, this approach leads to substantial gains in efficiency and performance. Further, it is model-agnostic and can be combined with previously-proposed multitask architectures for enhanced performance.},
  archiveprefix = {arXiv},
  eprint = {2001.06782},
  eprinttype = {arxiv},
  file = {Yu et al. - 2020 - Gradient Surgery for Multi-Task Learning.pdf},
  journal = {arXiv:2001.06782 [cs, stat]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},
  language = {en},
  primaryclass = {cs, stat}
}

@article{Yua,
  title = {Adaptive {{Behavior}}: {{Humans Act}} as {{Bayesian Learners}}},
  author = {Yu, Angela J},
  volume = {17},
  pages = {4},
  file = {Yu - Adaptive Behavior Humans Act as Bayesian Learners.pdf},
  language = {en},
  number = {22}
}

@article{Yuan2016,
  title = {Theoretical {{Analysis}} of {{Transcranial Magneto}}-{{Acoustical Stimulation}} with {{Hodgkin}}-{{Huxley Neuron Model}}},
  author = {Yuan, Yi and Chen, Yudong and Li, Xiaoli},
  year = {2016},
  month = apr,
  volume = {10},
  issn = {1662-5188},
  doi = {10.3389/fncom.2016.00035},
  file = {Yuan et al. - 2016 - Theoretical Analysis of Transcranial Magneto-Acous 2.pdf;Yuan et al. - 2016 - Theoretical Analysis of Transcranial Magneto-Acous.pdf},
  journal = {Frontiers in Computational Neuroscience},
  language = {en}
}

@article{Yule1926,
  title = {Why Do We {{Sometimes}} Get {{Nonsense}}-{{Correlations}} between {{Time}}-{{Series}}?--{{A Study}} in {{Sampling}} and the {{Nature}} of {{Time}}-{{Series}}},
  shorttitle = {Why Do We {{Sometimes}} Get {{Nonsense}}-{{Correlations}} between {{Time}}-{{Series}}?},
  author = {Yule, G. Udny},
  year = {1926},
  month = jan,
  volume = {89},
  pages = {1},
  issn = {09528385},
  doi = {10.2307/2341482},
  file = {Yule - 1926 - Why do we Sometimes get Nonsense-Correlations betw.pdf},
  journal = {Journal of the Royal Statistical Society},
  number = {1}
}

@article{Yuval-Greenberg2008,
  title = {Transient {{Induced Gamma}}-{{Band Response}} in {{EEG}} as a {{Manifestation}} of {{Miniature Saccades}}},
  author = {{Yuval-Greenberg}, Shlomit and Tomer, Orr and Keren, Alon S. and Nelken, Israel and Deouell, Leon Y.},
  year = {2008},
  month = may,
  volume = {58},
  pages = {429--441},
  issn = {08966273},
  doi = {10.1016/j.neuron.2008.03.027},
  abstract = {The induced gamma-band EEG response (iGBR) recorded on the scalp is widely assumed to reflect synchronous neural oscillation associated with object representation, attention, memory, and consciousness. The most commonly reported EEG iGBR is a broadband transient increase in power at the gamma range \$200\textendash 300 ms following stimulus onset. A conspicuous feature of this iGBR is the trial-to-trial poststimulus latency variability, which has been insufficiently addressed. Here, we show, using singletrial analysis of concomitant EEG and eye tracking, that this iGBR is tightly time locked to the onset of involuntary miniature eye movements and reflects a saccadic ``spike potential.'' The time course of the iGBR is related to an increase in the rate of saccades following a period of poststimulus saccadic inhibition. Thus, whereas neuronal gamma-band oscillations were shown conclusively with other methods, the broadband transient iGBR recorded by scalp EEG reflects properties of miniature saccade dynamics rather than neuronal oscillations.},
  file = {2008 - Yuval-Greenberg et al. - Transient Induced Gamma-Band Response in EEG as a Manifestation of Miniature Saccades.pdf},
  journal = {Neuron},
  language = {en},
  number = {3}
}

@article{Zador2019,
  title = {A Critique of Pure Learning and What Artificial Neural Networks Can Learn from Animal Brains},
  author = {Zador, Anthony M.},
  year = {2019},
  month = dec,
  volume = {10},
  pages = {3770},
  issn = {2041-1723},
  doi = {10.1038/s41467-019-11786-6},
  file = {Zador - 2019 - A critique of pure learning and what artificial ne.pdf},
  journal = {Nat Commun},
  language = {en},
  number = {1}
}

@article{Zandt2014,
  title = {A Neural Mass Model Based on Single Cell Dynamics to Model Pathophysiology},
  author = {Zandt, Bas-Jan and Visser, Sid and {van Putten}, Michel J. A. M. and {ten Haken}, Bennie},
  year = {2014},
  month = dec,
  volume = {37},
  pages = {549--568},
  issn = {0929-5313, 1573-6873},
  doi = {10.1007/s10827-014-0517-5},
  abstract = {Neural mass models are successful in modeling brain rhythms as observed in macroscopic measurements such as the electroencephalogram (EEG). While the synaptic current is explicitly modeled in current models, the single cell electrophysiology is not taken into account. To allow for investigations of the effects of channel pathologies, channel blockers and ion concentrations on macroscopic activity, we formulate neural mass equations explicitly incorporating the single cell dynamics by using a bottom-up approach. The mean and variance of the firing rate and synaptic input distributions are modeled. The firing rate curve (F(I)-curve) is used as link between the single cell and macroscopic dynamics. We show that this model accurately reproduces the behavior of two populations of synaptically connected Hodgkin-Huxley neurons, also in non-steady state.},
  file = {2014 - Zandt et al. - A neural mass model based on single cell dynamics to model pathophysiology.pdf;Zandt et al. - 2014 - A neural mass model based on single cell dynamics .pdf},
  journal = {Journal of Computational Neuroscience},
  language = {en},
  number = {3}
}

@article{Zanin2012,
  title = {Permutation {{Entropy}} and {{Its Main Biomedical}} and {{Econophysics Applications}}: {{A Review}}},
  shorttitle = {Permutation {{Entropy}} and {{Its Main Biomedical}} and {{Econophysics Applications}}},
  author = {Zanin, Massimiliano and Zunino, Luciano and Rosso, Osvaldo A. and Papo, David},
  year = {2012},
  month = aug,
  volume = {14},
  pages = {1553--1577},
  issn = {1099-4300},
  doi = {10.3390/e14081553},
  file = {2012 - Zanin et al. - Permutation entropy and its main biomedical and econophysics applications A review.pdf},
  journal = {Entropy},
  language = {en},
  number = {8}
}

@article{Zanos2015,
  title = {A {{Sensorimotor Role}} for {{Traveling Waves}} in {{Primate Visual Cortex}}},
  author = {Zanos, Theodoros P. and Mineault, Patrick J. and Nasiotis, Konstantinos T. and Guitton, Daniel and Pack, Christopher C.},
  year = {2015},
  month = feb,
  volume = {85},
  pages = {615--627},
  issn = {08966273},
  doi = {10.1016/j.neuron.2014.12.043},
  abstract = {Traveling waves of neural activity are frequently observed to occur in concert with the presentation of a sensory stimulus or the execution of a movement. Although such waves have been studied for decades, little is known about their function. Here we show that traveling waves in the primate extrastriate visual cortex provide a means of integrating sensory and motor signals. Specifically, we describe a traveling wave of local field potential (LFP) activity in cortical area V4 of macaque monkeys that is triggered by the execution of saccadic eye movements. These waves sweep across the V4 retinotopic map, following a consistent path from the foveal to the peripheral representations of space; their amplitudes correlate with the direction and size of each saccade. Moreover, these waves are associated with a reorganization of the postsaccadic neuronal firing patterns, which follow a similar retinotopic progression, potentially prioritizing the processing of behaviorally relevant stimuli.},
  file = {Zanos et al. - 2015 - A Sensorimotor Role for Traveling Waves in Primate.pdf},
  journal = {Neuron},
  language = {en},
  number = {3}
}

@article{Zanto2013,
  title = {Age-{{Related Changes}} in {{Expectation}}-{{Based Modulation}} of {{Motion Detectability}}},
  author = {Zanto, Theodore P. and Sekuler, Robert and Dube, Chad and Gazzaley, Adam},
  editor = {Rypma, Bart},
  year = {2013},
  month = aug,
  volume = {8},
  pages = {e69766},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0069766},
  abstract = {Expecting motion in some particular direction biases sensitivity to that direction, which speeds detection of motion. However, the neural processes underlying this effect remain underexplored, especially in the context of normal aging. To address this, we examined younger and older adults' performance in a motion detection task. In separate conditions, the probability was either 50\% or 100\% that a field of dots would move coherently in the direction a participant expected (either vertically or horizontally). Expectation and aging effects were assessed via response times (RT) to detect motion and electroencephalography (EEG). In both age groups, RTs were fastest when motion was similar to the expected direction of motion. RT tuning curves exhibited a characteristic U-shape such that detection time increased with an increasing deviation from the participant's expected direction. Strikingly, EEG results showed an analogous, hyperbolic curve for N1 amplitude, reflecting neural biasing. Though the form of behavioral and EEG curves did not vary with age, older adults displayed a clear decline in the speed of detection and a corresponding reduction in EEG N1 amplitude when horizontal (but not vertical) motion was expected. Our results suggest that expectation-based detection ability varies with age and, for older adults, also with axis of motion.},
  file = {2013 - Zanto et al. - Age-related changes in expectation-based modulation of motion detectability.pdf;Zanto et al. - 2013 - Age-Related Changes in Expectation-Based Modulatio.pdf},
  journal = {PLoS ONE},
  language = {en},
  number = {8}
}

@article{Zavala2017,
  title = {Human Subthalamic Nucleus Activity during Non-Motor Decision Making},
  author = {Zavala, Baltazar A and Jang, Anthony I and Zaghloul, Kareem A},
  year = {2017},
  month = dec,
  volume = {6},
  issn = {2050-084X},
  doi = {10.7554/eLife.31007},
  abstract = {Recent studies have implicated the subthalamic nucleus (STN) in decisions that involve 7 inhibiting movements. Many of the decisions that we make in our daily lives, however, do not 8 involve any motor actions. We studied non-motor decision making by recording intraoperative STN 9 and prefrontal cortex (PFC) electrophysiology as participants perform a novel task that required 10 them to decide whether to encode items into working memory. During all encoding trials, beta 11 band (15-30 Hz) activity decreased in the STN and PFC, and this decrease was progressively 12 enhanced as more items were stored into working memory. Crucially, the STN and lateral PFC beta 13 decrease was significantly attenuated during the trials in which participants were instructed not to 14 encode the presented stimulus. These changes were associated with increase lateral PFC-STN 15 coherence and altered STN neuronal spiking. Our results shed light on why states of altered basal 16 ganglia activity disrupt both motor function and cognition.},
  file = {Zavala et al. - 2017 - Human subthalamic nucleus activity during non-moto.pdf},
  journal = {eLife},
  language = {en}
}

@article{Zeldenrust,
  title = {Efficient and Robust Coding in Heterogeneous Recurrent Networks},
  author = {Zeldenrust, Fleur and Gutkin, Boris and Den{\'e}ve, Sophie},
  pages = {25},
  file = {Zeldenrust et al. - Efﬁcient and robust coding in heterogeneous recurr.pdf},
  language = {en}
}

@article{Zenke,
  title = {Improved Multitask Learning through Synaptic Intelligence},
  author = {Zenke, Friedemann and Poole, Ben and Ganguli, Surya},
  pages = {8},
  abstract = {Deep learning has led to remarkable advances when applied to problems where the data distribution does not change over the course of learning. In stark contrast, biological neural networks continually adapt to changing domains, and solve a diversity of tasks simultaneously. Furthermore, synapses in biological neurons are not simply real-valued scalars, but possess complex molecular machinery enabling non-trivial learning dynamics. In this study, we take a first step toward bringing this biological complexity into artificial neural networks. We introduce a model of intelligent synapses that accumulate task relevant information over time, and exploit this information to efficiently consolidate memories of old tasks to protect them from being overwritten as new tasks are learned. We apply our framework to learning sequences of related classification problems, and show that it dramatically reduces catastrophic forgetting while maintaining computational efficiency.},
  file = {Zenke et al. - Improved multitask learning through synaptic intel.pdf},
  language = {en}
}

@article{Zenke2018,
  title = {{{SuperSpike}}: {{Supervised Learning}} in {{Multilayer Spiking Neural Networks}}},
  shorttitle = {{{SuperSpike}}},
  author = {Zenke, Friedemann and Ganguli, Surya},
  year = {2018},
  month = jun,
  volume = {30},
  pages = {1514--1541},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/neco_a_01086},
  abstract = {A vast majority of computation in the brain is performed by spiking neural networks. Despite the ubiquity of such spiking, we currently lack an understanding of how biological spiking neural circuits learn and compute in-vivo, as well as how we can instantiate such capabilities in artificial spiking circuits in-silico. Here we revisit the problem of supervised learning in temporally coding multi-layer spiking neural networks. First, by using a surrogate gradient approach, we derive SuperSpike, a nonlinear voltage-based three factor learning rule capable of training multi-layer networks of deterministic integrateand-fire neurons to perform nonlinear computations on spatiotemporal spike patterns. Second, inspired by recent results on feedback alignment, we compare the performance of our learning rule under different credit assignment strategies for propagating output errors to hidden units. Specifically, we test uniform, symmetric and random feedback, finding that simpler tasks can be solved with any type of feedback, while more complex tasks require symmetric feedback. In summary, our results open the door to obtaining a better scientific understanding of learning and computation in spiking neural networks by advancing our ability to train them to solve nonlinear problems involving transformations between different spatiotemporal spike-time patterns.},
  file = {Zenke and Ganguli - 2018 - SuperSpike Supervised Learning in Multilayer Spik.pdf},
  journal = {Neural Computation},
  language = {en},
  number = {6}
}

@article{Zhang,
  title = {A {{Study}} on {{Overfitting}} in {{Deep Reinforcement Learning}}},
  author = {Zhang, Chiyuan and Vinyals, Oriol and Munos, Remi and Bengio, Samy},
  pages = {25},
  abstract = {Recent years have witnessed significant progresses in deep Reinforcement Learning (RL). Empowered with large scale neural networks, carefully designed architectures, novel training algorithms and massively parallel computing devices, researchers are able to attack many challenging RL problems. However, in machine learning, more training power comes with a potential risk of more overfitting. As deep RL techniques are being applied to critical problems such as healthcare and finance, it is important to understand the generalization behaviors of the trained agents. In this paper, we conduct a systematic study of standard RL agents and find that they could overfit in various ways. Moreover, overfitting could happen ``robustly'': commonly used techniques in RL that add stochasticity do not necessarily prevent or detect overfitting. In particular, the same agents and learning algorithms could have drastically different test performance, even when all of them achieve optimal rewards during training. The observations call for more principled and careful evaluation protocols in RL. We conclude with a general discussion on overfitting in RL and a study of the generalization behaviors from the perspective of inductive bias.},
  file = {Zhang et al. - A Study on Overfitting in Deep Reinforcement Learn.pdf},
  language = {en}
}

@article{Zhang2005,
  title = {Fixed Points and Stability in Differential Equations with Variable Delays},
  author = {Zhang, Bo},
  year = {2005},
  month = nov,
  volume = {63},
  pages = {e233-e242},
  issn = {0362546X},
  doi = {10.1016/j.na.2005.02.081},
  abstract = {In this paper we consider a linear scalar differential equation with variable delays and give conditions to ensure that the zero solution is asymptotically stable by means of fixed point theory. These conditions do not require the boundedness of delays, nor do they ask for a fixed sign on the coefficient functions. An asymptotic stability theorem with a necessary and sufficient condition is proved.},
  file = {2008 - Jin, Luo - Fixed points and stability in neutral differential equations with variable delays.pdf},
  journal = {Nonlinear Analysis: Theory, Methods \& Applications},
  language = {en},
  number = {5-7}
}

@article{Zhang2013,
  title = {Forgetful {{Bayes}} and Myopic Planning: {{Human}} Learning and Decision-Making in a Bandit Setting},
  author = {Zhang, Shunan and Yu, Angela J},
  year = {2013},
  volume = {26},
  abstract = {How humans achieve long-term goals in an uncertain environment, via repeated trials and noisy observations, is an important problem in cognitive science. We investigate this behavior in the context of a multi-armed bandit task. We compare human behavior to a variety of models that vary in their representational and computational complexity. Our result shows that subjects' choices, on a trial-totrial basis, are best captured by a ``forgetful'' Bayesian iterative learning model [21] in combination with a partially myopic decision policy known as Knowledge Gradient [7]. This model accounts for subjects' trial-by-trial choice better than a number of other previously proposed models, including optimal Bayesian learning and risk minimization, e-greedy and win-stay-lose-shift. It has the added benefit of being closest in performance to the optimal Bayesian model than all the other heuristic models that have the same computational complexity (all are significantly less complex than the optimal model). These results constitute an advancement in the theoretical understanding of how humans negotiate the tension between exploration and exploitation in a noisy, imperfectly known environment.},
  file = {Zhang and Yu - Forgetful Bayes and myopic planning Human learnin.pdf},
  journal = {NeurIPS},
  language = {en}
}

@article{Zhang2017,
  title = {Mixup: {{Beyond Empirical Risk Minimization}}},
  shorttitle = {Mixup},
  author = {Zhang, Hongyi and Cisse, Moustapha and Dauphin, Yann N. and {Lopez-Paz}, David},
  year = {2017},
  month = oct,
  abstract = {Large deep neural networks are powerful, but exhibit undesirable behaviors such as memorization and sensitivity to adversarial examples. In this work, we propose mixup, a simple learning principle to alleviate these issues. In essence, mixup trains a neural network on convex combinations of pairs of examples and their labels. By doing so, mixup regularizes the neural network to favor simple linear behavior in-between training examples. Our experiments on the ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show that mixup improves the generalization of state-of-the-art neural network architectures. We also find that mixup reduces the memorization of corrupt labels, increases the robustness to adversarial examples, and stabilizes the training of generative adversarial networks.},
  archiveprefix = {arXiv},
  eprint = {1710.09412},
  eprinttype = {arxiv},
  file = {Zhang et al. - 2017 - mixup Beyond Empirical Risk Minimization.pdf},
  journal = {arXiv:1710.09412 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryclass = {cs, stat}
}

@article{Zhang2017a,
  title = {Theta and Alpha Oscillations Are Traveling Waves in the Human Neocortex},
  author = {Zhang, Honghui and Watrous, Andrew J. and Patel, Ansh and Jacobs, Joshua},
  year = {2017},
  month = dec,
  doi = {10.1101/218198},
  abstract = {Human cognition requires the coordination of neural activity across widespread brain networks. Here we describe a new mechanism for large-scale coordination in the human brain: traveling waves of theta and alpha oscillations. Examining direct brain recordings from neurosurgical patients performing a memory task, we found that contiguous clusters of cortex in individual patients showed oscillations at specific frequencies in the range of 2 to 15 Hz. These clusters displayed spatial phase gradients, indicating that individual oscillation cycles moved across the cortex at {$\sim$}0.25\textendash 0.75 m/s. We found that traveling waves were relevant behaviorally because their propagation correlated with task events and was more consistent during good performance. Traveling waves showed a correlation between propagation speed and temporal frequency, which suggests that they propagate across the cortex following principles of phasecoupled oscillatory networks. By demonstrating that theta and alpha traveling waves are widespread and behaviorally relevant, our results suggest a broad role for brain oscillations in supporting cortical connectivity by organizing neural activity across space and time.},
  file = {Zhang et al. - 2017 - Theta and alpha oscillations are traveling waves i.pdf},
  journal = {bioRxiv},
  language = {en}
}

@article{Zhang2018,
  title = {Theta and {{Alpha Oscillations Are Traveling Waves}} in the {{Human Neocortex}}},
  author = {Zhang, Honghui and Watrous, Andrew J. and Patel, Ansh and Jacobs, Joshua},
  year = {2018},
  month = jun,
  volume = {98},
  pages = {1269-1281.e4},
  issn = {08966273},
  doi = {10.1016/j.neuron.2018.05.019},
  abstract = {Human cognition requires the coordination of neural activity across widespread brain networks. Here, we describe a new mechanism for large-scale coordination in the human brain: traveling waves of theta and alpha oscillations. Examining direct brain recordings from neurosurgical patients performing a memory task, we found contiguous clusters of cortex in individual patients with oscillations at specific frequencies within 2 to 15 Hz. These oscillatory clusters displayed spatial phase gradients, indicating that they formed traveling waves that propagated at \$0.25\textendash 0.75 m/s. Traveling waves were relevant behaviorally because their propagation correlated with task events and was more consistent when subjects performed the task well. Human traveling theta and alpha waves can be modeled by a network of coupled oscillators because the direction of wave propagation correlated with the spatial orientation of local frequency gradients. Our findings suggest that oscillations support brain connectivity by organizing neural processes across space and time.},
  file = {Zhang et al. - 2018 - Theta and Alpha Oscillations Are Traveling Waves i.pdf},
  journal = {Neuron},
  language = {en},
  number = {6}
}

@article{Zhang2018a,
  title = {A {{Dissection}} of {{Overfitting}} and {{Generalization}} in {{Continuous Reinforcement Learning}}},
  author = {Zhang, Amy and Ballas, Nicolas and Pineau, Joelle},
  year = {2018},
  month = jun,
  abstract = {The risks and perils of overfitting in machine learning are well known. However most of the treatment of this, including diagnostic tools and remedies, was developed for the supervised learning case. In this work, we aim to offer new perspectives on the characterization and prevention of overfitting in deep Reinforcement Learning (RL) methods, with a particular focus on continuous domains. We examine several aspects, such as how to define and diagnose overfitting in MDPs, and how to reduce risks by injecting sufficient training diversity. This work complements recent findings on the brittleness of deep RL methods and offers practical observations for RL researchers and practitioners.},
  archiveprefix = {arXiv},
  eprint = {1806.07937},
  eprinttype = {arxiv},
  file = {Zhang et al. - 2018 - A Dissection of Overfitting and Generalization in .pdf},
  journal = {arXiv:1806.07937 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryclass = {cs, stat}
}

@article{Zhang2019,
  title = {Efficient {{Novelty}}-{{Driven Neural Architecture Search}}},
  author = {Zhang, Miao and Li, Huiqi and Pan, Shirui and Liu, Taoping and Su, Steven},
  year = {2019},
  month = jul,
  abstract = {One-Shot Neural architecture search (NAS) attracts broad attention recently due to its capacity to reduce the computational hours through weight sharing. However, extensive experiments on several recent works show that there is no positive correlation between the validation accuracy with inherited weights from the supernet and the test accuracy after retraining for One-Shot NAS. Different from devising a controller to find the best performing architecture with inherited weights, this paper focuses on how to sample architectures to train the supernet to make it more predictive. A singlepath supernet is adopted, where only a small part of weights are optimized in each step, to reduce the memory demand greatly. Furthermore, we abandon devising complicated reward based architecture sampling controller, and sample architectures to train supernet based on novelty search. An efficient novelty search method for NAS is devised in this paper, and extensive experiments demonstrate the effectiveness and efficiency of our novelty search based architecture sampling method. The best architecture obtained by our algorithm with the same search space achieves the state-of-the-art test error rate of 2.51\% on CIFAR-10 with only 7.5 hours search time in a single GPU, and a validation perplexity of 60.02 and a test perplexity of 57.36 on PTB. We also transfer these search cell structures to larger datasets ImageNet and WikiText-2, respectively.},
  archiveprefix = {arXiv},
  eprint = {1907.09109},
  eprinttype = {arxiv},
  file = {Zhang et al. - 2019 - Efficient Novelty-Driven Neural Architecture Searc.pdf},
  journal = {arXiv:1907.09109 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  language = {en},
  primaryclass = {cs, stat}
}

@article{Zhang2019a,
  title = {Scheduled {{Intrinsic Drive}}: {{A Hierarchical Take}} on {{Intrinsically Motivated Exploration}}},
  shorttitle = {Scheduled {{Intrinsic Drive}}},
  author = {Zhang, Jingwei and Wetzel, Niklas and Dorka, Nicolai and Boedecker, Joschka and Burgard, Wolfram},
  year = {2019},
  month = jun,
  abstract = {Exploration in sparse reward reinforcement learning remains an open challenge. Many state-of-the-art methods use intrinsic motivation to complement the sparse extrinsic reward signal, giving the agent more opportunities to receive feedback during exploration. Commonly these signals are added as bonus rewards, which results in a mixture policy that neither conducts exploration nor task fulfillment resolutely. In this paper, we instead learn separate intrinsic and extrinsic task policies and schedule between these different drives to accelerate exploration and stabilize learning. Moreover, we introduce a new type of intrinsic reward denoted as successor feature control (SFC), which is general and not task-specific. It takes into account statistics over complete trajectories and thus differs from previous methods that only use local information to evaluate intrinsic motivation. We evaluate our proposed scheduled intrinsic drive (SID) agent using three different environments with pure visual inputs: VizDoom, DeepMind Lab and DeepMind Control Suite. The results show a substantially improved exploration efficiency with SFC and the hierarchical usage of the intrinsic drives. A video of our experimental results can be found at https://youtu.be/b0MbY3lUlEI.},
  archiveprefix = {arXiv},
  eprint = {1903.07400},
  eprinttype = {arxiv},
  file = {Zhang et al. - 2019 - Scheduled Intrinsic Drive A Hierarchical Take on .pdf},
  journal = {arXiv:1903.07400 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},
  language = {en},
  primaryclass = {cs, stat}
}

@inproceedings{Zhang2020,
  title = {One-{{Shot Neural Architecture Search}} via {{Novelty Driven Sampling}}},
  booktitle = {Proceedings of the {{Twenty}}-{{Ninth International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Zhang, Miao and Li, Huiqi and Pan, Shirui and Liu, Taoping and Su, Steven},
  year = {2020},
  month = jul,
  pages = {3188--3194},
  publisher = {{International Joint Conferences on Artificial Intelligence Organization}},
  address = {{Yokohama, Japan}},
  doi = {10.24963/ijcai.2020/441},
  abstract = {One-Shot Neural architecture search (NAS) has received wide attentions due to its computational efficiency. Most state-of-the-art One-Shot NAS methods use the validation accuracy based on inheriting weights from the supernet as the stepping stone to search for the best performing architecture, adopting a bilevel optimization pattern with assuming this validation accuracy approximates to the test accuracy after re-training. However, recent works have found that there is no positive correlation between the above validation accuracy and test accuracy for these One-Shot NAS methods, and this reward based sampling for supernet training also entails the rich-get-richer problem. To handle this deceptive problem, this paper presents a new approach, Efficient Novelty-driven Neural Architecture Search, to sample the most abnormal architecture to train the supernet. Specifically, a single-path supernet is adopted, and only the weights of a single architecture sampled by our novelty search are optimized in each step to reduce the memory demand greatly. Experiments demonstrate the effectiveness and efficiency of our novelty search based architecture sampling method.},
  file = {Zhang et al. - 2020 - One-Shot Neural Architecture Search via Novelty Dr.pdf},
  isbn = {978-0-9992411-6-5},
  language = {en}
}

@article{Zhanga,
  title = {Theory of {{Deep Learning III}}: {{Generalization Properties}} of {{SGD}}},
  author = {Zhang, Chiyuan and Liao, Qianli and Rakhlin, Alexander and Miranda, Brando and Golowich, Noah and Poggio, Tomaso},
  pages = {38},
  file = {Zhang et al. - Theory of Deep Learning III Generalization Proper.pdf},
  language = {en}
}

@article{Zhangb,
  title = {Understanding Deep Learning Requires Rethinking Generalization},
  author = {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  pages = {15},
  abstract = {Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training.},
  file = {Zhang et al. - Understanding deep learning requires rethinking ge.pdf},
  language = {en}
}

@article{Zhangc,
  title = {Cheap but {{Clever}}: {{Human Active Learning}} in a {{Bandit Setting}}},
  author = {Zhang, Shunan and Yu, Angela J},
  pages = {6},
  abstract = {How people achieve long-term goals in an imperfectly known environment, via repeated tries and noisy outcomes, is an important problem in cognitive science. There are two interrelated questions: how humans represent information, both what has been learned and what can still be learned, and how they choose actions, in particular how they negotiate the tension between exploration and exploitation. In this work, we examine human behavioral data in a multi-armed bandit setting, in which the subject choose one of four ``arms'' to pull on each trial and receives a binary outcome (win/lose). We implement both the Bayes-optimal policy, which maximizes the expected cumulative reward in this finite-horizon bandit environment, as well as a variety of heuristic policies that vary in their complexity of information representation and decision policy. We find that the knowledge gradient algorithm, which combines exact Bayesian learning with a decision policy that maximizes a combination of immediate reward gain and longterm knowledge gain, captures subjects' trial-by-trial choice best among all the models considered; it also provides the best approximation to the computationally intense optimal policy among all the heuristic policies.},
  file = {Zhang and Yu - Cheap but Clever Human Active Learning in a Bandi.pdf},
  language = {en}
}

@article{Zhao2017,
  title = {Variational {{Latent Gaussian Process}} for {{Recovering Single}}-{{Trial Dynamics}} from {{Population Spike Trains}}},
  author = {Zhao, Yuan and Park, Il Memming},
  year = {2017},
  month = may,
  volume = {29},
  pages = {1293--1316},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/NECO_a_00953},
  abstract = {A small number of common factors often explain most of the interdependence among simultaneously recorded neurons, a signature of underlying low-dimensional dynamics. We posit that simple neural coding and computation manifest as low-dimensional nonlinear dynamics implemented redundantly within a large population of neurons. Recovering the latent dynamics from observations can offer a deeper understanding of neural computation. We improve upon previously-proposed methods for recovering latent dynamics, which assume either an inappropriate observation model or linear dynamics. We propose a practical and efficient inference method for a generative model with explicit point process observations and an assumption of smooth nonlinear dynamics. We validate our method on both simulated data and population recording from primary visual cortex.},
  file = {Zhao and Park - 2017 - Variational Latent Gaussian Process for Recovering.pdf},
  journal = {Neural Computation},
  language = {en},
  number = {5}
}

@article{Zhao2019,
  title = {Inception of Memories That Guide Vocal Learning in the Songbird},
  author = {Zhao, Wenchan and {Garcia-Oscos}, Francisco and Dinh, Daniel and Roberts, Todd F.},
  year = {2019},
  month = oct,
  volume = {366},
  pages = {83--89},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aaw4226},
  abstract = {Animals learn many complex behaviors by emulating the behavior of more experienced individuals. This essential, yet still poorly understood, form of learning relies on the ability to encode lasting memories of observed behaviors. We identified a vocal-motor pathway in the zebra finch where memories that guide learning of song-element durations can be implanted. Activation of synapses in this pathway seeds memories that guide learning of song-element duration and can override learning from social interactions with other individuals. Genetic lesions of this circuit after memory formation, however, do not disrupt subsequent song imitation, which suggests that these memories are stored at downstream synapses. Thus, activity at these sensorimotor synapses can bypass learning from auditory and social experience and embed memories that guide learning of song timing.},
  file = {Zhao et al. - 2019 - Inception of memories that guide vocal learning in.pdf},
  journal = {Science},
  language = {en},
  number = {6461}
}

@article{Zhao2020,
  title = {Stimulus-Choice (Mis)Alignment in Primate Area {{MT}}},
  author = {Zhao, Yuan and Yates, Jacob L. and Levi, Aaron J. and Huk, Alexander C. and Park, Il Memming},
  editor = {Marinazzo, Daniele},
  year = {2020},
  month = may,
  volume = {16},
  pages = {e1007614},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1007614},
  abstract = {For stimuli near perceptual threshold, the trial-by-trial activity of single neurons in many sensory areas is correlated with the animal's perceptual report. This phenomenon has often been attributed to feedforward readout of the neural activity by the downstream decisionmaking circuits. The interpretation of choice-correlated activity is quite ambiguous, but its meaning can be better understood in the light of population-wide correlations among sensory neurons. Using a statistical nonlinear dimensionality reduction technique on single-trial ensemble recordings from the middle temporal (MT) area during perceptual-decision-making, we extracted low-dimensional latent factors that captured the population-wide fluctuations. We dissected the particular contributions of sensory-driven versus choice-correlated activity in the low-dimensional population code. We found that the latent factors strongly encoded the direction of the stimulus in single dimension with a temporal signature similar to that of single MT neurons. If the downstream circuit were optimally utilizing this information, choice-correlated signals should be aligned with this stimulus encoding dimension. Surprisingly, we found that a large component of the choice information resides in the subspace orthogonal to the stimulus representation inconsistent with the optimal readout view. This misaligned choice information allows the feedforward sensory information to coexist with the decision-making process. The time course of these signals suggest that this misaligned contribution likely is feedback from the downstream areas. We hypothesize that this noncorrupting choice-correlated feedback might be related to learning or reinforcing sensorymotor relations in the sensory population.},
  file = {Zhao et al. - 2020 - Stimulus-choice (mis)alignment in primate area MT.pdf},
  journal = {PLoS Comput Biol},
  language = {en},
  number = {5}
}

@article{Zhou2014,
  title = {Scaling down of Balanced Excitation and Inhibition by Active Behavioral States in Auditory Cortex},
  author = {Zhou, Mu and Liang, Feixue and Xiong, Xiaorui R and Li, Lu and Li, Haifu and Xiao, Zhongju and Tao, Huizhong W and Zhang, Li I},
  year = {2014},
  month = jun,
  volume = {17},
  pages = {841--850},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.3701},
  file = {2014 - Zhou et al. - Scaling down of balanced excitation and inhibition by active behavioral states in auditory cortex.pdf;Zhou et al. - 2014 - Scaling down of balanced excitation and inhibition.pdf},
  journal = {Nature Neuroscience},
  language = {en},
  number = {6}
}

@article{Zhou2015,
  title = {Establishing a {{Statistical Link}} between {{Network Oscillations}} and {{Neural Synchrony}}},
  author = {Zhou, Pengcheng and Burton, Shawn D. and Snyder, Adam C. and Smith, Matthew A. and Urban, Nathaniel N. and Kass, Robert E.},
  editor = {Sporns, Olaf},
  year = {2015},
  month = oct,
  volume = {11},
  pages = {e1004549},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1004549},
  abstract = {Pairs of active neurons frequently fire action potentials or ``spikes'' nearly synchronously (i.e., within 5 ms of each other). This spike synchrony may occur by chance, based solely on the neurons' fluctuating firing patterns, or it may occur too frequently to be explicable by chance alone. When spike synchrony above chances levels is present, it may subserve computation for a specific cognitive process, or it could be an irrelevant byproduct of such computation. Either way, spike synchrony is a feature of neural data that should be explained. A point process regression framework has been developed previously for this purpose, using generalized linear models (GLMs). In this framework, the observed number of synchronous spikes is compared to the number predicted by chance under varying assumptions about the factors that affect each of the individual neuron's firing-rate functions. An important possible source of spike synchrony is network-wide oscillations, which may provide an essential mechanism of network information flow. To establish the statistical link between spike synchrony and network-wide oscillations, we have integrated oscillatory field potentials into our point process regression framework. We first extended a previouslypublished model of spike-field association and showed that we could recover phase relationships between oscillatory field potentials and firing rates. We then used this new framework to demonstrate the statistical relationship between oscillatory field potentials and spike synchrony in: 1) simulated neurons, 2) in vitro recordings of hippocampal CA1 pyramidal cells, and 3) in vivo recordings of neocortical V4 neurons. Our results provide a rigorous method for establishing a statistical link between network oscillations and neural synchrony.},
  file = {Zhou et al. - 2015 - Establishing a Statistical Link between Network Os.pdf},
  journal = {PLOS Computational Biology},
  language = {en},
  number = {10}
}

@article{Zhou2020,
  title = {The Growth and Form of Knowledge Networks by Kinesthetic Curiosity},
  author = {Zhou, Dale and {Lydon-Staley}, David M. and Zurn, Perry and Bassett, Danielle S.},
  year = {2020},
  month = jun,
  abstract = {Throughout life, we might seek a calling, companions, skills, entertainment, truth, self-knowledge, beauty, and edification. The practice of curiosity can be viewed as an extended and open-ended search for valuable information with hidden identity and location in a complex space of interconnected information. Despite its importance, curiosity has been challenging to computationally model because the practice of curiosity often flourishes without specific goals, external reward, or immediate feedback. Here, we show how network science, statistical physics, and philosophy can be integrated into an approach that coheres with and expands the psychological taxonomies of specific-diversive and perceptual-epistemic curiosity. Using this interdisciplinary approach, we distill functional modes of curious information seeking as searching movements in information space. The kinesthetic model of curiosity offers a vibrant counterpart to the deliberative predictions of model-based reinforcement learning. In doing so, this model unearths new computational opportunities for identifying what makes curiosity curious.},
  archiveprefix = {arXiv},
  eprint = {2006.02949},
  eprinttype = {arxiv},
  file = {Zhou et al. - 2020 - The growth and form of knowledge networks by kines.pdf},
  journal = {arXiv:2006.02949 [cs, q-bio]},
  keywords = {Computer Science - Artificial Intelligence,Quantitative Biology - Neurons and Cognition},
  language = {en},
  primaryclass = {cs, q-bio}
}

@article{Zhou2020a,
  title = {The Growth and Form of Knowledge Networks by Kinesthetic Curiosity},
  author = {Zhou, Dale and {Lydon-Staley}, David M. and Zurn, Perry and Bassett, Danielle S.},
  year = {2020},
  month = jun,
  abstract = {Throughout life, we might seek a calling, companions, skills, entertainment, truth, self-knowledge, beauty, and edification. The practice of curiosity can be viewed as an extended and open-ended search for valuable information with hidden identity and location in a complex space of interconnected information. Despite its importance, curiosity has been challenging to computationally model because the practice of curiosity often flourishes without specific goals, external reward, or immediate feedback. Here, we show how network science, statistical physics, and philosophy can be integrated into an approach that coheres with and expands the psychological taxonomies of specific-diversive and perceptual-epistemic curiosity. Using this interdisciplinary approach, we distill functional modes of curious information seeking as searching movements in information space. The kinesthetic model of curiosity offers a vibrant counterpart to the deliberative predictions of model-based reinforcement learning. In doing so, this model unearths new computational opportunities for identifying what makes curiosity curious.},
  archiveprefix = {arXiv},
  eprint = {2006.02949},
  eprinttype = {arxiv},
  file = {Zhou et al. - 2020 - The growth and form of knowledge networks by kines 2.pdf},
  journal = {arXiv:2006.02949 [cs, q-bio]},
  keywords = {Computer Science - Artificial Intelligence,Quantitative Biology - Neurons and Cognition},
  language = {en},
  primaryclass = {cs, q-bio}
}

@techreport{Zhou2021,
  title = {Alpha Oscillations Shape Sensory Representation and Perceptual Accuracy},
  author = {Zhou, Ying Joey and Iemi, Luca and Schoffelen, Jan-Mathijs and {de Lange}, Floris P. and Haegens, Saskia},
  year = {2021},
  month = feb,
  institution = {{Neuroscience}},
  doi = {10.1101/2021.02.02.429418},
  abstract = {Alpha oscillatory activity (8\textendash 14 Hz) is the dominant rhythm in the awake brain, and is thought to play an important role in setting the brain's internal state. Previous work has associated states of decreased alpha-band oscillatory power with enhanced neural excitability. However, opinions differ on whether and how such excitability enhancement modulates sensory signals of interest versus noise differently, and what, if any, the consequences are for the subsequent perceptual process. To address these questions, we used a novel paradigm to experimentally manipulate human subjects' decision criteria in a visual detection task. In different blocks, we primed subjects with clearly visible stimuli to introduce either a liberal or conservative detection criterion for subsequent ambiguous stimuli. While we observed substantial criterion shifts under different priming conditions, such criterion shifts were not reflected in pre-stimulus alpha-band changes. Rather, we found that lower pre-stimulus alpha-band power in early visual cortices improved perceptual accuracy, accompanied by enhanced information content decodable from the neural activity patterns. Additionally, we showed that alpha oscillatory phase in parietal and frontal regions immediately before stimulus presentation modulated accuracy. Together, our results suggest that alpha-band dynamics modulate sensory signals of interest more strongly compared to noise, here resulting in enhanced information coding and improved perceptual accuracy.},
  file = {Zhou et al. - 2021 - Alpha oscillations shape sensory representation an.pdf},
  language = {en},
  type = {Preprint}
}

@article{Zhu1996,
  title = {No {{Free Lunch}} for {{Cross}}-{{Validation}}},
  author = {Zhu, Huaiyu and Rohwer, Richard},
  year = {1996},
  month = oct,
  volume = {8},
  pages = {1421--1426},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/neco.1996.8.7.1421},
  abstract = {It is known theoretically that an algorithm cannot be good for an arbitrary prior. We show that in practical terms this also applies to the technique of ``cross-validation,'' which has been widely regarded as defying this general rule. Numerical examples are analyzed in detail. Their implications to researches on learning algorithms are discussed.},
  file = {Zhu and Rohwer - 1996 - No Free Lunch for Cross-Validation.pdf},
  journal = {Neural Computation},
  language = {en},
  number = {7}
}

@article{Zhuang2020,
  title = {{{AdaBelief Optimizer}}: {{Adapting Stepsizes}} by the {{Belief}} in {{Observed Gradients}}},
  shorttitle = {{{AdaBelief Optimizer}}},
  author = {Zhuang, Juntang and Tang, Tommy and Ding, Yifan and Tatikonda, Sekhar and Dvornek, Nicha and Papademetris, Xenophon and Duncan, James S.},
  year = {2020},
  month = oct,
  abstract = {Most popular optimizers for deep learning can be broadly categorized as adaptive methods (e.g. Adam) and accelerated schemes (e.g. stochastic gradient descent (SGD) with momentum). For many models such as convolutional neural networks (CNNs), adaptive methods typically converge faster but generalize worse compared to SGD; for complex settings such as generative adversarial networks (GANs), adaptive methods are typically the default because of their stability.We propose AdaBelief to simultaneously achieve three goals: fast convergence as in adaptive methods, good generalization as in SGD, and training stability. The intuition for AdaBelief is to adapt the stepsize according to the "belief" in the current gradient direction. Viewing the exponential moving average (EMA) of the noisy gradient as the prediction of the gradient at the next time step, if the observed gradient greatly deviates from the prediction, we distrust the current observation and take a small step; if the observed gradient is close to the prediction, we trust it and take a large step. We validate AdaBelief in extensive experiments, showing that it outperforms other methods with fast convergence and high accuracy on image classification and language modeling. Specifically, on ImageNet, AdaBelief achieves comparable accuracy to SGD. Furthermore, in the training of a GAN on Cifar10, AdaBelief demonstrates high stability and improves the quality of generated samples compared to a well-tuned Adam optimizer. Code is available at https://github.com/juntang-zhuang/Adabelief-Optimizer},
  archiveprefix = {arXiv},
  eprint = {2010.07468},
  eprinttype = {arxiv},
  file = {Zhuang et al. - 2020 - AdaBelief Optimizer Adapting Stepsizes by the Bel.pdf},
  journal = {arXiv:2010.07468 [cs, stat]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryclass = {cs, stat}
}

@article{Zollman,
  title = {On the Normative Status of Mixed Strategies},
  author = {Zollman, Kevin J S},
  pages = {41},
  abstract = {Flipping a coin to decide what to do is a common feature of everyday life. Mixed strategies, as these are called, have a thorny status in normative decision theories. This paper explores various ways to justify choosing one's actions at random. I conclude that it is hard to make sense of this behavior without dealing with some difficult consequences.},
  file = {Zollman - On the normative status of mixed strategies.pdf},
  language = {en}
}

@article{Zollman2010,
  title = {The {{Epistemic Benefit}} of {{Transient Diversity}}},
  author = {Zollman, Kevin J. S.},
  year = {2010},
  month = jan,
  volume = {72},
  pages = {17--35},
  issn = {0165-0106, 1572-8420},
  doi = {10.1007/s10670-009-9194-6},
  file = {Zollman - 2010 - The Epistemic Benefit of Transient Diversity.pdf},
  journal = {Erkenn},
  language = {en},
  number = {1}
}

@article{Zollman2010a,
  title = {The {{Epistemic Benefit}} of {{Transient Diversity}}},
  author = {Zollman, Kevin J. S.},
  year = {2010},
  month = jan,
  volume = {72},
  pages = {17--35},
  issn = {0165-0106, 1572-8420},
  doi = {10.1007/s10670-009-9194-6},
  abstract = {There is growing interest in understanding and eliciting division of labor within groups of scientists. This paper illustrates the need for this division of labor through a historical example, and a formal model is presented to better analyze situations of this type. Analysis of this model reveals that a division of labor can be maintained in two different ways: by limiting information or by endowing the scientists with extreme beliefs. If both features are present however, cognitive diversity is maintained indefinitely, and as a result agents fail to converge to the truth. Beyond the mechanisms for creating diversity suggested here, this shows that the real epistemic goal is not diversity but transient diversity.},
  file = {Zollman - 2010 - The Epistemic Benefit of Transient Diversity 2.pdf},
  journal = {Erkenn},
  language = {en},
  number = {1}
}

@article{Zollmana,
  title = {On the Normative Status of Mixed Strategies},
  author = {Zollman, Kevin J S},
  pages = {41},
  abstract = {Flipping a coin to decide what to do is a common feature of everyday life. Mixed strategies, as these are called, have a thorny status in normative decision theories. This paper explores various ways to justify choosing one's actions at random. I conclude that it is hard to make sense of this behavior without dealing with some difficult consequences.},
  file = {Zollman - On the normative status of mixed strategies 2.pdf},
  language = {en}
}

@article{Zollmanb,
  title = {Between Cheap and Costly Signals: The Evolution of Partially Honest Communication},
  author = {Zollman, Kevin J S and Bergstrom, Carl T and Huttegger, Simon M},
  pages = {8},
  file = {Zollman et al. - Between cheap and costly signals the evolution of.pdf},
  language = {en}
}

@article{zotero-2129,
  title = {Continuity {{Debate}}},
  pages = {22},
  file = {2000 - Unknown - No Title.pdf},
  language = {en}
}

@misc{zotero-2170,
  title = {1929 - {{Hans}} - {{Uber}} Das Elektrenkephalogramm Des Menshen.Pdf},
  file = {1929 - Hans - Uber das elektrenkephalogramm des menshen.pdf}
}

@misc{zotero-2171,
  title = {1961 - {{Surwillo}} - {{Frequency}} of the `{{Alpha}}' {{Rhythm}}, {{Reaction Time}} and {{Age}}.Pdf},
  file = {1961 - Surwillo - Frequency of the ‘Alpha’ Rhythm, Reaction Time and Age.pdf}
}

@misc{zotero-2172,
  title = {1955 - {{Neyman}} - {{The}} Problem of Inductive Inference.Pdf},
  file = {1955 - Neyman - The problem of inductive inference.pdf}
}

@misc{zotero-2173,
  title = {1983 - {{Chan}}, {{Golub}}, {{LeVeque}} - {{Algorithms}} for {{Computing}} the {{Sample Variance Analysis}} and {{Recommendations}}.Pdf},
  file = {1983 - Chan, Golub, LeVeque - Algorithms for Computing the Sample Variance Analysis and Recommendations.pdf}
}

@misc{zotero-2174,
  title = {1984 - {{Hindmarsh}}, {{Rose}} - {{A Model}} of {{Neuronal Bursting Using Three Coupled First Order Differential Equations}}.Pdf},
  file = {1984 - Hindmarsh, Rose - A Model of Neuronal Bursting Using Three Coupled First Order Differential Equations.pdf}
}

@misc{zotero-2175,
  title = {1987 - {{Reiter}} - {{Nonmonotonic}} Reasoning.Pdf},
  file = {1987 - Reiter - Nonmonotonic reasoning.pdf}
}

@misc{zotero-2176,
  title = {1988 - {{Intended}}, {{Minsky}} - {{No Harm Intended}}.Pdf},
  file = {1988 - Intended, Minsky - No Harm Intended.pdf}
}

@misc{zotero-2180,
  title = {1989 - {{LeCun}} et al. - {{Backpropagation}} Applied to Handwritten Zip Code Recognition.Pdf},
  file = {1989 - LeCun et al. - Backpropagation applied to handwritten zip code recognition.pdf}
}

@misc{zotero-2182,
  title = {1992 - {{Heeger}} - {{Normalization}} of Cell Responses in Cat Striate Cortex.Pdf},
  file = {1992 - Heeger - Normalization of cell responses in cat striate cortex.pdf}
}

@misc{zotero-2183,
  title = {1992 - {{Williams}} - {{Simple}} Statistical Gradient-Following Algorithmns for Connectionist Reinforcement Learning.Pdf},
  file = {1992 - Williams - Simple statistical gradient-following algorithmns for connectionist reinforcement learning.pdf}
}

@misc{zotero-2184,
  title = {1996 - {{Mainen}}, {{Sejnowski}} - {{Influence}} of Dendritic Structure on Firing Pattern in Model Neocortical Neurons.Pdf},
  file = {1996 - Mainen, Sejnowski - Influence of dendritic structure on firing pattern in model neocortical neurons.pdf}
}

@article{zotero-2832,
  title = {Detailed Dendritic Excitatory/Inhibitory Balance through Heterosynaptic Spike-Timing-Dependent Plasticity},
  pages = {30},
  file = {2015 - Hiratani, Fukai - Detailed dendritic excitatory inhibitory balance through heterosynaptic spike-timing-dependent plasticity.pdf;Detailed dendritic excitatoryinhibitory balance t.pdf},
  language = {en}
}

@misc{zotero-3828,
  title = {[{{No}} Title Found]},
  file = {[No title found].pdf},
  language = {en}
}

@misc{zotero-4225,
  title = {[{{No}} Title Found]},
  file = {[No title found].pdf},
  language = {en}
}

@misc{zotero-4665,
  type = {Misc}
}

@article{zotero-5223,
  title = {The Neural Dynamics Underlying Prioritisation of Task-Relevant Information},
  pages = {23},
  file = {The neural dynamics underlying prioritisation of t.pdf},
  language = {en}
}

@article{zotero-5264,
  title = {Training Deep Neural Density Estimators to Identify Mechanistic Models of Neural Dynamics},
  pages = {39},
  file = {Training deep neural density estimators to identif.pdf},
  language = {en}
}

@book{zotero-5365,
  type = {Book}
}

@misc{zotero-5703,
  title = {Deep\_learning\_\_adaptive\_computation\_and\_machine\_learning\_\_{{PDFDrive}}.Com\_20190819-23861-Ck1r7v-with-Cover-Page.Pdf},
  file = {Deep_learning__adaptive_computation_and_machine_learning__PDFDrive.com_20190819-23861-ck1r7v-with-cover-page.pdf}
}

@article{Zylberberg2017,
  title = {Counterfactual Reasoning Underlies the Learning of Priors in Decision Making},
  author = {Zylberberg, Ariel and Wolpert, Daniel M and Shadlen, Michael N},
  year = {2017},
  month = nov,
  doi = {10.1101/227421},
  abstract = {Accurate decisions require knowledge of prior probabilities (e.g., prevalence or base rate) but it is unclear how prior probability is learned in the absence of a teacher. We hypothesized that humans could learn base rates from experience making decisions, even without feedback. Participants made difficult decisions about the direction of dynamic random dot motion. For each block of 15-42 trials, the base rate favored left or right by a different amount. Participants were not informed of the base rate, yet they gradually biased their choices and thereby increased accuracy and confidence in their decisions. They achieved this by updating knowledge of base rate after each decision, using a counterfactual representation of confidence that simulates a neutral prior. The strategy is consistent with Bayesian updating of belief and suggests that humans represent both true confidence, that incorporates the evolving belief of the prior, and counterfactual confidence that discounts the prior.},
  file = {Zylberberg et al. - 2017 - Counterfactual reasoning underlies the learning of.pdf},
  journal = {bioRxiv},
  language = {en}
}


